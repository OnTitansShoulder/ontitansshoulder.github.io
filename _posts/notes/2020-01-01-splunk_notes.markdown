---
layout: note_page
title: Begin to Advanced Splunk
title_short: splunk_notes_advanced
dateStr: 2020-01-01
category: Tool
categories: notes reference
---
# Splunk 7 Fundamentals (IOD)

This course teaches you how to search and navigate in Splunk, use _fields_, get _statistics_ from your data, create _reports_, _dashboards_, _lookups_, and _alerts_. Scenario-based examples and hands-on challenges will enable you to create robust searches, reports, and charts. It will also introduce you to Splunk's _datasets features_ and _Pivot interface_.

## Module 1 Machine data

**Machine data** can be any sort generated by a corporate system:
- Computers
- Network devices
- Virtual machines
- Internet devices
- Commnuication devices
- Sensors
- Databases
- Logs
- Configurations
- Messages
- Call detail records
- Clickstream
- Alerts
- Metrics
- Scripts
- Changes
- Tickets

https://docs.splunk.com/Documentation/Splunk

## Module 2 What is Splunk

Index Data -> Search & Investigate -> Add Knowledge -> Monitor & Alert -> Report & Analyze

**Splunk Modules**:
- Indexer - Process incoming data and string results in indexes as _events_
  - Create files organized in sets of directories by age
  - When searching, Splunk only open the dirs that match the time frame of the search
- Search Heads - all users to search the data using Splunk Search Language
  - Takes the search and distribute the request to indexers, and aggregate the results back
  - Include tools like dashboards, alerts, reports
- Forwarders - Splunk Enterprise component that consumes data and forward it to the indexers for processing
  - Requires minimal resources, has little impact on performance
  - Usually resides on the machines where the data origins
  - Primary way data is supplied for indexing

Splunk can **scale** to be a single instance or a full-distributed infrastructure.
- For a large scale of usage, it is better to split the Splunk installation into multiple specialized instances, needs Splunk Enterprise.
- Can have multiple Search Heads, multiple Indexers, and many Forwarders.
- Search Heads and Indexers can also be clusters to make it always available.

https://splunkbase.splunk.com/
http://dev.splunk.com/

## Module 3 Installing Splunk

Install in the /opt dir
Try not install Splunk as root user

Three types of users:
- Admin - install apps, create knowledge objects for all users
- Power user - create and share knowledge objects for users of an app and do realtime searches
- User - only see their own knowledge objects and those shared with them

Splunk Enterprise comes with the Home App and Search & Reporting App

## Module 4 Getting Data In

Ways to get data in:
- Through the Splunk web UI (as an Admin)
  - Upload a file, indexed once; good for data that never gets updated
  - There are pre-defined file source type which Splunk uses to index the file
- Monitor some files
  - Event Logs
  - File System Changes
  - Active Directory
  - Network Info
- Forwarder
  - Receive data from external Forwarders
  - Main source of data input
  - https://docs.splunk.com/Documentation/Splunk/latest/Data/Usingforwardingagents

Having different and specialized indexes helps make Splunk searches more efficient.

## Module 5 Basic Searching

The Search & Report app allows you to search and analyze data through creating knowledge objects, reports, dashboards and more.

The `Events` tab will display the events returned for your search and the fields extracted from the events.
The `patterns` tab allows you to see patterns in your data.
If your search generates statistics or visuals, they will appear in `Statistics` and `Visualization` tabs. Commands that create statistics and visualizations are called `transforming commands`, they transform data into data tables.

By default a search is active & valid for 10 minutes, otherwise needs rerun.
A shared search job is active for 7 days and its first-ran result visible by anyone you shared with.
Order of evaluation for boolean conditions: `NOT OR AND`

https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/CreateworkflowactionsinSplunkWeb
https://www.splunk.com/view/SP-CAAAPYB

## Module 6 Using Fields

Default fields selected are `hosts`, `source`, and `sourcetype`.
Interesting fields are those appear within at least 20% of the events
`a` denotes a String value, `#` denotes a numeral value
Searching fields are `key=value` string in the search query. the key is case-sensitive but the value is not.
`= !=` can be used on numeral and String values
`> >= < <=` can be used on numeral values only
There is a small difference between doing `key!=value` and `NOT key=value`. The latter will include entries even when the key doesn't exitst on those entries.
Instead of doing `(key=value1 OR key=value2 OR key=value3)`, can use alternatively `key IN ("value1", "value2", "value3")`

Use `Search & Reporting` -> `> Search History` to quickly view recent searches and reuse the searches if possible.
Use `Activity` -> `Jobs` to view recent search jobs (with data). Then further use `Job -> inspect job` to view the query performance.

## Module 7 Best Practices

**Search**
Using time to narrow down the search is the most effective way to improve query performance.
Then it comes to the default fields of index, source, host, and sourcetype are most powerful. They are extracted at index-time so won't be extracted for each search. So better to put these filters early in the front of the query.
The more you tell the search engine, the more likely you will get good results.
Inclusion is generally better than exclusion. Searching for "something" is better than searching for "NOT something"

**Time**
Can include time within the search query string using `earliest=-2h latest=-1h` or absolute time `earliest=01/08/2018:12:00:00`. Use '@' to denote round down time `earliest=-2@h`. i.e. `earliest=10/19/2018:00:00:00 latest=10/27/2018:00:00:00`

**Indexes**
It makes searches efficient, and also allows to limit who can access the data through roles.

## Module 8 SPL Fundamentals

The **SPL Components**:
- Search Terms - defines what data to return
- Commands - what to do with search results, like creating charts, computing statistics, and formatting results.
- Functions - defines how we want to chart, compute and evaluate the results
- Arguments - variables we want to apply to the function
- Clauses - how we want the results grouped

As you pipe through the commands, more unrelevant data remove from before the last pipe.

```
                           -------------------Pipes--------------------
sourcetype=acc* status=200 |  stats   list(    product_name   ) as "Games Sold"
-------Search Terms-------- Command-Function-----Argument-----Clause------------
```

Search UI **color code** parts of the query to help construct the query.
Boolean operators: Orange
Commands: Blue
Command-Arguments: Green
Functions: Purpule
Function-Arguments: Black
Clauses: Orange

The **Fields Command**, include or exclude fields from search results to limit the fields to display and also make search run faster. Internal fields like raw and time will always be extracted, but can also be removed from the search results.
```
index=web sourcetype=access_combined
| fields status clientip # only include fields status and clientip
index=web sourcetype=access_combined
| fields - status clientip # to exclude fields status and clientip
```
Field extraction is one of the most costly parts of searching in Splunk. Eliminate unnecessary fields will improve search speed since field includsion occurs before field extraction, while field exclusion happens after field extraction.

The **Table Command**, also specify fiedls kept in the results and retains the data in a tabulated format. The fields will be displayed in table header and each row in table has the corresponding values.
```
index=web sourcetype=access_combined
| table status clientip
```

The **Rename Command**, to rename a field
```
index=web sourcetype=access* status=200 product_name=*
| table JESSIONID, product_name, price
| rename JESSIONID as "User Session Id",
         product_name as "Purchased Game",
         price as "Purchase Price"
```
After the rename, subsequent commands must use the new names otherwise operation won't have any effects.

The **Dedup Command**, remove duplicated events from results that share common values.
```
index=security sourcetype=history* Address_Description="San Francisco"
| dedup Username # can be a single field or multiple fields
| table Username First_Name Last_Name
```

The **Sort Command**, to organize the results sorted by some fields
```
sourcetype=vendor_sales
| table Vendor product_name sale_price
| sort - sale_price Vendor
  # the '-' here affects all fields
| sort -sale_price Vendor
  # sort-decending will only affect sale_price while Vendor is sorted ascending
| sort -sale_price Vendor limit=20
  # will also limit the results for the first twenty in sorted order.
```

## Module 9 Transforming Commands

The **Top Command**, finds the most common values of given field(s) in a result set. Automatically returns count and percent columns, top 10 by default and can be set with `limit=n`. `limit=0` yields all results
```
index=sales sourcetype=vendor_sales
| top Vendor product_name limit=5
```
Top Command Clauses: limit=int countfield=string percentfield=string showcount=True/False showperc=True/False showother=True/False otherstr=string
```
# top command also supports results grouping by fields
index=sales sourcetype=vendor_sales
| top product_name by Vendor limit=3 countfield="Number of Sales" showperc=False
```

The **Rare Command**, shows the least common values of a field set. It is the opposite of _Top_ Command and accepts the same set of clauses.
```
index=sales sourcetype=vendor_sales
| rare Vendor limit=5 countfield="Number of Sales" showperc=False useother=True
```

The **Stats Command**, produces statistics of our search results. Need to use functions to produce stats
Some Common Stats Functions:
- _count_, returns the number of events matching search criteria
  - `index=sales sourcetype=vendor_sales | stats count as "Total Sells By Vendors" by product_name, categoryid`
  - shows vendor sells total by game title
  - can add any number of fields to split the count by
  - `| stats count(field)` get a count of the number of events where the field is present
  - `index=web sourcetype=access_combined | stats count(action) as "Action Events", count as "Total Events"`
- _distinct_count_ or _dc_, returns a count of unique values for a field. The same clauses that works with _count_ will work with _distinct_count_.
- _sum_, returns the sum of numerical values
  - `index=sales sourcetype=vendor_sales | stats count as "Unit Sold", sum(price) as "Gross Sales" by product_name`
  - here the 'by' clause works on both _count_ and _sum_
  - it has to be the same pipe; after this command, the information needed will no longer be available:
  - `index=sales sourcetype=vendor_sales | stats count as "Unit Sold" | stats sum(price) as "Gross Sales" by product_name` will yield no results
- _average_ or _avg_, _min_, _max_, returns an average/min/max of numerical values for a field
  - Any field values that are missing or formatted incorrectly will not be added into calculation
- _list_, returns all values of a field
  - `index=bcgassets sourcetype=asset_list | stats list(Asset) as "company assets" by Employee`
- _values_, returns unique values of a field

## Module 10 Reports and Dashboards

**Reports** allow people to easily store and share search results and queries used to make the search.
- It is best to have a good naming convention when creating reports.
- When a report is run, a fresh search is run.
- Control the access to see the results of this report.
- Can save report results to speed up search. See Accelerate Reports Doc https://docs.splunk.com/Documentation/Splunk/latest/Report/Acceleratereports

**Visualizations** Any searches that returns statistics information can be viewed as charts
- Splunk provides many type of charts to best reflect the information
- charts can be based on numbers, time, and location
- Can save visualizations as a report or a dashboard panel

**Dashboards** is a collection of reports.

## Module 11 Pivot and Datasets

Pivot and Datasets allows users to get knowledge from the data without learning Splunk in deep.
Data Models - knowledge objects that provide the data structure that drives Pivots
- Created by Admins and Power Users, with a solid understanding of the data.
- Data Model is like the framework, and Pivot is the simple interface to the data.
- Each Data Model is made of Datasets

Datasets - small subsets of data defined for specific purposes
- Defined like tables, with fields names as columns and fields values as cells

The rest can be done easily on the UI for Pivots

## Module 12 Lookups

Lookups allow you to add values to your events not included in the indexed data.
- A lookup is categorized as a dataset
- Combine fields from sources external to the index with searched events based on paired fields present in the events.
- includes csv files, scripts, or geospatial data
- i.e. data that might be useful for search, but not available in the index

Two steps to set it up, in Splunk UI -> Settings -> Lookups
Define a lookup table
Define the lookup
Optionally configure the lookup to run automatically
Use `| inputlookup <lookup_definition_name>` to verify lookup is setup correctly

Lookup field values are case-sensitive by default.

The **Lookup Command**
```
index=web sourcetype=access_combined NOT status=200
| lookup http_status code as status
# http_status is the name of the lookup definition
# code is one of the columns in the csv, won't show in the results
# defualt all fields in lookup table are returned except the input fields
# can choose what is shown by specifying them
index=web sourcetype=access_combined NOT status=200
| lookup http_status code as status,
         OUTPUT code as "HTTP Code",
         description as "HTTP Description"
```

Additional Lookup Options
- Populate lookup table with search results
- Define lookup based on external script or command
- Use Splunk DB Connect application
- Use geospatial lookups to create queries that can be used to generate choropleth map visualizations
- Populate events with KV Store fields
- https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Configureexternallookups
- https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/ConfigureKVstorelookups

## Module 13 Scheduled Reports and Alerts

Scheduled Reports can do weekly/monthly reports and automatically send results via emails.
Select a saved report and add a schedule on it. Only admin can set its priority.

Alerts are based on searches that run on scheduled intervals or in real time. It is triggered when the results of a search meet defined conditions. It can:
- list in interface, alerts can be viewed in Activity -> Triggered Alerts
- log events, create a log file with the events
- output to lookup, to apend or replace data in a lookup table
- send to a telemetry endpoint, call an endpoint
- trigger scripts, triggers a bash script stored on your instance
- send emails
- use a webhook
- run a custom alert, build a custom alert action


# Splunk 7 Fundamentals II (IOD)

This course focuses on _searching_ and _reporting_ commands as well as on the _creation of knowledge objects_.
Major topics include using _transforming commands_ and _visualizations_, _filtering and formatting results_, _correlating events_, _creating knowledge objects_, using _field aliases_ and _calculated fields_, creating _tags and event types_, using _macros_, creating _workflow actions and data models_, and normalizing data with the _Common Information Model_ (CIM).

## Module 1 Intro

Splunk Search Terms:
- Keywords, any keywords can be searched on; "AND" boolean is implied for multiple keywords
- Booleans, AND OR NOT
- Phrases, keywords surrounded by ""
- Fields, key=value pair search
- Wildcards, used in keywords and fields to match any chars; inefficient if used at the beginning of a word
- Comparisons, = != < <= > >=

The frequently used commands can be reviewed in last part.

## Module 2 Beyond Search Foundamentals

Go over why we want to searh in some specific ways in some situations.
- search terms, command names, function names are NOT case-sensitive.
  - exception is when using commands like _replace_, search term must be an exact match
  - field values from a Lookup are case sensitive by default

**Why Time** is the most efficient way to improve queries
- Splunk stores data in buckets (directories) containing raw data and indexing data
  - Splunk buckets have maximum size and maximum time span
- Three kinds of searchable buckets: hot, warm, cold
  - hot buckets: only writable buckets. It rolls back to warm bucket when Max size or time span reached, or indexer is restarted. It will be closed and renamed upon rolling to warm bucket.
  - warm buckets are renamed to a read-only bucket with naming of youngest and oldest event timestamp. i.e. `db_1389230491_1389230488_5`. It rolls back to a cold bucket when max size or time span reached.
  - cold buckets are stored in a different location than warm/hot buckets, likely slower but cost-effective infra.
- When search is run, it go look for the right bucket to open, uncompress the raw data and search the contents inside.

**Using wildcards** when to use and when to avoid
- using wildcard at the beginning of a word cause Splunk to search all events which causes degradation in performance.
- using wildcard in the middle of a string might cause inconsistent results
  - due to the way Splunk indexes data that contains punctuation, avoid using wildcards to match punctuation
- always best to make search as specific as possible, say `status=failure` instead of `status=fail*`

**Search Modes** use the right search mode helps improve the search speed and better access the data
- fast mode emphasis on performance and returns only essential data
  - non-transforming search will extract only fields required for the search
- verbose mode emphasizes completeness and returns all fields and event data
  - the field discovery comes at a cost of searching time
- smart mode returns the best results for the search being run

**Best Practices**
- The less data you have to search, the faster Splunk will be
- The order of effectiveness in filtering data:
  - time > index > source > host > sourcetype
  - they are extracted at index time so won't add time to extract at search time
  - use them to filter as early as possible in your search
- use the `fields` command to extract only the fields you need, as early as possible
- inclusion ("KEYWORD") is better than exclusion (NOT "KEYWORD").
- use appropriate search mode
- use Job Inspector to know the performance of the searches and determine which phase of the search too the most time
  - any search that has not expired can be inspected by this tool
  - accessible in the search results page, Job -> Inspect Job
  - https://docs.splunk.com/Documentation/Splunk/latest/Search/ViewsearchjobpropertieswiththeJobInspector

## Module 3 Commands for Visualizations

Any search that returns statistics value can be visualized as charts.
The **Chart Command**
- any stats function can be applied to the chart command
- can take two clauses: `over` and `by`
  - `over` tells Splunk which fields you want to be on the X axis; Y axis should always be the numerical value. i.e. `index=web sourcetype=access_combined status> 299 | chart count over status`
  - use `by` when we want to split the data by an additional field. i.e. `index=web sourcetype=access_combined status> 299 | chart count over status by host`
  - only one field can be used at a time for the `by` clause when using the `over` clause, unlike in stats command.
  - If more than one field is supplied to `by` clause without `over` clause, the first field is used as with a `over` clause. i.e. `index=web sourcetype=access_combined status> 299 | chart count by status, host`
- can use argument like `usenull=f` to ignore events with NULL value in the selected fields
  - might be more efficient to remove events with NULL value in those fields before piping into a command.
- chart is limited to display 10 columns by default, others will show up as an "other" column in the chart visualization. Can be turned off by passing an argument `useother=f`
  - use `limit=5` to control the max number of columns to display, or `limit=0` to display all columns

The **Timechart Command** performs stats aggregations against time, and time is always the x-axis
- like chart, any stats function can be applied to this command, and only one value can be supplied to the `by` clause. i.e. `index=web sourcetype=vendor_sales | timechart count by product_name`
- `useother` `usenull` and `limit` arguments are also available to this command.
- this command determines the time intervals from the time range selected.
  - can change the timespan by using an argument `span=12hr`

The **Timewrap Command** allows you to compare the data further over an older time range
- specify a time period to apply to the result of a timechart command, then display series of data based on this time periods, with the X axis display the increments of this period and the Y axis display the aggregated values over that period
- i.e. `index=sales sourcetype=vendor_sales product_name="Dream Crusher" | timechart span=1d sum(price) by product_name | timewrap 7d | rename _time as Day | eval Day=strftime(Day, "%A")`
- `strftime` is a handy function to format _time into something intuitive. i.e. `strftime(_time,"%m-%d %A")`

Creating customized visualizations http://docs.splunk.com/Documentation/Splunk/latest/AdvancedDev/CustomVizDevOverview

## Module 4 Advanced Visualizations

Splunk has commands to extract geographical info from data and display them in a good format.
The **Iplocation Command** lookup and add location information to events.
- data like city, country, region, latitude, and longitude can be added to events that include external ip addresses
- depending on the ip, not all location info might be available
- i.e. `index=web sourcetype=access_combined action=purchase status=200 | iplocation clientip`

The **Geostats Command** allows aggregates geographical data for use on a map visualization
- uses the same stats info like the _stats_ command
- i.e. `index=sales sourcetype=vendor_sales | geostats latfield=VendorLatitude longfield=VendorLongitude count by product_name globallimit=4`
- can be used in combination with _iplocation_ command

**Choropleth Maps** uses colored shadings to show differences in numbers over geographical locations
- needs a compressed Keyhole Markup Language (KMZ) file that defines region boundries
- Splunk ships with two KMZ files, geo_us_states for US, and geo_countries for the countries of the world
- other KMZ files can be used
- the **Geom Command** is the command to use to show choropleth map
  - it adds fields with geographical data structures matching polygons on map
  - i.e. `index=sales sourcetype=vendor_sales | stats count as Sales by VendorCountry | geom geo_countries featureIdField=VendorCountry`
  - the above example uses a field that maps back to the country name in the featureCollection

**Single Value Visualizations** when the results contain a single value, two types of visualizations can be used
- **single value graph** displays a single number, with formatting options to add caption, color, unit, ...
  - can also use the _timechart_ command to add a trend and a sparkline for that value
  - this will allow you to get info of this value compare to previous time period
  - search time range is related; formatting can change the time samples
- **gauges**, can be a radial, filler, or marker gauages provides a cooler visualization
  - can use format to set ranges, color; and able to swtich between the three types of gauges without losing them
  - can also be enabled by using _gauge_ command and pass in the ranges like `| gauge total 0 3000 6000 7000`

The **Trendline Command** computes moving averages of field values, gives a good understanding of how the data is trending
- three requred arguments: trendtype time-period field-for-calculation
- trendtypes
  - simple moving average (sma): compute the sum of data points over a period of time
  - expoential moving average (ema): assigns a heavier weighting to more current data points
  - weighted moving average (wma): assigns a heavier weighting to more current data points
- time-period for averaging the data points, an integer between 2 to 10000, and its unit depends on the time range of the search
- i.e. `index=web sourcetype=access_combined | timechart sum(price) as sales | trendline wma2(sales) as trend`
  - here the trendtype=wma time-period=2 field=sales

**Field Format** do styling changes of statistical tables
- wrap results, show row numbers, change click selection, or add data overlay (heat map or hightlight min/max)
- summary with totals or percentages

The **Addtotals Command** computes the sum of all numeric fields for each event (row) and creates a 'total' column
- i.e. `index=web sourcetype=access_combined | chart sum(bytes) over host by file | addtotals col=true label="Total"`
- will create a "Total" column in the results,
  - column name can be override with `fieldname="Total by host"`
  - can be removed by setting `row=false`
- the `col=true` creates another row that contains volumn totals
  - accepts a label name and a field variable `label=TOTALS labelfield=<field_name>`
  - can pass in the column names to select those to calculate totals

## Module 5 Filtering and Formatting

The **Eval Command** for calculate and manipulate field values
- arithmetic, concatenation, and boolean operators are supported
- results can be written to a new field or override existing field vlaues
- fields values created are case-sensitive
- i.e. `index=network sourcetype=cisco_wsa_squid | stats sum(sc_bytes) as Bytes by usage | eval bandwidth = round(Bytes/1024/1024, 2) | sort -bandwidth | rename bandwidth as "Bandwidth (MB)" | fields - Bytes`
  - creates a new field 'bandwidth' with rounded to 2 decimal places
  - after the eval, safe to remove fields that we don't need anymore
- Use _eval_ for mathematical functions
- Use _tostring_ function to convert numerical values to strings to join with other strings
  - can also format time for time, hexadecimal numbers, and commas for dollar numbers
  - string sort might yield different results from numerical sort, so sort numbers early or use _fieldformat_ command
- i.e. `index=web sourcetype=access_combined product_name=* action=purchase | stats sum(price) as total_list_price, sum(sale_price) as total_sale_price by product_name | eval total_list_price = "$" + tostring(total_list_price)`
- still, _eval_ creates new field values but not changing underlaying data in the index
- multiple _eval_ commands can be used in a search; subsequent _eval_ can operate on results of previous _eval_ commands
- _eval_ has _if_ function allows evaluate arguments and create values depending on the results
  - if (x,y,z)
  - x is a boolean expression
  - y will be executed if x is evaluated true
  - z will be executed if x is evaluated false
  - y and z must be double-quoted if not numerical
  - i.e. `| eval VendorTerritory=if(VendorId<4000, "North America", "Other")`
  - '*' cannot be used as wildcard in x. instead use the _like_ clause and '_' will match one char and '%' will match multiple chars
- _eval_ has _case_ function takes multiple boolean expressions and return the corresponding argument that is true
  - case(x1,y1,x2,y2,x3,y3, ...)
  - i.e. `| eval httpCategory=case(status>=200 AND status<300, "Success", status>=300 AND status<400, "Redirect", status>=400 AND status<500, "Client Error", status>=500, "Server Error", true(), "default catch all other cases")`
  - - '*' cannot be used as wildcard in x. instead use the _like_ clause and '_' will match one char and '%' will match multiple chars
- _eval_ can be used with _stats_
  - `index=web sourcetype=access_combined | stats count(eval(status<300)) as "Success", ...`
  - when _eval_ is used within a transforming command, the _as_ clause is required; double-quotes required for field values

The **FieldFormat Command** to format values without changing characteristics of underlying values
- uses same functions as the _eval_ command
- i.e. `index=web sourcetype=access_combined product_name=* action=purchase | stats sum(price) as total_list_price, sum(sale_price) as total_sale_price by product_name | fieldformat total_list_price = "$" + tostring(total_list_price)`
- now sort will still operate on original values, despite the field value was override

The **Search Command** to search terms further down the pipeline
- behaves exactly like the search terms before the first pipe, with the added benefit to filter results further down the search pipeline
- i.e. `index=network sourcetype=cisco_wsa_squid usage=Violation | stats count(usage) as Visits by cs_username | search Visits > 1`
- still best to filter as early as possible

The **Where Command** filters events to only keep the results that evaluate as true
- uses same expression syntax as _eval_ command and many of the same functions
- i.e. `index=network sourcetype=cisco_wsa_squid usage=Violation | stats count(eval(usage="Personal")) as Personal, count(eval(usage="Business")) as Business by username | where Personal > Business | where username!="lsagers" | sort -Personal`
  - without ""s, Splunk treat the argument 'lsagers' as a field, even surrounded by single-quotes!
- However, never use _where_ command when you can filter by search terms
- inside a _where_ command, '*' cannot be used as a wildcard; instead, use _like_ operator.
  - i.e. `| where product_name like "_World%"`
  - '_' will match one char and '%' will match multiple chars
  - can check null values using _isnull_ or _isnotnull_ functions
  - values are case-sensitive in _where_ evaluation

The **Fillnull Command** replaces any null values in your events
- i.e. `index=sales sourcetype=vendor_sales | chart sum(price) over product_name by VendorCountry | fillnull`
  - by default fill with '0'. Can set the fill value with `value="NULL"`

## Module 6 Correlating Events

**Transaction** is any group of related events that span time, can come from multiple applications or hosts
- i.e. an email sent out can generate many events along the way
- i.e. visiting a website can generate many http request
  - each event represents a user generating a single http request
- the **Transaction Command**
  - takes in one or multiple fields to make transactions
  - creates two fields in the raw event: _duration_ and _eventcount_
  - duration: the time between the first and last event in the transaction
  - eventcount: number of events in the transaction
- can be used with statistics and reporting commands
- i.e. `index=web sourcetype=access_combined | transaction clientip`
  - we get a list of events that share the same clientip, for each clientip as a group of events
  - format it better with `| table clientip, action, product_name`
- _tansaction_ command definitions: maxspan, maxpause, startswith, endswith
  - maxspan: allows setting a max total time between earliest and latest events
  - maxpause: the max total time allowed between events
  - startswith: forming transactions starting with specified (terms, field values, or evaluations)
  - endswith: forming transactions ending with specified (terms, field values, or evaluations)
  - http://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Transaction
- i.e. `index=web sourcetype=access_combined | transaction clientip startswith=action="addtocart" endswith=action="purchase"`
  - we get a list of events that share the same clientip, for each clientip as a group of events
  - can put expressions inside _eval_ function
  - format it better with `| table clientip, action, product_name` by default repeated values are ordered by an alphabetic sort discounting duplicates.

**Transaction vs. Stats**
- use transactions when:
  - to see events correlated together
  - when events need to be grouped on start and end values
  - by default limit 1000 events per transaction
- use stats when:
  - to see results of a calculation
  - when events need to be grouped on a field value
  - no limit on events per transaction
  - faster and more efficient

## Module 7 Knowledge Objects

**Knowledge Objects** like some tool to help discover and analyze data
- include data interpretation, classification, enrichment, normalization, and search-time-mapping of knowledge (Data Models)
- can be shared with permission settings, reused by multiple people and used in search
- Naming conventions
  - Suggested components: group type platform category time description
  - http://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Developnamingconventionsforknowledgeobjecttitles
  - i.e. `OPS_WFA_Network_Security_na_IPwhoisAction`
  - for a security-focused workflow action
- Permissions, three types
  - private, default for created by a User
  - specific app, only set by Power User and Admin
  - all apps, onyl set by Admin
- Managing Knowledge Objects through: Settings -> Searches, Reports, and Alerts, or Settings -> All Configurations
- **Common Information Model (CIM)** for normalizing values to a common field name across multiple indexes and sourcetypes
  - uses schema that defines standard fields b/w sources to create common base references
  - can use Knowledge Objects to help make these connections

## Module 8 Field Extractions

**The Field Extractor** allows to use a GUI to extract fields that persist as Knowledge Objects making them reusable in searches
- Useful if the data is badly structured (not in some key=value fashion)
- fields extracted are specific to a host, source, or sourcetype and are persistent
- Two different ways a field extractor can use: regex and delimiters
  - regex: using regular expression, work well when having unstructured data and events to extract fields from
  - delimiters: a delimiter such as comma, space, or any char. Use it for things like CSV files
- Three ways to access a field extractor:
  - Settings -> Fields
  - The field sidebar during a search
  - From an event's actions menu (easiest)
- http://docs.splunk.com/Documentation/Splunk/latest/Knowledge/AboutSplunkregularexpressions

## Module 9 Aliases and Calc Fields

**Field Aliases** give you a way to normalize data over multiple sources
- can assign one or more aliases to any extracted field and can apply them to lookups
- Good to reference CIM when creating aliases
- in GUI, Settings -> Fields -> Field aliases (add new)
  - set destination app
  - name (choose a meaningful name for the new field)
  - apply to (sourcetype, source, or host) and its matching value
  - Map fieldName=newFieldName
  - Map multiple fields by adding another field

**Calculated Field** saves frequently used calculation you would do in searches with complex _eval_ statements
- it is like a shorthand for an _eval_ pipe
- i.e. conversion between bytes to megabytes
- in GUI, Settings -> Fields -> Calculated fields (add new)
- must be based on extracted fields
  - output fields from a Lookup Table or fields generated from within a Search string are not supported

## Module 10 Tags and Event Types

**Tags** are Knowledge Objects allow you to designate descriptive names for key-value pairs
- enable search for events that contain particular field values
- in GUI, within a search, click on an event and see the list of key=vlaue pairs
  - in actions dropdown select Edit Tags to add new tags (comma separated)
- tags are value-specific. So it doesn't make sense to do this for a field having infinite possible values!
- To search with tags, enter `tag=<tag_value>` i.e. `index=security tag=SF`
  - tag values are case-sensitive in searches
  - matches all fields that are tagged with that value
- To limit a tag value to a field do `tag::<field>=<value>`
- manage tags in Settings -> Tags

**Event Types** allows categorize events based on search terms
- save a search as "Event Type"
- use _eventtype_ in a search will return those events that matches the definition of that eventtype
  - matching eventtypes will be color-coded
- i.e. `index=web sourcetype=access_combined eventtype=purchase_strategy`
- the priority settings affects the display order of returned results

**Event Types vs. Saved Reports**
- Event Types advantages:
  - categorize events based on search string
  - use tags to organize
  - can use the "eventtype" field within a search string
  - Downside: does not include a time range
- Saved Reports advantages:
  - fixed search criteria
  - time range & formatting needed
  - share with others
  - add to dashboards

## Module 11 Macros

**Macros** are reusable search strings or portions of search strings
- useful for frequent searches with complicated search syntax
  - can store entire search strings
  - time range independed; time selected at search time
  - can pass arguments to the search
- in GUI, Settings -> Advanced Search -> Search macros (add new)
- i.e. you saved some _eval_ expression as a macro, then used it in a search:
  - `index=sales sourcetype=vendor_sales | stats sum(sale_price) as total_sales by Vendor | ` \`convertUSD\`
  - using macro with its name surrounded by backticks
- For macro that accepts arguments
  - name: i.e. convertUSD(1)
  - arguments: argument_name
  - definition: use argument by using $argument_name$
  - validation: can ensure the argument passed in have good values
  - \`convertUSD("total_sales")\`
- ctrl-shift-E or cmd-shift-E can preview the expanded macro without running it

## Module 12 Workflow Actions

**Workflow Actions** create links to interact with external resources or narrow search
- uses hhtp GET/POST to pass info to external source
- or pass info back to Splunk for secondary searches
- Create workflow actions
  - GUI, Settings -> Fields -> Workflow Actions (add new)
  - can create actions for GET or POST request, or open another search
  - use field as variable like `$src_ip$` in the URI
  - if data value may contain chars need to be escaped for http URL, use `$!src_ip$`
- Workflow Action created will be available in the searched events that contains that field, as "event actions"
- If creating POST action, it only allows posting key=value raw data to the url specified
  - `age=1&hello=world&src_ip=10.2.10.163`
  - like moving the http get url parameters to the request body
  - may not be useful for many use cases

## Module 13 Data Models

**Data Models** hierarchically strucutred datasets
- three types: Events Searches Transactions
- Data Model is like a framework for the Pivot interface
  - Starting from a Root Object (can be a simple search for index and sourcetype)
  - Add Child Object (like a filter or constraint, or just something refine the search further)
  - Each Root Object or Child Object can have multiple Child Objects
- Create Data Model on GUI: Settings -> Data Models -> New Data Model
- Or create it from a valid Search -> Statistics -> Pivot -> Save as Report -> Edit Datasets
  - Root Event, enables you create hierarchies based on a set of events, commonly used
  - Root Search, builds hierarchies from a transforming search and does not benefit from data model acceleration
  - After creating Root Event:
  - Root Transaction allow you create datasets from groups of related events that span time. It uses an existing object from the data hierarchy. It also does not benefit from data model acceleration
  - Child allow you to constrain or narrow down the events in the objects above it in the hierarchical tree
- Add Field to the Data Model. Five types:
  - Auto-Extracted, fields Splunk extracts from the data
  - Eval Expression, fields created by running an eval expression on some fields
  - Lookup, fields created using existing tables
  - Regular Exp, fields created from regex on the data
  - Geo IP, fields created from Geo IP data in the events
- http://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Acceleratedatamodels

## Module 14 The Common Information Model (CIM)

Using **CIM** to make sure we map all data to defined method and normalize data to a common language.
- data can be normalized at index time or search time
- CIM schema should be used for
  - field extractions
  - aliases
  - event types
  - tags
- Knowledge Objects can be shared globally across all apps
- Can search the datamodel with `| datamodel keywords` command
- By default, CIM Add-on will search across all indexes.
  - this can be override in the CIM Add-on setting page
- https://splunkbase.splunk.com/app/1621/
- http://docs.splunk.com/Documentation/CIM/latest/User/Howtousethesereferencetables
- http://docs.splunk.com/Documentation/CIM/latest/User/Overview


# Splunk 7 Fundamentals III (IOD)

This course focuses on additional search commands as well as **advanced use of knowledge objects**. Major topics include **advanced statistics and eval** commands, **advanced lookup** topics, **advanced alert** actions, using **regex and erex to extract fields**, using **spath to work with self-referencing data**, creating **nested macros and macros with event types**, and **accelerating reports and data models**.

## Module 1 Intro

Review contents in previous courses.

## Module 2 Exploring Stats

**Stats Aggregate Functions** besides the _count, dc, sum, min, max, avg_ aggregation functions for summarizing values from events for _stats_ command, there are also: _median, range, stdev, var_ that can be used in _stats, chart, timechart_ commands
- _mean_ can be used in place of _avg_ function
- we need to use single quotes for names containing spaces in an _eval_ expression
- https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Aggregatefunctions

**Fieldsummary Command** is used to calculate summary statistics for fields in your events
- Given a search `index=web | fieldsummary` it gives insights of the "count, distinct_count, is_exact, min/max/mean/stdev, numberic_count, values" for each field
- adding a field name as the argument to this command, only summary for that field is returned
- by default, returns 100 distinct values per field, can be override with `maxvals` function

**Appendpipe Command** append subpipeline search data to your results
- it is not run until the command is reached
- it is not an additional search, but merely a pipeline for the previous results
- multiple appendpipe can be used in a search
- i.e. `index=network sourcetype=cisco_wsa_squid (usage=Borderline OR usage=Violation) | stats count by usage, cs_username | appendpipe [stats sum(count) as count by usage | eval cs_username= "TOTAL: ".usage] | appendpipe [search cs_username=Total* | stats sum(count) as count | eval cs_username = "GRAND TOTAL"] | sort usage, count`

**Count and List Functions** using them in conjunction can help make the results easier to read
- _list_ returns a list of values of a field as a multi-value result (will be put in one cell in the result table), by default up to 100 values
- i.e. `index=web sourcetype=access_combined action=purchase status=200 | stats count by host, product_name | sort -count | stats list(product_name) as "Product Name", list(count) as Count, sum(count) as total by host | sort -total | fields - total`

**Eventstats Command** generates statistics for fields in searched events and save them to new fields in the results
- the statistics are added inline to pertinent events
- i.e. the command `index=web sourcetype=access_combined | stats sum(bytes) as totalBytes by clientip | table clientip, bytes, totalBytes`
  - will yield a table with no data in 'bytes' column
  - this is because the _stats_ command generates the statistics info but won't preserve info used to do the calculation
- i.e. to fix that, `index=web sourcetype=access_combined | eventstats sum(bytes) as totalBytes by clientip | table clientip, bytes, totalBytes`
  - this command will do
  - because _eventstats_ adds the result as event data available in the search raw data
- i.e. another example `index=web sourcetype=access_combined action=remove | chart sum(price) as lostSales by product_name | eventstats avg(lostSales) as averageLoss | where lostSales > averageLoss | fields - averageLoss | sort -lostSales`
  - here after _eventstats_ command, you can see the data 'averageLoss' is available in each row in the table created by chart
  - then we use its value to filter out the rows we need to keep, then get rid of that field
- i.e. yet another example `index=web sourcetype=access_combined action=purchase status=200 | timechart sum(price) as totalSales | eventstats max(totalSales) as highest, min(totalSales) as lowest | where totalSales=highest OR totalSales=lowest | eval Outcome=if(totalSales=hightest, "Highest", "Lowest") | table _time, Outcome, totalSales`

**Streamstats Command** aggregates statistics to your searched events as Splunk sees the events in time (in a streaming manner)
- there three functions quite improtant than others
  - current - setting to 'f' tells Splunk to use the field value from previous event when performing statistical function
  - window - setting to N tells Splunk to only calculate previous N events at a time
  - time_window - setting to a time span to only calculate events happen in every that time span; with this function, events must be sorted by time
- i.e. `index=web sourcetype=access_combined action=purchase status=200 | stats sum(price) as orderAmount by JESSIONID | sort _time | streamstats avg(orderAmount) as averageOrder current=f window=20`
- https://www.splunk.com/blog/2014/04/01/search-command-stats-eventstats-and-streamstats-2.html

## Module 3 Conversion Functions

**Eval Command** Review
- Operates on existing fields and store result in a field
  - can override field value if it is the same field name
- Available arithmetic operators: + - * / %
- Available concatenation operator: .
- Available boolean operator: AND OR NOT XOR > < >= <= != = == LIKE IN

**Eval Command** Syntax
- multiple fields calculation can be created by one _eval_ command, separated by ','
  - ',' separated expressions are treated as functions, processed from left to right
  - fields created first can be used in subsequent calculation in the same _eval_ command
- three conversion functions for _eval, fieldformat, where_:
  - _tostring()_ casts a value to string; acceptable arguments: `(field, "commas"/"hex"/"duration")`, for formatting to "number with comma/hexadecimal number/time values"
  - _tonumber()_ casts a value to number (for calculation); acceptable arguments: `(field, 2)` (w is for example), for matting to base-10 number from a base-2 number
  - _printf()_ build a string from some formatting and arguments: `("formatString%s/float%.2f", Revenue)`; for `%f`, adding a ' char will show commas in the number: `%'.2f`; for `%f`, adding a + char will show either a plus or minus char in the value: `%+.2f`
  - mostly won't required unless used with arguments, as Splunk try to do them intelligently

## Module 4 Date and Time Functions

- _eval_ can also be used to calculate date and time
  - _now_ function returns the time a search is started
    - returns time in second precision
  - _time_ function returns the time an event was processed by the _eval_ command
    - returns time in a micro-second precision
  - _strftime_ allows convert unix epoch timestamp into a string
  - _strptime_ allows convert from string to a timestamp
    - i.e. `index=manufacture sourcetype=3dPrinterData | eval boot_ts=strptime(boot_time, "%b/%d/%y %H:%M:%S"), days_since_boot=round((now()-boot_ts)/86400) | stats values(days_since_boot) as "uptime_days" by printer_name`
    - `sourcetype=foo | eval date_hour=strftime(_time, "%H") | eval date_wday = strftime(_time, "%w") | search date_hour>=9 date_hour<=18 date_wday>=1 date_wday<=5`
  - _relative_time_ returns a timestamp relative to a specified time
    - 's m h d w mon y' represents 'seconds minutes hours days week month year' respectively
    - i.e. `index=manufacture sourcetype=3dPrinterData | eval boot_ts=strptime(boot_time, "%b/%d/%y %H:%M:%S"), rt=relative_time(boot_ts, "+30d"), reboot=strftime(rt, "%x") | stats values(reboot) as "day_to_reboot" by printer_name`
- These functions can also be used with _fieldformat_ and _where_ command

## Module 5 Text Functions

- _eval_ support functions to process text
  - _upper_ and _lower_ takes a value and return the string in upper or lower case respectively
    - use it to normalize data
    - i.e. `index=hr | eval Organization=lower(Organization), Is_In_Marketing=if(Organization=="marketing", "Yes", "No") | table Name, Organization, Is_In_Marketing`
  - _substr_ creates a field that is the value of doing substring from another field
    - i.e. `index=hr | eval Group=substr(Employee_ID, 1, 2), Location=substr(Employee_ID, -2) | table Name, Group, Location`
  - _replace_ apply a rule to replace text values in a field. regex is supported
    - i.e. `index=hr | eval Employee_Details=replace(Employee_ID, "^([A-Z]+)_(\d+)_([A-Z]+)", "Employee #\2 is a \1 in \3")`
  - _len_ returns the length of a field value
- These functions can also be used with _fieldformat_ and _where_ command

## Module 6 Comparison & Conditional Functions

- _eval_ support functions to aid condition check
  - _if_ and _case_ functions covered before
    - i.e. `index=sales sourcetype=vendor_sales | eval salesTerritory = case(VendorID<5000, "US", VendorID<7000, "EMEA", VendorID<9000, "APAC", 1==1, "Rest of World")`
  - _coalesce_ takes a list of arguments and returns the first value not null
    - useful when combing fields
    - i.e. `index=hr sourcetype=HD_DB | eval IP=coalesce(SF_IP, BO_IP, LN_IP) | table Name, System_ID, IP`
  - _match_ takes a field and a regex to match against
    - returns "True" if is a match, "False" otherwise
    - result can be used nested in an _if_ function
  - _cidrmatch_ specifically used to match ip addresses, takes in a "CIDR" ip string and then the field to match against
  - statements functions
    - _true() false() null()_ accepts no arguments and yields True, False, and null values
    - _nullif_ takes a field and a value against, and returns null value if it matches and returns its value if not a match
    - _tostring_ can cast a True, False, null value into its String counterpart
  - informational functions
    - _isbool isint isnotnull isnull isnum isstr in_ will return True or False on assertion of the field passed in
    - _typeof_ returns a string representation of the type of the field passed in; null value returned as "Invalid"
- These functions can also be used with _fieldformat_ and _where_ command

## Module 7 Stat & Math Functions

- _eval_ command's statistical functions
  - _min max_ takes some fields and return the lowest/hightest value from those
    - strings are compared lexicographically, and greater than numbers
  - _random_ returns a 32-bit random integer
    - i.e. `index=sales sourcetype=vendor_sales | streamstats count as eventNumber | where eventNumber = [ search index=sales sourcetype=vendor_sales | stats count as eventNumber | eval randomNumber=random() % eventNumber | return $randomNumber ] | table Vendor, VendorCity, VendorCountry`
  - mathematical functions
    - _abs ceiling exact exp floor ln log pi pow round sigfig sort_
  - cryptographic functions
    - _md5 sha1 sha256 sha512_
- These functions can also be used with _fieldformat_ and _where_ command

## Module 8 Other functions

- other infrequently used functions for _eval_
  - trigonometry functions
    - _acos acosh asin asinh atan atan2 atanh cos cosh hypot sin sinh tan tanh_
  - multivalue functions, that can operate on multiple values
    - _mvappend mvcount mvdedup mvfilter mvfind mvindex mvjoin mvrange mvsort mvzip split_
- the **Makeresults Command** creates a defined number of search results
  - good for creating sample data for testing searches or building values to be used in searches
  - i.e. `| makeresults | eval tomorrow=relative_time(now(), "+1d"), tomorrow=strftime(tomorrow, "%A"), result=if(tomorrow="Saturday" OR tomorrow="Sunday", "Huzzah!", "Boo!"), msg=printf("Tomorrow is %s, %s", tomorrow, result)`
- These functions can also be used with _fieldformat_ and _where_ command

## Module 9 Exploring Lookups

It has been covered that **Lookups** can enrich the data in the index with static or relatively unchanging sets of data. Fields from Lookups can be used in searches and show up in the sidebar.
- _lookup_ command can be used to lookup values
- can also configured to automatically return values by hosts, source, sourcetype
- use _lookup_ in a **subsearch** can help narrow down the set of events to search based on the values in the lookup
  - _inputlookup_ is the command to use to return values from a lookup table
  - i.e. `[ | inputlookup API_Tokens | table VendorID ]`
  - the values that subsearch returned will be used as search terms of the field-value pairs, with an OR operator between
  - the outter search of above subsearch will include events with only those VendorID from the lookup
  - to exclude the results from the subsearch, add NOT before the subsearch `NOT [ ... ]`
- **Splunk Key Value Store**, aka KV Store, is a way to store/retrieve data in Splunk apps, or create lookups from it
  - add data using Json POSTs to the Splunk API or the _outputlookup_ command
  - allows per-record inserts and edits; data type enforcement; field acceleration; rest API access
  - does not support case-insensitive lookups
  - KV collections defined in collections.conf
- **File-based Lookups** such as CSV file
  - need to use _inputlookup_ command
  - good for small or rarely modified lookups
  - allow case-sensitive lookups, easy text editing
  - does not support rest API
  - does not do per-record edits
  - no data type enforcement
  - no multi-user access locking
- **External Lookups** using scripts to fetch values for lookups; the speed of the script determines the speed of the lookup
  - need to use _lookup_ command
  - underlying file type is csv
- **Geospatial Lookups** matches data with geographic feature collections in KMZ or KML files, and output fields with geographic information
  - output field represent geographic regions shared borders with other regions of the same type
- **DB Connect Lookups** bridges Splunk to a relational DB, allows use of Splunk queries and reports with them, and adds data to our search
  - use `dbxlookup lookup=<db_connect_source>` in Search app, after setting up the connect
- **Lookups Best Practices**
  - in lookup table, keep the key field the first column
  - put lookups at the end of the search
  - if a lookup usage is very common in search, make it automatic
  - use KV store or gzip CSVs for large lookup table
  - only put data being used in lookup table
- **Outputlookup Command** can output results of search to a static lookup table file or KV Store collection
  - can overwrite an existing lookup table file
  - _createinapp_ is the bool to control whether create in system lookup directory

## Module 10 Exploring Alerts

**Alerts** are triggered when specific conditions in search results is triggered
- alert actions including
  - sending emails
  - logging events
    - Admin or edit_tcp capability required
    - in log event, use `$result.fieldname$` to access field value
    - https://docs.splunk.com/Documentation/Splunk/latest/AdvancedDev/ModAlertsLog
  - output to a lookup
  - using webhooks
    - can POST Json data for the results of the search triggered that alert
  - custom actions

## Module 11 Advanced Field Creation

**Field Extractor** allows users to perform extractions from the GUI
- always best to check and edit generated regex for performance and accuracy
- Good regex resources
  - avoid backtracking (when a regex went through the entire string and misses a match and has to start over) in regex
    - i.e. given string "IPs (178.162.162.192,121.9.245.250)" and regex _\((.*)(.*)\)_ it takes three passes to get the match right
  - avoid using whildcards when possible, use '+' over '*', use '|' over greedy matches, add '?' to make greedy quantifiers non-greedy
  - alwasys test regex expression and benchmark its speed
  - https://regex101.com/
  - https://regexone.com/
  - https://www.rexegg.com/
- the **erex rex Command** allows extract fields temporarily for the duration of the search
  - _erex_ is like an auto field extractor, but had the same shortcomings as the regex field extractor: only look for based on the samples provided
    - i.e. `index=security sourcetype=linux_secure "port" | erex ipaddress examples="74.125.19.106, 88.12.32.208" | table ipaddress, src_port`
  - _rex_ allows use regex capture groups to extract values at search time
  - by default uses `field=_raw`, if not provided a field name to read from
    - i.e. `index=security sourcetype=linux_secure "port" | rex "(?i) from (?P<ipaddress>[^ ]+)" | table ipaddress, src_port`
    - i.e. `index=network sourcetype=cisco_esa | where isnotnull(mailfrom) | rex field=mailfrom "@(?P<mail_domain>\S+)"`
    - i.e. regex string `\<(?<local_part>.*)@(?<domain>.*)>`
  - use _rex_ is recommended

## Module 12 Searching Self-Describing Data

**Self-Describing Data** includes the schema embedded in the data
- the data does not need to conform to a formal structure, such as _JSON XML_
  - the schema includes markers to separate elements
  - the schema enforce hierarchies of records and fields in the data
- Splunk will automatically recognize _JSON_ data and extract fields, but not for _XML_ data
  - setting in props.conf file, set by Admin: AUTO_KV_JSON=true; KV_MODE=XML
- **spath Command**
  - _spath_ command extracts info from semi-structured data and store in fields
  - i.e. `index=systems sourcetype=system_info_xml | spath path=event.systeminfo.cpu.percent output=CPU`
  - by default uses `input=_raw`, if not provided a field name to read from
  - when working with _JSON_ formatted data, values in array will be indicated by curly braces
    - i.e. `index=systems sourcetype=system_info_json | spath path=CPU_CORES{}.idle output=CPU_IDLE | stats avg(CPU_IDLE) by SYSTEM{}`
- _spath_ function for _eval where fieldformat_ commands
  - as an alternative to _spath_ command
  - i.e. `index=hr sourcetype=HR_DB | eval RFID=spath(Access, "rfid.chip"), Location=spath(Access, "rfid.location") | table Name, Access, RFID, Location`
- **MultiKV Command** for data formatted as large and single events of tabular data, extracts field values from them.
  - each row becomes an event, header becomes the field names, and tabular data becomes values of the fields
  - i.e. `index=main sourcetype=netstat | multikv` give all fields extracted
  - i.e. `index=main sourcetype=netstat | multikv fields LocalAddress State filter LISTEN | rex field=LocalAddress ":(?P<Listening_Port>\d+)$" | dedup Listening_Port | table Listening_Port` the _fields_ function chooses which fields to extract; the _filter_ function get only events that contain at least once of all the strings provided

## Module 13 Advanced Search Macros

Review what has been covered for **Search Macros**
- Again, Search Macros are nothing more different than the Splunk Search Language
- Search Macros use time range set at search time
- Now is best to save complex regex for field extractions as Search Macros, that can be reused.
  - i.e. the use case of extracting emails from raw events `index=games sourcetype=simcubebeta `ExtractEmail(_raw, ExtractedEmail)` | table ExtractedEmail`
  - i.e. the ExtractEmail was created from: `| rex field=$infield$ "[\s'=<](?P<$outfield$>[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9.]+)"`
- Nested Macros is allowed, with multiple levels
  - it is like calling a function from another
  - i.e. ``ExtractEmail($infield$, $outfield$)`| rex field=$outfield$ "@(?P<$outfield$>[a-zA-Z0-9-]+\.[a-zA-Z0-9.]+)"`
- use Tags and Event Types with Macros as would do in a normal search
  - i.e. `tag=InternalFail | stats count as FailedAttempts by user | where FailedAttempts>$GT_misses$ | rename user as $checkfield$ | table $checkfield$`
  - i.e. `eventtype=online_purchase_http_errors` directly search with saved event type

## Module 14 Acceleration: Report & Summary Index

The number of events returned in a search affects search time.
Splunk allows creation of summary of event data, as smaller segments of event data that only include those info needed to fullfill the search.
- it will be faster for searches
- three data summary creation methods:
  - Report acceleration, uses automatically created summaries, accelerates individual reports
    - Splunk runs the report and populate an acceleration summary
    - acceleration summary basically are stored results populated every 10 minutes
    - also stored as file chunks alongside indexes buckets
  - Summary indexing, uses manually created summaries, indexes separated from deployment
  - Data model acceleration, accelerates all fields in a data model, easiest and most efficient option

- for **Report acceleration**, the search has to be in Fast/Smart mode; Users must have "schedule_search" privilege
- if deleting all reports that uses summary, the summary will be deleted too
- types of commands for acceleration
  - streaming command, operate on events as they are returned, such as _eval search fields rename_
  - non-streaming, wait until all events are returned before executing. Note that streaming commands like _eval rename_ becomes non-streaming after a transforming command
  - transforming commands, order results into a data table, such as _stats chart top rare_
- Accelerated reports must include a transforming command, and before the transforming command must be streaming commands and after the transforming command must be non-streaming commands.
- Note that if Splunks sees storage used for summary exceeds 10% of total bucket size in the deployment, it will suspend summary creation for 24 hours
- If knowledge objects related to the search change, fix the search and rebuild the summary.

**Summary Indexing** should be used as alternative for **Report Acceleration** when reports do not qualify for acceleration.
- created by running a search that uses special transforming commands and writes results to a Summary Index on the search head to be used for searching.
  - requires Admin role
  - commands used are the 'si'-prefixed commands such as _sichart sitimechart sistats sitop sirare_
  - i.e. `index=security | sitop src_ip_user`
  - should be used and saved as a report that will run repeatedly and more frequently than any searches that will run against the Summary Index it creates
  - Can also give optional name-value pair for this Summary Index for easier search, like `report=summary_page_views_to_purchase`
- search a Summary Index by using `index=<summary_index_group_name> | report=<summary_index_name>`
- Avoid gaps and overlaps in the data when using **Summary Indexing** which can occur when
  - a populating report runs too long, or its time range to be longer than the frequency of the report schedule
  - a real-time schedule is used
  - Splunk goes down
- if gaps in data happen, a backfill script is required https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Managesummaryindexgapsandoverlaps; schedule populating reports to avoid gaps and overlaps


## Module 15 Acceleration : Data Models & Tsidx

**datamodel command** allows display the structure of data models and search against them.
- i.e. `| datamodel`
  - supply the datamodel name as 1st arg to see a specific data model
  - supply object name as 2nd arg to see details of that object
  - add 'search' to enable search on the data model data
  - i.e. `| datamodel Buttercup_Games_Online_Sales http_request search | search http_request.action=purchase`

**Data Model Acceleration** speeds up reporting for all fiedls defined by the data model
- files are created and accessed to return accelerated search results, called High-Performance Analytics Store
- Two types of Data Model Acceleration
  - Adhoc, summary-creation happens automatically on data models that have not been accelerated; summary files created stays on the search head for the period that user is actively using the Pivot tool
    - Pivot search run for the first time is very slow as summary is created
    - reports created in the Pivot editor won't use the summary
    - it is a performance killer for a Splunk deployment if multiple users are doing adhoc acceleration
  - Persistent, summary files created are stored alongside the data buckets and exits as long as the data model exists
    - can be used by all users
    - Admin or 'accelerate_datamodel' role is required
    - private data models cannot be accelerated
    - data model will be Read-only after persistent acceleration
    - it also requires searches that only use streaming commands

**Time-Series Index Files (Tsidx files)** are files created for data model acceleration
- when data is pumped into Splunk, some files including _raw data files and tsidx files were created
- tsidx components:
  - lexicon, an alphanumerically ordered list of terms found in data at index-time
    - fields extracted at index-time showup as key-value pair in lexicon
    - Splunk searches lexicon first and only open and read raw event data matching the terms using the pointers
  - posting list, array of pointers that match each term to events in the raw data files
- building of tsidx files also happen when a data model is accelerated
  - for persistent acceleration, updated tsidx every 5 min and remove-outdated after 30 min

**Tstats Command** is for getting statistical info from tsidx files
- uses lexicon and won't open raw data file
- can only use fields designated in the lexicon
- i.e. `| tstats values(sourcetype) as sourcetype by index`
  - any _stats_ functions can be used
- i.e. `| tstats count from datamodel=linux_server_access by host`
  - the _from_ clause tells _tstats_ to use the files created when accelerated 'linux_server_access' data model
- i.e. `| tstats count as "count" from datamodel=linux_server_access where web_server_access.src_ip=10.* web_server_access.action=failure by web_server_access.src_ip, web_server_access.user summariesonly=t | where count > 300 | sort -count`
  - by default, Splunk will do full search if the search ran is outside of the summary range of the accelerated data model
  - can limit search to summary range with _summariesonly_ argument
- i.e. `| tstats count as "events" by _time span=1d | sort -_time`
  - 'span' can be used to limit the time block for creating the stats


# Splunk 7 Advanced Searching and Reporting

This course focuses on advanced search and reporting commands to create robust searches, reports, and charts. Major topics include optimizing searches, additional charting commands and functions, formatting and calculating results, correlating events, and using combined searches and subsearches.

## Module 1 Intro

Review contents in previous courses.

## Module 2 Splunk Architecture

Searches that are run frequently or scheduled timely are the primary candidates for optimization. Also consider optimizing searches that query a large volume of data. The goal is to work with the smallest set of event data possible.

Splunk store data in indexes which held data in groups of data buckets.
Data are stored in Hot buckets as they arrive, then warn buckets and finally cold buckets. Eventually they go to frozen buckets for deletion or archiving.

**Inside Buckets** each bucket contains:
- a journal.gz file, where Splunk stores raw event data, composed of many smaller and compressed slices (each about 128 kb)
- time-series index (.tsidx) files, serves as index keys to the journal file
  - Splunk uses this to know which slice to open up and search for events
  - created from raw events, where each line is tokenized with the token words written out to a lexicon which exists inside _each_ tsidx file
  - and each lexicon has a posting array which has the location of the events in the journal file

**Bloom Filters** is created based on tsidx files when a bucket roll over from Hot to Warm
- it is a data structure for quick elimination of data that doesn't match a search
  - allows Splunk to avoid reading from disk unless really necessary
- when creating a Bloom Filter, each term inside a bucket's tsidx files' lexicon is run through a hashing algorithm
  - resulting hash sets the bits in the Bloom filter to 0/1
  - When a search is run, it generates its own Bloom filter based on the search terms and compared with the buckets' Bloom Filters, resulting a fast filter out unrelevant buckets

**Inside a Search**
- i.e. `index=security failed user=root | timechart count span=1h | stats avg(count) as HourlyAverage`
  - Splunk extracts 'failed' and 'root' and creates a Bloom Filter from them
  - Splunk then identify buckets in `security` index for the past 24h
  - Splunk compares the search Bloom Filter with those from the buckets
  - Splunk rules out tsidx files that don't have matches
  - Splunk checks the tsidx files from the selected buckets for extracted terms
  - Splunk uses the terms to go through each posting list to obtain seek address in journal file for raw events
  - Splunk extracts search-time fields from the raw events
  - Splunk filters the events that contains our search terms 'failed' and 'root'
  - The remaining events are search results
- In the search's Job Inspector
  - command.search.index gives the time to get location info from tsidx files
  - command.search.rawdata tells the time to extract event data from the gzip journal files
  - command.search.kv tells the time took to perform search-time field extractions
  - command.search.filter shows the time took to filter out non-matching events

**Transforming vs. Streaming Commands**
- transforming commands operate on the entire result set of data
  - need to wait for full set of results
  - executed on the Search Head
  - change event data into results once its complete
  - example commands _stats timechart chart top rara_
- streaming commands has two types
  - centralized
    - aka "Stateful Streaming Commands"
    - executed on the Search Head
    - apply transformations to each event returned by a search
    - results order depends on the order the data came in
    - example commands _transaction streamstats_
  - distributable
    - can execute without waiting for the full set of data
    - order of events does not matter
    - can be executed on Search Head or different Indexers
    - example commands _rename eval fields regex_
    - If preceded by commands that have to run on a Search Head, all will run on a Search Head
- http://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Commandsbytype

**Examples**
- i.e. `index=security failed user=root | timechart count span=1h | stats avg(count) as HourlyAverage`
  - _timechart_ and _stats_ are both transforming commands
  - data arrives to the Search Head then executes these commands
- i.e. `index=network sourcetype=cisco_wsa_squid | eval Risk = case(x_wbrs_score >= 3, "1 Safe", x_wbrs_score >= 0, "3 Neutral", x_wbrs_score >= -5, "4 Dangerous", 1==1, "Unknown") | timechart count by Risk`
  - _eval_ is executed on the Indexers and results send back to Search Head
- i.e. `index=network sourcetype=cisco_wsa_squid usage="Personal" OR usage="Violation" | stats count as connections by suspect, usage | rename username as suspect`
  - _rename_ here has to run on the Search Head, since _stats_ is a non-streaming transforming command
  - optimization: move _rename_ up before _stats_, so that it can execute on the Indexers

## Module 3 Search Tuning

**Breakers and Segmentation**
- Segmentation: the process of tokenizing search terms at search-time. Two stages:
  - splitting events up by finding characters (major/minor breakers)
  - major breakers are used to isolate words, phrases, terms, and numerical data
    - example major breakers: _spaces newlines carriage_returns tabs brackets exclamation_points commas_
  - minor breakers go through results in the first pass and break them up further
    - example minor breakers: _forward_slashes colons periods hyphens dollar_signs_
- The goal is to quickly return a set of tokens
- The tokens become part of tsidx files lexicons at index-time and used to build bloom filters
- https://docs.splunk.com/Documentation/Splunk/latest/Data/Abouteventsegmentation

**Lispy Expressions**
- in a search's Job Inspector, you can access the _search.log_
- find the phrase "base lispy" in the _search.log_
  - this contains the Lispy Expression Splunk created from the search query
  - is used to build Bloom Filter and locate terms in tsidx files
  - i.e. `index=web clientip=76.169.7.252` yields Lispy: `[ AND 169 252 7 76 index::web ]`
- Lispy in Searches
  - in previous example, search of IP address breaks down the ip into four tokens, which can affect performance by letting Splunk return all events containing any of those numbers instead of just the IP address
  - phrases can also create an issue if they include a space (a major breaker)
    - i.e. `index=security "failed password"` yields Lispy: `[ AND failed index::web password ]`
    - will return events contain either 'failed' or 'password'
- To compensate the issue above for Lispy Expression, use the **TERM Directive** to specify a search term as a complete value
  - i.e. `index=web clientip=TERM(76.169.7.252)`
  - it by passes any minor breakers in the TERM argument
    - TERM's argument **MUST** be bound by major breakers in the raw events for events to be returned
    - i.e. this won't get returned: `index=cdn upload=TERM(image.gif)` for raw event `12/May/2019:09:42:36, Upload:'http//BCG.com/image.gif',User:'angrybird@k.net'`
    - TERM's argument must **NOT** include any major breakers
    - i.e. `index=games action=TERM("Got caught making fun")` is not indexed as a token and TERM has no effect here
  - You can use TERM to match key value pairs before field extraction
    - i.e. `index=games TERM(EventType=8)`
    - Note that TERM will not work with field aliases; if working heavily with CIM, i.e. `index=network sourcetype=cisco_wsa_squid src_ip=TERM(192.168.1.1)` can be replaced with `index=network sourcetype=cisco_wsa_squid TERM(192.168.1.1) src_ip=192.168.1.1`
- Negation in Lispy
  - i.e. `index=security NOT password` yields Lispy `[ AND index::security [ NOT password ] ]`
  - negating terms that including minor breakers is not helpful
    - i.e. `index=web NOT 192.168.1.1` yields Lispy `[ AND index::web ]`
    - This is a good place to use the TERM directive
- Wildcards and Lispy
  - wildcard in the middle is okay if the term doesn't include major/minor breakers; otherwise might cause in inconsistent search results
    - the fix is to not let wildcard consume the major/minor breaker in the term by including it: `slid*do` vs. `slid*.do`
    - still avoid wildcard in the middle of a term
  - wildcard in the end of the term is acceptable
  - wildcard at the beginning of a term will not create tokens
- Restrict search to use unique values and specific terms as much as possible
- using fields extracted at index-time early in the search would be very effective in creating efficient searches
- when searching for fields that are extracted at search-time, only the values are tokenized in Lispy and only if the '=' operator is used
  - others like '!=', '>', '<' will not tokenize field values
  - because these operators do not tell Splunk what exactly we are looking for
- Lookups and Lispy
  - values matches in a lookup are automatically added to the Lispy Expression

## Module 4 Subsearch

**Subsearch** is a search that passes its results to its outer search as search terms
- enclosed in brackets
- must start with generating commands, such as _search tstats_
- always executed first before passing results to the outer search
- i.e. `index=security sourcetype=linux_secure "accepted" [search index=security sourcetype=linux_secure "failed password" src_ip!=10.0.0.0/8 | stats count by src_ip | where count > 10 | fields src_ip] | dedup src_ip, user | table src_ip, user`
  - when the results of the subsearch returns, Splunk implicitly adds "OR" between each two results
  - then the whole results is combined with the outter search with an "AND" operator;
    - can be override with "NOT" before the subsearch
  - multiple fields can be returned from the subsearch, better use _fields table return_ to limit the fields returned
- the **return command** should be used within subsearches
  - it formats results from a search into a single result and places into a new field "search", `| return src_ip`
  - by default, returns the first value unless a number of results is specified: `| return 5 src_ip`
  - concatenate the values with "OR": `(src_ip="xxx.xxx.xxx.xxx") OR (src_ip=...) OR ...` the field name is included by default; can omit it with '$' `| return 5 $src_ip`
  - can also alias the field name: `| return 5 ip=src_ip`
- i.e. `index=sales sourcetype=vendor_sales [ search index=sales sourcetype=vendor_sales VendorID<=2000 product_name=Mediocre* | top limit=5 Vendor | return 5 Vendor ] | rare limit=1 product_name by Vendor | fields - percent, count`

**Subsearch: When to use**
- Limitations of subsearches
  - default time limit of 60 seconds to execute
    - right after time limit, result is finalized and returned to outer search
  - default returns up to 10000 results
    - once met the cap, result is finalized and returned to outer search
  - limits can be adjusted by Admin
- Subsearch should operate on a small set of data and produces a small set of results
- If the outter search is executed **real-time**, its subsearch is executed for **all-time**
  - can override the all-time with _earliest_ and _latest_ time modifiers in subsearch
- only use subsearch when there is no other way to achieve the same results
  - i.e. Compare the follow two queries, which returns the same results but the second one is significantly faster:
  - `index=security sourcetype=winauthentication_security (EventCode=4624 OR EventCode=540) NOT [search sourcetype=history_access | rename Username as User | fields User] | stats count by User | table User`
  - `index=security (sourcetype=winauthentication_security (EventCode=4624 OR EventCode=540)) OR (sourcetype=history_access) | eval badge_access=if(sourcetype="history_access", 1, 0) | stats max(badge_access) as badged_in by Username | where badged_in=0 | fields Username`

**Troubleshooting Subsearches** when subsearch doesn't return results we wanted
- run both outter and subsearches independently to verify the query's correctness
- can view results of the subsearch in Job Inspector -> Search job properties -> normalizedSearch

## Module 5 Combining Searches

The **Append Command**
- adds results by appending subsearch results to the end of the main search results as additional rows
- i.e. `index=security sourcetype=linux_secure "failed password" NOT "invalid user" | timechart count as known_users | append [search index=security sourcetype=linux_secure "failed password" "invalid user" | timechart count as unknown_users]`
  - this search allows to draw data on the same visualization by combining results from two searches
  - the final result data may look awkward as rows are not combined based on the save values in some of the fields
    - this can be fixed with the _first_ function, which returns the first value of a field by the order it appears in the results
    - i.e. add this at the end of last example `| timechart first(known_users), first(unknown_users)` or better: `| timechart first(*) as *`, which tells Splunk to apply _first_ on all fields and keep their original name
- This command does not auto-merge row values from two sets of results
- Note, _append_ does not work with real-time searches

The **Appendcols Command**
- adds results of a subsearch to the right of the main search's results
- i.e. the same example as used in Append Command:
  - `index=security sourcetype=linux_secure "failed password" NOT "invalid user" | timechart count as known_users | appendcols [search index=security sourcetype=linux_secure "failed password" "invalid user" | timechart count as unknown_users]`
  - with this way, _first_ function is no-longer needed, and we will see that "unknown_users" is added as a new column to the outter search's results and matches the _time field
  - this is a special example, as the timechart generates the field "time" the same way in both searches, so _appendcols_ can match on
- This command adds results of the subsearch as additional table field columns
  - there are times this command doesn't (and impossible) to auto-match values to proper rows

The **Join Command**
- combines results of subsearch with outter search using common fields
- requirement: both searches must share at least one field in common
- two types of join
  - inner join (default), only return results from outter search that have matches in the subsearch
  - outer/left join, returns all results from the outter search and those have matches in the subsearch; can be specified with argument `type=outer`
- i.e. `index="security" sourcetype=linux_secure src_ip=10.0.0.0/8 "Failed" | join src_ip [search index=security sourcetype=winauthentication_security bcg_ip=* | dedup bcg_workstation | rename bcg_ip as src_ip | fields src_ip, bcg_workstation] | stats count as access_attempts by user, dest, bcg_workstation | where access_attempts > 40 | sort - access_attempts`
  - in this example, the bcg_workstation is info from another sourcetype, we used (inner) join to make it available to the outter search while using the ip address as the common field to match for the join

The **Union Command**
- combines search results from two or more datasets into one
- Datasets can be: saved searches, inputlookups, data models, subsearches
  - corresponding keywords to use with _union_ command: _savedsearch lookup datamodel_
  - subsearches are unnamed
- i.e. `| union datamoel:Buttercup_Games_Online_Sales.successful_purchase, [search index=sales sourcetype=vendor_sales] | table sourcetype, product_name`
- when used with datasets that only use distributed streaming commands, _union_ invokes the **multisearch command**, which is a generating command that runs multiple streaming commands at the same time
- when used with one of the datasets not using streaming command, _union_ invokes the **append command** on the search head
- can tell which command used in Job Inspector -> Search job properties -> optimizedSearch property

**Other Search examples**
- i.e. `index=security sourcetype=linux_secure "fail*" earliest=-31d@d latest=-1d@d | timechart count as daily_total | stats avg(daily_total) as DailyAvg | appendcols [search index=security sourcetype=linux_secure "fail*" earliest=-1d@d latest=@d | stats count as Yesterday] | eval Averages="" | stats list(DailyAvg) as DailyAvg, list(Yesterday) as Yesterday by Averages`
  - this search shows a technique to create empty column to allow display data compared in a bar chart visualization
- i.e. `index=web sourcetype=access_combined status>=400 (host=www1) OR (host=www2) | fields host, status | stats dc(host) as hostCount by status | where hostCount=2 | fields - hostCount`
  - this search shows a technique to know whether two searches diff on a field have data shared in both

## Module 6 Manipulating Data

The **Bin Command** adjusts and groups numerical values into discrete sets (bins) so that all the items in a bin share the same value
- i.e. `index=sales sourcetype=vendor_sales | stats sum(price) as totalSales by product_name | bin totalSales | stats list(product_name) by totalSales | eval totalSales = "$".totalSales`
- can take a _span_ argument to specify the bin size to group on
- overrides the field values passed-in; does not create new field, need to create backup field using _eval_ if desired

The **xyseries Command** converts statistical results into a tabular format suitable for visualizations
- useful for additional process after initial statistical data is returned
  - solves the issue that after _chart_ and _timechart_, cannot use _eval_ to do additional data tweaking
- Syntax `| xyseries x-axis-field, y-axis-field, y-axis-data`
- i.e. `index=web sourcetype=access_combined | bin _time span=1h | stats sum(bytes) as totalBytes by _time, host | eval MB = round(totalBytes/(1024*1024),2) | xyseries _time, host, MB`
- to better elaborate, see below conversions

_time|host|MB
-----|----|--
12:00|www1|12
12:00|www2|11
12:00|www3|7
...|...|...

_time|www1|www2|www3
-----|----|----|----
12:00|12|11|7
...|...|...|...

The **untable Command** does the opposite of _xyseries_, takes chartable, tabular data and format it similar to stats output
- good when need to further extract from, and manipulate values after using commands that result in tabular data
- Syntax `| untable x-axis-field, label-field, data-field`
    - label-field: field with the values to use for label names for the data series
    - data-field: field that holds the data charted series
  - i.e. `index=sales sourcetype=vendor_sales (VendorID >= 9000) | chart count as prod_count by product_name, VendorCountry limit=5 useother=f | untable product_name, VendorCountry, prod_count | eventstats max(prod_count) as max by VendorCountry | where prod_count=max | stats list(product_name), list(prod_count) by VendorCountry`
    - data is formatted in a tabular format after _chart_ command

The **foreach Command** allows to run templated subsearches for each field in a specified list
- i.e. `index=web status>=500 | chart count by action, host | eval total=0 | foreach www1, www2, www3 [eval total=total+<<FIELD>>]`
  - "www1, www2, www3" are the columns after executing _chart_ command. Here _FIELD_ refers to the value of the column_name, and each row goes through the foreach command and subsearch calculation
- i.e. Back to the example for _xyseries_, we can also use _foreach_ to achieve the same result:
  - `index=web sourcetype=access_combined | timechart span=1h sum(bytes) as totalBytes by host | foreach www* [eval <<FIELD>> = round(<<FIELD>>/(1024*1024),2)]`
- i.e. Great example:
  - `index=web sourcetype=access_combined action=purchase | chart sum(price) by product_name, clientip limit=0 | addtotals | sort 5 Total | fields - Total | untable product_name, clientip, count | xyseries clientip, product_name, count | addtotals | sort 3 -Total | fields - Total`

## Module 7 Multivalue Fields

Splunk provides functions specifically work with multi-valued fields _mvsort mvfilter mvjoin mvzip mvcount split mvindex makemv mvexpand_
- _mvsort_ takes a given multi-valued field and sorts its values in lexicographical order (usually UTF encoding)
  - i.e. `index=systems sourcetype=system_info | rename ROOT_USERS{} as root_users, SYSTEM{} as system | dedup system root_users | eval root_users=mvsort(root_users) | stats list(root_users) by system`
    - the origin field names may look funky and must be renamed, as they are extracted from json data
  - note that _values_ function also auto-sort the values of a given field
- _mvfilter_ is useful to narrow results based off values in a multi-valued field
  - accepts a boolean expression and references only one field at a time
  - i.e. `index=systems sourcetype=system_info | rename SYSTEM{} as system | eval uk_system=mvfilter(match(system, "UK$")) | dedup uk_system | sort uk_system | table uk_system`
    - tells to filter out values of field 'system' that does not end with 'UK'
  - _mvfilter_ will return null values if they exist in the data
    - can make it not return null values by
    - `mvfilter(!=isnull(x))` or `mvfilter(isnotnull(x))`
- _mvjoin_ allows joining all values of a multi-valued field with a delimiter, resulting a single-value field
  - accepts a field name and a string delimiter
  - i.e. `index=systems sourcetype=system_info | rename ROOT_USERS{} as root_users, SYSTEM{} as system | eval root_users=mvsort(root_users), root_users=mvjoin(users, ",") | dedup system root_users | eval root_users=mvsort(root_users) | stats list(root_users) by system`
- _mvzip_ combines two sets of multi-valued fields, pairing their values together in a new set, with an optional delimiter to separate values
  - i.e. `index=systems sourcetype=system_info | rename CPU_CORES{}.core as core, CPU_CORES{}.core_percent_used as percent_used | eval core_percent_used=mvzip(core, percent_used, " : ") | table _time, SYSTEM{}, core_percent_used`
- _mvcount_ counts the number of results for a multi-valued field for each event
  - i.e. `index=systems sourcetype=system_info | rename CPU_CORES{}.core as core | eval number_of_cores=mvcount(cores) | where number_of_cores>4 | stats values(SYSTEM{}) as "server with +4 cores"`
- _split_ can create new multi-value field out of single key-value pairs
  - accepts a field name and a string delimiter
  - i.e. `index=sales sourcetype=vendor_sales | eval account_codes=split(productId, "-") | dedup product_name | table product_name, productId`
- _mvindex_ assigns an array index to a multi-value field
  - allows reference a value by its location in the array
  - index begins from 0
  - i.e. `index=sales sourcetype=vendor_sales | eval account_codes=split(productId, "-"), type_code=mvindex(account_codes, 2), product_type=case(like(type_code, "G%"), "Game", like(type_code, "A%"), "Accessory", like(type_code, "T%", "T-shirt")) | stats count by product_type`
- all above functions can be used with _eval where fieldformat_ commands
- The **Makemv Command** can also create multi-value fields
  - distributable streaming command
  - replaces existing single-value field
  - splits values using delimiter or regex
  - i.e. `index=sales sourcetype=vendor_sales | eval account_codes=productId | makemv delim="-", account_codes | dedup product_name | table product_name, productId, account_codes`
    - same results can be achieved by using regex and specifying capture groups:
    - `| makemv tokenizer="([A-Z]{2}|[A-Z]{1}[0-9]{2})", account_codes`
- The **Mvexpand Command** creates separate event for each value contained in a multi-value field
  - distributable streaming command
  - creates new events count as of the number of values in a multi-valued field and copy all other values to the new evnets
  - it does not create new events in the index, only created in memory for the period of the search
  - best to remove unused fields before the _mvexpand_ command
  - i.e. `index=systems sourcetype=system_info | mvexpand ROOT_USERS{} | dedup SYSTEM{} ROOT_USERS{} | stats list(SYSTEM{}) by ROOT_USERS{}`

## Module 8 Advanced Transactions

The **Transaction Command** is used to group events spanning a period of time
- it helps build a story of the journey for a certain action started and ended at some point
- for _transaction_ command to work properly, events should be in a reversed chronological order
- it is a resource-intensive command and should be avoided when you can
- i.e. `index=web sourcetype=access_combined | transaction JESSIONID endswith=(status=503) maxevents=5 | hightlight 503`
- The _coalesce_ function accepts any number of fields and returns the first value that is not null.
  - this helps to normalize data returned from different datasets so we can do _transaction_ on the data
  - i.e. `((index=network sourcetype=cisco_wsa_squid) OR (index=web sourcetype=access_combined (action=addtocart OR action=purchase))) | eval uniqueIp = coalesce(c_ip, clientip), Action=if(sourcetype="access_combined", action, null()), Employee=if(sourcetype="cisco_wsa_squid", username, null()) | transaction uniqueIp startswith=(Actions=addtocart) endswith=(Actions=purchase) | stats list(Actions) as Actions by Employee, uniqueIp | dedup Employee | fields Employee`
- The _case_ function is an alternative to _coalesce_ to normalize fields for _transaction_
  - i.e. changes on the above example replacing _coalesce_
  - `| eval uniqueIp=case(sourcetype="access_combined", clientip, sourcetype="cisco_wsa_squid", c_ip)`

**Complete & Incomplete Transactions**
- useful when using both _startswith endswith_ arguments and to keep those transactions where the _endswith_ condition was not met
  - add _keepevicted=true_ argument will keep incomplete transactions in results
  - an internal & hidden field 'closed_txn' will be added to indicate whether this transaction is completed
- i.e. `index=web sourcetype=access_combined | transaction JESSIONID startswith=(action="addtocart") endswith=(action="purchase") keepevicted=true | search action="addtocart" | stats count(eval(closed_txn=0)) as "Total Failed", count(eval(closed_txn=1)) as "Total Successful", count as "Total Attempted" | eval "Percent Completed" = round(((('Total Attempted'-'Total Failed')*100)/'Total Attempted'),0) | table "Total Attempted", "Total Successful", "Percent Completed"`

**Transaction vs Stats**
- to build a transaction, all events data is searched and required, and is done on the Search Head
- when using stats, we can limit the data operated on to only the set of data needed; can use _fields_ which is a distributable streaming command filter data on the indexers
- i.e. compare the following two searches
  - uses transaction: `index=web sourcetype=access_combined | transaction JESSIONID | where duration > 0 | stats avg(duration) as avgd, avg(eventcount) as avgc | eval "Average Time On Site"=round(avgd, 0), "Average Page Views"=round(avgc, 0) | table "Average Time On Site", "Average Page Views"`
  - uses stats: `index=web sourcetype=access_combined | fields JESSIONID | stats range(_time) as duration, count as eventcount by JESSIONID | where duration > 0 | stats avg(duration) as avgd, avg(eventcount) as avgc | eval "Average Time On Site"=round(avgd, 0), "Average Page Views"=round(avgc, 0) | table "Average Time On Site", "Average Page Views"`

## Module 9 Working With Time

**Review about time**
- _earliest_ and _latest_ overrides the time selectors for the search
  - timeunits: https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/SearchTimeModifiers
  - snap to beginning of timeunit using '@' followed by a timeunit

**Advanced Searches that uses time**
> Use case: when there is a need to aggregate information for two different time ranges and be able to compare them on the same visualization

- i.e. `index=security sourcetype=linux_secure "failed password" earliest=-24h@h latest=@h | timechart count span=1h | eventstats avg(count) as HourlyAverage`
  - tells to search over last 24 hours with each hour snap to beginning of hour
  - **shortcoming**: the average value is only done over the entire time range of the search, it won't be useful when we want to display an average value of a longer period of time to compare with
- i.e. `index=security sourcetype=linux_secure "failed password" earliest=-30d@d latest=@d | timechart count span=1h as HourlyCount | eval Hour=strftime(_time, "%H") | stats avg(HourlyCount) as AvgPerHour`
  - this search gives the average count per hour for the last 30 days
- i.e. to solve the **shortcoming** above
  - `index=security sourcetype=linux_secure "failed password" earliest=-30d@d latest=@h | eval Previous24h=relative_time(now(), "-24h@h"), Series=if(_time>=Previous24h, "last24h", "prior") | timechart count span=1h by Series | eval Hour=strftime(_time, "%H"), currentHour=strftime(now(), "%H"), offset=case(Hour<=currentHour, Hour-currentHour, Hour>currentHour, (Hour-24)-currentHour) | stats avg(prior) as "30 Day Average for Hour", sum(last24) as "Last 24 Hours" by Hour, offset | sort offset | fields - offset | rename Hour as "Hour of Day"`
  - Here it used _relative_time_ to set a timestamp to compare to, and marked events with a 'Series' field from the time comparision; this is used to filter events in _stats_ command
  - Then it marked each event with an 'Hour' field optained from the '_time' field, which is used to compare to current hour to precisely calculate average for the hours arranged related to current hour
  - the 'offset' field is used to sort the results in the correct chronological order

> Use case: when there is a need to display some aggregated information for a certain time window on many days

- i.e. `index=web sourcetype=access_combined action=purchase earliest=-3d@d latest=@d date_hour>=0 AND date_hour<6 | bin span=1h _time | stats sum(price) as hourlySales by _time | eval Hour=strftime(_time, "%b %d, %I %p"), "Hourly Sales"="$".tostring(hourlySales) | table Hour, "Hourly Sales"`
  - note that the generated fields like `date_hour date_minute ...` are not converted to the user's local timezone
    - extracted at search time
  - to do this, need to create the 'hour' field ourselves using `| eval localhour=tonumber(strftime(_time, "%H"))` then search the events by the 'localhour' `| search localhour>=0 AND localhour<6`

date format code can be found here https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior
Splunk Documentation https://docs.splunk.com/Documentation
