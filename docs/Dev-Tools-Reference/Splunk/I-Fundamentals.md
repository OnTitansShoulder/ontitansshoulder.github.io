---
layout: note_page
title: Begin to Advanced Splunk I
title_short: splunk_notes_advanced
dateStr: 2020-01-01
category: Tool
tags: notes reference check
---

## Splunk 7 Fundamentals (IOD)

This course teaches you how to search and navigate in Splunk, use _fields_, get _statistics_ from your data, create _reports_, _dashboards_, _lookups_, and _alerts_. Scenario-based examples and hands-on challenges will enable you to create robust searches, reports, and charts. It will also introduce you to Splunk's _datasets features_ and _Pivot interface_.

<br/>

### Module 1 Machine data

**Machine data** can be any sort generated by a corporate system:

- Computers
- Network devices
- Virtual machines
- Internet devices
- Commnuication devices
- Sensors
- Databases
- Logs
- Configurations
- Messages
- Call detail records
- Clickstream
- Alerts
- Metrics
- Scripts
- Changes
- Tickets

https://docs.splunk.com/Documentation/Splunk

<br/>

### Module 2 What is Splunk

Index Data -> Search & Investigate -> Add Knowledge -> Monitor & Alert -> Report & Analyze

**Splunk Modules**:

- Indexer - Process incoming data and string results in indexes as _events_
    - Create files organized in sets of directories by age
    - When searching, Splunk only open the dirs that match the time frame of the search
- Search Heads - all users to search the data using Splunk Search Language
    - Takes the search and distribute the request to indexers, and aggregate the results back
    - Include tools like dashboards, alerts, reports
- Forwarders - Splunk Enterprise component that consumes data and forward it to the indexers for processing
    - Requires minimal resources, has little impact on performance
    - Usually resides on the machines where the data origins
    - Primary way data is supplied for indexing

Splunk can **scale** to be a single instance or a full-distributed infrastructure.

- For a large scale of usage, it is better to split the Splunk installation into multiple specialized instances, needs Splunk Enterprise.
- Can have multiple Search Heads, multiple Indexers, and many Forwarders.
- Search Heads and Indexers can also be clusters to make it always available.

https://splunkbase.splunk.com/

http://dev.splunk.com/

<br/>

### Module 3 Installing Splunk

Install in the /opt dir

Try not install Splunk as root user

Three types of users:

- Admin - install apps, create knowledge objects for all users
- Power user - create and share knowledge objects for users of an app and do realtime searches
- User - only see their own knowledge objects and those shared with them

Splunk Enterprise comes with the Home App and Search & Reporting App

<br/>

### Module 4 Getting Data In

Ways to get data in:

- Through the Splunk web UI (as an Admin)
    - Upload a file, indexed once; good for data that never gets updated
    - There are pre-defined file source type which Splunk uses to index the file
- Monitor some files
    - Event Logs
    - File System Changes
    - Active Directory
    - Network Info
- Forwarder
    - Receive data from external Forwarders
    - Main source of data input
    - https://docs.splunk.com/Documentation/Splunk/latest/Data/Usingforwardingagents

Having different and specialized indexes helps make Splunk searches more efficient.

<br/>

### Module 5 Basic Searching

The Search & Report app allows you to search and analyze data through creating knowledge objects, reports, dashboards and more.

The `Events` tab will display the events returned for your search and the fields extracted from the events.

The `patterns` tab allows you to see patterns in your data.

If your search generates statistics or visuals, they will appear in `Statistics` and `Visualization` tabs. Commands that create statistics and visualizations are called `transforming commands`, they transform data into data tables.

By default a search is active & valid for 10 minutes, otherwise needs rerun.

A shared search job is active for 7 days and its first-ran result visible by anyone you shared with.

Order of evaluation for boolean conditions: `NOT OR AND`

https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/CreateworkflowactionsinSplunkWeb

https://www.splunk.com/view/SP-CAAAPYB

<br/>

### Module 6 Using Fields

Default fields selected are `hosts`, `source`, and `sourcetype`.

Interesting fields are those appear within at least 20% of the events

`a` denotes a String value, `#` denotes a numeral value

Searching fields are `key=value` string in the search query. the key is case-sensitive but the value is not.

`= !=` can be used on numeral and String values

`> >= < <=` can be used on numeral values only

There is a small difference between doing `key!=value` and `NOT key=value`. The latter will include entries even when the key doesn't exitst on those entries.

Instead of doing `(key=value1 OR key=value2 OR key=value3)`, can use alternatively `key IN ("value1", "value2", "value3")`

Use `Search & Reporting` -> `> Search History` to quickly view recent searches and reuse the searches if possible.

Use `Activity` -> `Jobs` to view recent search jobs (with data). Then further use `Job -> inspect job` to view the query performance.

<br/>

### Module 7 Best Practices

**Search**

Using time to narrow down the search is the most effective way to improve query performance.

Then it comes to the default fields of index, source, host, and sourcetype are most powerful. They are extracted at index-time so won't be extracted for each search. So better to put these filters early in the front of the query.

The more you tell the search engine, the more likely you will get good results.

Inclusion is generally better than exclusion. Searching for "something" is better than searching for "NOT something"

**Time**

Can include time within the search query string using `earliest=-2h latest=-1h` or absolute time `earliest=01/08/2018:12:00:00`. Use '@' to denote round down time `earliest=-2@h`. i.e. `earliest=10/19/2018:00:00:00 latest=10/27/2018:00:00:00`

**Indexes**

It makes searches efficient, and also allows to limit who can access the data through roles.

<br/>

### Module 8 SPL Fundamentals

The **SPL Components**:

- Search Terms - defines what data to return
- Commands - what to do with search results, like creating charts, computing statistics, and formatting results.
- Functions - defines how we want to chart, compute and evaluate the results
- Arguments - variables we want to apply to the function
- Clauses - how we want the results grouped

As you pipe through the commands, more unrelevant data remove from before the last pipe.

```
                           -------------------Pipes--------------------
sourcetype=acc* status=200 |  stats   list(    product_name   ) as "Games Sold"
-------Search Terms-------- Command-Function-----Argument-----Clause------------
```

Search UI **color code** parts of the query to help construct the query.

- Boolean operators: Orange
- Commands: Blue
- Command-Arguments: Green
- Functions: Purpule
- Function-Arguments: Black
- Clauses: Orange

The **Fields Command**, include or exclude fields from search results to limit the fields to display and also make search run faster. Internal fields like raw and time will always be extracted, but can also be removed from the search results.

```sh
index=web sourcetype=access_combined
| fields status clientip # only include fields status and clientip
index=web sourcetype=access_combined
| fields - status clientip # to exclude fields status and clientip
```

Field extraction is one of the most costly parts of searching in Splunk. Eliminate unnecessary fields will improve search speed since field includsion occurs before field extraction, while field exclusion happens after field extraction.

The **Table Command**, also specify fiedls kept in the results and retains the data in a tabulated format. The fields will be displayed in table header and each row in table has the corresponding values.

```sh
index=web sourcetype=access_combined
| table status clientip
```

The **Rename Command**, to rename a field

```sh
index=web sourcetype=access* status=200 product_name=*
| table JESSIONID, product_name, price
| rename JESSIONID as "User Session Id",
         product_name as "Purchased Game",
         price as "Purchase Price"
```

After the rename, subsequent commands must use the new names otherwise operation won't have any effects.

The **Dedup Command**, remove duplicated events from results that share common values.

```sh
index=security sourcetype=history* Address_Description="San Francisco"
| dedup Username # can be a single field or multiple fields
| table Username First_Name Last_Name
```

The **Sort Command**, to organize the results sorted by some fields

```sh
sourcetype=vendor_sales
| table Vendor product_name sale_price
| sort - sale_price Vendor
  # the '-' here affects all fields
| sort -sale_price Vendor
  # sort-decending will only affect sale_price while Vendor is sorted ascending
| sort -sale_price Vendor limit=20
  # will also limit the results for the first twenty in sorted order.
```

<br/>

### Module 9 Transforming Commands

The **Top Command**, finds the most common values of given field(s) in a result set. Automatically returns count and percent columns, top 10 by default and can be set with `limit=n`. `limit=0` yields all results

```sh
index=sales sourcetype=vendor_sales
| top Vendor product_name limit=5
```

Top Command Clauses: limit=int countfield=string percentfield=string showcount=True/False showperc=True/False showother=True/False otherstr=string

```sh
# top command also supports results grouping by fields
index=sales sourcetype=vendor_sales
| top product_name by Vendor limit=3 countfield="Number of Sales" showperc=False
```

The **Rare Command**, shows the least common values of a field set. It is the opposite of _Top_ Command and accepts the same set of clauses.

```sh
index=sales sourcetype=vendor_sales
| rare Vendor limit=5 countfield="Number of Sales" showperc=False useother=True
```

The **Stats Command**, produces statistics of our search results. Need to use functions to produce stats

Some Common Stats Functions:

- _count_, returns the number of events matching search criteria
    - `index=sales sourcetype=vendor_sales | stats count as "Total Sells By Vendors" by product_name, categoryid`
    - shows vendor sells total by game title
    - can add any number of fields to split the count by
    - `| stats count(field)` get a count of the number of events where the field is present
    - `index=web sourcetype=access_combined | stats count(action) as "Action Events", count as "Total Events"`
- _distinct_count_ or _dc_, returns a count of unique values for a field. The same clauses that works with _count_ will work with _distinct_count_.
- _sum_, returns the sum of numerical values
    - `index=sales sourcetype=vendor_sales | stats count as "Unit Sold", sum(price) as "Gross Sales" by product_name`
    - here the 'by' clause works on both _count_ and _sum_
    - it has to be the same pipe; after this command, the information needed will no longer be available:
    - `index=sales sourcetype=vendor_sales | stats count as "Unit Sold" | stats sum(price) as "Gross Sales" by product_name` will yield no results
- _average_ or _avg_, _min_, _max_, returns an average/min/max of numerical values for a field
    - Any field values that are missing or formatted incorrectly will not be added into calculation
- _list_, returns all values of a field
    - `index=bcgassets sourcetype=asset_list | stats list(Asset) as "company assets" by Employee`
- _values_, returns unique values of a field

<br/>

### Module 10 Reports and Dashboards

**Reports** allow people to easily store and share search results and queries used to make the search.

- It is best to have a good naming convention when creating reports.
- When a report is run, a fresh search is run.
- Control the access to see the results of this report.
- Can save report results to speed up search. See Accelerate Reports Doc https://docs.splunk.com/Documentation/Splunk/latest/Report/Acceleratereports

**Visualizations** Any searches that returns statistics information can be viewed as charts

- Splunk provides many type of charts to best reflect the information
- charts can be based on numbers, time, and location
- Can save visualizations as a report or a dashboard panel

**Dashboards** is a collection of reports.

- Use `$variable_name$` to refer to another variable that can be controlled by a dropdown or the drilldown
- When editing the source, add `depends="$variable_name$"` to an element tag to listen on changes to that variable.
- For a time picker variable, the earliest and latest time can be accessed through `$variable_name$.earliest` and `$variable_name$.latest`
- To use a subsearch to limit the main search's time range, let the subsearch output a table with a single row and columns `earliest` and `latest`

<br/>

### Module 11 Pivot and Datasets

Pivot and Datasets allows users to get knowledge from the data without learning Splunk in deep.

Data Models - knowledge objects that provide the data structure that drives Pivots

- Created by Admins and Power Users, with a solid understanding of the data.
- Data Model is like the framework, and Pivot is the simple interface to the data.
- Each Data Model is made of Datasets

Datasets - small subsets of data defined for specific purposes

- Defined like tables, with fields names as columns and fields values as cells

The rest can be done easily on the UI for Pivots

<br/>

### Module 12 Lookups

Lookups allow you to add values to your events not included in the indexed data.

- A lookup is categorized as a dataset
- Combine fields from sources external to the index with searched events based on paired fields present in the events.
- includes csv files, scripts, or geospatial data
- i.e. data that might be useful for search, but not available in the index

Two steps to set it up, in Splunk UI -> Settings -> Lookups

Define a lookup table

Define the lookup

Optionally configure the lookup to run automatically

Use `| inputlookup <lookup_definition_name>` to verify lookup is setup correctly

Lookup field values are case-sensitive by default.

The **Lookup Command**

```sh
index=web sourcetype=access_combined NOT status=200
| lookup http_status code as status
# http_status is the name of the lookup definition
# code is one of the columns in the csv, won't show in the results
# defualt all fields in lookup table are returned except the input fields
# can choose what is shown by specifying them
index=web sourcetype=access_combined NOT status=200
| lookup http_status code as status,
         OUTPUT code as "HTTP Code",
         description as "HTTP Description"
```

Additional Lookup Options

- Populate lookup table with search results
- Define lookup based on external script or command
- Use Splunk DB Connect application
- Use geospatial lookups to create queries that can be used to generate choropleth map visualizations
- Populate events with KV Store fields
- https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Configureexternallookups
- https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/ConfigureKVstorelookups

<br/>

### Module 13 Scheduled Reports and Alerts

Scheduled Reports can do weekly/monthly reports and automatically send results via emails.

Select a saved report and add a schedule on it. Only admin can set its priority.

Alerts are based on searches that run on scheduled intervals or in real time. It is triggered when the results of a search meet defined conditions. It can:

- list in interface, alerts can be viewed in Activity -> Triggered Alerts
- log events, create a log file with the events
- output to lookup, to apend or replace data in a lookup table
- send to a telemetry endpoint, call an endpoint
- trigger scripts, triggers a bash script stored on your instance
- send emails
- use a webhook
- run a custom alert, build a custom alert action
