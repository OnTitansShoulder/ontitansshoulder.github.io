{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This section is a collection of my notes taken while reading and learning various of things. In such age of information boom, we are taking in lots of information everyday. And being a software engineer make it worse since you also have to keep learning loads of new stuff everyday in order to do a better job for the work assigned to you. Sometimes you learn something new but the learning curve forces you to drop it when you not use it regularly. I just recently started organizing my notes because I want to have a place to serve as a handy reference to look for pieces of information that I need to recall. By putting the notes all together with the ability to search through them, it will save some time for re-learning the stuff I certainly have visited before. I also enjoy taking notes as it let me grasp and memorize new things faster. So I am posting my notes here and plan to continue supplying new notes once a while. Hope they can be somewhat a useful reference to you as well .","title":"Welcome"},{"location":"Good-Reads/","text":"Tech Blogs \u00b6 highscalability.com netflixtechblog.com blog.google azure architecture aws.amazon.com/blogs news.ycombinator.com insights.sei.cmu.edu particular.net/blog konghq.com/blog lecloud.net Inspiring Private Blogs \u00b6 luanjunyi.medium.com allthingsdistributed.com Dive Deep \u00b6 A re-introduction to JavaScript systems-performance-2nd-edition The Twelve Factors App Sharding explained in deep Numbers Everyone Should Know Books \u00b6 Designing Data-Intensive Applications by Martin Kleppmann Enterprise Integration Patterns by Gregor Hohpe Learnings from Incidents \u00b6 Facebook outage 2021-10 Roblox outage 2021-10-28","title":"Good Reads"},{"location":"Good-Reads/#tech-blogs","text":"highscalability.com netflixtechblog.com blog.google azure architecture aws.amazon.com/blogs news.ycombinator.com insights.sei.cmu.edu particular.net/blog konghq.com/blog lecloud.net","title":"Tech Blogs"},{"location":"Good-Reads/#inspiring-private-blogs","text":"luanjunyi.medium.com allthingsdistributed.com","title":"Inspiring Private Blogs"},{"location":"Good-Reads/#dive-deep","text":"A re-introduction to JavaScript systems-performance-2nd-edition The Twelve Factors App Sharding explained in deep Numbers Everyone Should Know","title":"Dive Deep"},{"location":"Good-Reads/#books","text":"Designing Data-Intensive Applications by Martin Kleppmann Enterprise Integration Patterns by Gregor Hohpe","title":"Books"},{"location":"Good-Reads/#learnings-from-incidents","text":"Facebook outage 2021-10 Roblox outage 2021-10-28","title":"Learnings from Incidents"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/","text":"This notes is taken from Microsoft Azure Architecture Center . These principles still apply to any cloud applications not just limited to Azure. Architecture styles \u00b6 An architecture style is a family of architectures that share certain characteristics. An architecture style places constraints on the design, including the set of elements that can appear and the allowed relationships between those elements. Before choosing an architecture style, make sure that you understand the underlying principles and constraints of that style. Otherwise, you can end up with a design that conforms to the style at a superficial level, but does not achieve the full potential of that style. It's also important to be pragmatic. Sometimes it's better to relax a constraint, rather than insist on architectural purity. Weigh the tradeoffs \u00b6 Constraints also create challenges, so it's important to understand the trade-offs when adopting any of these styles. Some main challenges to consider: Complexity - is the style too simplistic or too complex for your domain? Is it managable? Asynchronous messaging and eventual consistency - is eventual consistency guaranteed? Any possiblity for lost message or duplicated messages? Inter-service communication - how much overall latency will be increased from the separated components? Will there be network ingestion? Manageability - how hard to monitor, deploy, extend the app? Common Styles \u00b6 N-tier \u00b6 N-tier - a traditional architecture for enterprise apps, where dependencies are managed by dividing the application into layers that perform logical functions , such as presentation, business logic, and data access. Layers is a way to separate responsibilities and manage dependencies . A layer can only call into layers that sit below it, which makes it hard to introduce changes in one part of the application without touching the rest of the application. Tiers are physically separated, running on separate machines. A tier can call to another tier directly, or use asynchronous messaging (message queue). Several layers might be hosted on the same tier. Physically separating the tiers improves scalability and resiliency , but also adds latency . N-tier is most often seen in infrastructure as a service (IaaS) solutions. An N-tier application can have a closed layer architecture (a layer can only call the next layer immediately down) or an open layer architecture (a layer can call any of the layers below it). Benefits: portabilityp simplicity in design Challenges: largely monolithic design prevents independent deployment of features Best practices: avoid a middle layer that only does CRUD operations to data tier which adds latency use autoscaling to handle load changes use asynchronous messaging to decouple tiers cache semistatic data make database tier highly available place Web Application Firewall (WAF) between frontend and the Internet to block abuse place each tier in its subnet and use subnets as a security boundary restrict data tier to only middle tier Web-Queue-Worker \u00b6 A Web-Queue-Worker app has a web frontend that handles HTTP requests and a backend worker that performs CPU-intensive tasks or long-running operations. The front end communicates to the worker through an asynchronous message queue . Other components that can be commonly incorporated: database cache CDN remote service identity provider for authentication The web frontend and worker should be stateless . Session state can be stored in a distributed cache. Any long-running or resource-intensive work is done by the worker asynchronously through active poll or periodical batch-processing, otherwise the worker is optional . The frontend and the worker can easily become large, monolithic components, and involves complex dependencies, which makes it hard to maintain and update. Benefits: simplicity in design, deploy, and manage separation of concerns good scalability for the decoupled componets Challenges: components can still get large and monolithic, making it hard to maintain and update hidden dependencies in terms of data schemas or modules for communication Best practices: expose a well-designed API to client use autoscaling to handle load changes cache semistatic data use a CDN to host static content partition data to improve scalability, reduce contention, and optimize performance Microservices \u00b6 A microservice app is composed of many small, independent services. Each service implements a single business capability. Services are loosely coupled , communicating through API contracts . Each service can be built by a small, focused development team. Individual services can be deployed without a lot of coordination between teams, which encourages frequent updates . Services are responsible for persisting their own data or external state. However, this architecture is more complex to build and manage and requires a mature development and DevOps culture. Done right, this style can lead to higher release velocity, faster innovation, and a more resilient architecture . Management/orchestration component is responsible for placing services on nodes, identifying failures, and rebalancing services across nodes. For example, Kubernetes, and certain public cloud services. API Gateway - the entry point for clients which forwards the call to the appropriate services on the back end. Benefits: Agile development Small, focused teams for each service Small code base on each service Mix of technologies, some services can be built with different stacks Fault isolation, some services failure will not bring down the whole service Better scalability, independent scaling Data isolation, each service owns its data. But common schema changes are tricky. Challenges: Complexity, more moving parts and need to make sure they work in harmony Development and testing, hard to test service dependencies Lack of goverance, may end up with things built with different stacks and styles and making it harder to maintain and see the entire picture Network ingestion and latency, more components lead to more inter-service communication. Reduce unnecessary calls, use more efficient serialization formats, and switch to asynchronous communication patterns helps Data integrity, data consistency can become a problem as each service keeps its own state Management, logging and tracing, DevOps Versioning, potential backward or forward compatibility Skill set for building highly distributed systems Best practices: Decentralize as much as possible API should be designed around the business domain offload auth and SSL termination to the Gateway The gateway should handle and route client requests without any knowledge of the business rules or domain logic Event-driven \u00b6 An application of event-driven architecture use a publish-subscribe (pub-sub) model, where producers publish events, and consumers subscribe to them. The producers are independent from the consumers, and consumers are independent from each other. Events are delivered in near real time, so consumers can respond immediately to events as they occur, and every consumer sees all of the events. This architecture is good for apps that ingest and process large volume of data with low latency, or when multiple different subsystems must perform different types of processing on the same event data. Two Event-driven architecture variations: The Pub/Sub model - sends published events to each subscriber and the events cannot be replayed and new subscribers do not see past events. The Event streaming model - writes events to a log and keeps the events strictly ordered and persistent. Consumers can read from any part of the stream and is responsible for advancing its position in the stream. A consumer can join any time and replay past events. Three common consumer variations: Simple event processing, each event immediately triggers an action complex event processing, the consumer processes a series of events to look for a pattern, like aggregation over sliding time windows event stream processing, use a pipeline to ingest events and to process or transform the stream. Good for IoT workloads. Benefits: decoupled design of producers and consumers easy to add or remove consumers consumers respond to events immediately highly scalable and distributed consumers can have independent view of the event stream Challenges: Guaranteed delivery is essential Process events in order or exactly once can be hard to guarantee Best practices: keep events as small as possible services should really only share IDs and maybe a timestamp to indicate when the information was effective think about which service should own which piece of data, and avoid sharing those pieces in a single giant event. Another great read about Event-driven design https://particular.net/blog/putting-your-events-on-a-diet and RPC's disadvantage against messaging or event-driven design https://particular.net/blog/rpc-vs-messaging-which-is-faster Big Data \u00b6 Big data and Big Compute are specialized architecture for workloads that fit certain specific profiles. Big data divides a very large dataset into chunks, performing parallel processing across the entire set, for analysis and reporting. It usually involves: Batch processing of big data stored app data in relational DB, static files, logs source data stored in Data Lake, typically distributed storage, high volumes, various formats, include blobs (Binary Large Object) Map/Reduce are often used repeated processing operations encapsulated in workflows Real-time processing of big data in motion IoT event data events typically are compressed and serialized into binary format a message ingestion store to act as a buffer for messages is needed support scale-out processing, reliable delivery, message queuing Interactive exploration of big data Predictive analytics and machine learning data modeling layer Benefits: Parallelism Elastic scale, scale up both borizontally and vertically on the components as needed Interoperability and extensibility, new workload can integrate into existing solutions Challenges: Complex to build and configure for each system in the picture Skillset is specialized, many frameworks and languages are not used in general application architectures Evolving technology, some are still evolving fast and introduce big changes across releases Security, properly govering data access Best practices: partition the data to reduce ingestion apply schema-on-read semantics, which project a schema onto the data when the data is processing, not when the data is stored. It is less likely to cause slow downs from bottlenecks such as validation and type checking. adjust the parallelism for the best balance in utilization costs and time scrub sensitive data early in the data pipeline Big Compute \u00b6 Big compute , aka high-performance computing (HPC), makes parallel computations across a large number (thousands) of cores , mostly used on simulations, modeling, rendering, or any jobs that requires too much memory to fit on single machine. The work must be able to split into discrete tasks, and each task is finite. The application can be spun up ad-hoc. For tightly coupled tasks, high-speed networking is required for exchanging intermediate results fast. Challenges: Manage VM infrastructure Provision thousands of cores timely Experiment to find optimum number of cores, parallelism, and cost Design Principles \u00b6 Design for self-healing \u00b6 A three-pronged approach: Detect failures Respond to failures gracefully Log and monitor failures to give operational insight Recommendations: Retry failed operations for transient failures Protect failing remote services via circuit breaker for persistent failures to prevent overwhelming retries causing cascading failures Isolate critical resources , partition a system into isolated groups, so that a failure in one partition does not bring down the entire system Perform load leveling , use a queue-based load leveling pattern to queue work items to run asynchronously and prevent sudden spikes take down the entire system Fail over to backup region or cluster. For stateless services, put instances behind a load balancer. For stateful services, use replicas for fail over (and handle eventual consistentcy). Compensate failed transactions , compose an operation from smaller individual transactions. Rewind back if the operation fails midway through. Checkpoint long-running transactions , allow the operation resume from last checkpoint in event of a failure. Degrade gracefully , provide reduced functionality when the primary function is broken. Throttle bad clients , throttle clients making excessive load abusively Block bad actors , define out-of-band process to detect bad clients and block their access for some period of time Leader election on coordinators to avoid single-point-failure Perform fault injection tests , test the resiliency of the system responding possible failure-scenarios either by triggering actual failures or simulate them Chaos engineering by randomly injecting failure conditions into production instances Build redundancy into application \u00b6 Identify the critical paths in your application. Is there redundancy at each point in the path? When a subsystem fails, will the application fail over to something else? Recommendations: Weigh the cost increase, the complexity, and operational procedures introduced from the amount of redundancy built into a system and decided if it justify the business scenario Put multiple VMs behind the load balancer . Replicate databases across regions to have quick fail over. Document fail over procedures or implement the observability layer to do so promptly Partition for availability. If one shard goes down, the other shards can still be reached. Deploy to multiple regions for higher availability. Synchronize frontend, backend, and database failover . Depending on the system, the backup region may require the whole stack to be failover to function properly. Use automatic failover but manual failback . Involve engineers to verify that all application subsystems are healthy before manually failing back. Also check dta consistency before failing back. Redundancy for Traffic Manager , as it is a possible failure point. Minimize coordination \u00b6 Most cloud apps runs multiple instances for scalability and reliability. When several instances try to perform concurrent operations that affect a shared state, there must be coordination across these instances. Coordination through locks creates performance bottlenecks, and as more instances are scaled up, the lock contention increases. A better approach is to partition the work space from number of workers, each worker only affects its assigned portion. Recommendataions: Embrace eventual consistency . The Compensating Transaction pattern can be used to incrementally apply transactions and roll back everything when necessary. Use domain events to synchronize state. Domain event is an event that has significance within the domain. Interested services can listen for the event and act accordingly, rather than relying on coordination. Partition data into shards Design idempotent operations so they can be handled with at-least-once semantics and making it easy to retry upon failures and crashes Use optimistic concurrency when possible. Pessimistic concurrency control uses database locks to prevent conflicts. Optimistic concurrency let each transaction modifies a copy or snapshot of the data, and have database engine validate the committed transaction and rejects others that breaks database consistency. Design to scale \u00b6 Design your application so that it can scale horizontally and quickly, adding or removing new instances as demand requires. Recommendations: Avoid instance stickiness/affinity , or when requests from the same client are always routed to the same server. Traffic from a high-volume user will not be distributed across instances. Identify bottlenecks and resolve them first before scaling up. Stateful parts of the system are the most likely cause of bottlenecks. Offload resource-intensive tasks as offline jobs or asynchronously handled workloads if possible, to easy the laod on the frontend. Graceful scale down , instances being removed should handle the termination gracefully and clean up its states. listen for shutdown events clients should support transient fault handling and retries, in the event its remote origin gets terminated set breaking points for long-running tasks to be stopped and resumed put work items on a queue so another instance can pick up the work Partition around limits \u00b6 In the cloud services have limits in their ability to scale up, such as number of cores, memory available, database size, query throughput, and network throughput. Use partition to work around these limits. partition a database to avoid limits on database size, data I/O, number of concurrent sessions. partition a queue or message bus to avoid limits on number of requests or the number of concurrent connections. partition an App Service web app to avoid limits on the number of instances per App Service plan. A database partition can be: horizontally - aka sharding, each partition holds data for a subset of the total data set. The partitions share the same data schema. vertically - each partition holds a subset of the fields for the items in the data store functionally - data is partitioned according to how it is used by each bounded context in the system, and the schemas are independent. Recommendataions: Besides database, consider also partition storage, cache, queues, and compute instances Design the partition key to avoid hotspots. Refer to consistent hashing, sharding When it is possible to partition at multiple layers, it is easier to partition lower layers first in the hierarchy. Design operations tools \u00b6 Some important operations functions for cloud applications: Deployment Logging and tracing Monitoring Escalation Incident response Security auditing Recommendataions: Make all things observable . Logging captures individual events, app states, errors, exceptions. Tracing records a path through the system and is useful to pinpoint bottlenecks, performance issues, and failure points. Instrument for monitoring , it should be as close to real-time as possible to discover issues that breach SLA Instrument for root cause analysis Use distributed tracing. Traces should include a correlation ID that flows across service boundaries Standardize logs and metrics. Define a common schema that can be derived to add custom fields, but all logs should be capable for tracing and aggregation. Treat configuration as code. Check configuration files into a version control system. Use managed services \u00b6 IaaS is like having a box of parts. You can build anything, but you have to assemble it yourself. PaaS options are easier to configure and administer. You are delegating some operational responsibility to a more focused team that manages it for everyone else. Use suitable data store \u00b6 In any large solution, it's likely that a single data store technology won't fill all your needs. Alternatives to relational databases include key/value stores, document databases, search engine databases, time series databases, column family databases, and graph databases. Embrace polyglot persistence and put different types of data into the storage that best fit its type. For example, put transactional data into SQL, put JSON documents into a document database, put telemetry data into a time series data base, put application logs in Elasticsearch, and put blobs in HBase or other blob storage. Recommendations: Prefer availability over (strong) consistency. Achieve higher availability by adopting an eventual consistency model. Consider the skillset of the development team. To adopt a new data storage technology, the development team must learn and understand appropriate usage patterns, how to optimize queries, tune for performance, and so on to get the most out of it. Use compsensating transactions to undo any completed steps in a failed transaction. Look at bounded contexts . A bounded context is an explicit boundary around a domain model, and defines which parts of the domain the model applies to. A bounded context maps to a subdomain of the business domain. The bounded contexts in your system are a natural place to consider polyglot persistence. Design for evolution \u00b6 All successful applications change over time, whether to fix bugs, add new features, bring in new technologies, replace existing components, or make existing systems more scalable and resilient. Recommendations: Enforce high cohesion and loose coupling. If you find that updating a service requires coordinated updates to other services, it may be a sign that your services are not cohesive. a service is cohesive if it groups functionalities that logically belongs together loosely coupled services allows changing one without affect another Use asynchronous messaging, it is effective in creating loosely coupled systems Expose open interfaces . A service should expose an API with a well-defined API contract, and avoid creating custom translation layers that sit between services. The API should be versioned for maintaining backward compatibility. Public facing services should expose a RESTful API over HTTP. Backend services might use an RPC-style messaging protocol for performance reasons. Design and test against service contracts. When services expose well-defined APIs, you can develop and test against those APIs. Abstract infrastructure away from domain logic. Don't let domain logic get mixed up with infrastructure-related functionality, such as messaging or persistence. Offload shared functionality to a separate service, which allows it getting evloved independently. Deploy services independently. Updates can happen more quickly and safely. Build for the business needs \u00b6 Ultimately, every design decision must be justified by a business requirement. Do you anticipate millions of users, or a few thousand? Is a one-hour application outage acceptable? Do you expect large bursts in traffic or a predictable workload? so on Recommendations: Define business objectives , including the recovery time objective (RTO), recovery point objective (RPO), and maximum tolerable outage (MTO). These numbers should inform decisions about the architecture. Document service level agreements (SLA) and service level objectives (SLO). It is a business decision to set these numbers to define how reliable the system must be built for. Model the application around the business domain . Analyzing the business requirements. Consider using a domain-driven design (DDD) approach to create domain models that reflect the business processes and use cases. Capture both functional and nonfunctional requirements . Functional requirements let you judge whether the application does the right thing . Nonfunctional requirements let you judge whether the application does those things well . Understand your requirements for scalability, availability, and latency Plan for growth . A solution might meet your current needs, in terms of number of users, volume of transactions, data storage, and so forth. A robust application can handle growth without major architectural changes. Manage costs . Traditional on-premises application pays upfront for hardware as a capital expense. In a cloud application, you pay for the resources that you consume so understand the pricing model for the services that you consume. Also consider your operations costs.","title":"Cloud App Architecture"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#architecture-styles","text":"An architecture style is a family of architectures that share certain characteristics. An architecture style places constraints on the design, including the set of elements that can appear and the allowed relationships between those elements. Before choosing an architecture style, make sure that you understand the underlying principles and constraints of that style. Otherwise, you can end up with a design that conforms to the style at a superficial level, but does not achieve the full potential of that style. It's also important to be pragmatic. Sometimes it's better to relax a constraint, rather than insist on architectural purity.","title":"Architecture styles"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#weigh-the-tradeoffs","text":"Constraints also create challenges, so it's important to understand the trade-offs when adopting any of these styles. Some main challenges to consider: Complexity - is the style too simplistic or too complex for your domain? Is it managable? Asynchronous messaging and eventual consistency - is eventual consistency guaranteed? Any possiblity for lost message or duplicated messages? Inter-service communication - how much overall latency will be increased from the separated components? Will there be network ingestion? Manageability - how hard to monitor, deploy, extend the app?","title":"Weigh the tradeoffs"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#common-styles","text":"","title":"Common Styles"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#n-tier","text":"N-tier - a traditional architecture for enterprise apps, where dependencies are managed by dividing the application into layers that perform logical functions , such as presentation, business logic, and data access. Layers is a way to separate responsibilities and manage dependencies . A layer can only call into layers that sit below it, which makes it hard to introduce changes in one part of the application without touching the rest of the application. Tiers are physically separated, running on separate machines. A tier can call to another tier directly, or use asynchronous messaging (message queue). Several layers might be hosted on the same tier. Physically separating the tiers improves scalability and resiliency , but also adds latency . N-tier is most often seen in infrastructure as a service (IaaS) solutions. An N-tier application can have a closed layer architecture (a layer can only call the next layer immediately down) or an open layer architecture (a layer can call any of the layers below it). Benefits: portabilityp simplicity in design Challenges: largely monolithic design prevents independent deployment of features Best practices: avoid a middle layer that only does CRUD operations to data tier which adds latency use autoscaling to handle load changes use asynchronous messaging to decouple tiers cache semistatic data make database tier highly available place Web Application Firewall (WAF) between frontend and the Internet to block abuse place each tier in its subnet and use subnets as a security boundary restrict data tier to only middle tier","title":"N-tier"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#web-queue-worker","text":"A Web-Queue-Worker app has a web frontend that handles HTTP requests and a backend worker that performs CPU-intensive tasks or long-running operations. The front end communicates to the worker through an asynchronous message queue . Other components that can be commonly incorporated: database cache CDN remote service identity provider for authentication The web frontend and worker should be stateless . Session state can be stored in a distributed cache. Any long-running or resource-intensive work is done by the worker asynchronously through active poll or periodical batch-processing, otherwise the worker is optional . The frontend and the worker can easily become large, monolithic components, and involves complex dependencies, which makes it hard to maintain and update. Benefits: simplicity in design, deploy, and manage separation of concerns good scalability for the decoupled componets Challenges: components can still get large and monolithic, making it hard to maintain and update hidden dependencies in terms of data schemas or modules for communication Best practices: expose a well-designed API to client use autoscaling to handle load changes cache semistatic data use a CDN to host static content partition data to improve scalability, reduce contention, and optimize performance","title":"Web-Queue-Worker"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#microservices","text":"A microservice app is composed of many small, independent services. Each service implements a single business capability. Services are loosely coupled , communicating through API contracts . Each service can be built by a small, focused development team. Individual services can be deployed without a lot of coordination between teams, which encourages frequent updates . Services are responsible for persisting their own data or external state. However, this architecture is more complex to build and manage and requires a mature development and DevOps culture. Done right, this style can lead to higher release velocity, faster innovation, and a more resilient architecture . Management/orchestration component is responsible for placing services on nodes, identifying failures, and rebalancing services across nodes. For example, Kubernetes, and certain public cloud services. API Gateway - the entry point for clients which forwards the call to the appropriate services on the back end. Benefits: Agile development Small, focused teams for each service Small code base on each service Mix of technologies, some services can be built with different stacks Fault isolation, some services failure will not bring down the whole service Better scalability, independent scaling Data isolation, each service owns its data. But common schema changes are tricky. Challenges: Complexity, more moving parts and need to make sure they work in harmony Development and testing, hard to test service dependencies Lack of goverance, may end up with things built with different stacks and styles and making it harder to maintain and see the entire picture Network ingestion and latency, more components lead to more inter-service communication. Reduce unnecessary calls, use more efficient serialization formats, and switch to asynchronous communication patterns helps Data integrity, data consistency can become a problem as each service keeps its own state Management, logging and tracing, DevOps Versioning, potential backward or forward compatibility Skill set for building highly distributed systems Best practices: Decentralize as much as possible API should be designed around the business domain offload auth and SSL termination to the Gateway The gateway should handle and route client requests without any knowledge of the business rules or domain logic","title":"Microservices"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#event-driven","text":"An application of event-driven architecture use a publish-subscribe (pub-sub) model, where producers publish events, and consumers subscribe to them. The producers are independent from the consumers, and consumers are independent from each other. Events are delivered in near real time, so consumers can respond immediately to events as they occur, and every consumer sees all of the events. This architecture is good for apps that ingest and process large volume of data with low latency, or when multiple different subsystems must perform different types of processing on the same event data. Two Event-driven architecture variations: The Pub/Sub model - sends published events to each subscriber and the events cannot be replayed and new subscribers do not see past events. The Event streaming model - writes events to a log and keeps the events strictly ordered and persistent. Consumers can read from any part of the stream and is responsible for advancing its position in the stream. A consumer can join any time and replay past events. Three common consumer variations: Simple event processing, each event immediately triggers an action complex event processing, the consumer processes a series of events to look for a pattern, like aggregation over sliding time windows event stream processing, use a pipeline to ingest events and to process or transform the stream. Good for IoT workloads. Benefits: decoupled design of producers and consumers easy to add or remove consumers consumers respond to events immediately highly scalable and distributed consumers can have independent view of the event stream Challenges: Guaranteed delivery is essential Process events in order or exactly once can be hard to guarantee Best practices: keep events as small as possible services should really only share IDs and maybe a timestamp to indicate when the information was effective think about which service should own which piece of data, and avoid sharing those pieces in a single giant event. Another great read about Event-driven design https://particular.net/blog/putting-your-events-on-a-diet and RPC's disadvantage against messaging or event-driven design https://particular.net/blog/rpc-vs-messaging-which-is-faster","title":"Event-driven"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#big-data","text":"Big data and Big Compute are specialized architecture for workloads that fit certain specific profiles. Big data divides a very large dataset into chunks, performing parallel processing across the entire set, for analysis and reporting. It usually involves: Batch processing of big data stored app data in relational DB, static files, logs source data stored in Data Lake, typically distributed storage, high volumes, various formats, include blobs (Binary Large Object) Map/Reduce are often used repeated processing operations encapsulated in workflows Real-time processing of big data in motion IoT event data events typically are compressed and serialized into binary format a message ingestion store to act as a buffer for messages is needed support scale-out processing, reliable delivery, message queuing Interactive exploration of big data Predictive analytics and machine learning data modeling layer Benefits: Parallelism Elastic scale, scale up both borizontally and vertically on the components as needed Interoperability and extensibility, new workload can integrate into existing solutions Challenges: Complex to build and configure for each system in the picture Skillset is specialized, many frameworks and languages are not used in general application architectures Evolving technology, some are still evolving fast and introduce big changes across releases Security, properly govering data access Best practices: partition the data to reduce ingestion apply schema-on-read semantics, which project a schema onto the data when the data is processing, not when the data is stored. It is less likely to cause slow downs from bottlenecks such as validation and type checking. adjust the parallelism for the best balance in utilization costs and time scrub sensitive data early in the data pipeline","title":"Big Data"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#big-compute","text":"Big compute , aka high-performance computing (HPC), makes parallel computations across a large number (thousands) of cores , mostly used on simulations, modeling, rendering, or any jobs that requires too much memory to fit on single machine. The work must be able to split into discrete tasks, and each task is finite. The application can be spun up ad-hoc. For tightly coupled tasks, high-speed networking is required for exchanging intermediate results fast. Challenges: Manage VM infrastructure Provision thousands of cores timely Experiment to find optimum number of cores, parallelism, and cost","title":"Big Compute"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#design-principles","text":"","title":"Design Principles"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#design-for-self-healing","text":"A three-pronged approach: Detect failures Respond to failures gracefully Log and monitor failures to give operational insight Recommendations: Retry failed operations for transient failures Protect failing remote services via circuit breaker for persistent failures to prevent overwhelming retries causing cascading failures Isolate critical resources , partition a system into isolated groups, so that a failure in one partition does not bring down the entire system Perform load leveling , use a queue-based load leveling pattern to queue work items to run asynchronously and prevent sudden spikes take down the entire system Fail over to backup region or cluster. For stateless services, put instances behind a load balancer. For stateful services, use replicas for fail over (and handle eventual consistentcy). Compensate failed transactions , compose an operation from smaller individual transactions. Rewind back if the operation fails midway through. Checkpoint long-running transactions , allow the operation resume from last checkpoint in event of a failure. Degrade gracefully , provide reduced functionality when the primary function is broken. Throttle bad clients , throttle clients making excessive load abusively Block bad actors , define out-of-band process to detect bad clients and block their access for some period of time Leader election on coordinators to avoid single-point-failure Perform fault injection tests , test the resiliency of the system responding possible failure-scenarios either by triggering actual failures or simulate them Chaos engineering by randomly injecting failure conditions into production instances","title":"Design for self-healing"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#build-redundancy-into-application","text":"Identify the critical paths in your application. Is there redundancy at each point in the path? When a subsystem fails, will the application fail over to something else? Recommendations: Weigh the cost increase, the complexity, and operational procedures introduced from the amount of redundancy built into a system and decided if it justify the business scenario Put multiple VMs behind the load balancer . Replicate databases across regions to have quick fail over. Document fail over procedures or implement the observability layer to do so promptly Partition for availability. If one shard goes down, the other shards can still be reached. Deploy to multiple regions for higher availability. Synchronize frontend, backend, and database failover . Depending on the system, the backup region may require the whole stack to be failover to function properly. Use automatic failover but manual failback . Involve engineers to verify that all application subsystems are healthy before manually failing back. Also check dta consistency before failing back. Redundancy for Traffic Manager , as it is a possible failure point.","title":"Build redundancy into application"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#minimize-coordination","text":"Most cloud apps runs multiple instances for scalability and reliability. When several instances try to perform concurrent operations that affect a shared state, there must be coordination across these instances. Coordination through locks creates performance bottlenecks, and as more instances are scaled up, the lock contention increases. A better approach is to partition the work space from number of workers, each worker only affects its assigned portion. Recommendataions: Embrace eventual consistency . The Compensating Transaction pattern can be used to incrementally apply transactions and roll back everything when necessary. Use domain events to synchronize state. Domain event is an event that has significance within the domain. Interested services can listen for the event and act accordingly, rather than relying on coordination. Partition data into shards Design idempotent operations so they can be handled with at-least-once semantics and making it easy to retry upon failures and crashes Use optimistic concurrency when possible. Pessimistic concurrency control uses database locks to prevent conflicts. Optimistic concurrency let each transaction modifies a copy or snapshot of the data, and have database engine validate the committed transaction and rejects others that breaks database consistency.","title":"Minimize coordination"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#design-to-scale","text":"Design your application so that it can scale horizontally and quickly, adding or removing new instances as demand requires. Recommendations: Avoid instance stickiness/affinity , or when requests from the same client are always routed to the same server. Traffic from a high-volume user will not be distributed across instances. Identify bottlenecks and resolve them first before scaling up. Stateful parts of the system are the most likely cause of bottlenecks. Offload resource-intensive tasks as offline jobs or asynchronously handled workloads if possible, to easy the laod on the frontend. Graceful scale down , instances being removed should handle the termination gracefully and clean up its states. listen for shutdown events clients should support transient fault handling and retries, in the event its remote origin gets terminated set breaking points for long-running tasks to be stopped and resumed put work items on a queue so another instance can pick up the work","title":"Design to scale"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#partition-around-limits","text":"In the cloud services have limits in their ability to scale up, such as number of cores, memory available, database size, query throughput, and network throughput. Use partition to work around these limits. partition a database to avoid limits on database size, data I/O, number of concurrent sessions. partition a queue or message bus to avoid limits on number of requests or the number of concurrent connections. partition an App Service web app to avoid limits on the number of instances per App Service plan. A database partition can be: horizontally - aka sharding, each partition holds data for a subset of the total data set. The partitions share the same data schema. vertically - each partition holds a subset of the fields for the items in the data store functionally - data is partitioned according to how it is used by each bounded context in the system, and the schemas are independent. Recommendataions: Besides database, consider also partition storage, cache, queues, and compute instances Design the partition key to avoid hotspots. Refer to consistent hashing, sharding When it is possible to partition at multiple layers, it is easier to partition lower layers first in the hierarchy.","title":"Partition around limits"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#design-operations-tools","text":"Some important operations functions for cloud applications: Deployment Logging and tracing Monitoring Escalation Incident response Security auditing Recommendataions: Make all things observable . Logging captures individual events, app states, errors, exceptions. Tracing records a path through the system and is useful to pinpoint bottlenecks, performance issues, and failure points. Instrument for monitoring , it should be as close to real-time as possible to discover issues that breach SLA Instrument for root cause analysis Use distributed tracing. Traces should include a correlation ID that flows across service boundaries Standardize logs and metrics. Define a common schema that can be derived to add custom fields, but all logs should be capable for tracing and aggregation. Treat configuration as code. Check configuration files into a version control system.","title":"Design operations tools"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#use-managed-services","text":"IaaS is like having a box of parts. You can build anything, but you have to assemble it yourself. PaaS options are easier to configure and administer. You are delegating some operational responsibility to a more focused team that manages it for everyone else.","title":"Use managed services"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#use-suitable-data-store","text":"In any large solution, it's likely that a single data store technology won't fill all your needs. Alternatives to relational databases include key/value stores, document databases, search engine databases, time series databases, column family databases, and graph databases. Embrace polyglot persistence and put different types of data into the storage that best fit its type. For example, put transactional data into SQL, put JSON documents into a document database, put telemetry data into a time series data base, put application logs in Elasticsearch, and put blobs in HBase or other blob storage. Recommendations: Prefer availability over (strong) consistency. Achieve higher availability by adopting an eventual consistency model. Consider the skillset of the development team. To adopt a new data storage technology, the development team must learn and understand appropriate usage patterns, how to optimize queries, tune for performance, and so on to get the most out of it. Use compsensating transactions to undo any completed steps in a failed transaction. Look at bounded contexts . A bounded context is an explicit boundary around a domain model, and defines which parts of the domain the model applies to. A bounded context maps to a subdomain of the business domain. The bounded contexts in your system are a natural place to consider polyglot persistence.","title":"Use suitable data store"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#design-for-evolution","text":"All successful applications change over time, whether to fix bugs, add new features, bring in new technologies, replace existing components, or make existing systems more scalable and resilient. Recommendations: Enforce high cohesion and loose coupling. If you find that updating a service requires coordinated updates to other services, it may be a sign that your services are not cohesive. a service is cohesive if it groups functionalities that logically belongs together loosely coupled services allows changing one without affect another Use asynchronous messaging, it is effective in creating loosely coupled systems Expose open interfaces . A service should expose an API with a well-defined API contract, and avoid creating custom translation layers that sit between services. The API should be versioned for maintaining backward compatibility. Public facing services should expose a RESTful API over HTTP. Backend services might use an RPC-style messaging protocol for performance reasons. Design and test against service contracts. When services expose well-defined APIs, you can develop and test against those APIs. Abstract infrastructure away from domain logic. Don't let domain logic get mixed up with infrastructure-related functionality, such as messaging or persistence. Offload shared functionality to a separate service, which allows it getting evloved independently. Deploy services independently. Updates can happen more quickly and safely.","title":"Design for evolution"},{"location":"Cloud-and-Containers/Architecture/Cloud-App-Architecture/#build-for-the-business-needs","text":"Ultimately, every design decision must be justified by a business requirement. Do you anticipate millions of users, or a few thousand? Is a one-hour application outage acceptable? Do you expect large bursts in traffic or a predictable workload? so on Recommendations: Define business objectives , including the recovery time objective (RTO), recovery point objective (RPO), and maximum tolerable outage (MTO). These numbers should inform decisions about the architecture. Document service level agreements (SLA) and service level objectives (SLO). It is a business decision to set these numbers to define how reliable the system must be built for. Model the application around the business domain . Analyzing the business requirements. Consider using a domain-driven design (DDD) approach to create domain models that reflect the business processes and use cases. Capture both functional and nonfunctional requirements . Functional requirements let you judge whether the application does the right thing . Nonfunctional requirements let you judge whether the application does those things well . Understand your requirements for scalability, availability, and latency Plan for growth . A solution might meet your current needs, in terms of number of users, volume of transactions, data storage, and so forth. A robust application can handle growth without major architectural changes. Manage costs . Traditional on-premises application pays upfront for hardware as a capital expense. In a cloud application, you pay for the resources that you consume so understand the pricing model for the services that you consume. Also consider your operations costs.","title":"Build for the business needs"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/","text":"This set of notes is taken from the book Docker High Performance by Allan Espinosa, Russ McKendrick and from the official Docker documentation . Optimizing Docker Images \u00b6 As you keep building and iterating a Docker image, eventually the small container and fast build time goes away when a docker image becomes huge in the order of gigabytes. Then it is worth a while to seek ways to optimize the image building process. Reducing deployment time \u00b6 If deployments happen within your network, consider a local registry . By setting up a local registry, it saves time and bandwidth when distributing Docker images without relying on external hubs. # will set a local registry running at tcp://dockerhost:5000 docker run --network = host -d registry:2 docker tag someimage dockerhost:5000/someimage docker push dockerhost:5000/someimage docker pull dockerhost:5000/someimage More details on setting up a managed Docker registry https://docs.docker.com/registry/deploying Improving image build time \u00b6 Using registry mirrors saves time when fetching upstream images. If the FROM <image> is quite large, definitely consider using a local registry mirror to speed up the fetch within local network. In this way, a large image will be fetched only once into the mirror and distributed fast within local network. After a managed local registry mirror is set, add the registry mirror host or ip address to the Docker daemon by updating /etc/docker/daemon.json and add into registry-mirrors and restart the docker service: systemctl restart docker.service { \"registry-mirrors\" : [ \"http://127.0.0.1:5000\" ] } Read more https://docs.docker.com/registry/recipes/mirror/ Reusing image layers helps speed up the image build process, as a Docker image consists of a series of layers combined with the union filesystem of a single image, and reused image layers won't need to be build or fetched again. How Dockerfile instructions are cached https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache Group the tasks of things that won't expect to change often, i.e. dependency installation, and do them early in the Dockerfile will help reuse those layers. For example, copy a dependency file and run the installation command before copy the source files into the image, since the source files are expected to change more often . Reducing the build context size \u00b6 Try to avoid copy unnecessary files into the image. Use .dockerignore file in the same directory as the Dockerfile to omit some kind of directories or files from being copied into the image. More on .dockerignore https://docs.docker.com/reference/builder/#dockerignore-file Omitting the build context can be useful in situations where your Dockerfile does NOT require files to be copied into the image, and improves the build-speed, as no files are sent to the daemon. It can be done by passing the build context through STDIN docker build -t myimage:latest - <<EOF FROM busybox RUN echo \"hello world\" EOF docker build [ OPTIONS ] -f- CONTEXT_PATH # read Dockerfile from STDIN Using caching proxies \u00b6 Another common problem that causes long runtimes in building Docker images is instructions that download dependencies , such as fetching packages from yum repositories or python modules. A useful technique to reduce the time for these build instructions is to introduce proxies that cache such dependency packages, such as apt-cacher-ng : supports caching Debian , RPM , and other distribution-specific packages https://www.unix-ag.uni-kl.de/~bloch/acng Sonatype Nexus : supports Maven , Ruby Gems , PyPI , and NuGet packages out of the box http://www.sonatype.org/nexus Polipo : a generic caching proxy that is useful for development http://www.pps.univ-paris-diderot.fr/~jch/software/polipo Squid : another popular caching proxy that can work with other types of network traffic http://www.squid-cache.org/ This technique is useful when we develop base Docker images for our team or organization. In general, it is recommended that we verify the contents of public Docker images in environments, such as Docker Hub, instead of blindly trusting them. Reducing Docker image size \u00b6 While the increase of the image size is inevitable as more changes and features added to the program being containerized, there are some good practices to help reduce the image size or speed up the build . Avoid unnecessary packages \u00b6 Avoid installing extra or unnecessary packages just because they might be \"nice to have.\" Docker images grow big because some instructions are added that are unnecessary to build or run an image. Limiting each container to one process is a good rule of thumb, but it is not a hard and fast rule. Use your best judgment to keep containers as clean and modular as possible. Chaining commands \u00b6 Packaging metadata and cache are the common parts of the code that are usually increased in size. A Docker image's size is basically the sum of each individual layer image (more specifically, only RUN COPY ADD creates layers, other instructions creates temporary intermediate layers that won't add up image size); this is how union filesystems work. That's why installing packages and removing cache in a separate step will not reduce the image size, like following practice: FROM debian:stretch RUN echo deb http://httpredir.debian.org/debian stretch-backports main \\ > /etc/apt/sources.list.d/stretch-backports.list RUN apt-get update RUN apt-get --no-install-recommends install -y openjdk-11-jre-headless RUN rm -rfv /var/lib/apt/lists/* # won't reduce the final image size There is no such thing as negative layer size , and so each instruction in a Dockerfile can only keep the image size constant or increase it. Instead, the cleaning steps should be performed in the same image layer as where those changes were introduced . Docker uses /bin/sh to run each instruction given to RUN , so can use && to chain commands . Alternatively, (when there are many instructions) put the commands in a shell script , copy the script in and run the script. Whenever possible, ease later changes by sorting multi-line arguments alphanumerically. This helps to avoid duplication of packages and make the list much easier to update. FROM debian:stretch RUN echo deb http://httpredir.debian.org/debian stretch-backports main \\ > /etc/apt/sources.list.d/stretch-backports.list && \\ apt-get update && \\ apt-get --no-install-recommends install -y openjdk-11-jre-headless && \\ rm -rfv /var/lib/apt/lists/* Separating build and deployment images \u00b6 Source libraries , such as compilers and source header files, are only necessary when building an application inside a Docker image. After the application is built, only the compiled binary and related shared libraries are needed to run the application. For example, use an image with jdk installed to build jar files then use an image with jvm to run the jars; use an image with golang installed to build the binary from go source and use a small Linux base-image (better, alpine or busybox ) to run the binary. This build is bad, as the image is used to build the app and also run the app. The go compilers and static libraries for the build are unnecessary when running the app. FROM golang:1.11-stretch ADD hello.go RUN go build hello.go EXPOSE 8080 ENTRYPOINT [ \"./hello\" ] Multi-stage image build \u00b6 # this base-image serve as a build stage for the final image FROM golang:1.11-stretch ADD hello.go hello.go RUN go build hello.go # Good to have a base image that has some debugging tools FROM busybox COPY --from = 0 /go/hello /app/hello # The libraries are obtained from running `ldd hello` which prints shared object dependencies on the binary COPY --from = 0 /lib/x86_64-linux-gnu/libpthread.so.0 \\ /lib/x86_64-linux-gnu/libpthread.so.0 COPY --from = 0 /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libc.so.6 COPY --from = 0 /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2 WORKDIR /app EXPOSE 8080 ENTRYPOINT [ \"./hello\" ] Multi-stage builds allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files. the image is built during the final stage of the build process, you can minimize image layers by leveraging build cache order the instructions from the less frequently changed to the more frequently changed to ensure the build cache is reusable install tools for building the app install or update library dependencies generate the app FROM golang:1.11-alpine AS build # Install tools required for project # Run `docker build --no-cache .` to update dependencies RUN apk add --no-cache git RUN go get github.com/golang/dep/cmd/dep # List project dependencies with Gopkg.toml and Gopkg.lock # These layers are only re-built when Gopkg files are updated COPY Gopkg.lock Gopkg.toml /go/src/project/ WORKDIR /go/src/project/ # Install library dependencies RUN dep ensure -vendor-only # Copy the entire project and build it # This layer is rebuilt when a file changes in the project directory COPY . /go/src/project/ RUN go build -o /bin/project # This results in a single layer image FROM scratch COPY --from = build /bin/project /bin/project ENTRYPOINT [ \"/bin/project\" ] CMD [ \"--help\" ] Docker Disk Usage Cleanup \u00b6 # prunes stopped containers, unused networks, dangling images, volumes, and cache docker system prune --all docker system prune docker system prune --volumes # clean up specific component docker rmi $( docker images -qf dangling = true ) docker rmi $( docker images | grep <image-name> | awk '{print $3}' ) docker volume rm $( docker volume ls -qf dangling = true ) Frequently-used Docker CLI commands reference \u00b6 docker info # prints a summary of current docker environment docker help # see a list of docker commands docker pull <image> # pull down an image docker images # list local cached docker images docker run <image> # run a container docker ps # see running containers docker kill <container_id>.. # stop containers docker rm <container_id>.. # remove containers docker rmi <image>.. # remove docker images docker build -t <image>:<tag> . # create image using current directory's Dockerfile # handy operations docker rmi $( docker images -f \"dangling=true\" -q ) # remove dangling images docker run -p 4000 :80 nginx # run a container mapping internal port 4000 to external port 80 docker run -d helloworld # run a container detached (in background) Dockerfile Reference \u00b6 How a Docker image is built \u00b6 A Dockerfile is a text document that describes how a Docker image is built. The docker build command builds an image from a Dockerfile and a context . build context \u00b6 The build's context is the set of files at a specified location , PATH , or URL . The PATH is a directory on your local filesystem . The URL is a Git repository location. The build is run by the Docker daemon , not by the docker CLI. The first thing a build process does is send the entire context ( recursively ) to the daemon. It's best to start with an empty directory as context and keep your Dockerfile in that directory. Add only the files needed for building the Dockerfile . Exclude files and directories by adding a .dockerignore file. By convention a Dockerfile is located in the root of the build context. use the -f <path> flag with docker build to point to a Dockerfile anywhere in your file system. specify a repository and tag at where to save the new image if the build succeeds, with -t <tag> multiple -t can be used each instruction is run independently , and causes a new image (layer) to be created whenever possible, Docker will re-use the intermediate images (cache) to accelerate the build process Build cache is only used from images that have a local parent chain . this means that these images were created by previous builds or the whole chain of images was loaded with docker load . images specified with docker build --cache-from <image> do not need to have a parent chain and may be pulled from other registries. once a layer is invalidated by the cache, all subsequent instructions generate new layers Starting with version 18.09 , Docker supports a new backend for executing your builds, the BuildKit which can be enabled by setting an environment variable DOCKER_BUILDKIT=1 before building an image. The BuildKit backend provides many benefits compared to the old implementation. Learning about BuildKit for new features Dockerfile instructions \u00b6 FROM \u00b6 Code FROM [ --platform = <platform> ] <image> [ AS <name> ] FROM [ --platform = <platform> ] <image> [ :<tag> ] [ AS <name> ] FROM [ --platform = <platform> ] <image> [ @<digest> ] [ AS <name> ] # example ARG VERSION = latest FROM busybox: $VERSION ARG VERSION RUN echo $VERSION > image_version FROM extras: ${ VERSION } CMD /code/run-extras FROM initializes a new build stage and sets the Base Image for subsequent instructions. can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another a valid Dockerfile must start with a FROM instruction --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image, i.e. linux/amd64 a name can be given to a new build stage by adding AS <name> and used in COPY --from=<name|index> tag or digest values are optional, default use latest tag FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM An ARG declared before a FROM is outside of a build stage , so it can\u2019t be used in any instruction after a FROM . To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside a build stage Tip use alpine as the baseimage whenever you can RUN \u00b6 Code # Following both are valid RUN /bin/bash -c 'source $HOME/.bashrc; \\ echo $HOME' RUN [ \"/bin/bash\" , \"-c\" , \"echo hello\" ] RUN instruction will execute any commands in a new layer on top of the current image and commit the results; the committed image will be used for the next step/instruction Docker commits are cheap and containers can be created from any point in an image's history, much like source control. the cache for a RUN will be reused during the next build. the cache can be invalidated by using the docker build --no-cache flag the cache for RUN instructions can be invalidated by ADD and COPY instructions two forms of RUN instruction RUN <command/script> is called the shell form implicitly invoking /bin/sh -c on the command passed in can do variable substitution RUN [\"executable\", \"param1\", \"param2\"] is called the exec form pass the executable with full path does NOT invoke a command shell, NO variable substitution , more like to concatenate the array into a string command must use double quotes as it is parsed as JSON array can run commands using a different shell executable necessary to escape backslashes Tip split long or complex RUN statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable. Or better: put them in a script always combine update and install statements in the same RUN instruction, as well as steps to clean up the installation cache Using RUN apt-get update && apt-get install -y ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting . You can also achieve cache-busting by specifying a package version. This is known as version pinning . if using pipes, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding CMD \u00b6 Code CMD [ \"/usr/bin/wc\" , \"--help\" ] CMD echo \"This is a test.\" | wc - CMD instruction provides defaults for an executing container. there can only be one CMD instruction in a Dockerfile , otherwise, only the last CMD will be used if CMD is used to provide default arguments for the ENTRYPOINT , then both of them should be specified with the JSON array format three forms of CMD instruction CMD [\"executable\", \"param1\", \"param2\"] is called the exec form , and is preferred form pass the executable with full path does NOT invoke a command shell, NO variable substitution , more like to concatenate the array into a string command must use double quotes as it is parsed as JSON array CMD [\"param1\",\"param2\"] provides default parameters to ENTRYPOINT which is necessary to be present CMD command param1 param2 is called the shell form implicitly invoking /bin/sh -c on the command passed in LABEL \u00b6 Code LABEL multi.label1 = \"value1\" multi.label2 = \"value2\" other = \"value3\" LABEL instruction adds metadata to an image. it is a key-value pair , multiple can be specified on the same instruction, separated with spaces key can contain periods and dashes use double quotes to include spaces in the value or a backslash to span string to multiple lines Labels included in base or parent images are inherited use docker image inspect --format='' <image> to see just the labels EXPOSE \u00b6 Code EXPOSE <port> [ <port>/<protocol>... ] EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. specify whether the port listens on TCP or UDP, default is TCP it does not actually publish the port but to serve as a documentation to tell the user which ports are intended to be published to actually map and publish the port when running the container, use docker run -p <internal-port>:<external-port> <image> this method takes precedence than what is specified in the Dockerfile use docker run -P <image> to publish all exposed ports and map to high-order ports i.e. 80:80 ENV \u00b6 Code ENV <key> <value> ENV <key> = <value> ... ENV myName = \"John Doe\" myDog = Rex \\ The \\ Dog \\ myCat = fluffy ENV myDog Rex The Dog ENV instruction sets the environment variable <key> to the value <value> this value will be in the environment for all subsequent instructions in the build stage to set a value for a single command , use RUN <key>=<value> <command> ( RUN 's shell form) environment variables set will persist when a container is run from the resulting image can view the environment variables values using docker inspect on an image environment variables can also be set when running docker run --env <key>=<value> variable expansion is supported by ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR ONBUILD two forms: ENV <key> <value> sets a single variable to a value entire string after the first space will be treated as the value ENV <key>=<value> ... sets multiple variables to be set at one time Tip if you don't want to have an ENV var persist to the container, use the define, use, unset approach in a single instruction ADD \u00b6 Code ADD [ --chown = <user>:<group> ] <src>... <dest> ADD [ --chown = <user>:<group> ] [ \"<src>\" ,... \"<dest>\" ] ADD instruction copies new files , directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest> --chown is for building on Linux system only new files and directories are created with a UID and GID of 0 , unless specified otherwise providing a username without groupname or a UID without GID will use the same numeric UID as the GID source paths are interpreted as relative to the source of the context of the build CANNOT use .. to leave the context directory source paths can contain wildcards if source path is a directory , the entire contents of the directory are copied, including filesystem metadata if source path is an URL and destination path does NOT end with a backslash , then the file is downloaded as the destination path ; otherwise, the filename is inferred from the URL and saved in the destination path directory if source path is a local compressed tarball archive , it is unpacked as a directory in the destination path; URL downloaded archive will NOT be auto decompressed if multiple source paths are specified, the destination path must be a directory and ends with a backslash destination path is an absolute path or relative path to WORKDIR inside the image and will be created if not exist You can also pass a compressed archive through STDIN: ( docker build - < archive.tar.gz ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build. COPY \u00b6 Code COPY [ --chown = <user>:<group> ] <src>... <dest> COPY [ --chown = <user>:<group> ] [ \"<src>\" ,... \"<dest>\" ] COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest> like ADD but work only for files and directories optionally accepts a flag --from=<name|index> that can be used to set the source location to a previous build stage (created with FROM .. AS <name> ) that will be used instead of a build context sent by the user the first encountered COPY instruction will invalidate the cache for all following instructions if the CONTENTS of one of its source paths have changed Tip prefer COPY over ADD unless using the convenience provided by ADD use curl or wget to fetch files allows having the chance to discard unwanted files in the same instruction if you have multiple Dockerfile steps that use different files from your context, COPY them individually , rather than all at once. This ensures that each step's build cache is only invalidated if the specifically required files change. ENTRYPOINT \u00b6 Code ENTRYPOINT [ \"/script/start.sh\" ] ENTRYPOINT instruction configures how a container will run as an executable two forms ENTRYPOINT [\"executable\", \"param1\", \"param2\"] is called the exec form ENTRYPOINT command param1 param2 is called the shell form command line arguments to docker run <image> will be APPENDED after all elements in an exec form ENTRYPOINT and will override all elements specified using CMD you can override the ENTRYPOINT instruction using the docker run --entrypoint flag shell form PREVENTS any CMD or docker run command line arguments from being used shell form will start the command with /bin/sh -c and has some disadvantages executable will NOT be the container's PID 1 executable will NOT receive Unix signals and it will NOT receive SIGTERM signal from docker stop <container> to fix the above two issues, make sure to start a command with exec which will invoke the command in another shell session, i.e. ENTRYPOINT exec top -b only the last ENTRYPOINT in the Dockerfile will be used, if multiple are provided if CMD is defined from the base image , setting ENTRYPOINT will RESET CMD to an empty value. VOLUME \u00b6 Code VOLUME [ \"/var/log/\" ] VOLUME /var/log /var/db VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. if any build steps change the data within the volume after it has been declared, those changes will be discarded the host directory (the mountpoint ) is declared at container run-time for portability, a given host directory can't be guaranteed to be available on all hosts, thus you can\u2019t mount a host directory from within the Dockerfile VOLUME does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container Tip VOLUME should be used to expose any database storage area, configuration storage, or files/folders created by your docker container. You are strongly encouraged to use VOLUME for any mutable and/or user-serviceable parts of your image. More on Docker volumes is here USER \u00b6 Code USER <user> [ :<group> ] USER <UID> [ :<GID> ] USER instruction sets the user name (or UID ) and optionally the user group (or GID ) to use when running the image and for any RUN , CMD and ENTRYPOINT instructions follows when specifying a group for the user, the user will have ONLY the specified group membership when the user doesn\u2019t have a primary group then the image (or the next instructions) will be run with the root group Tip if a service can run without privileges, use USER to change to a non-root user avoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems if you absolutely need functionality similar to sudo , such as initializing the daemon as root but running it as non-root, consider using gosu avoid switching USER back and forth frequently WORKDIR \u00b6 WORKDIR instruction sets the working directory for any RUN , CMD , ENTRYPOINT , COPY and ADD instructions follows if the WORKDIR doesn\u2019t exist, it will be created it can be used multiple times WORKDIR /path/to/workdir it could be a relative path to previous WORKDIR it can interpret variables set with ENV without a WORKDIR instruction, the work directory in the image is the root Tip always use absolute paths for WORKDIR use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something , which are hard to read, troubleshoot, and maintain ARG \u00b6 Code ARG user1 = someuser ARG ${ user2 :- some_user } ARG buildno ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag a default value can be set with ARG undefined variable results in empty string do not use it to pass secrets; build-time variables are visible in the image with docker history command use BuildKit instead ARG instruction goes out of scope at the end of the build stage where it was defined to use an arg in multiple stages, each stage must include the ARG instruction variables defined using the ENV instruction always override an ARG instruction of the same name some special predefined ARG variables can be set and used without an ARG instruction: HTTP_PROXY HTTPS_PROXY FTP_PROXY NO_PROXY and their lowercase version they won't be saved in docker history neither unless they are included with ARG some predefined platform ARG variables are set automatically: TARGETPLATFORM TARGETOS TARGETARCH TARGETVARIANT BUILDPLATFORM BUILDOS BUILDARCH BUILDVARIANT they are not automatically included so need to include an ARG instruction to make it available if an ARG value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition ONBUILD \u00b6 Code ONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build it won't affect current build and can be viewed by docker inspect <image> the trigger will be executed in the context of the downstream build; if all triggers succeed, the FROM instruction completes and the build continues any build instruction can be registered as a trigger; an image can have multiple ONBUILD instructions it will NOT be inherited to the downstream build Tip images built with ONBUILD should get a separate tag, for example: ruby:1.9-onbuild STOPSIGNAL \u00b6 STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit can be a valid unsigned number that matches a position in the kernel's syscall table HEALTHCHECK \u00b6 Code HEALTHCHECK --interval = 5m --timeout = 3s \\ CMD curl -f http://localhost/ || exit 1 HEALTHCHECK instruction tells Docker how to test a container to check that it is still working when a container has a healthcheck specified, it has a health status in addition to its normal status can only be one HEALTHCHECK instruction in a Dockerfile HEALTHCHECK [OPTIONS] CMD command and OPTIONS can be: --interval=DURATION default 30s, time between every other check --timeout=DURATION default 30s, fails if a check takes longer than this timeout --start-period=DURATION default 0s, initialization time before starting the check --retries=N default 3 command could be a shell command or an exec JSON array any output from the check will be stored in the health status and visible in docker inspect SHELL \u00b6 Code SHELL [ \"/bin/bash\" , \"-c\" ] SHELL instruction allows overriding the default shell used for the shell form of commands default shell on Linux is [\"/bin/sh\", \"-c\"] it must be written in exec JSON form it can appear multiple times each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent RUN CMD ENTRYPOINT instructions","title":"Docker Practical"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#optimizing-docker-images","text":"As you keep building and iterating a Docker image, eventually the small container and fast build time goes away when a docker image becomes huge in the order of gigabytes. Then it is worth a while to seek ways to optimize the image building process.","title":"Optimizing Docker Images"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#reducing-deployment-time","text":"If deployments happen within your network, consider a local registry . By setting up a local registry, it saves time and bandwidth when distributing Docker images without relying on external hubs. # will set a local registry running at tcp://dockerhost:5000 docker run --network = host -d registry:2 docker tag someimage dockerhost:5000/someimage docker push dockerhost:5000/someimage docker pull dockerhost:5000/someimage More details on setting up a managed Docker registry https://docs.docker.com/registry/deploying","title":"Reducing deployment time"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#improving-image-build-time","text":"Using registry mirrors saves time when fetching upstream images. If the FROM <image> is quite large, definitely consider using a local registry mirror to speed up the fetch within local network. In this way, a large image will be fetched only once into the mirror and distributed fast within local network. After a managed local registry mirror is set, add the registry mirror host or ip address to the Docker daemon by updating /etc/docker/daemon.json and add into registry-mirrors and restart the docker service: systemctl restart docker.service { \"registry-mirrors\" : [ \"http://127.0.0.1:5000\" ] } Read more https://docs.docker.com/registry/recipes/mirror/ Reusing image layers helps speed up the image build process, as a Docker image consists of a series of layers combined with the union filesystem of a single image, and reused image layers won't need to be build or fetched again. How Dockerfile instructions are cached https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache Group the tasks of things that won't expect to change often, i.e. dependency installation, and do them early in the Dockerfile will help reuse those layers. For example, copy a dependency file and run the installation command before copy the source files into the image, since the source files are expected to change more often .","title":"Improving image build time"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#reducing-the-build-context-size","text":"Try to avoid copy unnecessary files into the image. Use .dockerignore file in the same directory as the Dockerfile to omit some kind of directories or files from being copied into the image. More on .dockerignore https://docs.docker.com/reference/builder/#dockerignore-file Omitting the build context can be useful in situations where your Dockerfile does NOT require files to be copied into the image, and improves the build-speed, as no files are sent to the daemon. It can be done by passing the build context through STDIN docker build -t myimage:latest - <<EOF FROM busybox RUN echo \"hello world\" EOF docker build [ OPTIONS ] -f- CONTEXT_PATH # read Dockerfile from STDIN","title":"Reducing the build context size"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#using-caching-proxies","text":"Another common problem that causes long runtimes in building Docker images is instructions that download dependencies , such as fetching packages from yum repositories or python modules. A useful technique to reduce the time for these build instructions is to introduce proxies that cache such dependency packages, such as apt-cacher-ng : supports caching Debian , RPM , and other distribution-specific packages https://www.unix-ag.uni-kl.de/~bloch/acng Sonatype Nexus : supports Maven , Ruby Gems , PyPI , and NuGet packages out of the box http://www.sonatype.org/nexus Polipo : a generic caching proxy that is useful for development http://www.pps.univ-paris-diderot.fr/~jch/software/polipo Squid : another popular caching proxy that can work with other types of network traffic http://www.squid-cache.org/ This technique is useful when we develop base Docker images for our team or organization. In general, it is recommended that we verify the contents of public Docker images in environments, such as Docker Hub, instead of blindly trusting them.","title":"Using caching proxies"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#reducing-docker-image-size","text":"While the increase of the image size is inevitable as more changes and features added to the program being containerized, there are some good practices to help reduce the image size or speed up the build .","title":"Reducing Docker image size"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#avoid-unnecessary-packages","text":"Avoid installing extra or unnecessary packages just because they might be \"nice to have.\" Docker images grow big because some instructions are added that are unnecessary to build or run an image. Limiting each container to one process is a good rule of thumb, but it is not a hard and fast rule. Use your best judgment to keep containers as clean and modular as possible.","title":"Avoid unnecessary packages"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#chaining-commands","text":"Packaging metadata and cache are the common parts of the code that are usually increased in size. A Docker image's size is basically the sum of each individual layer image (more specifically, only RUN COPY ADD creates layers, other instructions creates temporary intermediate layers that won't add up image size); this is how union filesystems work. That's why installing packages and removing cache in a separate step will not reduce the image size, like following practice: FROM debian:stretch RUN echo deb http://httpredir.debian.org/debian stretch-backports main \\ > /etc/apt/sources.list.d/stretch-backports.list RUN apt-get update RUN apt-get --no-install-recommends install -y openjdk-11-jre-headless RUN rm -rfv /var/lib/apt/lists/* # won't reduce the final image size There is no such thing as negative layer size , and so each instruction in a Dockerfile can only keep the image size constant or increase it. Instead, the cleaning steps should be performed in the same image layer as where those changes were introduced . Docker uses /bin/sh to run each instruction given to RUN , so can use && to chain commands . Alternatively, (when there are many instructions) put the commands in a shell script , copy the script in and run the script. Whenever possible, ease later changes by sorting multi-line arguments alphanumerically. This helps to avoid duplication of packages and make the list much easier to update. FROM debian:stretch RUN echo deb http://httpredir.debian.org/debian stretch-backports main \\ > /etc/apt/sources.list.d/stretch-backports.list && \\ apt-get update && \\ apt-get --no-install-recommends install -y openjdk-11-jre-headless && \\ rm -rfv /var/lib/apt/lists/*","title":"Chaining commands"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#separating-build-and-deployment-images","text":"Source libraries , such as compilers and source header files, are only necessary when building an application inside a Docker image. After the application is built, only the compiled binary and related shared libraries are needed to run the application. For example, use an image with jdk installed to build jar files then use an image with jvm to run the jars; use an image with golang installed to build the binary from go source and use a small Linux base-image (better, alpine or busybox ) to run the binary. This build is bad, as the image is used to build the app and also run the app. The go compilers and static libraries for the build are unnecessary when running the app. FROM golang:1.11-stretch ADD hello.go RUN go build hello.go EXPOSE 8080 ENTRYPOINT [ \"./hello\" ]","title":"Separating build and deployment images"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#multi-stage-image-build","text":"# this base-image serve as a build stage for the final image FROM golang:1.11-stretch ADD hello.go hello.go RUN go build hello.go # Good to have a base image that has some debugging tools FROM busybox COPY --from = 0 /go/hello /app/hello # The libraries are obtained from running `ldd hello` which prints shared object dependencies on the binary COPY --from = 0 /lib/x86_64-linux-gnu/libpthread.so.0 \\ /lib/x86_64-linux-gnu/libpthread.so.0 COPY --from = 0 /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libc.so.6 COPY --from = 0 /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2 WORKDIR /app EXPOSE 8080 ENTRYPOINT [ \"./hello\" ] Multi-stage builds allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files. the image is built during the final stage of the build process, you can minimize image layers by leveraging build cache order the instructions from the less frequently changed to the more frequently changed to ensure the build cache is reusable install tools for building the app install or update library dependencies generate the app FROM golang:1.11-alpine AS build # Install tools required for project # Run `docker build --no-cache .` to update dependencies RUN apk add --no-cache git RUN go get github.com/golang/dep/cmd/dep # List project dependencies with Gopkg.toml and Gopkg.lock # These layers are only re-built when Gopkg files are updated COPY Gopkg.lock Gopkg.toml /go/src/project/ WORKDIR /go/src/project/ # Install library dependencies RUN dep ensure -vendor-only # Copy the entire project and build it # This layer is rebuilt when a file changes in the project directory COPY . /go/src/project/ RUN go build -o /bin/project # This results in a single layer image FROM scratch COPY --from = build /bin/project /bin/project ENTRYPOINT [ \"/bin/project\" ] CMD [ \"--help\" ]","title":"Multi-stage image build"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#docker-disk-usage-cleanup","text":"# prunes stopped containers, unused networks, dangling images, volumes, and cache docker system prune --all docker system prune docker system prune --volumes # clean up specific component docker rmi $( docker images -qf dangling = true ) docker rmi $( docker images | grep <image-name> | awk '{print $3}' ) docker volume rm $( docker volume ls -qf dangling = true )","title":"Docker Disk Usage Cleanup"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#frequently-used-docker-cli-commands-reference","text":"docker info # prints a summary of current docker environment docker help # see a list of docker commands docker pull <image> # pull down an image docker images # list local cached docker images docker run <image> # run a container docker ps # see running containers docker kill <container_id>.. # stop containers docker rm <container_id>.. # remove containers docker rmi <image>.. # remove docker images docker build -t <image>:<tag> . # create image using current directory's Dockerfile # handy operations docker rmi $( docker images -f \"dangling=true\" -q ) # remove dangling images docker run -p 4000 :80 nginx # run a container mapping internal port 4000 to external port 80 docker run -d helloworld # run a container detached (in background)","title":"Frequently-used Docker CLI commands reference"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#dockerfile-reference","text":"","title":"Dockerfile Reference"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#how-a-docker-image-is-built","text":"A Dockerfile is a text document that describes how a Docker image is built. The docker build command builds an image from a Dockerfile and a context .","title":"How a Docker image is built"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#build-context","text":"The build's context is the set of files at a specified location , PATH , or URL . The PATH is a directory on your local filesystem . The URL is a Git repository location. The build is run by the Docker daemon , not by the docker CLI. The first thing a build process does is send the entire context ( recursively ) to the daemon. It's best to start with an empty directory as context and keep your Dockerfile in that directory. Add only the files needed for building the Dockerfile . Exclude files and directories by adding a .dockerignore file. By convention a Dockerfile is located in the root of the build context. use the -f <path> flag with docker build to point to a Dockerfile anywhere in your file system. specify a repository and tag at where to save the new image if the build succeeds, with -t <tag> multiple -t can be used each instruction is run independently , and causes a new image (layer) to be created whenever possible, Docker will re-use the intermediate images (cache) to accelerate the build process Build cache is only used from images that have a local parent chain . this means that these images were created by previous builds or the whole chain of images was loaded with docker load . images specified with docker build --cache-from <image> do not need to have a parent chain and may be pulled from other registries. once a layer is invalidated by the cache, all subsequent instructions generate new layers Starting with version 18.09 , Docker supports a new backend for executing your builds, the BuildKit which can be enabled by setting an environment variable DOCKER_BUILDKIT=1 before building an image. The BuildKit backend provides many benefits compared to the old implementation. Learning about BuildKit for new features","title":"build context"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#dockerfile-instructions","text":"","title":"Dockerfile instructions"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#from","text":"Code FROM [ --platform = <platform> ] <image> [ AS <name> ] FROM [ --platform = <platform> ] <image> [ :<tag> ] [ AS <name> ] FROM [ --platform = <platform> ] <image> [ @<digest> ] [ AS <name> ] # example ARG VERSION = latest FROM busybox: $VERSION ARG VERSION RUN echo $VERSION > image_version FROM extras: ${ VERSION } CMD /code/run-extras FROM initializes a new build stage and sets the Base Image for subsequent instructions. can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another a valid Dockerfile must start with a FROM instruction --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image, i.e. linux/amd64 a name can be given to a new build stage by adding AS <name> and used in COPY --from=<name|index> tag or digest values are optional, default use latest tag FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM An ARG declared before a FROM is outside of a build stage , so it can\u2019t be used in any instruction after a FROM . To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside a build stage Tip use alpine as the baseimage whenever you can","title":"FROM"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#run","text":"Code # Following both are valid RUN /bin/bash -c 'source $HOME/.bashrc; \\ echo $HOME' RUN [ \"/bin/bash\" , \"-c\" , \"echo hello\" ] RUN instruction will execute any commands in a new layer on top of the current image and commit the results; the committed image will be used for the next step/instruction Docker commits are cheap and containers can be created from any point in an image's history, much like source control. the cache for a RUN will be reused during the next build. the cache can be invalidated by using the docker build --no-cache flag the cache for RUN instructions can be invalidated by ADD and COPY instructions two forms of RUN instruction RUN <command/script> is called the shell form implicitly invoking /bin/sh -c on the command passed in can do variable substitution RUN [\"executable\", \"param1\", \"param2\"] is called the exec form pass the executable with full path does NOT invoke a command shell, NO variable substitution , more like to concatenate the array into a string command must use double quotes as it is parsed as JSON array can run commands using a different shell executable necessary to escape backslashes Tip split long or complex RUN statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable. Or better: put them in a script always combine update and install statements in the same RUN instruction, as well as steps to clean up the installation cache Using RUN apt-get update && apt-get install -y ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting . You can also achieve cache-busting by specifying a package version. This is known as version pinning . if using pipes, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding","title":"RUN"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#cmd","text":"Code CMD [ \"/usr/bin/wc\" , \"--help\" ] CMD echo \"This is a test.\" | wc - CMD instruction provides defaults for an executing container. there can only be one CMD instruction in a Dockerfile , otherwise, only the last CMD will be used if CMD is used to provide default arguments for the ENTRYPOINT , then both of them should be specified with the JSON array format three forms of CMD instruction CMD [\"executable\", \"param1\", \"param2\"] is called the exec form , and is preferred form pass the executable with full path does NOT invoke a command shell, NO variable substitution , more like to concatenate the array into a string command must use double quotes as it is parsed as JSON array CMD [\"param1\",\"param2\"] provides default parameters to ENTRYPOINT which is necessary to be present CMD command param1 param2 is called the shell form implicitly invoking /bin/sh -c on the command passed in","title":"CMD"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#label","text":"Code LABEL multi.label1 = \"value1\" multi.label2 = \"value2\" other = \"value3\" LABEL instruction adds metadata to an image. it is a key-value pair , multiple can be specified on the same instruction, separated with spaces key can contain periods and dashes use double quotes to include spaces in the value or a backslash to span string to multiple lines Labels included in base or parent images are inherited use docker image inspect --format='' <image> to see just the labels","title":"LABEL"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#expose","text":"Code EXPOSE <port> [ <port>/<protocol>... ] EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. specify whether the port listens on TCP or UDP, default is TCP it does not actually publish the port but to serve as a documentation to tell the user which ports are intended to be published to actually map and publish the port when running the container, use docker run -p <internal-port>:<external-port> <image> this method takes precedence than what is specified in the Dockerfile use docker run -P <image> to publish all exposed ports and map to high-order ports i.e. 80:80","title":"EXPOSE"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#env","text":"Code ENV <key> <value> ENV <key> = <value> ... ENV myName = \"John Doe\" myDog = Rex \\ The \\ Dog \\ myCat = fluffy ENV myDog Rex The Dog ENV instruction sets the environment variable <key> to the value <value> this value will be in the environment for all subsequent instructions in the build stage to set a value for a single command , use RUN <key>=<value> <command> ( RUN 's shell form) environment variables set will persist when a container is run from the resulting image can view the environment variables values using docker inspect on an image environment variables can also be set when running docker run --env <key>=<value> variable expansion is supported by ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR ONBUILD two forms: ENV <key> <value> sets a single variable to a value entire string after the first space will be treated as the value ENV <key>=<value> ... sets multiple variables to be set at one time Tip if you don't want to have an ENV var persist to the container, use the define, use, unset approach in a single instruction","title":"ENV"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#add","text":"Code ADD [ --chown = <user>:<group> ] <src>... <dest> ADD [ --chown = <user>:<group> ] [ \"<src>\" ,... \"<dest>\" ] ADD instruction copies new files , directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest> --chown is for building on Linux system only new files and directories are created with a UID and GID of 0 , unless specified otherwise providing a username without groupname or a UID without GID will use the same numeric UID as the GID source paths are interpreted as relative to the source of the context of the build CANNOT use .. to leave the context directory source paths can contain wildcards if source path is a directory , the entire contents of the directory are copied, including filesystem metadata if source path is an URL and destination path does NOT end with a backslash , then the file is downloaded as the destination path ; otherwise, the filename is inferred from the URL and saved in the destination path directory if source path is a local compressed tarball archive , it is unpacked as a directory in the destination path; URL downloaded archive will NOT be auto decompressed if multiple source paths are specified, the destination path must be a directory and ends with a backslash destination path is an absolute path or relative path to WORKDIR inside the image and will be created if not exist You can also pass a compressed archive through STDIN: ( docker build - < archive.tar.gz ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.","title":"ADD"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#copy","text":"Code COPY [ --chown = <user>:<group> ] <src>... <dest> COPY [ --chown = <user>:<group> ] [ \"<src>\" ,... \"<dest>\" ] COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest> like ADD but work only for files and directories optionally accepts a flag --from=<name|index> that can be used to set the source location to a previous build stage (created with FROM .. AS <name> ) that will be used instead of a build context sent by the user the first encountered COPY instruction will invalidate the cache for all following instructions if the CONTENTS of one of its source paths have changed Tip prefer COPY over ADD unless using the convenience provided by ADD use curl or wget to fetch files allows having the chance to discard unwanted files in the same instruction if you have multiple Dockerfile steps that use different files from your context, COPY them individually , rather than all at once. This ensures that each step's build cache is only invalidated if the specifically required files change.","title":"COPY"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#entrypoint","text":"Code ENTRYPOINT [ \"/script/start.sh\" ] ENTRYPOINT instruction configures how a container will run as an executable two forms ENTRYPOINT [\"executable\", \"param1\", \"param2\"] is called the exec form ENTRYPOINT command param1 param2 is called the shell form command line arguments to docker run <image> will be APPENDED after all elements in an exec form ENTRYPOINT and will override all elements specified using CMD you can override the ENTRYPOINT instruction using the docker run --entrypoint flag shell form PREVENTS any CMD or docker run command line arguments from being used shell form will start the command with /bin/sh -c and has some disadvantages executable will NOT be the container's PID 1 executable will NOT receive Unix signals and it will NOT receive SIGTERM signal from docker stop <container> to fix the above two issues, make sure to start a command with exec which will invoke the command in another shell session, i.e. ENTRYPOINT exec top -b only the last ENTRYPOINT in the Dockerfile will be used, if multiple are provided if CMD is defined from the base image , setting ENTRYPOINT will RESET CMD to an empty value.","title":"ENTRYPOINT"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#volume","text":"Code VOLUME [ \"/var/log/\" ] VOLUME /var/log /var/db VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. if any build steps change the data within the volume after it has been declared, those changes will be discarded the host directory (the mountpoint ) is declared at container run-time for portability, a given host directory can't be guaranteed to be available on all hosts, thus you can\u2019t mount a host directory from within the Dockerfile VOLUME does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container Tip VOLUME should be used to expose any database storage area, configuration storage, or files/folders created by your docker container. You are strongly encouraged to use VOLUME for any mutable and/or user-serviceable parts of your image. More on Docker volumes is here","title":"VOLUME"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#user","text":"Code USER <user> [ :<group> ] USER <UID> [ :<GID> ] USER instruction sets the user name (or UID ) and optionally the user group (or GID ) to use when running the image and for any RUN , CMD and ENTRYPOINT instructions follows when specifying a group for the user, the user will have ONLY the specified group membership when the user doesn\u2019t have a primary group then the image (or the next instructions) will be run with the root group Tip if a service can run without privileges, use USER to change to a non-root user avoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems if you absolutely need functionality similar to sudo , such as initializing the daemon as root but running it as non-root, consider using gosu avoid switching USER back and forth frequently","title":"USER"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#workdir","text":"WORKDIR instruction sets the working directory for any RUN , CMD , ENTRYPOINT , COPY and ADD instructions follows if the WORKDIR doesn\u2019t exist, it will be created it can be used multiple times WORKDIR /path/to/workdir it could be a relative path to previous WORKDIR it can interpret variables set with ENV without a WORKDIR instruction, the work directory in the image is the root Tip always use absolute paths for WORKDIR use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something , which are hard to read, troubleshoot, and maintain","title":"WORKDIR"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#arg","text":"Code ARG user1 = someuser ARG ${ user2 :- some_user } ARG buildno ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag a default value can be set with ARG undefined variable results in empty string do not use it to pass secrets; build-time variables are visible in the image with docker history command use BuildKit instead ARG instruction goes out of scope at the end of the build stage where it was defined to use an arg in multiple stages, each stage must include the ARG instruction variables defined using the ENV instruction always override an ARG instruction of the same name some special predefined ARG variables can be set and used without an ARG instruction: HTTP_PROXY HTTPS_PROXY FTP_PROXY NO_PROXY and their lowercase version they won't be saved in docker history neither unless they are included with ARG some predefined platform ARG variables are set automatically: TARGETPLATFORM TARGETOS TARGETARCH TARGETVARIANT BUILDPLATFORM BUILDOS BUILDARCH BUILDVARIANT they are not automatically included so need to include an ARG instruction to make it available if an ARG value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition","title":"ARG"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#onbuild","text":"Code ONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build it won't affect current build and can be viewed by docker inspect <image> the trigger will be executed in the context of the downstream build; if all triggers succeed, the FROM instruction completes and the build continues any build instruction can be registered as a trigger; an image can have multiple ONBUILD instructions it will NOT be inherited to the downstream build Tip images built with ONBUILD should get a separate tag, for example: ruby:1.9-onbuild","title":"ONBUILD"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#stopsignal","text":"STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit can be a valid unsigned number that matches a position in the kernel's syscall table","title":"STOPSIGNAL"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#healthcheck","text":"Code HEALTHCHECK --interval = 5m --timeout = 3s \\ CMD curl -f http://localhost/ || exit 1 HEALTHCHECK instruction tells Docker how to test a container to check that it is still working when a container has a healthcheck specified, it has a health status in addition to its normal status can only be one HEALTHCHECK instruction in a Dockerfile HEALTHCHECK [OPTIONS] CMD command and OPTIONS can be: --interval=DURATION default 30s, time between every other check --timeout=DURATION default 30s, fails if a check takes longer than this timeout --start-period=DURATION default 0s, initialization time before starting the check --retries=N default 3 command could be a shell command or an exec JSON array any output from the check will be stored in the health status and visible in docker inspect","title":"HEALTHCHECK"},{"location":"Cloud-and-Containers/Docker/Docker-Practical/#shell","text":"Code SHELL [ \"/bin/bash\" , \"-c\" ] SHELL instruction allows overriding the default shell used for the shell form of commands default shell on Linux is [\"/bin/sh\", \"-c\"] it must be written in exec JSON form it can appear multiple times each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent RUN CMD ENTRYPOINT instructions","title":"SHELL"},{"location":"Cloud-and-Containers/Docker/Docker/","text":"This set of notes is taken from the book Docker In Action by Jeffrey Nickoloff and Stephen Kuenzli, Docker Deep Dive by Nigel Poulton, and from the official Docker documentation . Docker at High Level \u00b6 The main difference between a container and a VM is that a running container shares the kernel of the host machine it is running on. On a Windows machine, a Linux container run either inside a lightweight Hyper-V VM or using the Windows Subsystem for Linux (WSL), which offers better performance and compatibility. There are three things regarding how Docker works: The runtime - operates at the lowest level buliding OS constructs, namespaces, cgroups responsible for starting and stopping containers The daemon - performs high-level tasks and exposing APIs The orchestrator Docker runtime \u00b6 The low-level runtime is called runc and is the reference implementation of Open Containers Initiative ( OCI ) runtime-spec. OCI sets the standards for the low-level fundamental components of container infrastructure (mainly image-spec and runtime-spec ). That's how Kubernetes can support other container runtime technologies that follows the OCI standards to replace Docker's runc as the runtime. runc is an interface with the underlying OS and start and stop containers. Every running container needs to fork a new instance of runc when it is being created; the runc exits after successful container creation. The higher-level runtime is called containerd which manages the entire lifecycle of a container, pulling images, creating network interfaces, and managing lower-level runc instances. Docker daemon \u00b6 Docker daemon is above containerd and provides a standard interface that abstracts the lower levels for performing top level actions. Orchestractor \u00b6 Docker has native support for managing clusters of nodes running Docker, called Swarm, and managed through Docker Compose. Docker Engine \u00b6 The Docker engine is the core software that runs and manages containers. It is modular in design and built from many small specialised tools. The earlier versions of Docker Engine relies on LXC, which is a Linux-specific runtime, and the Docker daemon is designed in a monolithic nature which becomes problematic. Later Docker engine removed container execution and runtime from the daemon and refactoring them into small, modular, and specialized tools. All container executions are refactored into containerd. The main components of Docker Engine in a Linux environment: dockerd docker-containerd docker-containerd-shim docker-runc dockerd \u00b6 Main functionality of the Docker daemon are: image management image builds exposing REST API authentication security core networking orchestration (Swarm) docker-containerd-shim \u00b6 Whenever a container\u2019s parent runc process exits, the associated containerd-shim process becomes the container\u2019s parent. A shim process is responsible for: keeping any STDIN and STDOUT streams open, separate from the daemon reports container's exit status back to daemon Docker Images \u00b6 A Docker image is a unit of packaging that contains everything required for an application to run. It works like a stopped container. Docker images are saved to an image registry . A Docker daemon must pull an image to local before running containers out of it. Images are considered build-time constructs, whereas containers are run-time constructs. Images are made up of multiple layers that are stacked on top of each other and represented as a single object. Inside of the image is a cut-down operating system and all of the files and dependencies required to run an application. The whole purpose of a container is to run a single application or service, so an image should be small and stripped off all unnecessary files. Containers Deep Dive \u00b6 Historically, UNIX-style operating systems have used the term jail to describe a modified runtime environment that limits the scope of resources that a jailed program can access. Containers are not virtualization. Containers and isolation features have existed for decades. Docker uses Linux namespaces and cgroups, which have been part of Linux since 2007. As a side note: Hyper-V containers run a single container inside of a dedicated lightweight VM. The container leverages the kernel of the OS running inside the VM. In Linux, programs that run on top of the operating system runs in the user space memory which is isolated from the kernel space memory . Running Docker means running two programs in user space: the Docker engine and Docker CLI. Each running container has its own subspace of the user space memory. Programs running inside a container thus can access ONLY their own memory and resources as scoped by the container. Docker builds containers using 10 major system features : PID namespace - Process identifiers and capabilities UTS namespace - Host and domain name MNT namespace - Filesystem access and structure IPC namespace - Process communication over shared memory NET namespace - Network access and structure USR namespace - User names and identifiers chroot syscall - sets the location of the filesystem root cgroups - Resource protection CAP drop - Operating system feature restrictions security modules - Mandatory access controls","title":"Docker and Containers"},{"location":"Cloud-and-Containers/Docker/Docker/#docker-at-high-level","text":"The main difference between a container and a VM is that a running container shares the kernel of the host machine it is running on. On a Windows machine, a Linux container run either inside a lightweight Hyper-V VM or using the Windows Subsystem for Linux (WSL), which offers better performance and compatibility. There are three things regarding how Docker works: The runtime - operates at the lowest level buliding OS constructs, namespaces, cgroups responsible for starting and stopping containers The daemon - performs high-level tasks and exposing APIs The orchestrator","title":"Docker at High Level"},{"location":"Cloud-and-Containers/Docker/Docker/#docker-runtime","text":"The low-level runtime is called runc and is the reference implementation of Open Containers Initiative ( OCI ) runtime-spec. OCI sets the standards for the low-level fundamental components of container infrastructure (mainly image-spec and runtime-spec ). That's how Kubernetes can support other container runtime technologies that follows the OCI standards to replace Docker's runc as the runtime. runc is an interface with the underlying OS and start and stop containers. Every running container needs to fork a new instance of runc when it is being created; the runc exits after successful container creation. The higher-level runtime is called containerd which manages the entire lifecycle of a container, pulling images, creating network interfaces, and managing lower-level runc instances.","title":"Docker runtime"},{"location":"Cloud-and-Containers/Docker/Docker/#docker-daemon","text":"Docker daemon is above containerd and provides a standard interface that abstracts the lower levels for performing top level actions.","title":"Docker daemon"},{"location":"Cloud-and-Containers/Docker/Docker/#orchestractor","text":"Docker has native support for managing clusters of nodes running Docker, called Swarm, and managed through Docker Compose.","title":"Orchestractor"},{"location":"Cloud-and-Containers/Docker/Docker/#docker-engine","text":"The Docker engine is the core software that runs and manages containers. It is modular in design and built from many small specialised tools. The earlier versions of Docker Engine relies on LXC, which is a Linux-specific runtime, and the Docker daemon is designed in a monolithic nature which becomes problematic. Later Docker engine removed container execution and runtime from the daemon and refactoring them into small, modular, and specialized tools. All container executions are refactored into containerd. The main components of Docker Engine in a Linux environment: dockerd docker-containerd docker-containerd-shim docker-runc","title":"Docker Engine"},{"location":"Cloud-and-Containers/Docker/Docker/#dockerd","text":"Main functionality of the Docker daemon are: image management image builds exposing REST API authentication security core networking orchestration (Swarm)","title":"dockerd"},{"location":"Cloud-and-Containers/Docker/Docker/#docker-containerd-shim","text":"Whenever a container\u2019s parent runc process exits, the associated containerd-shim process becomes the container\u2019s parent. A shim process is responsible for: keeping any STDIN and STDOUT streams open, separate from the daemon reports container's exit status back to daemon","title":"docker-containerd-shim"},{"location":"Cloud-and-Containers/Docker/Docker/#docker-images","text":"A Docker image is a unit of packaging that contains everything required for an application to run. It works like a stopped container. Docker images are saved to an image registry . A Docker daemon must pull an image to local before running containers out of it. Images are considered build-time constructs, whereas containers are run-time constructs. Images are made up of multiple layers that are stacked on top of each other and represented as a single object. Inside of the image is a cut-down operating system and all of the files and dependencies required to run an application. The whole purpose of a container is to run a single application or service, so an image should be small and stripped off all unnecessary files.","title":"Docker Images"},{"location":"Cloud-and-Containers/Docker/Docker/#containers-deep-dive","text":"Historically, UNIX-style operating systems have used the term jail to describe a modified runtime environment that limits the scope of resources that a jailed program can access. Containers are not virtualization. Containers and isolation features have existed for decades. Docker uses Linux namespaces and cgroups, which have been part of Linux since 2007. As a side note: Hyper-V containers run a single container inside of a dedicated lightweight VM. The container leverages the kernel of the OS running inside the VM. In Linux, programs that run on top of the operating system runs in the user space memory which is isolated from the kernel space memory . Running Docker means running two programs in user space: the Docker engine and Docker CLI. Each running container has its own subspace of the user space memory. Programs running inside a container thus can access ONLY their own memory and resources as scoped by the container. Docker builds containers using 10 major system features : PID namespace - Process identifiers and capabilities UTS namespace - Host and domain name MNT namespace - Filesystem access and structure IPC namespace - Process communication over shared memory NET namespace - Network access and structure USR namespace - User names and identifiers chroot syscall - sets the location of the filesystem root cgroups - Resource protection CAP drop - Operating system feature restrictions security modules - Mandatory access controls","title":"Containers Deep Dive"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/","text":"This set of notes is taken straight from kubernetes.io Breif Kubernetes Overview \u00b6 Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. k8s provides: Service discovery and load balancing expose a container using DNS name or IP address balance loads among many instances of a service Storage orchestration mount storage from local or other cloud providers Automated rollouts and rollbacks describe desired state of containers and have k8s monitor it Automatic bin packing k8s can make the best use of resources by fitting containers among the nodes Self-healing k8s manages the avaibility, restart, replacement, and termination of containers Secret and configuration management k8s manages and stores sensitive info k8s is not a traditional Platform as a Service system. Kubernetes Components \u00b6 When you deploy Kubernetes, you get a cluster . At an enterprise grade k8s deployment, there would be multiple clusters for fault-tolerance and high availablility . These components exist within a cluster: nodes - a number of worker machines control plane - a set of programs that together manages the nodes and the Pods may span across multiple physical machines consists of: kube-apiserver, etcd, kube-scheduler, kube-controller-manager, cloud-controller-manager Control Plane Components \u00b6 The control plane's components can be run on any machine in the cluster and they make global decisions about the cluster as well as detecting and responding to cluster events . See Building High-Availability Clusters for an example multi-master-VM setup. kube-apiserver \u00b6 The apiserver is the front end for the k8s control plane and it is designed to scale horizontally. Multiple kube-apiserver can be deployed to balance traffic. kube-apiserver serves REST http requests which can be consumed from apps built by any languages. kubectl and kubeadm also consumes this API. List of client libraries for consuming k8s API. Full k8s API reference is available, but it is better to use the client libraries. etcd \u00b6 etcd provides a consistent and highly-available key value store for all cluster data . K8s allows you to use alternative key-value store for the cluster data. Be sure to have back up plan for the data no matter which store you use. etcd official documentation kube-scheduler \u00b6 kube-scheduler watches for newly created Pods with no assigned node and selects a node for them to run on. Some decision-making factors : collective resource requirements hardware/software/policy constraints affinity and anti-affinity specs data locality inter-workload interference deadlines kube-controller-manager \u00b6 kube-controller-manager runs controller processes, combined in a single binary. Node controller - notices and reports when nodes go down Replication controller - maintains the correct number of pods for every replication controller object Endpoints controller - populates the endpoints object (joins Services & Pods) Service Account & Token controller - creates default accounts and API access tokens for new namespaces cloud-controller-manager \u00b6 cloud-controller-manager has cloud-specific control logic. Lets you link your cluster into your external cloud provider's API. Only runs controllers that are specific to your cloud provider. Node Components \u00b6 Node components run on every node , maintaining running pods and providing the Kubernetes runtime environment. kubelet \u00b6 kubelet makes sure containers are running healthy in a Pod described by the PodSpecs . It doesn't manage containers that were not create by Kubernetes. kube-proxy \u00b6 kube-proxy is a network proxy that maintains network rules, implementing part of the Kubernetes Service concept. It allows communication to Pods inside or outside the cluster. container runtime \u00b6 container runtime is the software responsible for running containers. Typically are one of: Docker, containerd, CRI-O , or any that implements Kubernetes CRI (Container Runtime Interface) Addons \u00b6 Addons use Kubernetes resources to implement cluster features . Cluster DNS \u00b6 A DNS server that serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server. Web UI Dashboard \u00b6 The general web UI for clusters allow manage and troubleshoot applications running in the cluster. Container Resource Monitoring \u00b6 Records generic time-series metrics about containers . Cluster-level logging \u00b6 Saving container logs to a central log store with search/browsing interface. The Kubernetes API \u00b6 The control plane is the apiserver and it exposes a HTTP API for querying and manipulating the state of objects in Kubernetes. Tools like kubectl and kubeadm use this API. You can also consume it with REST calls. Client libraries exist for many languages for consuming the API. Extend Kubernetes API by adding CustomResourceDefinition . Kubernetes Objects \u00b6 Kubernetes objects are persistent entities to represent state of your cluster . what applications are running the resources available to the applications policies around how the applications behave Kubernetes objects are represented in the Kubernetes API and can be expressed in .yaml format. Kubernetes API consumes the converted JSON of the configs specified in the yaml file. Almost every k8s object includes spec and status . spec describes the desired state for the resource, and status describes the current state of the object. spec is done by you when the resource is first created, and status is updated by k8s. metadata is used to identify the resource . Required fields: apiVersion - the version of the k8s API used when creating this object kind - what kind of object metadata - object identifier, usually a name, UID, and optional namespace each object has a unique name for that specific type of resource in one cluster for name restriction check here each object has a unique UID that is unique across the whole cluster generated by k8s spec - desired state for the object precise spec is different for each k8s object Namespaces \u00b6 Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces . Namespaces are intended for use in environments with many users spread across multiple teams, or projects . Namespaces provide a scope for names . Names of resources need to be unique within a namespace, but not across namespaces. Namespaces provide a way to divide cluster resources between multiple users via resource quota . Some resources are in a namespace and some are not. To find out: # In a namespace kubectl api-resources --namespaced = true # Not in a namespace kubectl api-resources --namespaced = false Labels and Selectors \u00b6 labels , under metadata , are key-value pairs that are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users , but do not directly imply semantics to the core system. Labels can be attached to objects at creation time and added/modified any time and can be used to organize and to select subsets of objects. Labels are made up from an optional prefix and a name , connected by a slash . labels are NOT unique among multiple objects prefix must be a DNS subdomain if prefix is omitted, label key is presumed to be private to the user automated system components must specify a prefix name must be less or equals to 63 chars length , starts and ends with alphanumerics and can have dashes, underscores, or dots between. Using a label selector to identify a set of objects, which is the core grouping primitive in k8s. A label selector can be made of multiple requirements of comma-separated string , for which all must satisify. A label selector requirement can be specified with equality-based operators, =, ==, != for inequality, resources without the label key ( null ) will also be selected. for objects definitions in JSON or YAML, some only support equality requirement selectors and some supports set requirements see https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#resources-that-support-set-based-requirements A label selector requirement can also be a set of values with operators, in, notin, exists i.e. environment in (prod, qa) , tier notin (fe, be) , partition (only the key for exists), !partition (opposite of exists) Common labels Key Description Example Type app.kubernetes.io/name The name of the application mysql string app.kubernetes.io/instance A unique name identifying the instance of an application mysql-abcxzy string app.kubernetes.io/version The current version of the application (e.g., a semantic version, revision hash, etc.) 5.7.21 string app.kubernetes.io/component The component within the architecture database string app.kubernetes.io/part-of The name of a higher level application this one is part of wordpress string app.kubernetes.io/managed-by The tool being used to manage the operation of an application helm string Annotations \u00b6 annotations are not used to identify and select objects. It serves as additional information for the application or for debugging purposes. The metadata in an annotation can be small or large, structured or unstructured, and CAN include characters not permitted by labels. The naming restrictions of annotation keys are the SAME as that for label keys. Field Selectors \u00b6 field selectors allow selecting resources based on the value of one or more resource fields . This is broader than the label selector, but it works only on API and can't be specified in the resource itself to select other objects. Supported field selectors vary by the resource type. Using unsupported field selectors produces an error. Operators =, ==, != can be used; multiple requirements can be combined with a comma. i.e. metadata.name=my-service,status.phase!=Running Cluster Architecture \u00b6 Nodes \u00b6 Kubernetes runs workload by placing containers into Pods that run on Nodes . A node can be a virtual or physical machine. Each node contains services necessary to run Pods which are managed by the control plane . A node has components: kubelet , a container runtime , and the kube-proxy . A node can be added to the k8s apiserver either by the kubelet self-register or by a mnaully-created Node resource object, for which the control plane server will validate and healthcheck. The Node object must have a valid DNS subdomain name . Node can be marked unscheduleable before maintenance or upgrades. The node lifecycle controller automatically creates taints that represent conditions . The Pod scheduler takes the Node's taints into consideration when assigning a Pod to a Node. Pods can also have tolerations which let them tolerate a Node's taints. The node controller is responsible for assigning Classless Inter-Domain Routing (CIDR) block to the node when it is registered, keeping the node controller's internal list of nodes up-to-date with the list of available machines , and monitoring the nodes' health . For reliability , spread your nodes across availability zones so that the workload can be shifted to healthy zones when one entire zone goes down. Control Plane <--> Node Communication \u00b6 All API usage from nodes terminate at the apiserver . The apiserver is configured to listen for remote connections on a secure HTTPS port (typically 443) with one or more forms of client authentication enabled. Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the apiserver along with valid client credentials. Pods that wish to connect to the apiserver can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in all namespaces) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the apiserver. Control Plane can talk to the nodes from the apiserver to the kubelet process on each node in the cluster, or from the apiserver to any node, pod, or service through the apiserver's proxy. Kubernetes supports SSH tunnels to protect the control plane to nodes communication paths but it is deprecated. Konnectivity service is the replacement for this communication channel. Controllers \u00b6 In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. A controller might handle the action itself or messages the apiserver to do so. Built-in controllers run inside the kube-controller-manager and manage state by interacting with the cluster apiserver. Controllers that interact with external state find their desired state from the apiserver, then communicate directly with an external system to bring the current state closer in line. There can be several controllers that create or update the same kind of object. Behind the scenes, Kubernetes controllers make sure that they only change the resources linked to their controlling resource. You can find controllers that run outside the control plane, to extend Kubernetes. Or, if you want, you can write a new controller yourself. You can run your own controller as a set of Pods, or externally to Kubernetes (via CustomResourceDefinition). Cloud Controller Manager \u00b6 The cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic and allow linking your cluster into your cloud provider's API. It works like an adaptor . The cloud-controller-manager is structured using a plugin mechanism that allows different cloud providers to integrate their platforms with Kubernetes. The cloud controller manager runs in the control plane as a replicated set of processes and each cloud-controller-manager implements multiple controllers in a single process. It can also run as a k8s addon rather than part of the control plane. Containers \u00b6 Images \u00b6 A container image is a ready-to-run software package , containing everything needed to run an application: the code and any runtime it requires, application and system libraries, and default values for any essential settings. An image makes very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a Pod. An image name can include a registry hostname and an optional port number. If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry. After the image name part you can add a tag to identify different versions of that image. If you don't specify a tag, Kubernetes assumes you mean the tag latest. Avoid using latest tag when deploying containers in production, as it is harder to track which version of the image is running and more difficult to roll back to a working version. Use a pinned version instead. A container registry can also serve a container image index to allow different systems to fetch the right binary image for the machine architecture they are using. Kubernetes itself typically names container images with a suffix -$(ARCH). A container is immutable : you cannot change the code of a container that is already running. To do so you need to build a new image that includes the change, then recreate the container to start from the updated image. The container runtime is the software that is responsible for running containers. K8s suports Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI. Container Environment \u00b6 The resources available to containers in the container environment : filesystem , a combination of image and volumes information about the container i.e. env variables like Pod name and namespace, other user defined env variables and others statically specified in the image information about other objects in the cluster a list of all services that were running when the container is created, available as env variables i.e. FOO_SERVICE_HOST and FOO_SERVICE_PORT is available as env variables for a service named foo that maps to that container Runtime Class \u00b6 RuntimeClass is a feature for selecting the container runtime configuration, which is used to run a Pod's containers. You can set a different RuntimeClass between different Pods to balance performance and security . You can also use RuntimeClass to run different Pods with the same container runtime but with different settings. Container Lifecycle Hooks \u00b6 kubelet managed containers can use the container lifecycle hook framework to run code triggered by events during their management lifecycle. The hooks enable containers to be aware of events in their management lifecycle and run code implemented in a handler when the corresponding lifecycle hook is executed. Two hooks exposed to containers: PostStart - executed immediately after a container is created PreStop - called immediately before a container is terminated due to an API request or management event such as liveness probe failure , preemption , resource contention , cordon , container exit , and others. Containers can access a hook by implementing and registering a handler for that hook. Two types of hook handlers: exec - executes a specific command or script , such as pre-stop.sh, inside the cgroups and namespaces of the container http - executes an HTTP request against a specific endpoint on the container Hook handler calls are synchronous within the context of the Pod containing the container. The hook need to finish before the container reach a running state. Users should make their hook handlers as lightweight as possible . PostStart or PreStop hook failures kills the container . Hook delivery is intended to be at least once and generally only once . It doesn't get resend if delivery fails. If a handler fails for some reason, it broadcasts an event, which is viewable by kubectl describe . Workloads \u00b6 A workload is an application running on Kubernetes, usually in a set of Pods . A Pod represents a set of running containers on your cluster. You can use workload resources that manage a set of Pods on your behalf. Workload resources: Deployment and ReplicaSet StatefulSet DaemonSet Job and CronJob Garbage collection tidies up objects from your cluster after their owning resource has been removed. The time-to-live after finished controller removes Jobs once a defined time has passed since they completed. Once your application is running, you might want to make it available on the internet as a Service or, for web application only, using an Ingress . Pods \u00b6 Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod is a group of one or more containers , with shared storage/network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \" logical host \": it contains one or more application containers which are relatively tightly coupled . A Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging if your cluster offers this. Init containers run and complete before the app containers are started. The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation. Pods are generally used to run a single container or run multiple containers that need to work together. For example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The containers in a Pod are automatically co-located and co-scheduled on the same physical or virtual machine in the cluster. The containers can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated. A Pod is not a process, but an environment for running container(s) . A Pod persists until it is deleted. Within a Pod, containers share an IP address and port space , and can find each other via localhost. The containers in a Pod can also communicate with each other using standard inter-process communications like SystemV semaphores or POSIX shared memory. Containers that want to interact with a container running in a different Pod can use IP networking to communicate. Schema Reference \u00b6 Pod Example apiVersion : v1 kind : Pod metadata : name : pod-example spec : containers : - name : ubuntu image : ubuntu:trusty command : [ \"echo\" ] args : [ \"Hello World\" ] Frequently used commands kubectl get pods kubectl get pods --show-labels kubectl describe pod/<pod> kubectl exec -it pod/<pod> -c <container> <command> kubectl logs pod/<pod> -c <container> Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. The main use for static Pods is to run a self-hosted control plane (or manage its components). Pod lifecycle \u00b6 Pod lifecycle : starting in the Pending phase, moving through Running if at least one of its primary containers starts OK, and then through either the Succeeded or Failed phases depending on whether any container in the Pod terminated in failure. Pods are considered to be relatively ephemeral (rather than durable) entities. Pods do not, by themselves, self-heal. If a Pod is scheduled to a node that then fails, or if the scheduling operation itself fails, the Pod is deleted. A given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead, that Pod can be replaced by a new, near-identical Pod, with even the same name if desired, but with a different UID. The same rule applies to the volumes attached to a given Pod. You can use container lifecycle hooks to trigger events to run at certain points in a container's lifecycle. Pod has three states: Waiting - a container is running the operations it requires in order to complete start up Running - a container is executing without issues, and the postStart hook has been executed and finished Terminated - a container has begun execution and either ran to completion or failed for some reason. The preStop hook is executed and finished before a container enters the Terminated state A Pod's restartPolicy controls restarts of Terminated containers. A Pod has PodStatus , which contains an array of PodConditions which the Pod has/hasn't passed: PodScheduled - the Pod has been scheduled to a node ContainersReady - all containers in the Pod are ready Initialized - all init containers have started successfully Ready - the Pod is able to serve requests and should be added to the load balancing pools of all matching Services Probe \u00b6 A Probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet calls a Handler implemented by the container. There are three types of handlers: ExecAction - Executes a specified command or script inside the container and expects it exits with 0 TCPSocketAction - Performs a TCP check against the Pod's IP address on a specified port and expects the port to be open HTTPGetAction - Performs an HTTP GET request against the Pod's IP address on a specified port and path and expect response status code, x, to be 200 <= x < 400 The kubelet can optionally perform and react to three kinds of probes on running containers: livenessProbe - whether the container is running . kubelet kills the container according to its restart policy if the probe fails readinessProbe - whether the container is ready to respond to requests .The endpoints controller removes the Pod's IP from the Services that match the Pod, if the probe fails. startupProbe - whether the application within the container is started. Other probes are disabled until this probe succeeds. kubelet kills the container according to its restart policy if the probe fails Pods represent processes running on nodes in the cluster, it is important to allow those processes to gracefully terminate when they are no longer needed. The container runtime sends a TERM signal to the main process (PID=1) in each container. Once the PodTerminationGracePeriod has expired, the KILL signal is sent to any remaining processes , and the Pod is then deleted from the API Server. If the order of shutting down the containers within a Pod matters, consider using the preStop hook to synchronize . initContainers \u00b6 initContainers can contain utilities or setup scripts not present in an app image, which are run before the app containers are started. They always run to completion and must complete sequentially and successfully before next one starts. If one fails, the kubelet repeatedly restarts that init container until it succeeds unless the restartPolicy is Never, then the Pod fails to start and pending deletion. Init containers offer a mechanism to block or delay app container startup until a set of preconditions are met, and can securely run utilities or custom code that would otherwise make an app container image less secure, as they access to Secrets that app containers cannot access. You can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization. Directives related to \"Affinity\" control how Pods are scheduled - more packed or more scattered. PodPresets are API resource objects for injecting additional runtime information such as secrets, volumes, volume mounts, and environment variables, into pods at creation time. Pod template authors to not have to explicitly provide all information for every pod. Pod Disruptions \u00b6 Pods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system software error. Unavoidable Pod disruptions are involuntary disruptions . Examples: hardware failure from physical machine cluster admin deletes VM by mistake cloud provider hypervisor failure causes VM disappear kernel panic node disappears from the cluster due to network partition eviction of Pods due to out-of-resource errors Other cases are voluntary disruptions . Examples: deleting a deployment or the controller that manages the Pods updating a deployment's Pod template causing a restart directly deleting a Pod by accident cluster admin drain a node for maintenance removing a Pod from a node to free up resource To mitigate involuntary disruptions ensure the Pod requests just enough resources it needs replicate application for higher availability spread applications across racks or across zones A PodDisruptionBudget(PDB) limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions. However, deleting deployments or pods bypasses this rule. When a pod is evicted using the eviction API, it is gracefully terminated. As a cluster admin to perform a disruptive action on all the nodes in your cluster, some options: accept downtime during the upgrade failover to another complete replica cluster, maybe costly write disruption-rolerant applications and use PDBs, can be tricky on the application effort Ephemeral containers are intended to help troubleshoot a hard-to-reproduce bug through running within a Pod and inspect its state and run arbitrary commands. It is a way to run containers with many limitations and has to be added to a Pod through k8s API. Deployments \u00b6 A Deployment provides declarative updates for Pods and ReplicaSets . Creating a new Deployment creates a ReplicaSet, which in turn creates Pods per the PodTemplateSpec of the Deployment. With a Deployment: Scale up/down the Deployment Rollback the Deployment and apply an earlier ReplicaSet Make changes to PodTemplateSpec and rollout new a RelicaSet Schema Reference \u00b6 Deployment Template apiVersion : apps/v1 # required kind : Deployment # required metadata : # required name : nginx-deployment labels : # must match spec.template.metadata.labels app : nginx spec : # required minReadySeconds : 0 # default 0, time that newly created Pod should be # ready without any of its containers crashing # for it to be considered available progressDeadlineSeconds : 600 # default 600, time to wait for your Deployment to # progress before the system reports back that # the Deployment has failed progressing # must > minReadySeconds paused : false # default false, for pausing and resuming a Deployment replicas : 3 # default 1 revisionHistoryLimit : 10 # default 10, number of old ReplicaSets kept for rollback selector : # required matchLabels : # must match spec.template.metadata.labels app : nginx strategy : # strategy used to replace old Pods by new ones type : RollingUpdate # default RollingUpdate, also possible: Recreate rollingUpdate : maxUnavailable : 25% # default 25%, max number of Pods over desired replicas # that can be unavailable during the update process # can be an absolute number or percentage (rounding down) maxSurge : 25% # default 25%, max number of Pods that can be created # over desired replicas # can be an absolute number or percentage (rounding up) template : # required, aka Pod template, # same schema as Pod, apiVersion and kind excluded metadata : # required labels : app : nginx spec : # required containers : # required - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 restartPolicy : Always # default Always, also available: Never, OnFailure A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template or if the total number of such Pods exceeds .spec.replicas . For labels, make sure not to overlap with other controllers, otherwise the controllers won't behave correctly Frequently used commands kubectl apply -f <.yaml> kubectl get deployment kubectl describe deployment/<deployment_name> kubectl scale deployment/<deployment_name> --replicas = 10 kubectl autoscale deployment/<deployment_name> --min = 10 --max = 15 --cpu-percent = 80 kubectl rollout status deployment/<deployment_name> kubectl rollout history deployment/<deployment_name> kubectl rollout undo deployment/<deployment_name> --to-revision = 2 kubectl rollout pause/resume deployment/<deployment_name> kubectl edit deployment/<deployment_name> kubectl get rs RollingUpdate vs. Recreate \u00b6 A Deployment's revision is created when a Deployment's rollout is triggered. New revision is created each time the PodTemplate (.spec.templates) is changed. Set .spec.revisionHistoryLimit field (default 10) to specify how many old ReplicaSets for this Deployment you want to retain. During a RollingUpdate , Deployment controller creates new Pods per the new ReplicaSet and termiantes existing replicas in the active ReplicaSets, in a controlled rate in order to mitigate risk. With propertional scaling , Deployment controller decide where to add new replicas instead of adding all blindly to the new ReplicaSet. Parameters affecting this are maxSurge and maxUnavailable When setting .spec.strategy.type to Recreate , existing Pods are killed before new ones are created, which guarantee Pod termination previous to creation for upgrades. Successful removal is awaited before any Pod of the new revision is created. Deployment statuses \u00b6 A Deployment can have various states during its lifecycle: progressing, complete, fail to progress Progressing state: creating a new ReplicaSet scaling up newest ReplicaSet scaling down older ReplicaSet(s) wait new Pods to become Ready Complete state: all replicas updated to latest version per the PodTemplateSpec and Ready no old replicas present Failed state: progressing got stuck for a while and cannot complete by the .spec.progressDeadlineSeconds configured Possible causes: insufficient quota readiness probe fails image pull errors insufficient permissions application runtime misconfiguration causing container exits Kubernetes takes NO action on a stalled Deployment other than to report a status condition .status.conditions : \"Type=Progressing\" \"Status=False\" \"Reason=ProgressDeadlineExceeded\" Higher level orchestrators can take advantage of it and act accordingly, such as rollback to previous version ReplicaSet \u00b6 A ReplicaSet maintains a stable set of replica Pods running at any given time. When a ReplicaSet needs to create new Pods, it uses its Pod template . ReplicaSet and its Links with Pods \u00b6 A ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences field, which specifies what resource the current object is owned by. A ReplicaSet identifies new Pods to acquire by using its selector on Pods missing metadata.ownerReferences or the reference is an invalid controller, for which the reference is established. While you can create bare Pods , it is important to make sure they DO NOT have labels which match the selector of one of exiting ReplicaSets, otherwise they can be acquired by the ReplicaSets and got deleted (because exceeding the desired number of replica). You can remove Pods from a ReplicaSet ( orphan Pods ) by changing their labels so they do not match any ReplicaSet's selector. This technique may be used to remove Pods from service for debugging, data recovery, etc . Deployment is a higher-level resource that manages ReplicaSets, and you may never need to manipulate ReplicaSet objects directly. Use a Deployment instead, and define your application in the spec section. Schema Reference \u00b6 ReplicaSet Example apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend labels : app : guestbook tier : frontend spec : replicas : 3 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 Frequently used commands kubectl get rs kubectl delete rs/<rs_name> kubectl delete replicaset/<rs_name> kubectl delete replicaset/<rs_name> --cascade = orphan # keep the Pods managed by this ReplicaSet ReplicaSet as a Horizontal Pod Autoscaler (HPA) Target \u00b6 ReplicaSet can be auto-scaled by an HPA according to settings such as CPU usage, cron expression, etc. HPA Example apiVersion : autoscaling/v1 kind : HorizontalPodAutoscaler metadata : name : frontend-scaler spec : scaleTargetRef : kind : ReplicaSet name : frontend minReplicas : 3 maxReplicas : 10 targetCPUUtilizationPercentage : 50 Frequently used commands kubectl autoscale rs frontend --max = 10 --min = 3 --cpu-percent = 50 StatefulSets \u00b6 StatefulSets are like Deployments that manages the deployment and scaling of a set of Pods, while providing guarantees about the ordering and uniqueness of these Pods. Pods created are not interchangeable and each has a persistent identifier that it maintains across any rescheduling. StatefulSets are valuable for applications that require one or more of: Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates. Limitations/Behaviors \u00b6 Storage for a Pod must be provisioned by a PersistentVolumeProvisioner based on the storage class , or pre-provisioned by an admin Deleting/scaling down StatefulSet will NOT delete the volumes to ensure data safety A Headless Service is required for the network identity of the Pods Deletion of the Pods when a StatefulSet is deleted, is not guaranteed. try scale down the StatefulSet to 0 prior its deletion RollingUpdates with default podManagementPolicy as OrderedReady can run into a broken state that requires manual intervention Schema Reference \u00b6 StatefulSet Example apiVersion : apps/v1 # required kind : StatefulSet # required metadata : # required name : web spec : # required podManagementPolicy : OrderedReady # default OrderedReady, also possible: Parallel replicas : 3 # default 1 selector : # required matchLabels : # must match spec.template.metadata.labels app : nginx serviceName : \"nginx\" # must match the Service name template : # required metadata : # required labels : app : nginx spec : terminationGracePeriodSeconds : 10 # should be non-0 containers : # required - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html updateStrategy : type : RollingUpdate # default RollingUpdate, also possible: OnDelete, Partitions volumeClaimTemplates : # provide stable storage using PersistentVolumes - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"my-storage-class\" resources : requests : storage : 1Gi --- # Headless Service apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx StatefulSet Pods \u00b6 StatefulSet Pods have a unique identity that is comprised of an ordinal : each Pod assigned an integer ordinal from 0 up through N-1 , for N replicas. Pod will be named $(statefulset name)-$(ordinal) and getting a matching DNS subdomain $(podname).$(governing service domain) . The domain managed by the Headless Service has form $(service name).$(namespace).svc.$(cluster_domain) . cluster_domain will be cluster.local unless configured . Pod will also be added a label statefulset.kubernetes.io/pod-name with it name, for attaching a Service to specific Pods. Scaling \u00b6 StatefulSet being deployed will do updates to the Pods sequentially in the order from {0..N-1} ; and Pod N must wait ALL Pods N-1 and below to have started and available, before being created and replaced. Upon StatefulSet deletion, the Pods will be terminatated sequentially in the order from {N-1..0} ; and Pod N must wait ALL Pods N+1 and above to have completed terminated and deleted, before going into termination. Parallel Pod Management \u00b6 Specifying .spec.podManagementPolicy with Parallel will launch or terminate all Pods in parallel regardless the ordering. Update Strategies \u00b6 StatefulSet's .spec.updateStrategy field allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet. OnDelete \u00b6 OnDelete tells StatefulSet controller to not automatically update Pods; users must manually delete Pods to cause new Pods being created to reflect changes to .spec.template . RollingUpdate \u00b6 RollingUpdate strategy implements automated, rolling update for the Pods in a StatefulSet. Partitions \u00b6 Partitions is a setting under RollingUpdate, by specifying a .spec.updateStrategy.rollingUpdate.partition number. All Pods with an ordinal >= partition number will be updated when the StatefulSet's .spec.template is updated; the rest won't ever be updated even on deletion (recreated at previous version) DaemonSet \u00b6 DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them, and vice-versa. Deleting a DaemonSet will clean up the Pods it created. Typical use of DaemonSet: running a cluster storage daemon on every node running a logs collection daemon on every node running a node monitoring daemon on every node Schema Reference \u00b6 DaemonSet Template apiVersion : apps/v1 # required kind : DaemonSet # required metadata : # required name : fluentd-elasticsearch namespace : kube-system labels : k8s-app : fluentd-logging spec : # required selector : # required, if both types of matchers specified, ANDed matchLabels : # must match spec.template.metadata.labels name : fluentd-elasticsearch matchExpressions : ... # allows more sophisticated selectors by key or list values template : # required metadata : labels : name : fluentd-elasticsearch spec : containers : - name : fluentd-elasticsearch image : quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources : limits : memory : 200Mi requests : cpu : 100m memory : 200Mi volumeMounts : - name : varlog mountPath : /var/log - name : varlibdockercontainers mountPath : /var/lib/docker/containers readOnly : true affinity : # default unset, decide what nodes to schedule Pods nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchFields : - key : metadata.name operator : In values : - target-host-name nodeName : ... # default unset (schedule on all nodes) restartPolicy : Always # default Always, must be Always terminationGracePeriodSeconds : 30 tolerations : # have the daemonset runnable on master nodes # remove if your masters can't run pods - key : node-role.kubernetes.io/master effect : NoSchedule volumes : - name : varlog hostPath : path : /var/log - name : varlibdockercontainers hostPath : path : /var/lib/docker/containers Once a DaemonSet is created, do not edit its .spec.selector as it creates orphaned Pods. Instead, delete the old DaemonSet and deploy a new one if it is necessary to update the selector. Normally, the node that a Pod runs on is selected by the Kubernetes scheduler. DaemonSet pods are created and scheduled by the DaemonSet controller . By adding the NodeAffinity term to the DaemonSet pods, ScheduleDaemonSetPods allows you to schedule DaemonSets using the default scheduler instead of the DaemonSet controller. Jobs \u00b6 A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. Deleting a Job cleans up the Pods created. Job is used to run some task to completion, and run multiple tasks in parallel from multiple Pods. Parallel Jobs are good for processing of a set of independent but related work items. Parallel executions \u00b6 Non-parallel default Jobs only one Pod started, Job completes as soon as the Pod terminates successfully Parallel Jobs with a fixed completion count Multiple Pods started, each successful Pod termination get completion count increment by 1 Job completes when there is one successful Pod for each value in the range 1 to .spec.completions you may specify .spec.parallelism to speed up Job completion actual number of pods running in parallel will not exceed the number of remaining completions Parallel Jobs with a work queue specify .spec.parallelism , leave .spec.completions unset Multiple Pods started, Job completes only when at least one Pod terminates successfully and all Pods are terminated requires Pods coordinate amongst themselves or an external service to determine what each should work on requires each Pod independently capable of determining whether or not all its peers are done No new Pods scheduled once one of the Pods terminates successfully When running a Job in parallel, the Pods should be tolerant to concurrency. Schema Reference \u00b6 Job example apiVersion : batch/v1 # required kind : Job # required metadata : # required name : pi spec : # required activeDeadlineSeconds : 100 # default unset, Job must complete within this period ttlSecondsAfterFinished : 100 # default unset, time to delete Job after its completion # currently an Alpha feature, needs feature gate backoffLimit : 6 # default 6, fail a Job after some amount of retries # with an exponential back-off delay completions : 3 # default 1, run parallel jobs for fixed completion count parallelism : 3 # default 1, run parallel jobs with work queue manualSelector : false # only when manually specifying the selector # optional and best to omit selector : ... template : # required spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never # only Never or OnFailure allowed Frequently used commands kubectl get jobs kubectl get pods --selector = job-name = pi --output = jsonpath = '{.items[*].metadata.name}' Job termination \u00b6 When a Job completes, no more Pods are created, and the terminated Pods are around for viewing the logs and diagnostics. The job object also remains after it is completed. It must be manually cleaned up or set ttlSecondsAfterFinished for auto cleanup. Or switch to a CronJob instead. If .spec.activeDeadlineSeconds is set, the Job must complete within the period and will fail otherwise and terminate all its running Pods. CronJob \u00b6 A CronJob creates Jobs on a repeating schedule. One CronJob object is like one line of a crontab (cron table) file. All CronJob schedule: times are based on the timezone of the kube-controller-manager . CronJob name must be <= 52 chars because 11 chars are usually generated and the max length for name is 63 chars. A cron job creates a job object about once per execution time of its schedule. CronJobs scheduled should be idempotent since in rare circumstances it might be that 2 jobs are created or none created. Schema Reference \u00b6 CronJob Example apiVersion : batch/v1beta1 # required kind : CronJob # required metadata : # required name : hello spec : # required schedule : \"*/1 * * * *\" # required: minute[0-59], hour[0-23], day of the month[1-31], month[1-12], day of week[0-6](Sunday to Saturday) # consult tool at https://crontab.guru/ concurrencyPolicy : Allow # default Forbid startingDeadlineSeconds : 200 # default unset jobTemplate : # required spec : template : spec : containers : - name : hello image : busybox imagePullPolicy : IfNotPresent args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure CronJob limitations \u00b6 A CronJob is counted as missed if it has failed to create Job at its scheduled time. If concurrencyPolicy is set to Forbid and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed. CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs an error. If the startingDeadlineSeconds field is set, controller counts how many missed jobs occurred from this value rather than from the last scheduled time until now. If startingDeadlineSeconds is set to a large value or left unset (the default) and if concurrencyPolicy is set to Allow, the jobs will always run at least once . ReplicationController \u00b6 ReplicationController ensures that a specified number of pod replicas are running and available at any time . Extra Pods are terminated to match the desired number of Pods and vice-versa. ReplicationController supervises multiple pods across multiple nodes . The ReplicationController is designed to facilitate simple rolling updates to a service by replacing pods one-by-one , and does NOT perform readiness nor liveness probes. ReplicationController is intended to be managed by external auto-scaler and not he HPA and used as a composable building-block primitive. Schema Reference \u00b6 ReplicationController example apiVersion : v1 # required kind : ReplicationController # required metadata : # required name : nginx spec : # required replicas : 3 # default 1 selector : # required app : nginx # default spec.template.metadata.labels and must match template : # required metadata : name : nginx labels : app : nginx spec : containers : - name : nginx image : nginx ports : - containerPort : 80 restartPolicy : Always # default Always and must be Always Garbage Collector \u00b6 Kubernetes garbage collector (GC) deletes certain objects that no longer have an owner . The k8s GC does NOT delete bare Pods or orphan Pods since they are not the case of losing the ownership. Ownership \u00b6 Objects can own other objects within the same namespace. Owned objects are dependents of the owner object and must have a metadata.ownerReferences field point to its owner. The metadata.ownerReferences is automatically set for objects created or adopted by ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job, and CronJob. It can also be manually set on objects to establish ownership. Schema Reference \u00b6 Ownership Example apiVersion : v1 kind : Pod metadata : ... ownerReferences : - apiVersion : apps/v1 controller : true blockOwnerDeletion : true kind : ReplicaSet name : my-repset uid : d9607e19-f88f-11e6-a518-42010a800195 ... Cascading Deletion \u00b6 When you delete an object, the object's dependents are also deleted automatically if Cascading Deletion is enabled, otherwise the dependents are orphaned . Foreground \u00b6 In foreground cascading deletion, root object enters a \"deletion in progress\" state: object still visible in API object deletionTimestamp is set object metadata.finalizers contains \" foregroundDeletion \" next, GC deletes root object's dependents once ALL dependents are deleted, GC deletes the root object only blocked by dependents with ownerReference.blockOwnerDeletion=true Background \u00b6 In background cascading deletion, Kubernetes deletes the owner object immediately and the garbage collector then deletes the dependents in the background . Schema Reference \u00b6 The json example is for when calling k8s API to delete an object while setting the deletion options. DeleteOptions example { \"kind\" : \"DeleteOptions\" , \"apiVersion\" : \"v1\" , \"propagationPolicy\" : \"Orphan\" // also possible : Foregrou n d , Backgrou n d } Frequently used commands # call k8s API kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\" # directly use kubectl delete kubectl delete replicaset my-repset --cascade = orphan TTL Controller \u00b6 TTL controller provides a TTL ( time to live ) mechanism to limit the lifetime of resource objects that have finished execution . It only handles Jobs for now, specified with .spec.ttlSecondsAfterFinished , and it is currently an alpha feature. The .spec.ttlSecondsAfterFinished of a Job can be modified after the resource is created or has finished. The Job's existance will NOT be guaranteed after its TTL (if set) expires. TTL controller will assume that a resource is eligible to be cleaned up when its TTL has expired, and will delete it cascadingly. Services, LB, Networking \u00b6 Service \u00b6 Service is an abstract way to expose an application running on a set of Pods as a network service . It defines a logical set of Pods (governed by a selector) and a policy by which to access them. Kubernetes creates Endpoint objects and associates them with the Service objects. This is done automatically when a Service has a selector defined. Kubernetes gives Pods their own IP addresses. With Service, Kubernetes assigns this Service an IP address ( cluster IP ) and a single DNS name to represent the set of Pods matching its selector, and can load-balance among the Pods. Schema Reference \u00b6 Service template apiVersion : v1 kind : Service metadata : name : my-service spec : type : ClusterIP # default ClusterIP, also possible: NodePort, LoadBalancer, ExternalName externalName : ... # required only when using ExternalName as type clusterIP : ... # optional and only when specifying your own cluster IP address # must be a valid IPv4 or IPv6 address from within # the service-cluster-ip-range CIDR range configured # for the API server selector : app : MyApp sessionAffinityConfig:# default unset clientIP : # pass particular client request to the same Pod each time timeoutSeconds : 600 ports : - name : http # required if specifying multiple ports protocol : TCP # default TCP, also possible: UDP, SCTP (alpha), HTTP, PROXY port : 80 # incoming port targetPort : 9376 # default same as port, the target port within Pod nodePort : 30007 # optional and only when using NodePort as type externalIPs : # optional and only when forcing on external nodes - 80.11.12.11 status : loadBalancer : # available when using LoadBalancer and after being provisioned ingress : - ip : 192.0.2.127 If you created a Service without a selector , you need to manually map the Service to the network address and port where it's running, by adding an Endpoint object manually. Endpoint template apiVersion : v1 kind : Endpoints metadata : name : my-service subsets : - addresses : - ip : 192.0.2.42 ports : - port : 9376 Note The endpoint IPs must NOT be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6). Endpoint IP addresses CANNOT be the cluster IP s of other Kubernetes Services, because kube-proxy does NOT support virtual IPs as a destination. kubectl port-forward --address 0.0.0.0 8080:8080 ServiceType \u00b6 Service's ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP. ClusterIP - Exposes the Service on a cluster-internal (virtual) IP. Service is only reachable from within the cluster NodePort - Exposes the Service on each Node's IP at a static port Service reachable from outside the cluster, by requesting <NodeIP>:<NodePort> . The port allocated is from a range specified by --service-node-port-rang , default 30000-32767 reported to the Service object in .spec.ports[*].nodePort can request specific port number from the valid range with nodePort , but need to take care of port collision yourself Use --nodeport-addresses in kube-proxy to specify only particular IP block(s) to proxy the port A ClusterIP is created to be routed from the NodePort Service. each node proxies that port into this Service LoadBalancer - Exposes the Service externally using a cloud provider 's load balancer. A NodePort and a ClusterIP is created to be routed from the external load balancer NodePort allocation can be disabled via an alpha feature ServiceLBNodePortControl with spec.allocateLoadBalancerNodePorts=false Internal load balancer support see here Actual creation of the load balancer happens asynchronously and available after it is provisioned, in .status.loadBalancer field Some cloud providers allow you to specify the loadBalancerIP All ports must have the same protocol on the LoadBalancer Service available protocol is defined by the cloud provider alpha feature MixedProtocolLBService allows different protocols ExternalName - Maps the Service to the contents of the .spec.externalName field Value should be a canonical DNS name Returning a CNAME record with its value NO proxy is set up Read more Alternative to a Service is the Ingress. It acts as the entry point for your cluster and lets you consolidate your routing rules into a single resource as it can expose multiple services under the same IP address. If there are external IP s that route to one or more cluster nodes , Kubernetes Services can be exposed on these nodes via .spec.externalIPs Application protocol \u00b6 This is an alpha feature, by specifying appProtocol , you can specify an application protocol for each Service port. For example, an application can serve 8889 with ftp and 8090 with smtp. The value must be valid IANA standard service names Virtual IPs and proxies \u00b6 Every node in a Kubernetes cluster runs a kube-proxy , which is responsible for implementing a form of virtual IP (cluster IP) for Services of type other than ExternalName . kube-proxy uses iptables (packet processing logic in Linux) to define virtual IP addresses which are transparently redirected as needed. kube-proxy can run in several modes. User space mode \u00b6 In this mode, kube-proxy on each node watches the Kubernetes control plane for the addition and removal of Service and Endpoint objects. For each new Service it opens a randomly chosen proxy port on the local node. It then adds new iptables rules which capture traffic to the Service's clusterIP and port ( request -> clusterIP:port -> nodeIP:proxy_port ), which redirect that traffic to the proxy port which proxies to the Service's backend Pod (through Endpoints), using a round-robin algorithm. Service owners can choose ANY port they want without risk of collision. The .spec.sessionAffinity setting on the Service affects which Pod gets the traffic at any moment. iptables mode \u00b6 In this mode, kube-proxy also does the watches on the Service and Endpoint objects and install iptable rules which capture traffic to the Service's clusterIP and port. Additionally, it installs iptables rules which redirect from the clusterIP and port to per-Service rules, which further link to backend Pods for each Endpoint object using destination NAT. When a Service is accessed, kube-proxy redirects the packets to a backend Pod at random or via session affinity . This mode is likely more reliable since traffic is handled by Linux netfilter without the need to switch between userspace and the kernel space . Packets are never copied to userspace, the kube-proxy does NOT have to be running for the virtual IP address to work, and Nodes see traffic arriving from the unaltered client IP address . However if traffic comes in through a node-port or through a load-balancer, the client IP does get altered. IPVS mode \u00b6 In this mode, kube-proxy watches Kubernetes Services and Endpoints, calls netlink interface to create IPVS rules accordingly and synchronizes IPVS rules with Kubernetes Services and Endpoints periodically . When accessing a Service, IPVS directs traffic to one of the backend Pods. This mode is based on netfilter hook function that is similar to iptables mode, but uses a hash table as the underlying data structure and works in the kernel space . This mode is likely better, serves with lower latency, higher throughput, better performance in synchronization of rules, and provides more load-balancing algorithms. IPVS kernel modules must be installed before enabling kube-proxy with this mode. Note Both the userspace proxy mode and the iptables proxy mode obscures the source (external client) IP address of a packet accessing a Service. One way to fight this is to add a proxy before Kubernetes Network and filtering each request and add source IP address as a header. Discovering services \u00b6 Env Vars \u00b6 When a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service : {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables, where the Service name is upper-cased and dashes are converted to underscores. i.e. redis-master which exposes TCP port 6379 will be available as: REDIS_MASTER_SERVICE_HOST = 10 .0.0.11 REDIS_MASTER_SERVICE_PORT = 6379 REDIS_MASTER_PORT = tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP = tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO = tcp REDIS_MASTER_PORT_6379_TCP_PORT = 6379 REDIS_MASTER_PORT_6379_TCP_ADDR = 10 .0.0.11 Note When you have a Pod that needs to access a Service via the environment variables, you must create the Service before the client Pods come into existence. DNS \u00b6 You should ALWAYS set up a DNS service for your Kubernetes cluster using an add-on , such as CoreDNS. Doing so will allow Pods within the same namespace as a Service to lookup its IP using its DNS name. Pods within a different namespace can also do with its full DNS name. The Kubernetes DNS server is the ONLY way to access ExternalName Services . Headless Services \u00b6 When you do NOT need load-balancing and Service IP, you can create a Headless Service by explicitly specifying \" None \" for the cluster IP ( .spec.clusterIP ). For headless Services that define selectors , the endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return records (addresses) that point directly to the Pods backing the Service. For those that do not define selectors, no Endpoints created. However, the DNS system looks for and configures either: CNAME records for ExternalName-type Services A records for any Endpoints that share a name with the Service, for all other types. Service Topology \u00b6 Service Topology enables a service to route traffic based upon the Node topology of the cluster . A service can specify that traffic be preferentially routed to endpoints that are on the same Node as the client, or in the same availability zone . Service creator can define a policy for routing traffic based upon the Node labels for the originating and destination Nodes. You can control Service traffic routing by specifying the .spec.topologyKeys field on the Service object, which should be a preference-ordered list of Node labels to be used to sort endpoints when accessing the Service. If no match to any of the Node labels is found, the traffic will be rejected . The special value * may be used to mean \"any topology\" and only makes sense as the last value in the list, if used. If .spec.topologyKeys is not specified or empty, NO topology constraints will be applied. Consider a cluster with Nodes that are labeled with their hostname, zone name, and region name . Then you can set the topologyKeys values of a service to direct traffic: Only to endpoints on the same node, failing if no endpoint exists on the node: [\"kubernetes.io/hostname\"] Preferentially to endpoints on the same node, falling back to endpoints in the same zone, followed by the same region, and failing otherwise: [\"kubernetes.io/hostname\", \"topology.kubernetes.io/zone\", \"topology.kubernetes.io/region\"] Preferentially to the same zone, fallback on any available endpoint otherwise: [\"topology.kubernetes.io/zone\", \"*\"] Valid topology keys are currently limited to kubernetes.io/hostname , topology.kubernetes.io/zone , and topology.kubernetes.io/region YAML Reference \u00b6 Service Topology example apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : my-app ports : - protocol : TCP port : 80 targetPort : 9376 topologyKeys : - \"kubernetes.io/hostname\" - \"*\" Kubernetes DNS \u00b6 Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelets to tell individual containers to use the DNS Service's IP to resolve DNS names. Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name . By default, a client Pod's DNS search list will include the Pod's own namespace and the cluster's default domain . for Services \u00b6 Normal Services are assigned a DNS A or AAAA record , depending on the IP family of the service, in the form of svc-name.namespace.svc.cluster-domain.example , which resolves to the cluster IP of the Service. A Headless Service gets the same record, except that it resolves to the set of IPs of the pods selected by the Service. SRV records \u00b6 SRV Records are created for named ports that are part of normal or Headless Services, in the form of _port-name._port-protocol.svc-name.namespace.svc.cluster-domain.example . For a normal Service, it resolves to the domain name and the port number, svc-name.namespace.svc.cluster-domain.example:port . For a Headless Service, it resolves to multiple answers, one domain and port for each pod that is backing the service, auto-generated-name.svc-name.namespace.svc.cluster-domain.example:port . for Pods \u00b6 A/AAAA records \u00b6 A normal Pod gets DNS resolution in the form of pod-ip-address.namespace.pod.cluster-domain.example . Any pods created by a Deployment or DaemonSet exposed by a Service have the DNS resolution in the form of pod-ip-address.deployment-name.namespace.svc.cluster-domain.example . hostnmae and subdomain \u00b6 When a Pod is created, its hostname is the Pod's metadata.name value. There is also .spec.hostname to be used to specify the Pod's hostname, which takes precedence over the Pod's name. The Pod also has an optional subdomain field .spec.subdomain which can be used to specify its subdomain . Then its fully qualified domain name ( FQDN ) becomes hostname.subdomain.namespace.svc.cluster-domain.example Pod needs to become READY in order to have a record unless publishNotReadyAddresses=True is set on the Service. setHostnameAsFQDN \u00b6 When a Pod is configured to have FQDN, its hostname command returns the short hostname set on the Pod, and the hostname --fqdn command returns the FQDN. When you set .spec.setHostnameAsFQDN=true on the Pod, both hostname and hostname --fqdn return the Pod's FQDN. Note In Linux, the hostname field of the kernel (the nodename field of struct utsname) is limited to 64 characters . If a Pod enables .spec.setHostnameAsFQDN and its FQDN is longer than 64 character, it will stuck in Pending(ContainerCreating) status. One way to solve this issue is to create an admission webhook controller to control FQDN size when users create top level objects ike Deployments. DNS Policy \u00b6 DNS policies can be set on a per-pod basis via .spec.dnsPolicy : Default - Pod inherits the DNS name resolution configuration from the node that the pods run on ClusterFirst (default policy) - Any DNS query that does NOT match the configured cluster domain suffix , such as \"www.kubernetes.io\", is forwarded to the upstream DNS nameserver inherited from the node Cluster administrators may have extra stub-domain and upstream DNS servers configured ClusterFirstWithHostNet - only for Pods running with hostNetwork None - allows Pod to ignore DNS settings from the Kubernetes environment All DNS settings are expected to be provided using the .spec.dnsConfig field in the Pod Spec DNS Config \u00b6 Pod's DNS Config is optional but it allows users more control on the DNS settings for a Pod. It will be required if .spec.dnsPolicy=None . Some properties: nameservers - a list of DNS servers' IP addresses at most 3 can be provided optional unless .spec.dnsPolicy=None the list will be combined to the base nameservers generated from the DNS Policy searches - a list of DNS search domains for hostname lookup in the Pod. at most 6 can be provided optional the list will be combined to the base search domains generated from the DNS Policy options - a list of objects where each object may have a name property (required) and a value property (optional) optional the list will be combined to the base options generated from the DNS Policy DNS Config Example apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 1.2.3.4 searches : - ns1.svc.cluster-domain.example - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0 Will generate /etc/resolv.conf : nameserver 1.2.3.4 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0 EndpointSlices \u00b6 EndpointSlices allow for distributing network endpoints across multiple resources . Each slice can hold upto 100 endpoints.","title":"K8s Knowledge"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#breif-kubernetes-overview","text":"Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. k8s provides: Service discovery and load balancing expose a container using DNS name or IP address balance loads among many instances of a service Storage orchestration mount storage from local or other cloud providers Automated rollouts and rollbacks describe desired state of containers and have k8s monitor it Automatic bin packing k8s can make the best use of resources by fitting containers among the nodes Self-healing k8s manages the avaibility, restart, replacement, and termination of containers Secret and configuration management k8s manages and stores sensitive info k8s is not a traditional Platform as a Service system.","title":"Breif Kubernetes Overview"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kubernetes-components","text":"When you deploy Kubernetes, you get a cluster . At an enterprise grade k8s deployment, there would be multiple clusters for fault-tolerance and high availablility . These components exist within a cluster: nodes - a number of worker machines control plane - a set of programs that together manages the nodes and the Pods may span across multiple physical machines consists of: kube-apiserver, etcd, kube-scheduler, kube-controller-manager, cloud-controller-manager","title":"Kubernetes Components"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#control-plane-components","text":"The control plane's components can be run on any machine in the cluster and they make global decisions about the cluster as well as detecting and responding to cluster events . See Building High-Availability Clusters for an example multi-master-VM setup.","title":"Control Plane Components"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kube-apiserver","text":"The apiserver is the front end for the k8s control plane and it is designed to scale horizontally. Multiple kube-apiserver can be deployed to balance traffic. kube-apiserver serves REST http requests which can be consumed from apps built by any languages. kubectl and kubeadm also consumes this API. List of client libraries for consuming k8s API. Full k8s API reference is available, but it is better to use the client libraries.","title":"kube-apiserver"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#etcd","text":"etcd provides a consistent and highly-available key value store for all cluster data . K8s allows you to use alternative key-value store for the cluster data. Be sure to have back up plan for the data no matter which store you use. etcd official documentation","title":"etcd"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kube-scheduler","text":"kube-scheduler watches for newly created Pods with no assigned node and selects a node for them to run on. Some decision-making factors : collective resource requirements hardware/software/policy constraints affinity and anti-affinity specs data locality inter-workload interference deadlines","title":"kube-scheduler"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kube-controller-manager","text":"kube-controller-manager runs controller processes, combined in a single binary. Node controller - notices and reports when nodes go down Replication controller - maintains the correct number of pods for every replication controller object Endpoints controller - populates the endpoints object (joins Services & Pods) Service Account & Token controller - creates default accounts and API access tokens for new namespaces","title":"kube-controller-manager"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cloud-controller-manager","text":"cloud-controller-manager has cloud-specific control logic. Lets you link your cluster into your external cloud provider's API. Only runs controllers that are specific to your cloud provider.","title":"cloud-controller-manager"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#node-components","text":"Node components run on every node , maintaining running pods and providing the Kubernetes runtime environment.","title":"Node Components"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kubelet","text":"kubelet makes sure containers are running healthy in a Pod described by the PodSpecs . It doesn't manage containers that were not create by Kubernetes.","title":"kubelet"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kube-proxy","text":"kube-proxy is a network proxy that maintains network rules, implementing part of the Kubernetes Service concept. It allows communication to Pods inside or outside the cluster.","title":"kube-proxy"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#container-runtime","text":"container runtime is the software responsible for running containers. Typically are one of: Docker, containerd, CRI-O , or any that implements Kubernetes CRI (Container Runtime Interface)","title":"container runtime"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#addons","text":"Addons use Kubernetes resources to implement cluster features .","title":"Addons"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cluster-dns","text":"A DNS server that serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server.","title":"Cluster DNS"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#web-ui-dashboard","text":"The general web UI for clusters allow manage and troubleshoot applications running in the cluster.","title":"Web UI Dashboard"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#container-resource-monitoring","text":"Records generic time-series metrics about containers .","title":"Container Resource Monitoring"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cluster-level-logging","text":"Saving container logs to a central log store with search/browsing interface.","title":"Cluster-level logging"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#the-kubernetes-api","text":"The control plane is the apiserver and it exposes a HTTP API for querying and manipulating the state of objects in Kubernetes. Tools like kubectl and kubeadm use this API. You can also consume it with REST calls. Client libraries exist for many languages for consuming the API. Extend Kubernetes API by adding CustomResourceDefinition .","title":"The Kubernetes API"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kubernetes-objects","text":"Kubernetes objects are persistent entities to represent state of your cluster . what applications are running the resources available to the applications policies around how the applications behave Kubernetes objects are represented in the Kubernetes API and can be expressed in .yaml format. Kubernetes API consumes the converted JSON of the configs specified in the yaml file. Almost every k8s object includes spec and status . spec describes the desired state for the resource, and status describes the current state of the object. spec is done by you when the resource is first created, and status is updated by k8s. metadata is used to identify the resource . Required fields: apiVersion - the version of the k8s API used when creating this object kind - what kind of object metadata - object identifier, usually a name, UID, and optional namespace each object has a unique name for that specific type of resource in one cluster for name restriction check here each object has a unique UID that is unique across the whole cluster generated by k8s spec - desired state for the object precise spec is different for each k8s object","title":"Kubernetes Objects"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#namespaces","text":"Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces . Namespaces are intended for use in environments with many users spread across multiple teams, or projects . Namespaces provide a scope for names . Names of resources need to be unique within a namespace, but not across namespaces. Namespaces provide a way to divide cluster resources between multiple users via resource quota . Some resources are in a namespace and some are not. To find out: # In a namespace kubectl api-resources --namespaced = true # Not in a namespace kubectl api-resources --namespaced = false","title":"Namespaces"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#labels-and-selectors","text":"labels , under metadata , are key-value pairs that are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users , but do not directly imply semantics to the core system. Labels can be attached to objects at creation time and added/modified any time and can be used to organize and to select subsets of objects. Labels are made up from an optional prefix and a name , connected by a slash . labels are NOT unique among multiple objects prefix must be a DNS subdomain if prefix is omitted, label key is presumed to be private to the user automated system components must specify a prefix name must be less or equals to 63 chars length , starts and ends with alphanumerics and can have dashes, underscores, or dots between. Using a label selector to identify a set of objects, which is the core grouping primitive in k8s. A label selector can be made of multiple requirements of comma-separated string , for which all must satisify. A label selector requirement can be specified with equality-based operators, =, ==, != for inequality, resources without the label key ( null ) will also be selected. for objects definitions in JSON or YAML, some only support equality requirement selectors and some supports set requirements see https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#resources-that-support-set-based-requirements A label selector requirement can also be a set of values with operators, in, notin, exists i.e. environment in (prod, qa) , tier notin (fe, be) , partition (only the key for exists), !partition (opposite of exists) Common labels Key Description Example Type app.kubernetes.io/name The name of the application mysql string app.kubernetes.io/instance A unique name identifying the instance of an application mysql-abcxzy string app.kubernetes.io/version The current version of the application (e.g., a semantic version, revision hash, etc.) 5.7.21 string app.kubernetes.io/component The component within the architecture database string app.kubernetes.io/part-of The name of a higher level application this one is part of wordpress string app.kubernetes.io/managed-by The tool being used to manage the operation of an application helm string","title":"Labels and Selectors"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#annotations","text":"annotations are not used to identify and select objects. It serves as additional information for the application or for debugging purposes. The metadata in an annotation can be small or large, structured or unstructured, and CAN include characters not permitted by labels. The naming restrictions of annotation keys are the SAME as that for label keys.","title":"Annotations"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#field-selectors","text":"field selectors allow selecting resources based on the value of one or more resource fields . This is broader than the label selector, but it works only on API and can't be specified in the resource itself to select other objects. Supported field selectors vary by the resource type. Using unsupported field selectors produces an error. Operators =, ==, != can be used; multiple requirements can be combined with a comma. i.e. metadata.name=my-service,status.phase!=Running","title":"Field Selectors"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cluster-architecture","text":"","title":"Cluster Architecture"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#nodes","text":"Kubernetes runs workload by placing containers into Pods that run on Nodes . A node can be a virtual or physical machine. Each node contains services necessary to run Pods which are managed by the control plane . A node has components: kubelet , a container runtime , and the kube-proxy . A node can be added to the k8s apiserver either by the kubelet self-register or by a mnaully-created Node resource object, for which the control plane server will validate and healthcheck. The Node object must have a valid DNS subdomain name . Node can be marked unscheduleable before maintenance or upgrades. The node lifecycle controller automatically creates taints that represent conditions . The Pod scheduler takes the Node's taints into consideration when assigning a Pod to a Node. Pods can also have tolerations which let them tolerate a Node's taints. The node controller is responsible for assigning Classless Inter-Domain Routing (CIDR) block to the node when it is registered, keeping the node controller's internal list of nodes up-to-date with the list of available machines , and monitoring the nodes' health . For reliability , spread your nodes across availability zones so that the workload can be shifted to healthy zones when one entire zone goes down.","title":"Nodes"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#control-plane-node-communication","text":"All API usage from nodes terminate at the apiserver . The apiserver is configured to listen for remote connections on a secure HTTPS port (typically 443) with one or more forms of client authentication enabled. Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the apiserver along with valid client credentials. Pods that wish to connect to the apiserver can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in all namespaces) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the apiserver. Control Plane can talk to the nodes from the apiserver to the kubelet process on each node in the cluster, or from the apiserver to any node, pod, or service through the apiserver's proxy. Kubernetes supports SSH tunnels to protect the control plane to nodes communication paths but it is deprecated. Konnectivity service is the replacement for this communication channel.","title":"Control Plane &lt;--&gt; Node Communication"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#controllers","text":"In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. A controller might handle the action itself or messages the apiserver to do so. Built-in controllers run inside the kube-controller-manager and manage state by interacting with the cluster apiserver. Controllers that interact with external state find their desired state from the apiserver, then communicate directly with an external system to bring the current state closer in line. There can be several controllers that create or update the same kind of object. Behind the scenes, Kubernetes controllers make sure that they only change the resources linked to their controlling resource. You can find controllers that run outside the control plane, to extend Kubernetes. Or, if you want, you can write a new controller yourself. You can run your own controller as a set of Pods, or externally to Kubernetes (via CustomResourceDefinition).","title":"Controllers"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cloud-controller-manager_1","text":"The cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic and allow linking your cluster into your cloud provider's API. It works like an adaptor . The cloud-controller-manager is structured using a plugin mechanism that allows different cloud providers to integrate their platforms with Kubernetes. The cloud controller manager runs in the control plane as a replicated set of processes and each cloud-controller-manager implements multiple controllers in a single process. It can also run as a k8s addon rather than part of the control plane.","title":"Cloud Controller Manager"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#containers","text":"","title":"Containers"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#images","text":"A container image is a ready-to-run software package , containing everything needed to run an application: the code and any runtime it requires, application and system libraries, and default values for any essential settings. An image makes very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a Pod. An image name can include a registry hostname and an optional port number. If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry. After the image name part you can add a tag to identify different versions of that image. If you don't specify a tag, Kubernetes assumes you mean the tag latest. Avoid using latest tag when deploying containers in production, as it is harder to track which version of the image is running and more difficult to roll back to a working version. Use a pinned version instead. A container registry can also serve a container image index to allow different systems to fetch the right binary image for the machine architecture they are using. Kubernetes itself typically names container images with a suffix -$(ARCH). A container is immutable : you cannot change the code of a container that is already running. To do so you need to build a new image that includes the change, then recreate the container to start from the updated image. The container runtime is the software that is responsible for running containers. K8s suports Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI.","title":"Images"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#container-environment","text":"The resources available to containers in the container environment : filesystem , a combination of image and volumes information about the container i.e. env variables like Pod name and namespace, other user defined env variables and others statically specified in the image information about other objects in the cluster a list of all services that were running when the container is created, available as env variables i.e. FOO_SERVICE_HOST and FOO_SERVICE_PORT is available as env variables for a service named foo that maps to that container","title":"Container Environment"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#runtime-class","text":"RuntimeClass is a feature for selecting the container runtime configuration, which is used to run a Pod's containers. You can set a different RuntimeClass between different Pods to balance performance and security . You can also use RuntimeClass to run different Pods with the same container runtime but with different settings.","title":"Runtime Class"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#container-lifecycle-hooks","text":"kubelet managed containers can use the container lifecycle hook framework to run code triggered by events during their management lifecycle. The hooks enable containers to be aware of events in their management lifecycle and run code implemented in a handler when the corresponding lifecycle hook is executed. Two hooks exposed to containers: PostStart - executed immediately after a container is created PreStop - called immediately before a container is terminated due to an API request or management event such as liveness probe failure , preemption , resource contention , cordon , container exit , and others. Containers can access a hook by implementing and registering a handler for that hook. Two types of hook handlers: exec - executes a specific command or script , such as pre-stop.sh, inside the cgroups and namespaces of the container http - executes an HTTP request against a specific endpoint on the container Hook handler calls are synchronous within the context of the Pod containing the container. The hook need to finish before the container reach a running state. Users should make their hook handlers as lightweight as possible . PostStart or PreStop hook failures kills the container . Hook delivery is intended to be at least once and generally only once . It doesn't get resend if delivery fails. If a handler fails for some reason, it broadcasts an event, which is viewable by kubectl describe .","title":"Container Lifecycle Hooks"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#workloads","text":"A workload is an application running on Kubernetes, usually in a set of Pods . A Pod represents a set of running containers on your cluster. You can use workload resources that manage a set of Pods on your behalf. Workload resources: Deployment and ReplicaSet StatefulSet DaemonSet Job and CronJob Garbage collection tidies up objects from your cluster after their owning resource has been removed. The time-to-live after finished controller removes Jobs once a defined time has passed since they completed. Once your application is running, you might want to make it available on the internet as a Service or, for web application only, using an Ingress .","title":"Workloads"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#pods","text":"Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod is a group of one or more containers , with shared storage/network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \" logical host \": it contains one or more application containers which are relatively tightly coupled . A Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging if your cluster offers this. Init containers run and complete before the app containers are started. The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation. Pods are generally used to run a single container or run multiple containers that need to work together. For example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The containers in a Pod are automatically co-located and co-scheduled on the same physical or virtual machine in the cluster. The containers can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated. A Pod is not a process, but an environment for running container(s) . A Pod persists until it is deleted. Within a Pod, containers share an IP address and port space , and can find each other via localhost. The containers in a Pod can also communicate with each other using standard inter-process communications like SystemV semaphores or POSIX shared memory. Containers that want to interact with a container running in a different Pod can use IP networking to communicate.","title":"Pods"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference","text":"Pod Example apiVersion : v1 kind : Pod metadata : name : pod-example spec : containers : - name : ubuntu image : ubuntu:trusty command : [ \"echo\" ] args : [ \"Hello World\" ] Frequently used commands kubectl get pods kubectl get pods --show-labels kubectl describe pod/<pod> kubectl exec -it pod/<pod> -c <container> <command> kubectl logs pod/<pod> -c <container> Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. The main use for static Pods is to run a self-hosted control plane (or manage its components).","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#pod-lifecycle","text":"Pod lifecycle : starting in the Pending phase, moving through Running if at least one of its primary containers starts OK, and then through either the Succeeded or Failed phases depending on whether any container in the Pod terminated in failure. Pods are considered to be relatively ephemeral (rather than durable) entities. Pods do not, by themselves, self-heal. If a Pod is scheduled to a node that then fails, or if the scheduling operation itself fails, the Pod is deleted. A given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead, that Pod can be replaced by a new, near-identical Pod, with even the same name if desired, but with a different UID. The same rule applies to the volumes attached to a given Pod. You can use container lifecycle hooks to trigger events to run at certain points in a container's lifecycle. Pod has three states: Waiting - a container is running the operations it requires in order to complete start up Running - a container is executing without issues, and the postStart hook has been executed and finished Terminated - a container has begun execution and either ran to completion or failed for some reason. The preStop hook is executed and finished before a container enters the Terminated state A Pod's restartPolicy controls restarts of Terminated containers. A Pod has PodStatus , which contains an array of PodConditions which the Pod has/hasn't passed: PodScheduled - the Pod has been scheduled to a node ContainersReady - all containers in the Pod are ready Initialized - all init containers have started successfully Ready - the Pod is able to serve requests and should be added to the load balancing pools of all matching Services","title":"Pod lifecycle"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#probe","text":"A Probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet calls a Handler implemented by the container. There are three types of handlers: ExecAction - Executes a specified command or script inside the container and expects it exits with 0 TCPSocketAction - Performs a TCP check against the Pod's IP address on a specified port and expects the port to be open HTTPGetAction - Performs an HTTP GET request against the Pod's IP address on a specified port and path and expect response status code, x, to be 200 <= x < 400 The kubelet can optionally perform and react to three kinds of probes on running containers: livenessProbe - whether the container is running . kubelet kills the container according to its restart policy if the probe fails readinessProbe - whether the container is ready to respond to requests .The endpoints controller removes the Pod's IP from the Services that match the Pod, if the probe fails. startupProbe - whether the application within the container is started. Other probes are disabled until this probe succeeds. kubelet kills the container according to its restart policy if the probe fails Pods represent processes running on nodes in the cluster, it is important to allow those processes to gracefully terminate when they are no longer needed. The container runtime sends a TERM signal to the main process (PID=1) in each container. Once the PodTerminationGracePeriod has expired, the KILL signal is sent to any remaining processes , and the Pod is then deleted from the API Server. If the order of shutting down the containers within a Pod matters, consider using the preStop hook to synchronize .","title":"Probe"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#initcontainers","text":"initContainers can contain utilities or setup scripts not present in an app image, which are run before the app containers are started. They always run to completion and must complete sequentially and successfully before next one starts. If one fails, the kubelet repeatedly restarts that init container until it succeeds unless the restartPolicy is Never, then the Pod fails to start and pending deletion. Init containers offer a mechanism to block or delay app container startup until a set of preconditions are met, and can securely run utilities or custom code that would otherwise make an app container image less secure, as they access to Secrets that app containers cannot access. You can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization. Directives related to \"Affinity\" control how Pods are scheduled - more packed or more scattered. PodPresets are API resource objects for injecting additional runtime information such as secrets, volumes, volume mounts, and environment variables, into pods at creation time. Pod template authors to not have to explicitly provide all information for every pod.","title":"initContainers"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#pod-disruptions","text":"Pods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system software error. Unavoidable Pod disruptions are involuntary disruptions . Examples: hardware failure from physical machine cluster admin deletes VM by mistake cloud provider hypervisor failure causes VM disappear kernel panic node disappears from the cluster due to network partition eviction of Pods due to out-of-resource errors Other cases are voluntary disruptions . Examples: deleting a deployment or the controller that manages the Pods updating a deployment's Pod template causing a restart directly deleting a Pod by accident cluster admin drain a node for maintenance removing a Pod from a node to free up resource To mitigate involuntary disruptions ensure the Pod requests just enough resources it needs replicate application for higher availability spread applications across racks or across zones A PodDisruptionBudget(PDB) limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions. However, deleting deployments or pods bypasses this rule. When a pod is evicted using the eviction API, it is gracefully terminated. As a cluster admin to perform a disruptive action on all the nodes in your cluster, some options: accept downtime during the upgrade failover to another complete replica cluster, maybe costly write disruption-rolerant applications and use PDBs, can be tricky on the application effort Ephemeral containers are intended to help troubleshoot a hard-to-reproduce bug through running within a Pod and inspect its state and run arbitrary commands. It is a way to run containers with many limitations and has to be added to a Pod through k8s API.","title":"Pod Disruptions"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#deployments","text":"A Deployment provides declarative updates for Pods and ReplicaSets . Creating a new Deployment creates a ReplicaSet, which in turn creates Pods per the PodTemplateSpec of the Deployment. With a Deployment: Scale up/down the Deployment Rollback the Deployment and apply an earlier ReplicaSet Make changes to PodTemplateSpec and rollout new a RelicaSet","title":"Deployments"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_1","text":"Deployment Template apiVersion : apps/v1 # required kind : Deployment # required metadata : # required name : nginx-deployment labels : # must match spec.template.metadata.labels app : nginx spec : # required minReadySeconds : 0 # default 0, time that newly created Pod should be # ready without any of its containers crashing # for it to be considered available progressDeadlineSeconds : 600 # default 600, time to wait for your Deployment to # progress before the system reports back that # the Deployment has failed progressing # must > minReadySeconds paused : false # default false, for pausing and resuming a Deployment replicas : 3 # default 1 revisionHistoryLimit : 10 # default 10, number of old ReplicaSets kept for rollback selector : # required matchLabels : # must match spec.template.metadata.labels app : nginx strategy : # strategy used to replace old Pods by new ones type : RollingUpdate # default RollingUpdate, also possible: Recreate rollingUpdate : maxUnavailable : 25% # default 25%, max number of Pods over desired replicas # that can be unavailable during the update process # can be an absolute number or percentage (rounding down) maxSurge : 25% # default 25%, max number of Pods that can be created # over desired replicas # can be an absolute number or percentage (rounding up) template : # required, aka Pod template, # same schema as Pod, apiVersion and kind excluded metadata : # required labels : app : nginx spec : # required containers : # required - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 restartPolicy : Always # default Always, also available: Never, OnFailure A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template or if the total number of such Pods exceeds .spec.replicas . For labels, make sure not to overlap with other controllers, otherwise the controllers won't behave correctly Frequently used commands kubectl apply -f <.yaml> kubectl get deployment kubectl describe deployment/<deployment_name> kubectl scale deployment/<deployment_name> --replicas = 10 kubectl autoscale deployment/<deployment_name> --min = 10 --max = 15 --cpu-percent = 80 kubectl rollout status deployment/<deployment_name> kubectl rollout history deployment/<deployment_name> kubectl rollout undo deployment/<deployment_name> --to-revision = 2 kubectl rollout pause/resume deployment/<deployment_name> kubectl edit deployment/<deployment_name> kubectl get rs","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#rollingupdate-vs-recreate","text":"A Deployment's revision is created when a Deployment's rollout is triggered. New revision is created each time the PodTemplate (.spec.templates) is changed. Set .spec.revisionHistoryLimit field (default 10) to specify how many old ReplicaSets for this Deployment you want to retain. During a RollingUpdate , Deployment controller creates new Pods per the new ReplicaSet and termiantes existing replicas in the active ReplicaSets, in a controlled rate in order to mitigate risk. With propertional scaling , Deployment controller decide where to add new replicas instead of adding all blindly to the new ReplicaSet. Parameters affecting this are maxSurge and maxUnavailable When setting .spec.strategy.type to Recreate , existing Pods are killed before new ones are created, which guarantee Pod termination previous to creation for upgrades. Successful removal is awaited before any Pod of the new revision is created.","title":"RollingUpdate vs. Recreate"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#deployment-statuses","text":"A Deployment can have various states during its lifecycle: progressing, complete, fail to progress Progressing state: creating a new ReplicaSet scaling up newest ReplicaSet scaling down older ReplicaSet(s) wait new Pods to become Ready Complete state: all replicas updated to latest version per the PodTemplateSpec and Ready no old replicas present Failed state: progressing got stuck for a while and cannot complete by the .spec.progressDeadlineSeconds configured Possible causes: insufficient quota readiness probe fails image pull errors insufficient permissions application runtime misconfiguration causing container exits Kubernetes takes NO action on a stalled Deployment other than to report a status condition .status.conditions : \"Type=Progressing\" \"Status=False\" \"Reason=ProgressDeadlineExceeded\" Higher level orchestrators can take advantage of it and act accordingly, such as rollback to previous version","title":"Deployment statuses"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#replicaset","text":"A ReplicaSet maintains a stable set of replica Pods running at any given time. When a ReplicaSet needs to create new Pods, it uses its Pod template .","title":"ReplicaSet"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#replicaset-and-its-links-with-pods","text":"A ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences field, which specifies what resource the current object is owned by. A ReplicaSet identifies new Pods to acquire by using its selector on Pods missing metadata.ownerReferences or the reference is an invalid controller, for which the reference is established. While you can create bare Pods , it is important to make sure they DO NOT have labels which match the selector of one of exiting ReplicaSets, otherwise they can be acquired by the ReplicaSets and got deleted (because exceeding the desired number of replica). You can remove Pods from a ReplicaSet ( orphan Pods ) by changing their labels so they do not match any ReplicaSet's selector. This technique may be used to remove Pods from service for debugging, data recovery, etc . Deployment is a higher-level resource that manages ReplicaSets, and you may never need to manipulate ReplicaSet objects directly. Use a Deployment instead, and define your application in the spec section.","title":"ReplicaSet and its Links with Pods"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_2","text":"ReplicaSet Example apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend labels : app : guestbook tier : frontend spec : replicas : 3 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 Frequently used commands kubectl get rs kubectl delete rs/<rs_name> kubectl delete replicaset/<rs_name> kubectl delete replicaset/<rs_name> --cascade = orphan # keep the Pods managed by this ReplicaSet","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#replicaset-as-a-horizontal-pod-autoscaler-hpa-target","text":"ReplicaSet can be auto-scaled by an HPA according to settings such as CPU usage, cron expression, etc. HPA Example apiVersion : autoscaling/v1 kind : HorizontalPodAutoscaler metadata : name : frontend-scaler spec : scaleTargetRef : kind : ReplicaSet name : frontend minReplicas : 3 maxReplicas : 10 targetCPUUtilizationPercentage : 50 Frequently used commands kubectl autoscale rs frontend --max = 10 --min = 3 --cpu-percent = 50","title":"ReplicaSet as a Horizontal Pod Autoscaler (HPA) Target"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#statefulsets","text":"StatefulSets are like Deployments that manages the deployment and scaling of a set of Pods, while providing guarantees about the ordering and uniqueness of these Pods. Pods created are not interchangeable and each has a persistent identifier that it maintains across any rescheduling. StatefulSets are valuable for applications that require one or more of: Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates.","title":"StatefulSets"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#limitationsbehaviors","text":"Storage for a Pod must be provisioned by a PersistentVolumeProvisioner based on the storage class , or pre-provisioned by an admin Deleting/scaling down StatefulSet will NOT delete the volumes to ensure data safety A Headless Service is required for the network identity of the Pods Deletion of the Pods when a StatefulSet is deleted, is not guaranteed. try scale down the StatefulSet to 0 prior its deletion RollingUpdates with default podManagementPolicy as OrderedReady can run into a broken state that requires manual intervention","title":"Limitations/Behaviors"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_3","text":"StatefulSet Example apiVersion : apps/v1 # required kind : StatefulSet # required metadata : # required name : web spec : # required podManagementPolicy : OrderedReady # default OrderedReady, also possible: Parallel replicas : 3 # default 1 selector : # required matchLabels : # must match spec.template.metadata.labels app : nginx serviceName : \"nginx\" # must match the Service name template : # required metadata : # required labels : app : nginx spec : terminationGracePeriodSeconds : 10 # should be non-0 containers : # required - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html updateStrategy : type : RollingUpdate # default RollingUpdate, also possible: OnDelete, Partitions volumeClaimTemplates : # provide stable storage using PersistentVolumes - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"my-storage-class\" resources : requests : storage : 1Gi --- # Headless Service apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#statefulset-pods","text":"StatefulSet Pods have a unique identity that is comprised of an ordinal : each Pod assigned an integer ordinal from 0 up through N-1 , for N replicas. Pod will be named $(statefulset name)-$(ordinal) and getting a matching DNS subdomain $(podname).$(governing service domain) . The domain managed by the Headless Service has form $(service name).$(namespace).svc.$(cluster_domain) . cluster_domain will be cluster.local unless configured . Pod will also be added a label statefulset.kubernetes.io/pod-name with it name, for attaching a Service to specific Pods.","title":"StatefulSet Pods"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#scaling","text":"StatefulSet being deployed will do updates to the Pods sequentially in the order from {0..N-1} ; and Pod N must wait ALL Pods N-1 and below to have started and available, before being created and replaced. Upon StatefulSet deletion, the Pods will be terminatated sequentially in the order from {N-1..0} ; and Pod N must wait ALL Pods N+1 and above to have completed terminated and deleted, before going into termination.","title":"Scaling"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#parallel-pod-management","text":"Specifying .spec.podManagementPolicy with Parallel will launch or terminate all Pods in parallel regardless the ordering.","title":"Parallel Pod Management"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#update-strategies","text":"StatefulSet's .spec.updateStrategy field allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet.","title":"Update Strategies"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#ondelete","text":"OnDelete tells StatefulSet controller to not automatically update Pods; users must manually delete Pods to cause new Pods being created to reflect changes to .spec.template .","title":"OnDelete"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#rollingupdate","text":"RollingUpdate strategy implements automated, rolling update for the Pods in a StatefulSet.","title":"RollingUpdate"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#partitions","text":"Partitions is a setting under RollingUpdate, by specifying a .spec.updateStrategy.rollingUpdate.partition number. All Pods with an ordinal >= partition number will be updated when the StatefulSet's .spec.template is updated; the rest won't ever be updated even on deletion (recreated at previous version)","title":"Partitions"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#daemonset","text":"DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them, and vice-versa. Deleting a DaemonSet will clean up the Pods it created. Typical use of DaemonSet: running a cluster storage daemon on every node running a logs collection daemon on every node running a node monitoring daemon on every node","title":"DaemonSet"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_4","text":"DaemonSet Template apiVersion : apps/v1 # required kind : DaemonSet # required metadata : # required name : fluentd-elasticsearch namespace : kube-system labels : k8s-app : fluentd-logging spec : # required selector : # required, if both types of matchers specified, ANDed matchLabels : # must match spec.template.metadata.labels name : fluentd-elasticsearch matchExpressions : ... # allows more sophisticated selectors by key or list values template : # required metadata : labels : name : fluentd-elasticsearch spec : containers : - name : fluentd-elasticsearch image : quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources : limits : memory : 200Mi requests : cpu : 100m memory : 200Mi volumeMounts : - name : varlog mountPath : /var/log - name : varlibdockercontainers mountPath : /var/lib/docker/containers readOnly : true affinity : # default unset, decide what nodes to schedule Pods nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchFields : - key : metadata.name operator : In values : - target-host-name nodeName : ... # default unset (schedule on all nodes) restartPolicy : Always # default Always, must be Always terminationGracePeriodSeconds : 30 tolerations : # have the daemonset runnable on master nodes # remove if your masters can't run pods - key : node-role.kubernetes.io/master effect : NoSchedule volumes : - name : varlog hostPath : path : /var/log - name : varlibdockercontainers hostPath : path : /var/lib/docker/containers Once a DaemonSet is created, do not edit its .spec.selector as it creates orphaned Pods. Instead, delete the old DaemonSet and deploy a new one if it is necessary to update the selector. Normally, the node that a Pod runs on is selected by the Kubernetes scheduler. DaemonSet pods are created and scheduled by the DaemonSet controller . By adding the NodeAffinity term to the DaemonSet pods, ScheduleDaemonSetPods allows you to schedule DaemonSets using the default scheduler instead of the DaemonSet controller.","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#jobs","text":"A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. Deleting a Job cleans up the Pods created. Job is used to run some task to completion, and run multiple tasks in parallel from multiple Pods. Parallel Jobs are good for processing of a set of independent but related work items.","title":"Jobs"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#parallel-executions","text":"Non-parallel default Jobs only one Pod started, Job completes as soon as the Pod terminates successfully Parallel Jobs with a fixed completion count Multiple Pods started, each successful Pod termination get completion count increment by 1 Job completes when there is one successful Pod for each value in the range 1 to .spec.completions you may specify .spec.parallelism to speed up Job completion actual number of pods running in parallel will not exceed the number of remaining completions Parallel Jobs with a work queue specify .spec.parallelism , leave .spec.completions unset Multiple Pods started, Job completes only when at least one Pod terminates successfully and all Pods are terminated requires Pods coordinate amongst themselves or an external service to determine what each should work on requires each Pod independently capable of determining whether or not all its peers are done No new Pods scheduled once one of the Pods terminates successfully When running a Job in parallel, the Pods should be tolerant to concurrency.","title":"Parallel executions"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_5","text":"Job example apiVersion : batch/v1 # required kind : Job # required metadata : # required name : pi spec : # required activeDeadlineSeconds : 100 # default unset, Job must complete within this period ttlSecondsAfterFinished : 100 # default unset, time to delete Job after its completion # currently an Alpha feature, needs feature gate backoffLimit : 6 # default 6, fail a Job after some amount of retries # with an exponential back-off delay completions : 3 # default 1, run parallel jobs for fixed completion count parallelism : 3 # default 1, run parallel jobs with work queue manualSelector : false # only when manually specifying the selector # optional and best to omit selector : ... template : # required spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never # only Never or OnFailure allowed Frequently used commands kubectl get jobs kubectl get pods --selector = job-name = pi --output = jsonpath = '{.items[*].metadata.name}'","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#job-termination","text":"When a Job completes, no more Pods are created, and the terminated Pods are around for viewing the logs and diagnostics. The job object also remains after it is completed. It must be manually cleaned up or set ttlSecondsAfterFinished for auto cleanup. Or switch to a CronJob instead. If .spec.activeDeadlineSeconds is set, the Job must complete within the period and will fail otherwise and terminate all its running Pods.","title":"Job termination"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cronjob","text":"A CronJob creates Jobs on a repeating schedule. One CronJob object is like one line of a crontab (cron table) file. All CronJob schedule: times are based on the timezone of the kube-controller-manager . CronJob name must be <= 52 chars because 11 chars are usually generated and the max length for name is 63 chars. A cron job creates a job object about once per execution time of its schedule. CronJobs scheduled should be idempotent since in rare circumstances it might be that 2 jobs are created or none created.","title":"CronJob"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_6","text":"CronJob Example apiVersion : batch/v1beta1 # required kind : CronJob # required metadata : # required name : hello spec : # required schedule : \"*/1 * * * *\" # required: minute[0-59], hour[0-23], day of the month[1-31], month[1-12], day of week[0-6](Sunday to Saturday) # consult tool at https://crontab.guru/ concurrencyPolicy : Allow # default Forbid startingDeadlineSeconds : 200 # default unset jobTemplate : # required spec : template : spec : containers : - name : hello image : busybox imagePullPolicy : IfNotPresent args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cronjob-limitations","text":"A CronJob is counted as missed if it has failed to create Job at its scheduled time. If concurrencyPolicy is set to Forbid and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed. CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs an error. If the startingDeadlineSeconds field is set, controller counts how many missed jobs occurred from this value rather than from the last scheduled time until now. If startingDeadlineSeconds is set to a large value or left unset (the default) and if concurrencyPolicy is set to Allow, the jobs will always run at least once .","title":"CronJob limitations"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#replicationcontroller","text":"ReplicationController ensures that a specified number of pod replicas are running and available at any time . Extra Pods are terminated to match the desired number of Pods and vice-versa. ReplicationController supervises multiple pods across multiple nodes . The ReplicationController is designed to facilitate simple rolling updates to a service by replacing pods one-by-one , and does NOT perform readiness nor liveness probes. ReplicationController is intended to be managed by external auto-scaler and not he HPA and used as a composable building-block primitive.","title":"ReplicationController"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_7","text":"ReplicationController example apiVersion : v1 # required kind : ReplicationController # required metadata : # required name : nginx spec : # required replicas : 3 # default 1 selector : # required app : nginx # default spec.template.metadata.labels and must match template : # required metadata : name : nginx labels : app : nginx spec : containers : - name : nginx image : nginx ports : - containerPort : 80 restartPolicy : Always # default Always and must be Always","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#garbage-collector","text":"Kubernetes garbage collector (GC) deletes certain objects that no longer have an owner . The k8s GC does NOT delete bare Pods or orphan Pods since they are not the case of losing the ownership.","title":"Garbage Collector"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#ownership","text":"Objects can own other objects within the same namespace. Owned objects are dependents of the owner object and must have a metadata.ownerReferences field point to its owner. The metadata.ownerReferences is automatically set for objects created or adopted by ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job, and CronJob. It can also be manually set on objects to establish ownership.","title":"Ownership"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_8","text":"Ownership Example apiVersion : v1 kind : Pod metadata : ... ownerReferences : - apiVersion : apps/v1 controller : true blockOwnerDeletion : true kind : ReplicaSet name : my-repset uid : d9607e19-f88f-11e6-a518-42010a800195 ...","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#cascading-deletion","text":"When you delete an object, the object's dependents are also deleted automatically if Cascading Deletion is enabled, otherwise the dependents are orphaned .","title":"Cascading Deletion"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#foreground","text":"In foreground cascading deletion, root object enters a \"deletion in progress\" state: object still visible in API object deletionTimestamp is set object metadata.finalizers contains \" foregroundDeletion \" next, GC deletes root object's dependents once ALL dependents are deleted, GC deletes the root object only blocked by dependents with ownerReference.blockOwnerDeletion=true","title":"Foreground"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#background","text":"In background cascading deletion, Kubernetes deletes the owner object immediately and the garbage collector then deletes the dependents in the background .","title":"Background"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_9","text":"The json example is for when calling k8s API to delete an object while setting the deletion options. DeleteOptions example { \"kind\" : \"DeleteOptions\" , \"apiVersion\" : \"v1\" , \"propagationPolicy\" : \"Orphan\" // also possible : Foregrou n d , Backgrou n d } Frequently used commands # call k8s API kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\" # directly use kubectl delete kubectl delete replicaset my-repset --cascade = orphan","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#ttl-controller","text":"TTL controller provides a TTL ( time to live ) mechanism to limit the lifetime of resource objects that have finished execution . It only handles Jobs for now, specified with .spec.ttlSecondsAfterFinished , and it is currently an alpha feature. The .spec.ttlSecondsAfterFinished of a Job can be modified after the resource is created or has finished. The Job's existance will NOT be guaranteed after its TTL (if set) expires. TTL controller will assume that a resource is eligible to be cleaned up when its TTL has expired, and will delete it cascadingly.","title":"TTL Controller"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#services-lb-networking","text":"","title":"Services, LB, Networking"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#service","text":"Service is an abstract way to expose an application running on a set of Pods as a network service . It defines a logical set of Pods (governed by a selector) and a policy by which to access them. Kubernetes creates Endpoint objects and associates them with the Service objects. This is done automatically when a Service has a selector defined. Kubernetes gives Pods their own IP addresses. With Service, Kubernetes assigns this Service an IP address ( cluster IP ) and a single DNS name to represent the set of Pods matching its selector, and can load-balance among the Pods.","title":"Service"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#schema-reference_10","text":"Service template apiVersion : v1 kind : Service metadata : name : my-service spec : type : ClusterIP # default ClusterIP, also possible: NodePort, LoadBalancer, ExternalName externalName : ... # required only when using ExternalName as type clusterIP : ... # optional and only when specifying your own cluster IP address # must be a valid IPv4 or IPv6 address from within # the service-cluster-ip-range CIDR range configured # for the API server selector : app : MyApp sessionAffinityConfig:# default unset clientIP : # pass particular client request to the same Pod each time timeoutSeconds : 600 ports : - name : http # required if specifying multiple ports protocol : TCP # default TCP, also possible: UDP, SCTP (alpha), HTTP, PROXY port : 80 # incoming port targetPort : 9376 # default same as port, the target port within Pod nodePort : 30007 # optional and only when using NodePort as type externalIPs : # optional and only when forcing on external nodes - 80.11.12.11 status : loadBalancer : # available when using LoadBalancer and after being provisioned ingress : - ip : 192.0.2.127 If you created a Service without a selector , you need to manually map the Service to the network address and port where it's running, by adding an Endpoint object manually. Endpoint template apiVersion : v1 kind : Endpoints metadata : name : my-service subsets : - addresses : - ip : 192.0.2.42 ports : - port : 9376 Note The endpoint IPs must NOT be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6). Endpoint IP addresses CANNOT be the cluster IP s of other Kubernetes Services, because kube-proxy does NOT support virtual IPs as a destination. kubectl port-forward --address 0.0.0.0 8080:8080","title":"Schema Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#servicetype","text":"Service's ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP. ClusterIP - Exposes the Service on a cluster-internal (virtual) IP. Service is only reachable from within the cluster NodePort - Exposes the Service on each Node's IP at a static port Service reachable from outside the cluster, by requesting <NodeIP>:<NodePort> . The port allocated is from a range specified by --service-node-port-rang , default 30000-32767 reported to the Service object in .spec.ports[*].nodePort can request specific port number from the valid range with nodePort , but need to take care of port collision yourself Use --nodeport-addresses in kube-proxy to specify only particular IP block(s) to proxy the port A ClusterIP is created to be routed from the NodePort Service. each node proxies that port into this Service LoadBalancer - Exposes the Service externally using a cloud provider 's load balancer. A NodePort and a ClusterIP is created to be routed from the external load balancer NodePort allocation can be disabled via an alpha feature ServiceLBNodePortControl with spec.allocateLoadBalancerNodePorts=false Internal load balancer support see here Actual creation of the load balancer happens asynchronously and available after it is provisioned, in .status.loadBalancer field Some cloud providers allow you to specify the loadBalancerIP All ports must have the same protocol on the LoadBalancer Service available protocol is defined by the cloud provider alpha feature MixedProtocolLBService allows different protocols ExternalName - Maps the Service to the contents of the .spec.externalName field Value should be a canonical DNS name Returning a CNAME record with its value NO proxy is set up Read more Alternative to a Service is the Ingress. It acts as the entry point for your cluster and lets you consolidate your routing rules into a single resource as it can expose multiple services under the same IP address. If there are external IP s that route to one or more cluster nodes , Kubernetes Services can be exposed on these nodes via .spec.externalIPs","title":"ServiceType"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#application-protocol","text":"This is an alpha feature, by specifying appProtocol , you can specify an application protocol for each Service port. For example, an application can serve 8889 with ftp and 8090 with smtp. The value must be valid IANA standard service names","title":"Application protocol"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#virtual-ips-and-proxies","text":"Every node in a Kubernetes cluster runs a kube-proxy , which is responsible for implementing a form of virtual IP (cluster IP) for Services of type other than ExternalName . kube-proxy uses iptables (packet processing logic in Linux) to define virtual IP addresses which are transparently redirected as needed. kube-proxy can run in several modes.","title":"Virtual IPs and proxies"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#user-space-mode","text":"In this mode, kube-proxy on each node watches the Kubernetes control plane for the addition and removal of Service and Endpoint objects. For each new Service it opens a randomly chosen proxy port on the local node. It then adds new iptables rules which capture traffic to the Service's clusterIP and port ( request -> clusterIP:port -> nodeIP:proxy_port ), which redirect that traffic to the proxy port which proxies to the Service's backend Pod (through Endpoints), using a round-robin algorithm. Service owners can choose ANY port they want without risk of collision. The .spec.sessionAffinity setting on the Service affects which Pod gets the traffic at any moment.","title":"User space mode"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#iptables-mode","text":"In this mode, kube-proxy also does the watches on the Service and Endpoint objects and install iptable rules which capture traffic to the Service's clusterIP and port. Additionally, it installs iptables rules which redirect from the clusterIP and port to per-Service rules, which further link to backend Pods for each Endpoint object using destination NAT. When a Service is accessed, kube-proxy redirects the packets to a backend Pod at random or via session affinity . This mode is likely more reliable since traffic is handled by Linux netfilter without the need to switch between userspace and the kernel space . Packets are never copied to userspace, the kube-proxy does NOT have to be running for the virtual IP address to work, and Nodes see traffic arriving from the unaltered client IP address . However if traffic comes in through a node-port or through a load-balancer, the client IP does get altered.","title":"iptables mode"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#ipvs-mode","text":"In this mode, kube-proxy watches Kubernetes Services and Endpoints, calls netlink interface to create IPVS rules accordingly and synchronizes IPVS rules with Kubernetes Services and Endpoints periodically . When accessing a Service, IPVS directs traffic to one of the backend Pods. This mode is based on netfilter hook function that is similar to iptables mode, but uses a hash table as the underlying data structure and works in the kernel space . This mode is likely better, serves with lower latency, higher throughput, better performance in synchronization of rules, and provides more load-balancing algorithms. IPVS kernel modules must be installed before enabling kube-proxy with this mode. Note Both the userspace proxy mode and the iptables proxy mode obscures the source (external client) IP address of a packet accessing a Service. One way to fight this is to add a proxy before Kubernetes Network and filtering each request and add source IP address as a header.","title":"IPVS mode"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#discovering-services","text":"","title":"Discovering services"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#env-vars","text":"When a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service : {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables, where the Service name is upper-cased and dashes are converted to underscores. i.e. redis-master which exposes TCP port 6379 will be available as: REDIS_MASTER_SERVICE_HOST = 10 .0.0.11 REDIS_MASTER_SERVICE_PORT = 6379 REDIS_MASTER_PORT = tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP = tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO = tcp REDIS_MASTER_PORT_6379_TCP_PORT = 6379 REDIS_MASTER_PORT_6379_TCP_ADDR = 10 .0.0.11 Note When you have a Pod that needs to access a Service via the environment variables, you must create the Service before the client Pods come into existence.","title":"Env Vars"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#dns","text":"You should ALWAYS set up a DNS service for your Kubernetes cluster using an add-on , such as CoreDNS. Doing so will allow Pods within the same namespace as a Service to lookup its IP using its DNS name. Pods within a different namespace can also do with its full DNS name. The Kubernetes DNS server is the ONLY way to access ExternalName Services .","title":"DNS"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#headless-services","text":"When you do NOT need load-balancing and Service IP, you can create a Headless Service by explicitly specifying \" None \" for the cluster IP ( .spec.clusterIP ). For headless Services that define selectors , the endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return records (addresses) that point directly to the Pods backing the Service. For those that do not define selectors, no Endpoints created. However, the DNS system looks for and configures either: CNAME records for ExternalName-type Services A records for any Endpoints that share a name with the Service, for all other types.","title":"Headless Services"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#service-topology","text":"Service Topology enables a service to route traffic based upon the Node topology of the cluster . A service can specify that traffic be preferentially routed to endpoints that are on the same Node as the client, or in the same availability zone . Service creator can define a policy for routing traffic based upon the Node labels for the originating and destination Nodes. You can control Service traffic routing by specifying the .spec.topologyKeys field on the Service object, which should be a preference-ordered list of Node labels to be used to sort endpoints when accessing the Service. If no match to any of the Node labels is found, the traffic will be rejected . The special value * may be used to mean \"any topology\" and only makes sense as the last value in the list, if used. If .spec.topologyKeys is not specified or empty, NO topology constraints will be applied. Consider a cluster with Nodes that are labeled with their hostname, zone name, and region name . Then you can set the topologyKeys values of a service to direct traffic: Only to endpoints on the same node, failing if no endpoint exists on the node: [\"kubernetes.io/hostname\"] Preferentially to endpoints on the same node, falling back to endpoints in the same zone, followed by the same region, and failing otherwise: [\"kubernetes.io/hostname\", \"topology.kubernetes.io/zone\", \"topology.kubernetes.io/region\"] Preferentially to the same zone, fallback on any available endpoint otherwise: [\"topology.kubernetes.io/zone\", \"*\"] Valid topology keys are currently limited to kubernetes.io/hostname , topology.kubernetes.io/zone , and topology.kubernetes.io/region","title":"Service Topology"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#yaml-reference","text":"Service Topology example apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : my-app ports : - protocol : TCP port : 80 targetPort : 9376 topologyKeys : - \"kubernetes.io/hostname\" - \"*\"","title":"YAML Reference"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#kubernetes-dns","text":"Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelets to tell individual containers to use the DNS Service's IP to resolve DNS names. Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name . By default, a client Pod's DNS search list will include the Pod's own namespace and the cluster's default domain .","title":"Kubernetes DNS"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#for-services","text":"Normal Services are assigned a DNS A or AAAA record , depending on the IP family of the service, in the form of svc-name.namespace.svc.cluster-domain.example , which resolves to the cluster IP of the Service. A Headless Service gets the same record, except that it resolves to the set of IPs of the pods selected by the Service.","title":"for Services"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#srv-records","text":"SRV Records are created for named ports that are part of normal or Headless Services, in the form of _port-name._port-protocol.svc-name.namespace.svc.cluster-domain.example . For a normal Service, it resolves to the domain name and the port number, svc-name.namespace.svc.cluster-domain.example:port . For a Headless Service, it resolves to multiple answers, one domain and port for each pod that is backing the service, auto-generated-name.svc-name.namespace.svc.cluster-domain.example:port .","title":"SRV records"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#for-pods","text":"","title":"for Pods"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#aaaaa-records","text":"A normal Pod gets DNS resolution in the form of pod-ip-address.namespace.pod.cluster-domain.example . Any pods created by a Deployment or DaemonSet exposed by a Service have the DNS resolution in the form of pod-ip-address.deployment-name.namespace.svc.cluster-domain.example .","title":"A/AAAA records"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#hostnmae-and-subdomain","text":"When a Pod is created, its hostname is the Pod's metadata.name value. There is also .spec.hostname to be used to specify the Pod's hostname, which takes precedence over the Pod's name. The Pod also has an optional subdomain field .spec.subdomain which can be used to specify its subdomain . Then its fully qualified domain name ( FQDN ) becomes hostname.subdomain.namespace.svc.cluster-domain.example Pod needs to become READY in order to have a record unless publishNotReadyAddresses=True is set on the Service.","title":"hostnmae and subdomain"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#sethostnameasfqdn","text":"When a Pod is configured to have FQDN, its hostname command returns the short hostname set on the Pod, and the hostname --fqdn command returns the FQDN. When you set .spec.setHostnameAsFQDN=true on the Pod, both hostname and hostname --fqdn return the Pod's FQDN. Note In Linux, the hostname field of the kernel (the nodename field of struct utsname) is limited to 64 characters . If a Pod enables .spec.setHostnameAsFQDN and its FQDN is longer than 64 character, it will stuck in Pending(ContainerCreating) status. One way to solve this issue is to create an admission webhook controller to control FQDN size when users create top level objects ike Deployments.","title":"setHostnameAsFQDN"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#dns-policy","text":"DNS policies can be set on a per-pod basis via .spec.dnsPolicy : Default - Pod inherits the DNS name resolution configuration from the node that the pods run on ClusterFirst (default policy) - Any DNS query that does NOT match the configured cluster domain suffix , such as \"www.kubernetes.io\", is forwarded to the upstream DNS nameserver inherited from the node Cluster administrators may have extra stub-domain and upstream DNS servers configured ClusterFirstWithHostNet - only for Pods running with hostNetwork None - allows Pod to ignore DNS settings from the Kubernetes environment All DNS settings are expected to be provided using the .spec.dnsConfig field in the Pod Spec","title":"DNS Policy"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#dns-config","text":"Pod's DNS Config is optional but it allows users more control on the DNS settings for a Pod. It will be required if .spec.dnsPolicy=None . Some properties: nameservers - a list of DNS servers' IP addresses at most 3 can be provided optional unless .spec.dnsPolicy=None the list will be combined to the base nameservers generated from the DNS Policy searches - a list of DNS search domains for hostname lookup in the Pod. at most 6 can be provided optional the list will be combined to the base search domains generated from the DNS Policy options - a list of objects where each object may have a name property (required) and a value property (optional) optional the list will be combined to the base options generated from the DNS Policy DNS Config Example apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 1.2.3.4 searches : - ns1.svc.cluster-domain.example - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0 Will generate /etc/resolv.conf : nameserver 1.2.3.4 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0","title":"DNS Config"},{"location":"Cloud-and-Containers/k8s/K8s-Knowledge/#endpointslices","text":"EndpointSlices allow for distributing network endpoints across multiple resources . Each slice can hold upto 100 endpoints.","title":"EndpointSlices"},{"location":"Cloud-and-Containers/k8s/service_mesh/","text":"This set of notes was taken from a Linux Foundation course. Service Mesh Fundamentals \u00b6 Cloud-native applications \u00b6 A cloud-native app is those apps designed to run in the cloud and take advantage of the cloud infrastructure properties and features, often packaged and run in containers . The underlying cloud infrastructure often runs on shared commodity hardware that is regularly changing, restarting, or failing . The microservice should be designed to be ephemeral in nature. It should start up quickly, locate its dependent network services rapidly, and fail fast. In addition to developing cloud-native app, a cloud-native organization should also have adopted development and operational practices that enable agility for both the app and the organization itself. Common Architectures \u00b6 In its architectures, each cloud-native app is composed of a number of loosely-coupled and highly-cohesive microservices working together to form a distributed system . loosely-coupled - the microservice can be changed internally with minimal impact on any other microservices highly-cohesive - microservices are built around a well-defined business context, and typically any modifications required are focused around a single area of responsibility or functionality Developer's primary benefit of cloud-native apps development is that each app can be developed and deployed separately from the others, which enables faster development and release of apps. Operation's primary benefit of cloud-native apps is the ability to easily deploy, scale up or down, upgrade, and replace those apps. Common Challenges \u00b6 Challenge 1: service discovery - being able to find all the microservices that are dynamically deployed. It becomes increasingly difficult to keep track of where all the instances are currently deployed at any given time. Challenge 2: security and observability - cloud-native apps have larger attack surfaces and more logical pieces to protect, and it is harder to understand what's happening when things go wrong. Challenge 3: system-wide fault-tolerance - while its architecture enables a crash in one service to be isolated, it is quite possible that a failure in one service can cascade throughout the entire application if this is not handled properly. Resilience for distributed systems \u00b6 Resilience is the use of strategies for improving availability . Ensuring the distributed system is available by reducing its downtime increases the system's resilience. The ultimate goal of resilience is to ensure that failures or degradations of particular microservice instances don't cause cascading failures that cause downtime for the entire distributed system. ( Application Layer ) Developers could design and code an app so it will continue to work in a degraded state, providing vital functions, even when other functions have failed due to bugs, compromises, or other issues with one or more of the microservices. ( Network Layer ) Network-level resilience strategies work to monitor the network performance of deployed instances of each microservice. Requests should be redirected to avoid unresponsive instances automatically to ensure availbility. Load Balancing \u00b6 Load blalancing (LB) for cloud-native apps can be performed at both the Network Layer or the Application Layer. The key difference is that Network level LB is per connection , while Application level LB is per request . Service Mesh is Application level LB. For Kubernetes, Network Layer load balancing is by default implemented using kube-proxy . The management of Pod IP addresses and routing of traffic between virtual/physical network adapters is handled via the Container Networking Interface (CNI) implementation or an overlay network, such as Calico or Weave Net. For cloud-native apps, load balancing refers to balancing the app's requests amongst the running instances of the microservices. Having multiple instances of each microservice distributed on several machines or nodes provides redundancy . LB algorithms are mostly Round Robin, Least Request, Session Affinity (aka sticky sessions), and weighted versions of the first two. Timeouts and Retries \u00b6 When part of the system fails to handle the request in a certain period of time, the request times out and the requester can retry the request with a redundant instance of the failed system. Requests should NOT ALWAYS be automatically retried. Main reason is to avoid duplicating a transaction that has already succeeded. A safe transaction (aka idempotent transaction ) is that the same request causes the same result if ran multiple times. Deadlines \u00b6 Distributed timeouts (aka deadlines ) involves more than one services that form a chain of requests. Each node of the chain adds processing time and needs to consider the time used from all of previous nodes' processing time against the deadline set when calling the first node. If the deadline is passed on any node from the call chain, the request should return as failure immediately. Circuit Breakers \u00b6 Circuit breaker works by setting a limit for the degree of service degradation or failures for a single instance of a microservice. The goal of a circuit breaker is to prevent an issue with one microservice instance from negatively affecting other microservices and potentially causing a cascading failure . Resilience with Proxies \u00b6 Over time, library-based implementations of resilience strategies have been supplanted by proxy-based implementations. Proxies can implement resilience strategies between the instances of microservices. For example, when an instance of microservice A sends a request to microservice B, that request actually goes to a proxy (controlled with CNAMEs ). The proxy would process A's request and decide which instance of microservice B it should go to (through endpoint mappings and LB), and then it would issue the request on A's behalf where some minor-altering to the request can happen such as special heads to indicate source IP, or upgrade request with mTLS. Using proxy may also do cached responses for heavily requested contents to reduce the load on downstream requests. Data Planes and Control Planes \u00b6 Data Plane \u00b6 The data plane is a group of proxies directly manipulates traffic and is responsible for service discovery, resilience, observability, and security for the microservices. It consists of one or more service proxies within a cluster, each of which typically runs next to a single service or microservice of an app, called a service (mesh) proxy , or a sidecar proxy running alongside a service. A service proxy is actually both a proxy and a reverse proxy . It can accept incoming requests and make out-going requests, and handles proxy-to-proxy remote procedure calls (RPCs). Data plane features: Service discovery - find suitable service proxies to make requests to Resilience - implement resilience strategies Observability - observe each request and reply that it receives, collect performance metrics (throughput, latency, failure rates, etc) Security - reject requests that violate policies Common service mesh data plane solutions include Envoy Proxy, linkerd-proxy, NGINX, and HAProxy. Control Plane \u00b6 The control plane manages and coordinates the data plane, and defines policy and ensuring every service proxy in the data plane follows that policy. Requests do NOT pass through the control plane. A service mesh is one of the patterns for architecture designs considering control plane and data plane together. Ingress and API Gateway \u00b6 A service mesh handles an app's service-to-service (east-west) traffic within a cluster. An Ingress controller manages the app's ingress (north-south), traffic entering the cluster . The integration between an Ingress controller and a service mesh: at control plane - share service discovery information so that traffic is consistently routed to the correct target at data plane - share encryption secrets so that all communications can be trusted and secured Some Ingress controllers also function as API Gateways . An API Gateway provides additional management capabilities such as authentication, rate limiting, and API management. An Ingress controller is particular to Kubernetes, while API gateways are used in many environments and for purposes in addition to Ingress control. Common Ingress controllers: Ambassador, Contour, ingress-nginx Orchestration \u00b6 The service mesh control plane frequently updates the Ingress controller on the state of the microservices in its data plane. The Ingress controller uses that information to decide how to handle incoming requests entering the cluster from the external load balancer. Ingress controllers provide a feature known as TLS termination . This is the process of serving digital certificates to clients so that they can use TLS and also decrypt the TLS connections, and let the Ingress controller read the contents of incoming requests. Service Mesh Benefits \u00b6 Service Discovery \u00b6 In a service mesh, service discovery refers to the processes that find microservices and keep track of where they are on the network. It can be performed in many ways, inluding a DNS infrastructure, a consistent key value datastore , or a specialized solution. Service mesh needs to have up-to-date information on what microservices are currently deployed and how to communicate with them. Ensure as each microservice instance is deployed, it is automatically assigned a service proxy (aka automatic sidecar/proxy injection) that is deployed within its pod. The communications between the microservice and the service proxy in a pod utilize the localhost loopback adapter , 127.0.0.1. Resilience \u00b6 Timeouts and Retries \u00b6 Timeouts and automatic retries need proper configurations, otherwise can cause large number of retries that overwhelm systems and slow down all services. Service meshes can address these problems, methods commonly used are known as using a service profile (Linkerd) or a virtual service (Istio). They both define routes for a microservice. Another method is called a retry budget , for which the service mesh keeps track of the first-time requests and retried requests , then make sure the ratio of first-time requests to concurrently retried requests stays at an acceptable level. Circuit Breakers \u00b6 Circuit breakers are defined through groups of configuration settings in the service mesh, and closely monitor those conditions and trip the actions from the configuration. Load Balancing \u00b6 Every service mesh supports at least one type of load balancing for requests, or using the general terms: traffic shifting or traffic splitting . Blue-green deployment refers to having two production environments, one designated by blue and the other by green. When users are interacting with the blue, new deployments and testing will be done in green, then switch users to green. Canary deployment, canary rollout, and canary release all refer to testing a new microservice release (called canary) by sending a percentage of production requests to the new microservice and gradually increasing that percentage over time. The older version ends when all requests are going to the new version. Fault Injection \u00b6 Service mesh technologies support Fault injection , which means that you can test your app by having faults or errors purposely generated to see how they affect the app. This can be a great way to simulate various conditions and ensure that the app behaves the way you expect it to. The service mesh can inject many of the error conditions and other adverse circumstances on its own without having to alter the app code at all. Observability \u00b6 Telemetry and Metrics \u00b6 Observability is being able to monitor an app's state and to determine a high level cause when there was a system failure in the microservice. Telemetry (aka Metrics ) means the collection of data that measures something. Google's Four Goden Signals, those metrics important to collect when monitoring distributed systems: Latency - time takes to receive a reply for a request Traffic, rps - requests per second, or what reflects the demand of the system Errors - percentage of requests result in an error Saturation - the utilization of resources such as CPU and Memory allocated Service mesh technologies offer multiple ways to access their metrics, such as a GUI or API. Distributed Tracing \u00b6 Distributed tracing is to have a special and unique ID-based trace header added to each request. Typically this unique ID is a universally unique identifier (UUID) that is added at the point of Ingress and binds to a user-initiated request, which can be useful for troubleshooting. Distributed tracing is intended to be used when metrics and other information already collected by the service mesh doesn't provide enough information to troubleshoot a problem or understand an unexpected behavior. All the service meshes use the same specification for their trace headers: B3 . Read more about openzipkin/b3-propagation Security \u00b6 Mutual TLS \u00b6 Service meshes can protect the communications between pods by using Transport Layer Security (TLS). TLS uses cryptography to ensure that the information being communicated can't be monitored or altered by others. In service meshes, TLS is used between service proxies in the form called mutual TLS : Each service proxy has a secret cryptographic key that confirms its identity and allows it to decrypt the communications it receives. Each service proxy also has a certificate (aka public key ) for every other service proxy that allows it to encrypt the communications it sends out to other service proxies. When initiating pod-to-pod communications, each service proxy first verifies the identity of the other service proxy through authentication, then encrypts their communications. Because each microservice instance has a unique private key , each session is encrypted in a way that only that particular microservice instance can decrypt . Instead of the old security models that enforced policies based on IP address , new security models can enforce policies based on service proxy identity . Enforcing trust boundaries through identities verified by mutual TLS helps prevent attackers from laterally moving through microservices to reach the assets of greatest value. Service Mesh Disadvantages and Costs \u00b6 Increased complexity - more components in the architecture, and tasks like configuring the components properly and troubleshooting problems will be more challenging. More overhead - more compute resources and other resources will be needed, and higher latency in communications. Rapid evolution - service mesh technologies are still frequently evolving and not necessarily stable. Need to make a conscious effort to keep up with the latest developments in your service mesh technology. When to consider Service Mesh \u00b6 As the number of microservices in an app increases , the growing complexity of the app makes a service mesh more beneficial for supporting resilience, observability, and security among all the microservices and their interrelationships. Also in deeper microservice topologies , with many microservices sending requests to each other, service meshes can be invaluable in having visibility into these requests, as well as securing the communications and adding resilience features to prevent cascading failures and other issues. If you're not sure if a service mesh is needed for a particular situation, consider if an Ingress controller alone may be sufficient. Service Mesh Interface \u00b6 Service Mesh Interface (SMI) is a Kubernetes-native specification and supports interoperability for common service mesh features. It defines a standard set of Kubernetes Custom Resource Definitions (CRDs) and APIs for service meshes. Users can define applications that use service mesh technology without tightly binding to any specific implementation. Read more about smi specifications . Traffic Specs API \u00b6 Allow you to define routes that an app can use. For example, kind : HTTPRouteGroup metadata : name : m-routes spec : matches : - name : metrics pathRegex : \"/metrics\" methods : - GET defines a HTTPRouteGroup resource named m-routes, which will match on any HTTP GET request the app sees with the string \"/metrics\" in its path. By itself it doesn't do anything for the matched requests, other resources are responsible for acting on them. Traffic Split API \u00b6 Allow you to implement traffic splitting and traffic shifting methods like A/B testing , blue-green deployment , and canary deployment . For example, kind : TrafficSplit metadata : name : e8-feature-test namespace : e8app spec : service : e8-widget-svc backends : - service : e8-widget-svc-current weight : 75 - service : e8-widget-svc-patch weight : 25 Specify services under backends and give them weights for amount of traffic shed to them. The weights are not percentage, but the number over their total gives exact percentage (25-75 is the same as 1-3, or 250-750). It will be easier to make sure they add up to 100, or 1000. Traffic Access Control API \u00b6 Allow you to set access control policies for pod-to-pod ( service proxy to service proxy ) communications based on service proxy identity . By default ALL traffic is denied and need to explicitly grant permission for types of traffic to allow. For example, kind : TrafficTarget metadata : name : path-specific namespace : default spec : destination : - kind : ServiceAccount name : service-a namespace : default port : 8080 rules : - kind : HTTPRouteGroup name : m-routes matches : - metrics sources : - kind : ServiceAccount name : prometheus namespace : default The rules defines the characteristics the source traffic must have in order to be allowed to reach its destination. Traffic Metrics API \u00b6 Allow you to collect metrics on HTTP traffic and make those metrics available to other tools. Each metric involves a Kubernetes resource and limited to a particular source or destination (aka edge). For example, kind : TrafficMetrics # See ObjectReference v1 core for full spec resource : # source of traffic name : foo-775b9cbd88-ntxsl namespace : foobar kind : Pod edge : # destination of traffic direction : to side : client resource : name : baz-577db7d977-lsk2q namespace : foobar kind : Pod timestamp : 2019-04-08T22:25:55Z window : 30s metrics : - name : p99_response_latency unit : seconds value : 10m - name : p90_response_latency unit : seconds value : 10m - name : p50_response_latency unit : seconds value : 10m - name : success_count value : 100 - name : failure_count value : 100 Debug and Mitigate App Failures \u00b6 Service Mesh Status Checks \u00b6 Check of the status of the service mesh components first, some app failures you are seeing may actually be caused by a problem in service mesh. This can be done with the service mesh's cli tool. For example, linkerd check does following checks: Can the service mesh communicate with Kubernetes? Is the Kubernetes version compatible with the service mesh version? Is the service mesh installed and running? Is the service mesh's control plane properly configured? Are the service mesh's credentials valid and up to date? Is the API for the control plane running and ready? Is the service mesh installation up to date? Is the service mesh control plane up to date? And linkerd check --proxy checks more things: Are the credentials for each of the data plane proxies valid and up to date? Are the data plane proxies running and working fine? Service Route Metrics \u00b6 Check service app route metrics. Viewing the metrics for each route can indicate where slowdowns or failures are occurring within the mesh and narrow down the problem. Request Logging \u00b6 Log individual requests and responses. Be causcious that log data can grow very large rapidly, selectively log requests may help. Service Proxy Logging \u00b6 Configure the service proxy to record more events or record more details about each event. Be causcious as it may degrade the proxy's performance and also generate massive amount of logs. Inject Debug Container \u00b6 Have your service mesh inject a debug container into bad pod. Debug container (debug sidecar) is designed to monitor the activity within the pod and to collect information on that activity, such as capturing network packets. Telepresence Tool \u00b6 Using Telepresence, you can run a single process (a service or debug tool) locally, and a two-way network proxy enables that local service to effectively operate as part of the remote Kubernetes cluster. Telepresence is also intended for use in testing or staging.","title":"Service Mesh"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-mesh-fundamentals","text":"","title":"Service Mesh Fundamentals"},{"location":"Cloud-and-Containers/k8s/service_mesh/#cloud-native-applications","text":"A cloud-native app is those apps designed to run in the cloud and take advantage of the cloud infrastructure properties and features, often packaged and run in containers . The underlying cloud infrastructure often runs on shared commodity hardware that is regularly changing, restarting, or failing . The microservice should be designed to be ephemeral in nature. It should start up quickly, locate its dependent network services rapidly, and fail fast. In addition to developing cloud-native app, a cloud-native organization should also have adopted development and operational practices that enable agility for both the app and the organization itself.","title":"Cloud-native applications"},{"location":"Cloud-and-Containers/k8s/service_mesh/#common-architectures","text":"In its architectures, each cloud-native app is composed of a number of loosely-coupled and highly-cohesive microservices working together to form a distributed system . loosely-coupled - the microservice can be changed internally with minimal impact on any other microservices highly-cohesive - microservices are built around a well-defined business context, and typically any modifications required are focused around a single area of responsibility or functionality Developer's primary benefit of cloud-native apps development is that each app can be developed and deployed separately from the others, which enables faster development and release of apps. Operation's primary benefit of cloud-native apps is the ability to easily deploy, scale up or down, upgrade, and replace those apps.","title":"Common Architectures"},{"location":"Cloud-and-Containers/k8s/service_mesh/#common-challenges","text":"Challenge 1: service discovery - being able to find all the microservices that are dynamically deployed. It becomes increasingly difficult to keep track of where all the instances are currently deployed at any given time. Challenge 2: security and observability - cloud-native apps have larger attack surfaces and more logical pieces to protect, and it is harder to understand what's happening when things go wrong. Challenge 3: system-wide fault-tolerance - while its architecture enables a crash in one service to be isolated, it is quite possible that a failure in one service can cascade throughout the entire application if this is not handled properly.","title":"Common Challenges"},{"location":"Cloud-and-Containers/k8s/service_mesh/#resilience-for-distributed-systems","text":"Resilience is the use of strategies for improving availability . Ensuring the distributed system is available by reducing its downtime increases the system's resilience. The ultimate goal of resilience is to ensure that failures or degradations of particular microservice instances don't cause cascading failures that cause downtime for the entire distributed system. ( Application Layer ) Developers could design and code an app so it will continue to work in a degraded state, providing vital functions, even when other functions have failed due to bugs, compromises, or other issues with one or more of the microservices. ( Network Layer ) Network-level resilience strategies work to monitor the network performance of deployed instances of each microservice. Requests should be redirected to avoid unresponsive instances automatically to ensure availbility.","title":"Resilience for distributed systems"},{"location":"Cloud-and-Containers/k8s/service_mesh/#load-balancing","text":"Load blalancing (LB) for cloud-native apps can be performed at both the Network Layer or the Application Layer. The key difference is that Network level LB is per connection , while Application level LB is per request . Service Mesh is Application level LB. For Kubernetes, Network Layer load balancing is by default implemented using kube-proxy . The management of Pod IP addresses and routing of traffic between virtual/physical network adapters is handled via the Container Networking Interface (CNI) implementation or an overlay network, such as Calico or Weave Net. For cloud-native apps, load balancing refers to balancing the app's requests amongst the running instances of the microservices. Having multiple instances of each microservice distributed on several machines or nodes provides redundancy . LB algorithms are mostly Round Robin, Least Request, Session Affinity (aka sticky sessions), and weighted versions of the first two.","title":"Load Balancing"},{"location":"Cloud-and-Containers/k8s/service_mesh/#timeouts-and-retries","text":"When part of the system fails to handle the request in a certain period of time, the request times out and the requester can retry the request with a redundant instance of the failed system. Requests should NOT ALWAYS be automatically retried. Main reason is to avoid duplicating a transaction that has already succeeded. A safe transaction (aka idempotent transaction ) is that the same request causes the same result if ran multiple times.","title":"Timeouts and Retries"},{"location":"Cloud-and-Containers/k8s/service_mesh/#deadlines","text":"Distributed timeouts (aka deadlines ) involves more than one services that form a chain of requests. Each node of the chain adds processing time and needs to consider the time used from all of previous nodes' processing time against the deadline set when calling the first node. If the deadline is passed on any node from the call chain, the request should return as failure immediately.","title":"Deadlines"},{"location":"Cloud-and-Containers/k8s/service_mesh/#circuit-breakers","text":"Circuit breaker works by setting a limit for the degree of service degradation or failures for a single instance of a microservice. The goal of a circuit breaker is to prevent an issue with one microservice instance from negatively affecting other microservices and potentially causing a cascading failure .","title":"Circuit Breakers"},{"location":"Cloud-and-Containers/k8s/service_mesh/#resilience-with-proxies","text":"Over time, library-based implementations of resilience strategies have been supplanted by proxy-based implementations. Proxies can implement resilience strategies between the instances of microservices. For example, when an instance of microservice A sends a request to microservice B, that request actually goes to a proxy (controlled with CNAMEs ). The proxy would process A's request and decide which instance of microservice B it should go to (through endpoint mappings and LB), and then it would issue the request on A's behalf where some minor-altering to the request can happen such as special heads to indicate source IP, or upgrade request with mTLS. Using proxy may also do cached responses for heavily requested contents to reduce the load on downstream requests.","title":"Resilience with Proxies"},{"location":"Cloud-and-Containers/k8s/service_mesh/#data-planes-and-control-planes","text":"","title":"Data Planes and Control Planes"},{"location":"Cloud-and-Containers/k8s/service_mesh/#data-plane","text":"The data plane is a group of proxies directly manipulates traffic and is responsible for service discovery, resilience, observability, and security for the microservices. It consists of one or more service proxies within a cluster, each of which typically runs next to a single service or microservice of an app, called a service (mesh) proxy , or a sidecar proxy running alongside a service. A service proxy is actually both a proxy and a reverse proxy . It can accept incoming requests and make out-going requests, and handles proxy-to-proxy remote procedure calls (RPCs). Data plane features: Service discovery - find suitable service proxies to make requests to Resilience - implement resilience strategies Observability - observe each request and reply that it receives, collect performance metrics (throughput, latency, failure rates, etc) Security - reject requests that violate policies Common service mesh data plane solutions include Envoy Proxy, linkerd-proxy, NGINX, and HAProxy.","title":"Data Plane"},{"location":"Cloud-and-Containers/k8s/service_mesh/#control-plane","text":"The control plane manages and coordinates the data plane, and defines policy and ensuring every service proxy in the data plane follows that policy. Requests do NOT pass through the control plane. A service mesh is one of the patterns for architecture designs considering control plane and data plane together.","title":"Control Plane"},{"location":"Cloud-and-Containers/k8s/service_mesh/#ingress-and-api-gateway","text":"A service mesh handles an app's service-to-service (east-west) traffic within a cluster. An Ingress controller manages the app's ingress (north-south), traffic entering the cluster . The integration between an Ingress controller and a service mesh: at control plane - share service discovery information so that traffic is consistently routed to the correct target at data plane - share encryption secrets so that all communications can be trusted and secured Some Ingress controllers also function as API Gateways . An API Gateway provides additional management capabilities such as authentication, rate limiting, and API management. An Ingress controller is particular to Kubernetes, while API gateways are used in many environments and for purposes in addition to Ingress control. Common Ingress controllers: Ambassador, Contour, ingress-nginx","title":"Ingress and API Gateway"},{"location":"Cloud-and-Containers/k8s/service_mesh/#orchestration","text":"The service mesh control plane frequently updates the Ingress controller on the state of the microservices in its data plane. The Ingress controller uses that information to decide how to handle incoming requests entering the cluster from the external load balancer. Ingress controllers provide a feature known as TLS termination . This is the process of serving digital certificates to clients so that they can use TLS and also decrypt the TLS connections, and let the Ingress controller read the contents of incoming requests.","title":"Orchestration"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-mesh-benefits","text":"","title":"Service Mesh Benefits"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-discovery","text":"In a service mesh, service discovery refers to the processes that find microservices and keep track of where they are on the network. It can be performed in many ways, inluding a DNS infrastructure, a consistent key value datastore , or a specialized solution. Service mesh needs to have up-to-date information on what microservices are currently deployed and how to communicate with them. Ensure as each microservice instance is deployed, it is automatically assigned a service proxy (aka automatic sidecar/proxy injection) that is deployed within its pod. The communications between the microservice and the service proxy in a pod utilize the localhost loopback adapter , 127.0.0.1.","title":"Service Discovery"},{"location":"Cloud-and-Containers/k8s/service_mesh/#resilience","text":"","title":"Resilience"},{"location":"Cloud-and-Containers/k8s/service_mesh/#timeouts-and-retries_1","text":"Timeouts and automatic retries need proper configurations, otherwise can cause large number of retries that overwhelm systems and slow down all services. Service meshes can address these problems, methods commonly used are known as using a service profile (Linkerd) or a virtual service (Istio). They both define routes for a microservice. Another method is called a retry budget , for which the service mesh keeps track of the first-time requests and retried requests , then make sure the ratio of first-time requests to concurrently retried requests stays at an acceptable level.","title":"Timeouts and Retries"},{"location":"Cloud-and-Containers/k8s/service_mesh/#circuit-breakers_1","text":"Circuit breakers are defined through groups of configuration settings in the service mesh, and closely monitor those conditions and trip the actions from the configuration.","title":"Circuit Breakers"},{"location":"Cloud-and-Containers/k8s/service_mesh/#load-balancing_1","text":"Every service mesh supports at least one type of load balancing for requests, or using the general terms: traffic shifting or traffic splitting . Blue-green deployment refers to having two production environments, one designated by blue and the other by green. When users are interacting with the blue, new deployments and testing will be done in green, then switch users to green. Canary deployment, canary rollout, and canary release all refer to testing a new microservice release (called canary) by sending a percentage of production requests to the new microservice and gradually increasing that percentage over time. The older version ends when all requests are going to the new version.","title":"Load Balancing"},{"location":"Cloud-and-Containers/k8s/service_mesh/#fault-injection","text":"Service mesh technologies support Fault injection , which means that you can test your app by having faults or errors purposely generated to see how they affect the app. This can be a great way to simulate various conditions and ensure that the app behaves the way you expect it to. The service mesh can inject many of the error conditions and other adverse circumstances on its own without having to alter the app code at all.","title":"Fault Injection"},{"location":"Cloud-and-Containers/k8s/service_mesh/#observability","text":"","title":"Observability"},{"location":"Cloud-and-Containers/k8s/service_mesh/#telemetry-and-metrics","text":"Observability is being able to monitor an app's state and to determine a high level cause when there was a system failure in the microservice. Telemetry (aka Metrics ) means the collection of data that measures something. Google's Four Goden Signals, those metrics important to collect when monitoring distributed systems: Latency - time takes to receive a reply for a request Traffic, rps - requests per second, or what reflects the demand of the system Errors - percentage of requests result in an error Saturation - the utilization of resources such as CPU and Memory allocated Service mesh technologies offer multiple ways to access their metrics, such as a GUI or API.","title":"Telemetry and Metrics"},{"location":"Cloud-and-Containers/k8s/service_mesh/#distributed-tracing","text":"Distributed tracing is to have a special and unique ID-based trace header added to each request. Typically this unique ID is a universally unique identifier (UUID) that is added at the point of Ingress and binds to a user-initiated request, which can be useful for troubleshooting. Distributed tracing is intended to be used when metrics and other information already collected by the service mesh doesn't provide enough information to troubleshoot a problem or understand an unexpected behavior. All the service meshes use the same specification for their trace headers: B3 . Read more about openzipkin/b3-propagation","title":"Distributed Tracing"},{"location":"Cloud-and-Containers/k8s/service_mesh/#security","text":"","title":"Security"},{"location":"Cloud-and-Containers/k8s/service_mesh/#mutual-tls","text":"Service meshes can protect the communications between pods by using Transport Layer Security (TLS). TLS uses cryptography to ensure that the information being communicated can't be monitored or altered by others. In service meshes, TLS is used between service proxies in the form called mutual TLS : Each service proxy has a secret cryptographic key that confirms its identity and allows it to decrypt the communications it receives. Each service proxy also has a certificate (aka public key ) for every other service proxy that allows it to encrypt the communications it sends out to other service proxies. When initiating pod-to-pod communications, each service proxy first verifies the identity of the other service proxy through authentication, then encrypts their communications. Because each microservice instance has a unique private key , each session is encrypted in a way that only that particular microservice instance can decrypt . Instead of the old security models that enforced policies based on IP address , new security models can enforce policies based on service proxy identity . Enforcing trust boundaries through identities verified by mutual TLS helps prevent attackers from laterally moving through microservices to reach the assets of greatest value.","title":"Mutual TLS"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-mesh-disadvantages-and-costs","text":"Increased complexity - more components in the architecture, and tasks like configuring the components properly and troubleshooting problems will be more challenging. More overhead - more compute resources and other resources will be needed, and higher latency in communications. Rapid evolution - service mesh technologies are still frequently evolving and not necessarily stable. Need to make a conscious effort to keep up with the latest developments in your service mesh technology.","title":"Service Mesh Disadvantages and Costs"},{"location":"Cloud-and-Containers/k8s/service_mesh/#when-to-consider-service-mesh","text":"As the number of microservices in an app increases , the growing complexity of the app makes a service mesh more beneficial for supporting resilience, observability, and security among all the microservices and their interrelationships. Also in deeper microservice topologies , with many microservices sending requests to each other, service meshes can be invaluable in having visibility into these requests, as well as securing the communications and adding resilience features to prevent cascading failures and other issues. If you're not sure if a service mesh is needed for a particular situation, consider if an Ingress controller alone may be sufficient.","title":"When to consider Service Mesh"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-mesh-interface","text":"Service Mesh Interface (SMI) is a Kubernetes-native specification and supports interoperability for common service mesh features. It defines a standard set of Kubernetes Custom Resource Definitions (CRDs) and APIs for service meshes. Users can define applications that use service mesh technology without tightly binding to any specific implementation. Read more about smi specifications .","title":"Service Mesh Interface"},{"location":"Cloud-and-Containers/k8s/service_mesh/#traffic-specs-api","text":"Allow you to define routes that an app can use. For example, kind : HTTPRouteGroup metadata : name : m-routes spec : matches : - name : metrics pathRegex : \"/metrics\" methods : - GET defines a HTTPRouteGroup resource named m-routes, which will match on any HTTP GET request the app sees with the string \"/metrics\" in its path. By itself it doesn't do anything for the matched requests, other resources are responsible for acting on them.","title":"Traffic Specs API"},{"location":"Cloud-and-Containers/k8s/service_mesh/#traffic-split-api","text":"Allow you to implement traffic splitting and traffic shifting methods like A/B testing , blue-green deployment , and canary deployment . For example, kind : TrafficSplit metadata : name : e8-feature-test namespace : e8app spec : service : e8-widget-svc backends : - service : e8-widget-svc-current weight : 75 - service : e8-widget-svc-patch weight : 25 Specify services under backends and give them weights for amount of traffic shed to them. The weights are not percentage, but the number over their total gives exact percentage (25-75 is the same as 1-3, or 250-750). It will be easier to make sure they add up to 100, or 1000.","title":"Traffic Split API"},{"location":"Cloud-and-Containers/k8s/service_mesh/#traffic-access-control-api","text":"Allow you to set access control policies for pod-to-pod ( service proxy to service proxy ) communications based on service proxy identity . By default ALL traffic is denied and need to explicitly grant permission for types of traffic to allow. For example, kind : TrafficTarget metadata : name : path-specific namespace : default spec : destination : - kind : ServiceAccount name : service-a namespace : default port : 8080 rules : - kind : HTTPRouteGroup name : m-routes matches : - metrics sources : - kind : ServiceAccount name : prometheus namespace : default The rules defines the characteristics the source traffic must have in order to be allowed to reach its destination.","title":"Traffic Access Control API"},{"location":"Cloud-and-Containers/k8s/service_mesh/#traffic-metrics-api","text":"Allow you to collect metrics on HTTP traffic and make those metrics available to other tools. Each metric involves a Kubernetes resource and limited to a particular source or destination (aka edge). For example, kind : TrafficMetrics # See ObjectReference v1 core for full spec resource : # source of traffic name : foo-775b9cbd88-ntxsl namespace : foobar kind : Pod edge : # destination of traffic direction : to side : client resource : name : baz-577db7d977-lsk2q namespace : foobar kind : Pod timestamp : 2019-04-08T22:25:55Z window : 30s metrics : - name : p99_response_latency unit : seconds value : 10m - name : p90_response_latency unit : seconds value : 10m - name : p50_response_latency unit : seconds value : 10m - name : success_count value : 100 - name : failure_count value : 100","title":"Traffic Metrics API"},{"location":"Cloud-and-Containers/k8s/service_mesh/#debug-and-mitigate-app-failures","text":"","title":"Debug and Mitigate App Failures"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-mesh-status-checks","text":"Check of the status of the service mesh components first, some app failures you are seeing may actually be caused by a problem in service mesh. This can be done with the service mesh's cli tool. For example, linkerd check does following checks: Can the service mesh communicate with Kubernetes? Is the Kubernetes version compatible with the service mesh version? Is the service mesh installed and running? Is the service mesh's control plane properly configured? Are the service mesh's credentials valid and up to date? Is the API for the control plane running and ready? Is the service mesh installation up to date? Is the service mesh control plane up to date? And linkerd check --proxy checks more things: Are the credentials for each of the data plane proxies valid and up to date? Are the data plane proxies running and working fine?","title":"Service Mesh Status Checks"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-route-metrics","text":"Check service app route metrics. Viewing the metrics for each route can indicate where slowdowns or failures are occurring within the mesh and narrow down the problem.","title":"Service Route Metrics"},{"location":"Cloud-and-Containers/k8s/service_mesh/#request-logging","text":"Log individual requests and responses. Be causcious that log data can grow very large rapidly, selectively log requests may help.","title":"Request Logging"},{"location":"Cloud-and-Containers/k8s/service_mesh/#service-proxy-logging","text":"Configure the service proxy to record more events or record more details about each event. Be causcious as it may degrade the proxy's performance and also generate massive amount of logs.","title":"Service Proxy Logging"},{"location":"Cloud-and-Containers/k8s/service_mesh/#inject-debug-container","text":"Have your service mesh inject a debug container into bad pod. Debug container (debug sidecar) is designed to monitor the activity within the pod and to collect information on that activity, such as capturing network packets.","title":"Inject Debug Container"},{"location":"Cloud-and-Containers/k8s/service_mesh/#telepresence-tool","text":"Using Telepresence, you can run a single process (a service or debug tool) locally, and a two-way network proxy enables that local service to effectively operate as part of the remote Kubernetes cluster. Telepresence is also intended for use in testing or staging.","title":"Telepresence Tool"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/","text":"Notes taken from Splunk official trainings. It serves as a quick reference for Splunk Dashboard creation and tricks. Dashboard view are webpages driven by simple XML for their contents, HTML for their layout, and CSS and JavaScript to define appearance and interactions. Any searches that return statistical data (from using transforming commands , i.e. stats chart timechart top rare ) can be displayed as visualization in a dashboard. Making a Plan \u00b6 Splunk Dashboard serves a team audience. It can be used by engineers, stakeholders, devops, and customer support. Some questions worth getting clear answers before creating a dashboard: What are the end users and their skill level? Which metrics are critical to individual roles? What is the timespan of the data and how frequent it should refresh? What types of visualizations are required? Layout? Will users access this dashboard through Splunk or in a web app? Do you need custom stylesheet to define parameters for the visualizations? Javascript for additional interactivity? The important part of planning to to sketch out the dashboard's wireframes. Think about the panels you will need, types of visualizations, data you want, how they should arrange, and how the dashboard should look in the end. Next, plan for interactivity by adding inputs. For the search and data, it helps to create a chart listing the data group, the source type, and interesting fields. i.e. Type Source type Interesting Fields Online transactions access_combined action, bytes, categoryId, clientip, itemId, JSESSIONID, price, productId, product_name, referer, referer_domain, sale_price, status, user, useragent Retail sales vendor_sales AcctID, categoryId, product_name, productId, sale_price, Vendor, VendorCity, VendorCountry, VendorID, VendorStateProvince Server access data linux_secure action, app, dest, process, src_ip, src_port, user, vendor_action Event Annotations bcg_sale_dates Sale_Category, Sale_Date, message HTML Dashboard \u00b6 At the dashboards page, you can edit a dashboard and convert it to HTML dashboard. Always do \"Create New\" so you have a backup to return to when running into issues. HTML dashboards allows more customizability but cuts out editability of the dashboard contents. Learn more about it here . You can add texts, images, reference files from other apps, and use common HTML tags. You can also directly add <html> tags within the dashboard source, and reference tokens within it and have it display on the dashboard. i.e. <row> <panel> <title> Time Range: </title> <html> $timeInput.earliest$ to $timeInput.latest$ </html> </panel> </row> Edit Dashboard Source XML \u00b6 Panels can have seven XML visualization elements, each of which can have their own attributes and options. Full reference is available at here Source XML edit allows more advanced touch to the dashboard to improve efficiency, make appearance consistant cross panels, make more complex interactivity, <chart> <event> <map> <single> single value <table> <viz> custom visualization <html> html Three types of dashboard panels: report inline prebuilt can be referenced by multiple dashboards where they show the same view can be created from existing panels prebuilt panels can be accessed by Settings -> User interface -> Prebuilt panels edit prebuilt panels in XML Note panel refresh is only available to report and inline panels. Prebuilt panels Avoid searches using knowledge objects when creating prebuilt panels, such as event types, tags, lookups. This is because all users may not have access to the same knowledge objects. Add custom stylesheets and logic \u00b6 You can add on top of the dashboard XML with CSS, HTML, Javascript, using third-party libraries to add custom behaviors, graphics, and layouts. This can be very labor intensive, however. Dashboard Forms \u00b6 We can create user input widgets to take in form values, which are stored as tokens. Then tokens can be used as arguments in panels created by inline searches. Dashboard that has no inputs are wrapped in <dashboard> tags, while dashboard that has inputs are wrapped in <form> tags. Tokens \u00b6 Tokens represent variables whose value changes dynamically. It is used to pass search terms and field values to individual or multiple dashboard panels. Tokens can also be used for event handlers which listen for actions and respond with configured behaviors, such as showing/hiding a dashboard panel, useful for configuring drill downs. Tokens can be pre-defined or user-defined. More about dashboard tokens . Time Input \u00b6 Time input can be created as a time picker, which creates a token for panels searches to reference and affect the duration covered by the search. You can do so easily through the UI. Or from XML: <fieldset> <input type= \"time\" token= \"timeInput\" > ... <default> <earliest> -7d@d </earliest> <latest> now </latest> </default> </input> </fieldset> ... <search> <query> ... </query> <earliest> $timeInput.earliest$ </earliest> <latest> $timeInput.latest$ </latest> </search> If the time input is not given a token name, it is regarded as the global time picker. The panels without a time selected will use the global time by default. And its XML will show <search> <query> ... </query> <earliest> $earliest$ </earliest> <latest> $latest$ </latest> </search> Note that earliest and latest in the search query still overrides the time picker. It can be useful to put the same panel side-by-side but only allow timerange selection to change on the right panel to allow comparison. A time input can be put within a panel to denote it controls time for that panel (even though the token created is still sharable by other panels). You can still reference the time token within the Panel title to let it reflect the time selected. Text Input \u00b6 Manually supply any value to a field token that can be used by panels searches. This token is useful to replace query where a search condition has wildcard match in it, so that the text token can be used instead for filtering data for the panel. It is good to prefix and suffix the token value with \" in the search string so the token value can contain spaces. Default value overides initial value. Dropdown Menu \u00b6 Supply selectable values to users, whose values set is also searchable. Options can be static or from a search. There are ways to improve the user experience for the dropdown, such as sort the options alphabetically, list the options sorted by the frequency of appearance. Multiselect Input \u00b6 Allows user simultaneously search for multiple values. The token value prefix/suffix and delimiter are particularly useful here to supply token value as multiple conditions for the search. Cascading Input \u00b6 Feed variables from one input dropdowns to another to narrow down data being searched by categoy. i.e. select Country Code -> State -> City. Event Handler \u00b6 When providing cascading inputs, it is nice to provide a way to quickly clear/reset the form. We can do so with a Radio button that has only one option, and add some <unset> tags to the radio button's tags. i.e. <input type= \"radio\" token= \"reset_menus_tok\" searchWhenChanged= \"true\" > <label></label> <choice value= \"now\" > Reset Menus </choice> <default> now </default> <change> <unset token= \"form.v_country_tok\" ></unset> <unset token= \"form.v_state_tok\" ></unset> <unset token= \"form.v_city_tok\" ></unset> <set token= \"form.reset_menus_tok\" > no </set> </change> </input> Token filters \u00b6 Token filters allow you to manipulate and escape characters in token values. Splunk has five default Token filters. You can create you own using JavaScript, reference here . More examples here and Splunk JavaScript SDK |s Quote filter - You can use the |s operator in token to ensure it is always quoted, i.e. $mytoken|s$ |h HTML inclusion filter - makes token values valid for HTML, such as &amp; |u URL encoding filter - makes token value URL escaped, such as %20 |n - prevent any encoding to variables contained within the token wrapper, such as preventing URLs from being escaped. $$ - wrap the token with second pair of $ to prevent it from being rendered. Global environment tokens \u00b6 Tokens available by default and used as $env:token_name$ . Here is a list of global tokens . Customizing Dashboards \u00b6 Table Formatting \u00b6 Using the configuration options from the UI, you can make a statistics table more informative through use of Format and give it sort by field, highlights on values, and formatting on number precisions. The same set of customization rules can be easily copied to another in the XML (Source) editor. Trellis \u00b6 Use of Trellis allows us visualize the data by category, and create multiple instances of a visualization without running multiple ad hoc searches. Enabling this option may make the panel heights a bit awkward, which we can fix by giving the panel a fixed height or width, or updating the trellis size: <option name= \"height\" > 460 </option> Event Annotation \u00b6 Event Annotation allow you to add labels to a dashboard panel, adding context to the data behind it. You do this within an existing panel by editing its source XML. Add another search element with type=\"annotation\" . Then within the search string, use eval to create annotation_label which allows you to use a field name to group annotations. Event annotations require the _time field, so it must be available in the output of the search. The search's time range will inherit from the panel's time range unless specified otherwise in the query. i.e. <serach type= \"annotation\" > <query> index=... | eval annotation_label = \"User Removed from Cart\" | table _time annotation_label </query> </search> Some other useful properties can be added within the eval command are annotation_category : group annotations by category annotation_color : set annotation marker color alternatively can use chart option charting.annotation.categoryColors Hiding panel links \u00b6 You can choose to hide some links shown at bottom of the panel when hovering. A comprehensive list of options to show/hide can be found here . The options also allow further refine the linked search with other queries, useful for drill down more data or on a different or larger time span. Drilldowns \u00b6 Drilldowns allows users to run new searches and display additional details by interacting with dashboard panels. Four types of drilldowns: Event, Visualization, Dynamic, Contextual. Event drilldown involves user clicking on an event returned from a search result, allows users to add more terms to the search exclude terms create new search for all events containing the term Visualization drilldown is under the visualization tab, users clicks directs them to the events returned by the underlying search. Dynamic drilldowns allow user interactions to pass values to other searches, dashboards, reports, or external sources. This can be configured by the Drilldown editor for each panel: Link to Search allows using predefined tokens or other tokens available in the link to search with custom search string $click.value$ token is predefined for multiple visualization types when clicked, the visualization x-axis value is populated into $click.value$ token, and the y-axis value is populated to the $click.value2$ token Link to Dashboard allows further see other dashboards with more details for a specific category you can pass parameters to the downstream dashboard that depends on a token parameter name should be: form.<token_name> note the global time range can be directly used in parameter: earliest latest Link to reports allows redirects to reports to prevent unwanted adhoc searches Link to URL allows opening a page from a relative URL (for another Splunk view) or absolute URL (for external resources) Change the value of tokens or set a token A <drilldown> element will be add to the panel in its XML source when it is set. Contextual drilldowns allow dashboard panels to listen for specific events, then trigger custom actions in response, i.e. show/hide panels. Conditional Interactions \u00b6 Use of drilldown and change actions depending on the fields clicked, which requires change the XML and the use of <condition> elements. i.e. <drilldown> <condition field= \"Vendor Sales\" > <set token= \"params1\" > index=web </set> <link target= \"_self\" > /app/search/product_sales_by_source?form.Source=$params1$ &amp; form.dash2time.earliest=$form.dash1time.earliest$... </link> </condition> </drilldown> More elements that can be used within conditions can be found at here Advanced Behaviors \u00b6 Event Actions \u00b6 Four types of event actions elements: <eval> <link> <set> <unset> <eval> works like an eval command, by executing an expression and storing the results in a defined field it takes a component of an eval command and adapts for XML i.e. | eval bandwidth=round(Bytes/1024,1024,2) becomes <eval token=\"bandwidth\">round(Bytes/1024,1024,2)</eval> <link> allows setting a destination for a drilldown, input, or search. destination can be a dashboard (i.e. <link>app/search/[dashboard_name]</link> ), form (i.e. <link>app/search/[dashboard_name]?form.token=$token$</link> ), or URL (i.e. <link>[URL]?q=$token$</link> ) <set/unset> allows create new token values or remove a token These event action elements must be wrapped within elements such as <drilldown> <change> <search> , and optionally used within the <condition> element. When used with <search> , you can make the action dependent on the status of a search, from the event handlers <done> <error> <fail> <cancelled> <progress> . <link> can further be used with <finalize> <preview> elements. Comprehensive documentation for event actions is here . Search Event handlers \u00b6 Five types of search event handlers: <done> <error> <fail> <cancelled> <progress> They can be used with predefiend tokens $job$ $result$ . $result$ token returns the value from the first row of results for the specified field can be used with the <progress> and <done> elements $job$ token accesses properties from the search, i.e. resultCount runDuration can be used with the <progress> and <done> elements Example usage: show result count in the panel title <panel> <chart> <title> Vendor Sales - $results$ events </title> <query> ... </query> <earliest> ... </earliest> <latest> ... </latest> <done> <set token= \"results\" > $job.resultCount$ </set> <eval token= \"runTime\" > tostring(tonumber($job.runDuration$),\"duration\") </eval> </done> </chart> <html> <p> Job Duration: $runTime$ </p> </html> </panel> Visualization Event Handlers \u00b6 <drilldown> is a kind of visualization event handlers. There is also <selection> which allows users to pan and zoom timerange from within an area, column, and line charts. The token produced can be used anywhere on other panels. i.e. <chart> <search> ... </search> <selection> <set token= \"selection.earliest\" > $start$ </set> <set token= \"selection.latest\" > $end$ </set> </selection> </chart> Then create other panels to listen on the tokens created by the selection. The new panel will refresh whenenver the selection changes from the first panel. depends and rejects are attributes can be used on elements <row> <panel> <chart> <table> <events> <input> <search> to show/hide elements by checking whether the specified token exists. They can be used with event handlers including <drilldown> and <condition> depends and rejects can be used together in case different combination of inputs are used to display different panels. Form Input Event Handlers \u00b6 The <change> element can be used within <input> to control how the dashboard responds to user inputs. The supported input types are checkbox, dropdown, link, radio, text, time For example, when user selecting one input which should clear some other input, you can use <change> and <unset> to do so. One thing to note is that tokens used in <set> <unset> within <change> should be referenced by form.<token_name> for it to work properly. Custom Visualizations \u00b6 Custom visualizations can be downloaded from Splunkbase.com (or from Browse more Apps menu) or created yourself (helpful docs ). Installing new visualizations requires admin role. Simple XML extensions \u00b6 Files put under Search/Appserver/Static can be referenced by the dashboard XML as comma-separated string. For css it is referenced as attribute stylesheet=\"db_style.css\" , and for js it is script=\"db_script.js\" dashboard.css and dashboard.js will automatically be referenced without explicitly doing so in XML. Optimizing and Best Practices \u00b6 Power a Panel using Report \u00b6 When creating dashboards, we tend to create an empty dashboard, then create panels with queries as inline search. The dashboard is configured such that each time it loads, it runs those searches. And if the panels are configured to refresh on an interval, it does so for every panel and for every user having this dashboard opened. This puts more pressure to Splunk as more users open the same dashboard at the same time. There is a way to let all users see the \"cached\" dashboard panels, given that dashboard panels are not subject to taking user inputs. Power a Panel using Report Within the search page, save your search as Report (and make sure no time range picker, so it can be scheduled to run). Then you can immediately add a panel in the dashboard view using the Report you just created (make sure Report is selected rather than inline search). Now go to the Report page, configure the report just created and enable \"Schedule Report\", give it a recurring scheduled time to run so that each time the dashboard loads, it reuse the cached results from the scheduled Report. Prevent panels refresh when inactive \u00b6 onunloadCanelJobs We can set a dashboard attribute onunloadCanelJobs=\"true\" to prevent panels from auto refreshing when a user is navigated to a different browser tab. i.e. <dashboard onunloadCanelJobs= \"true\" > ... </dashboard> Load custom JavaScript \u00b6 custom JavaScript You can have a dashboard load custom JavaScript by providing the attribute script=\"<script_file1, script_file2 ...>\" on the dashboard root tag. This file must be present on the app server's static subfolder. <form script= \"customScript.js\" > ... </form> Show/Hide a panel \u00b6 Use token to control whether element renders You can use rejects or depends attributes on elements to control whether it renders. Both takes a comma separated list of tokens. rejects will hide an element if one the specified tokens is defined; depends will show an element only if all the specified tokens are defined. Complete charting properties \u00b6 Full Charting Configurations can be found on official doc. Using these options to further customize the charts. Visualization also have shared options can be found here . sparkline \u00b6 Add a sparkline to statistics table to show a small trend of data sparkline is a function to use to let a statistics table showing some \"snippet\" of what you get from a timechart on that field. Panels Best Practices \u00b6 Try limit the number of panels to five on a dashboard consider spreading to multiple dashboards if needed more panels As for Reports, saved searches, and other knowledge objects, make title as descriptive as possible recommend a global naming convention for the organization i.e. <sourcetype> by <team/org> - <time_range> title length of 45 chars or less is also recommended Use Report-backed panels, limiting time ranges for the searches, or use more of top and head commands in the search help speed up the panel load time You should consider power your panels with accelerated data models and using tstats Use a Base Search \u00b6 base search and post-process search When multiple panels within the dashboard have the same search terms (before the first pipe), you can pull the common part out as a base search, and reference it and keep build on top of it in other panels' searches (as post-process search). i.e. <dashboard> <search id= \"web_sales\" > <query> index=vendor-sales ... | fields vendor, sales, product_name </query> </search> <row> <panel> <chart> <search base= \"web_sales\" > <query> timechart count by product_name </query> </search> </chart> </panel> </row> </dashboard> This also allows those panels sharing the same base search to make one base search to Splunk, reducing loads on Splunk deployment. Troubleshooting \u00b6 Check the search string For inline search and Prebuilt panels, just paste the query string into a search and verify the data is expected. Also check macros that are powered by their own underlying searches. Short cut to show macro expanded is Ctrl/Command-Shift-E. Use history command to check previous searches stored as events. For Reports, view the report to ensure its functioning properly, note its schedule and acceleration settings. Check XML for illegal characters Characters such as ' \" < > have special meanings in XML and should be escaped using the CDATA tags. i.e. <!--CDATA[ Steve's Games ]--> Use job inspector Use the job inspector within the search page to see how a search was processed as well as many metrics related to the search. More see this video Create HTML panel with tokens It is useful to debug a dashboard tokens by creating a HTML panel showing all token values. i.e. <row> <panel> <html> <p> cat_tok_01 = $cat_tok_01$ </p> <p> cat_tok_02 = $cat_tok_02$ </p> ... </html> </panel> </row>","title":"Splunk Dashboarding"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#making-a-plan","text":"Splunk Dashboard serves a team audience. It can be used by engineers, stakeholders, devops, and customer support. Some questions worth getting clear answers before creating a dashboard: What are the end users and their skill level? Which metrics are critical to individual roles? What is the timespan of the data and how frequent it should refresh? What types of visualizations are required? Layout? Will users access this dashboard through Splunk or in a web app? Do you need custom stylesheet to define parameters for the visualizations? Javascript for additional interactivity? The important part of planning to to sketch out the dashboard's wireframes. Think about the panels you will need, types of visualizations, data you want, how they should arrange, and how the dashboard should look in the end. Next, plan for interactivity by adding inputs. For the search and data, it helps to create a chart listing the data group, the source type, and interesting fields. i.e. Type Source type Interesting Fields Online transactions access_combined action, bytes, categoryId, clientip, itemId, JSESSIONID, price, productId, product_name, referer, referer_domain, sale_price, status, user, useragent Retail sales vendor_sales AcctID, categoryId, product_name, productId, sale_price, Vendor, VendorCity, VendorCountry, VendorID, VendorStateProvince Server access data linux_secure action, app, dest, process, src_ip, src_port, user, vendor_action Event Annotations bcg_sale_dates Sale_Category, Sale_Date, message","title":"Making a Plan"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#html-dashboard","text":"At the dashboards page, you can edit a dashboard and convert it to HTML dashboard. Always do \"Create New\" so you have a backup to return to when running into issues. HTML dashboards allows more customizability but cuts out editability of the dashboard contents. Learn more about it here . You can add texts, images, reference files from other apps, and use common HTML tags. You can also directly add <html> tags within the dashboard source, and reference tokens within it and have it display on the dashboard. i.e. <row> <panel> <title> Time Range: </title> <html> $timeInput.earliest$ to $timeInput.latest$ </html> </panel> </row>","title":"HTML Dashboard"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#edit-dashboard-source-xml","text":"Panels can have seven XML visualization elements, each of which can have their own attributes and options. Full reference is available at here Source XML edit allows more advanced touch to the dashboard to improve efficiency, make appearance consistant cross panels, make more complex interactivity, <chart> <event> <map> <single> single value <table> <viz> custom visualization <html> html Three types of dashboard panels: report inline prebuilt can be referenced by multiple dashboards where they show the same view can be created from existing panels prebuilt panels can be accessed by Settings -> User interface -> Prebuilt panels edit prebuilt panels in XML Note panel refresh is only available to report and inline panels. Prebuilt panels Avoid searches using knowledge objects when creating prebuilt panels, such as event types, tags, lookups. This is because all users may not have access to the same knowledge objects.","title":"Edit Dashboard Source XML"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#add-custom-stylesheets-and-logic","text":"You can add on top of the dashboard XML with CSS, HTML, Javascript, using third-party libraries to add custom behaviors, graphics, and layouts. This can be very labor intensive, however.","title":"Add custom stylesheets and logic"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#dashboard-forms","text":"We can create user input widgets to take in form values, which are stored as tokens. Then tokens can be used as arguments in panels created by inline searches. Dashboard that has no inputs are wrapped in <dashboard> tags, while dashboard that has inputs are wrapped in <form> tags.","title":"Dashboard Forms"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#tokens","text":"Tokens represent variables whose value changes dynamically. It is used to pass search terms and field values to individual or multiple dashboard panels. Tokens can also be used for event handlers which listen for actions and respond with configured behaviors, such as showing/hiding a dashboard panel, useful for configuring drill downs. Tokens can be pre-defined or user-defined. More about dashboard tokens .","title":"Tokens"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#time-input","text":"Time input can be created as a time picker, which creates a token for panels searches to reference and affect the duration covered by the search. You can do so easily through the UI. Or from XML: <fieldset> <input type= \"time\" token= \"timeInput\" > ... <default> <earliest> -7d@d </earliest> <latest> now </latest> </default> </input> </fieldset> ... <search> <query> ... </query> <earliest> $timeInput.earliest$ </earliest> <latest> $timeInput.latest$ </latest> </search> If the time input is not given a token name, it is regarded as the global time picker. The panels without a time selected will use the global time by default. And its XML will show <search> <query> ... </query> <earliest> $earliest$ </earliest> <latest> $latest$ </latest> </search> Note that earliest and latest in the search query still overrides the time picker. It can be useful to put the same panel side-by-side but only allow timerange selection to change on the right panel to allow comparison. A time input can be put within a panel to denote it controls time for that panel (even though the token created is still sharable by other panels). You can still reference the time token within the Panel title to let it reflect the time selected.","title":"Time Input"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#text-input","text":"Manually supply any value to a field token that can be used by panels searches. This token is useful to replace query where a search condition has wildcard match in it, so that the text token can be used instead for filtering data for the panel. It is good to prefix and suffix the token value with \" in the search string so the token value can contain spaces. Default value overides initial value.","title":"Text Input"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#dropdown-menu","text":"Supply selectable values to users, whose values set is also searchable. Options can be static or from a search. There are ways to improve the user experience for the dropdown, such as sort the options alphabetically, list the options sorted by the frequency of appearance.","title":"Dropdown Menu"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#multiselect-input","text":"Allows user simultaneously search for multiple values. The token value prefix/suffix and delimiter are particularly useful here to supply token value as multiple conditions for the search.","title":"Multiselect Input"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#cascading-input","text":"Feed variables from one input dropdowns to another to narrow down data being searched by categoy. i.e. select Country Code -> State -> City.","title":"Cascading Input"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#event-handler","text":"When providing cascading inputs, it is nice to provide a way to quickly clear/reset the form. We can do so with a Radio button that has only one option, and add some <unset> tags to the radio button's tags. i.e. <input type= \"radio\" token= \"reset_menus_tok\" searchWhenChanged= \"true\" > <label></label> <choice value= \"now\" > Reset Menus </choice> <default> now </default> <change> <unset token= \"form.v_country_tok\" ></unset> <unset token= \"form.v_state_tok\" ></unset> <unset token= \"form.v_city_tok\" ></unset> <set token= \"form.reset_menus_tok\" > no </set> </change> </input>","title":"Event Handler"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#token-filters","text":"Token filters allow you to manipulate and escape characters in token values. Splunk has five default Token filters. You can create you own using JavaScript, reference here . More examples here and Splunk JavaScript SDK |s Quote filter - You can use the |s operator in token to ensure it is always quoted, i.e. $mytoken|s$ |h HTML inclusion filter - makes token values valid for HTML, such as &amp; |u URL encoding filter - makes token value URL escaped, such as %20 |n - prevent any encoding to variables contained within the token wrapper, such as preventing URLs from being escaped. $$ - wrap the token with second pair of $ to prevent it from being rendered.","title":"Token filters"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#global-environment-tokens","text":"Tokens available by default and used as $env:token_name$ . Here is a list of global tokens .","title":"Global environment tokens"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#customizing-dashboards","text":"","title":"Customizing Dashboards"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#table-formatting","text":"Using the configuration options from the UI, you can make a statistics table more informative through use of Format and give it sort by field, highlights on values, and formatting on number precisions. The same set of customization rules can be easily copied to another in the XML (Source) editor.","title":"Table Formatting"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#trellis","text":"Use of Trellis allows us visualize the data by category, and create multiple instances of a visualization without running multiple ad hoc searches. Enabling this option may make the panel heights a bit awkward, which we can fix by giving the panel a fixed height or width, or updating the trellis size: <option name= \"height\" > 460 </option>","title":"Trellis"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#event-annotation","text":"Event Annotation allow you to add labels to a dashboard panel, adding context to the data behind it. You do this within an existing panel by editing its source XML. Add another search element with type=\"annotation\" . Then within the search string, use eval to create annotation_label which allows you to use a field name to group annotations. Event annotations require the _time field, so it must be available in the output of the search. The search's time range will inherit from the panel's time range unless specified otherwise in the query. i.e. <serach type= \"annotation\" > <query> index=... | eval annotation_label = \"User Removed from Cart\" | table _time annotation_label </query> </search> Some other useful properties can be added within the eval command are annotation_category : group annotations by category annotation_color : set annotation marker color alternatively can use chart option charting.annotation.categoryColors","title":"Event Annotation"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#hiding-panel-links","text":"You can choose to hide some links shown at bottom of the panel when hovering. A comprehensive list of options to show/hide can be found here . The options also allow further refine the linked search with other queries, useful for drill down more data or on a different or larger time span.","title":"Hiding panel links"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#drilldowns","text":"Drilldowns allows users to run new searches and display additional details by interacting with dashboard panels. Four types of drilldowns: Event, Visualization, Dynamic, Contextual. Event drilldown involves user clicking on an event returned from a search result, allows users to add more terms to the search exclude terms create new search for all events containing the term Visualization drilldown is under the visualization tab, users clicks directs them to the events returned by the underlying search. Dynamic drilldowns allow user interactions to pass values to other searches, dashboards, reports, or external sources. This can be configured by the Drilldown editor for each panel: Link to Search allows using predefined tokens or other tokens available in the link to search with custom search string $click.value$ token is predefined for multiple visualization types when clicked, the visualization x-axis value is populated into $click.value$ token, and the y-axis value is populated to the $click.value2$ token Link to Dashboard allows further see other dashboards with more details for a specific category you can pass parameters to the downstream dashboard that depends on a token parameter name should be: form.<token_name> note the global time range can be directly used in parameter: earliest latest Link to reports allows redirects to reports to prevent unwanted adhoc searches Link to URL allows opening a page from a relative URL (for another Splunk view) or absolute URL (for external resources) Change the value of tokens or set a token A <drilldown> element will be add to the panel in its XML source when it is set. Contextual drilldowns allow dashboard panels to listen for specific events, then trigger custom actions in response, i.e. show/hide panels.","title":"Drilldowns"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#conditional-interactions","text":"Use of drilldown and change actions depending on the fields clicked, which requires change the XML and the use of <condition> elements. i.e. <drilldown> <condition field= \"Vendor Sales\" > <set token= \"params1\" > index=web </set> <link target= \"_self\" > /app/search/product_sales_by_source?form.Source=$params1$ &amp; form.dash2time.earliest=$form.dash1time.earliest$... </link> </condition> </drilldown> More elements that can be used within conditions can be found at here","title":"Conditional Interactions"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#advanced-behaviors","text":"","title":"Advanced Behaviors"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#event-actions","text":"Four types of event actions elements: <eval> <link> <set> <unset> <eval> works like an eval command, by executing an expression and storing the results in a defined field it takes a component of an eval command and adapts for XML i.e. | eval bandwidth=round(Bytes/1024,1024,2) becomes <eval token=\"bandwidth\">round(Bytes/1024,1024,2)</eval> <link> allows setting a destination for a drilldown, input, or search. destination can be a dashboard (i.e. <link>app/search/[dashboard_name]</link> ), form (i.e. <link>app/search/[dashboard_name]?form.token=$token$</link> ), or URL (i.e. <link>[URL]?q=$token$</link> ) <set/unset> allows create new token values or remove a token These event action elements must be wrapped within elements such as <drilldown> <change> <search> , and optionally used within the <condition> element. When used with <search> , you can make the action dependent on the status of a search, from the event handlers <done> <error> <fail> <cancelled> <progress> . <link> can further be used with <finalize> <preview> elements. Comprehensive documentation for event actions is here .","title":"Event Actions"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#search-event-handlers","text":"Five types of search event handlers: <done> <error> <fail> <cancelled> <progress> They can be used with predefiend tokens $job$ $result$ . $result$ token returns the value from the first row of results for the specified field can be used with the <progress> and <done> elements $job$ token accesses properties from the search, i.e. resultCount runDuration can be used with the <progress> and <done> elements Example usage: show result count in the panel title <panel> <chart> <title> Vendor Sales - $results$ events </title> <query> ... </query> <earliest> ... </earliest> <latest> ... </latest> <done> <set token= \"results\" > $job.resultCount$ </set> <eval token= \"runTime\" > tostring(tonumber($job.runDuration$),\"duration\") </eval> </done> </chart> <html> <p> Job Duration: $runTime$ </p> </html> </panel>","title":"Search Event handlers"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#visualization-event-handlers","text":"<drilldown> is a kind of visualization event handlers. There is also <selection> which allows users to pan and zoom timerange from within an area, column, and line charts. The token produced can be used anywhere on other panels. i.e. <chart> <search> ... </search> <selection> <set token= \"selection.earliest\" > $start$ </set> <set token= \"selection.latest\" > $end$ </set> </selection> </chart> Then create other panels to listen on the tokens created by the selection. The new panel will refresh whenenver the selection changes from the first panel. depends and rejects are attributes can be used on elements <row> <panel> <chart> <table> <events> <input> <search> to show/hide elements by checking whether the specified token exists. They can be used with event handlers including <drilldown> and <condition> depends and rejects can be used together in case different combination of inputs are used to display different panels.","title":"Visualization Event Handlers"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#form-input-event-handlers","text":"The <change> element can be used within <input> to control how the dashboard responds to user inputs. The supported input types are checkbox, dropdown, link, radio, text, time For example, when user selecting one input which should clear some other input, you can use <change> and <unset> to do so. One thing to note is that tokens used in <set> <unset> within <change> should be referenced by form.<token_name> for it to work properly.","title":"Form Input Event Handlers"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#custom-visualizations","text":"Custom visualizations can be downloaded from Splunkbase.com (or from Browse more Apps menu) or created yourself (helpful docs ). Installing new visualizations requires admin role.","title":"Custom Visualizations"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#simple-xml-extensions","text":"Files put under Search/Appserver/Static can be referenced by the dashboard XML as comma-separated string. For css it is referenced as attribute stylesheet=\"db_style.css\" , and for js it is script=\"db_script.js\" dashboard.css and dashboard.js will automatically be referenced without explicitly doing so in XML.","title":"Simple XML extensions"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#optimizing-and-best-practices","text":"","title":"Optimizing and Best Practices"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#power-a-panel-using-report","text":"When creating dashboards, we tend to create an empty dashboard, then create panels with queries as inline search. The dashboard is configured such that each time it loads, it runs those searches. And if the panels are configured to refresh on an interval, it does so for every panel and for every user having this dashboard opened. This puts more pressure to Splunk as more users open the same dashboard at the same time. There is a way to let all users see the \"cached\" dashboard panels, given that dashboard panels are not subject to taking user inputs. Power a Panel using Report Within the search page, save your search as Report (and make sure no time range picker, so it can be scheduled to run). Then you can immediately add a panel in the dashboard view using the Report you just created (make sure Report is selected rather than inline search). Now go to the Report page, configure the report just created and enable \"Schedule Report\", give it a recurring scheduled time to run so that each time the dashboard loads, it reuse the cached results from the scheduled Report.","title":"Power a Panel using Report"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#prevent-panels-refresh-when-inactive","text":"onunloadCanelJobs We can set a dashboard attribute onunloadCanelJobs=\"true\" to prevent panels from auto refreshing when a user is navigated to a different browser tab. i.e. <dashboard onunloadCanelJobs= \"true\" > ... </dashboard>","title":"Prevent panels refresh when inactive"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#load-custom-javascript","text":"custom JavaScript You can have a dashboard load custom JavaScript by providing the attribute script=\"<script_file1, script_file2 ...>\" on the dashboard root tag. This file must be present on the app server's static subfolder. <form script= \"customScript.js\" > ... </form>","title":"Load custom JavaScript"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#showhide-a-panel","text":"Use token to control whether element renders You can use rejects or depends attributes on elements to control whether it renders. Both takes a comma separated list of tokens. rejects will hide an element if one the specified tokens is defined; depends will show an element only if all the specified tokens are defined.","title":"Show/Hide a panel"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#complete-charting-properties","text":"Full Charting Configurations can be found on official doc. Using these options to further customize the charts. Visualization also have shared options can be found here .","title":"Complete charting properties"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#sparkline","text":"Add a sparkline to statistics table to show a small trend of data sparkline is a function to use to let a statistics table showing some \"snippet\" of what you get from a timechart on that field.","title":"sparkline"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#panels-best-practices","text":"Try limit the number of panels to five on a dashboard consider spreading to multiple dashboards if needed more panels As for Reports, saved searches, and other knowledge objects, make title as descriptive as possible recommend a global naming convention for the organization i.e. <sourcetype> by <team/org> - <time_range> title length of 45 chars or less is also recommended Use Report-backed panels, limiting time ranges for the searches, or use more of top and head commands in the search help speed up the panel load time You should consider power your panels with accelerated data models and using tstats","title":"Panels Best Practices"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#use-a-base-search","text":"base search and post-process search When multiple panels within the dashboard have the same search terms (before the first pipe), you can pull the common part out as a base search, and reference it and keep build on top of it in other panels' searches (as post-process search). i.e. <dashboard> <search id= \"web_sales\" > <query> index=vendor-sales ... | fields vendor, sales, product_name </query> </search> <row> <panel> <chart> <search base= \"web_sales\" > <query> timechart count by product_name </query> </search> </chart> </panel> </row> </dashboard> This also allows those panels sharing the same base search to make one base search to Splunk, reducing loads on Splunk deployment.","title":"Use a Base Search"},{"location":"Dev-Tools-Reference/Splunk/Splunk-Dashboarding/#troubleshooting","text":"Check the search string For inline search and Prebuilt panels, just paste the query string into a search and verify the data is expected. Also check macros that are powered by their own underlying searches. Short cut to show macro expanded is Ctrl/Command-Shift-E. Use history command to check previous searches stored as events. For Reports, view the report to ensure its functioning properly, note its schedule and acceleration settings. Check XML for illegal characters Characters such as ' \" < > have special meanings in XML and should be escaped using the CDATA tags. i.e. <!--CDATA[ Steve's Games ]--> Use job inspector Use the job inspector within the search page to see how a search was processed as well as many metrics related to the search. More see this video Create HTML panel with tokens It is useful to debug a dashboard tokens by creating a HTML panel showing all token values. i.e. <row> <panel> <html> <p> cat_tok_01 = $cat_tok_01$ </p> <p> cat_tok_02 = $cat_tok_02$ </p> ... </html> </panel> </row>","title":"Troubleshooting"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/","text":"Notes taken from Splunk official trainings. It serves as a quick reference for Splunk Search commands and tricks. Splunk Brief \u00b6 What Splunk is good for \u00b6 Monitoring, alerting, debugging issues and outages, generating reports and insights. Its workflow is roughly: Index Data -> Search & Investigate -> Add Knowledge -> Monitor & Alert -> Report & Analyze Splunk Data Sources can be from: Computers, network devices, virtual machines, Internet devices, commnuication devices, sensors, databases, logs, configurations, messages, call detail records, clickstream, alerts, metrics, scripts, changes, tickets, and more. Splunk Modules \u00b6 Indexer - Process incoming data and string results in indexes as events Create files organized in sets of directories by age When searching, Splunk only open the dirs that match the time frame of the search Search Heads - all users to search the data using Splunk Search Language Takes the search, divide and conquor and distribute work to indexers, and aggregate the results got back Also serve tools like dashboards, alerts, reports Forwarders - Splunk Enterprise component that consumes data and forward to the indexers for process and save Usually resides on the machines where the data origins and requires negligible resources Primary way data is supplied for indexing For a large scale of data sources, it is better to split the Splunk installation into multiple specialized instances : multiple Search Heads and Indexers (which can form clusters), and many Forwarders. Read More: https://splunkbase.splunk.com/ and http://dev.splunk.com/ Splunk User Types \u00b6 Admin User - access to install apps , create knowledge objects for all users Power User - access to create and share knowledge objects for users of an app and do realtime searches User - only see their own knowledge objects and those shared with them knowledge objects created must be made share-in-app by Power User for it to be seen by other Users Get Data into Splunk \u00b6 Admin User upload data files through web UI Configure Splunk to monitor some files locally on the machine Event Logs, File System changes, Active Directory, Network Info (Main) Receive data from external Forwarders Read more: https://docs.splunk.com/Documentation/Splunk/latest/Data/Usingforwardingagents How Data is stored \u00b6 Splunk store data in indexes which held data in groups of data buckets . Bucketing \u00b6 Data are stored in Hot buckets as they arrive , then Warm buckets and time elapses and finally Cold buckets. Eventually they go to Frozen buckets for deletion or archiving . Each bucket contains: a journal.gz file, where Splunk stores raw event data , composed of many smaller and compressed slices (each about 128 kb) time-series index (.tsidx) files, serves as index keys to the journal file Splunk uses this to know which slice to open up and search for events created from raw events, where each line is tokenized with the token words written out to a lexicon which exists inside each tsidx file and each lexicon has a posting array which has the location of the events in the journal file Bloom Filters \u00b6 Bloom Filters is created based on tsidx files when a bucket roll over from Hot to Warm it is a data structure for quick elimination of data that doesn't match a search allows Splunk to AVOID accessing disk unless really necessary when creating a Bloom Filter, each term inside a bucket's tsidx files' lexicon is run through a hashing algorithm resulting hash sets the bits in the Bloom filter to 0/1 When a search is run, it generates its own Bloom filter based on the search terms and compared with the buckets' Bloom Filters, resulting a fast filter on unrelevant buckets Segmentation \u00b6 The process of tokenizing search terms at search-time. Two stages: splitting events up by finding major/minor breaker characters major breakers are used to isolate words, phrases, terms, and numerical data example major breakers: space \\n \\r \\t [] ! , minor breakers go through results in the first pass and break them up further in the second pass example minor breakers: / : . - $ The tokens become part of tsidx files lexicons at index-time and used to build bloom filters Read more from doc page Inside a Search \u00b6 Take this search as an example: index = security failed user = root | timechart count span = 1h | stats avg ( count ) as HourlyAverage Splunk extracts 'failed' and 'root' and creates a Bloom Filter from them Splunk then identify buckets in security index for the past 24h Splunk compares the search Bloom Filter with those from the buckets Splunk rules out tsidx files that don't have matches Splunk checks the tsidx files from the selected buckets for extracted terms Splunk uses the terms to go through each posting list to obtain seek address in journal file for raw events Splunk extracts search-time fields from the raw events Splunk filters the events that contains our search terms 'failed' and 'root' The remaining events are search results Inspect a Search \u00b6 Within the Job Inspector: command.search.index gives the time to get location info from tsidx files command.search.rawdata tells the time to extract event data from the gzip journal files command.search.kv tells the time took to perform search-time field extractions command.search.filter shows the time took to filter out non-matching events command.search.log look for phrase \"base lispy\" that contains the Lispy Expression created from the query and is used to build Bloom Filter and locate terms in tsidx files Search and Report App \u00b6 The Search & Report app allows you to search and analyze data through creating knowledge objects, reports, dashboards and more. Search UI \u00b6 Events tab displays the events returned for your search and the fields extracted from the events. patterns tab allows you to see patterns in your data. If your search generates statistics or visuals, they will appear in Statistics and Visualization tabs. Commands that create statistics and visualizations are called transforming commands , they transform data into data tables. By default, a search is active & valid for 10 minutes , otherwise needs rerun. A shared search job is active for 7 days and its first-ran result visible by anyone you shared with. From the web UI, use Search & Reporting -> Search History to quickly view recent searches and reuse the searches if possible. Also from the web UI, use Activity -> Jobs to view recent search jobs (with data). Then further use Job -> inspect job to view the query performance. Search Fields \u00b6 Search fields are key=value string in the search query and will be automatically extracted and available for use by commands for creating insights. The key is case-sensitive but the value is not. Three fields ALWAYS available are hosts , source , and sourcetype . Interesting fields are those appear within at least 20% of the events From the extracted fields, a denotes a String value, # denotes a numeral value In the search, = != can be used on numeral and String values; > >= < <= can be used on numeral values only. Tags \u00b6 Tags are Knowledge Objects allow you to designate descriptive names for key-value pairs. It is useful when you have logs that multiple fields may have the same value(s) you want to filter with. Then you can tag those fields with the same tag name and in future searches do filter with the tag name. Tags are value-specific and are case-sensitive. When you tag a key-value pair you are associating a tag with that pair. When the field appears with another value then it is not tagged with this tag name. Do NOT tag a field having infinite possible values . Steps: in a Splunk search, click on an event and see the list of key=vlaue pairs and in actions dropdown select Edit Tags to add new tags (comma separated). To search with tags, enter tag=<tag_name> . For example, index=security tag=SF matches all fields that are tagged with that value. To limit a tag to a field do tag::<field>=<tag_name> Read more about tags in doc page Event Type \u00b6 Event Type allows saving a search's definition and use the definition in another search to highlight matching events. It is good for categorizing events based on search terms. The priority settings affects the display order of returned results. Steps: after a search, save as \"Event Type\" and use it in another search with eventtype . For example, index=web sourcetype=access_combined eventtype=purchase_strategy Knowledge Objects \u00b6 Knowledge Objects are like some tools to help discover and analyze data , which include data interpretation, classification, enrichment, normalization, and search-time-mapping of knowledge (Data Models). Saved Searches, Reports, Alerts, and Dashboards are all Knowledge Objects. Field Extractor \u00b6 The Field Extractor allows to use a GUI to extract fields that persist as Knowledge Objects and make them reusable in searches. It is useful if the information from events are badly structured (not in some key=value fashion), such as simple statements in a natural language but contains useful information in a specific pattern . Fields extracted are specific to a host, source, or sourcetype and are persistent . Two different ways a field extractor can use: regex and delimiters : regex : using regular expression, work well when having unstructured data and events to extract fields from delimiters : a delimiter such as comma, space, or any char. Use it for things like CSV files Three ways to access a field extractor: - Settings -> Fields - The field sidebar during a search - From an event's actions menu (easiest) Read more from the docs page for Splunk regex. Field Aliases \u00b6 Field Aliases give you a way to normalize data over multiple sources . For example, when aggregating results from multiple sources or indexes, but the data logged may have fields with different names such as url, uurl, dest_url, service_url, etc. which all refers to the same thing. Then this tool is handy. You can create one or more aliases to any extracted field and can apply them to lookup . Steps: Settings -> Fields -> Field aliases (add new) set destination app name (choose a meaningful name for the new field) apply to (sourcetype, source, or host) and its matching value Map fieldName=newFieldName Map multiple fields by adding another field Calculated Field \u00b6 Calculated Field saves frequently used calculation you would do in searches with complex eval statements and it is like a shorthand for a function made of SPL commands on extracted fields . For example, you might want to save something to do conversion between bytes to megabytes, or encoding some field to sha256, etc. Steps: Settings -> Fields -> Calculated fields (add new) Search Macros \u00b6 Macros are reusable search strings or portions of search strings. It is useful for frequent searches with complicated search syntax and can store entire search strings . You can pass arguments (fields or constants) to the search. Steps: Settings -> Advanced Search -> Search macros (add new) Then use it in the search by surrounding the macro name with backticks. For example: index=sales sourcetype=vendor_sales | stats sum(sale_price) as total_sales by Vendor | ``convertUSD`` Within a search, use ctrl-shift-E or cmd-shift-E to preview and expand the macro used without running it. Datasets \u00b6 Datasets are small subsets of data for specific purposes, defined like tables with fields names as columns and fields values as cells. Lookups \u00b6 Lookups are in fact datasets that allow you to add useful values to your events not included in the indexed data. Field values from a Lookup are case sensitive by default Set it up in Splunk UI -> Settings -> Lookups -> Define a lookup table Use | inputlookup <lookup_definition_name> to verify lookup is setup correctly. Additionally, you can populate lookup table with search results or from external script or linux executable. Read more about external lookups and KVstore lookups Some tips for using lookups: in lookup table, keep the key field the first column put lookups at the end of the search if a lookup usage is very common in search, make it automatic use KV store or gzip CSVs for large lookup table only put data being used in lookup table the outputlookup can output results of search to a static lookup table Pivot \u00b6 Pivot allows users to get knowledge from the data without learning SPL in deep; a UI-based tool can help the users with creating useful searches. It is like a tool created by knowledgable Splunk users to serve the users don't know much about Splunk. Data Models \u00b6 Data Models are made of Datasets and are knowledge objects that provide the data structure that drives Pivots. It can also be used to significantly speed up searches, reports, and dashboard. Data Models must be created by Admins and Power Users and are hierarchically strucutred datasets. Starting from a Root Object (a search with index and sourcetype), then add Child Objects (a filter or constraint). Data Models are good for accelerating searches . Doing searches on data models are like searching on some cached events that gets updated regularly. Steps: Settings -> Data Models -> New Data Model Then search the datamodel with | datamodel keywords Read more from the doc page Splunk Search Language (SPL) \u00b6 SPL Components \u00b6 Search Terms - defines what data to return i.e. host=xxx status>399 also includes Boolean operators ( NOT OR AND in its evaluation precedence) Commands - process search results and, create charts, compute statistics, and format results i.e. eval in eval foo=1 Functions - serves supplying additional conditions and algorithms to compute and evaluate the results in Commands i.e. avg in stats avg(latency) by host Arguments - variables we want to apply to the Functions i.e. latency in stats avg(latency) by host Clauses - how we want the results grouped i.e. by in stats avg(latency) by host Search Terms, Command names, Function names are NOT case-sensitive , with the exception to the command replace where search term must be an exact match. Search Terms \u00b6 Keywords , any keywords can be searched on; \"AND\" boolean is implied for multiple keywords Booleans , AND OR NOT Phrases , keywords surrounded by \"\" Fields , key=value pair search Wildcards , used in keywords and fields to match any chars; inefficient if used at the beginning of a word Comparisons , = != < <= > >= Subsearch \u00b6 Subsearch is a search enclosed in brackets that passes its results to its outer search as search terms . The return command can be used in subsearch to limit the first few number of values to return, i.e. | return src_ip . The field name is included by default; to omit returning values with the field name, use $ , i.e. | return 5 $src_ip Example index = security sourcetype = linux_secure \"accepted\" [ search index = security sourcetype = linux_secure \"failed password\" src_ip! = 10 .0.0.0/8 | stats count by src_ip | where count > 10 | fields src_ip ] # when subsearch returns, Splunk implicitly adds \"OR\" between each two results # then whole results is combined with the outter search with an implicit \"AND\" operator | dedup src_ip, user | table src_ip, user # subsearch with 'return' command index = sales sourcetype = vendor_sales [ search index = sales sourcetype = vendor_sales VendorID< = 2000 product_name = Mediocre* | top limit = 5 Vendor | return 5 Vendor ] | rare limit = 1 product_name by Vendor | fields - percent, count Some limitations of subsearches: default time limit of 60 seconds to execute right after time limit, result is finalized and returned to outer search default returns up to 10000 results if the outter search is executed real-time , its subsearch is executed for all-time best to use earliest and latest time modifiers in subsearch SPL Commands \u00b6 Transforming vs. Streaming Commands transforming commands operate on the entire result set of data executes on the Search Head and waits for full set of results change event data into results once its complete example commands stats timechart chart top rare streaming commands has two types centralized (aka \"Stateful Streaming Commands\") executes on the Search Head apply transformations to each event returned by a search results order depends on the order when data came in example commands transaction streamstats distributable executes on Search head or Indexers order of events does not matter when preceded by commands that have to run on a Search Head, ALL will run on a Search Head example commands rename eval fields regex Read more from doc page Streaming Commands \u00b6 append \u00b6 Add results by appending subsearch results to the end of the main search results as additional rows . Does NOT work with read-time searches. It is useful to draw data on the same visualization by combining results from two searches. However, rows are NOT combined based on the values in some of the fields. Example index = security sourcetype = linux_secure \"failed password\" NOT \"invalid user\" | timechart count as known_users | append [ search index = security sourcetype = linux_secure \"failed password\" \"invalid user\" | timechart count as unknown_users ] # Final result data may look awkward as rows are NOT combined based on the values in some of the fields. # Fix it with the `first` Function - return the first value of a field by the order it appears in the results. | timechart first ( * ) as * # apply first Function on all fields and keep their original name appendcols \u00b6 Add results of a subsearch to the right of the main search's results. It also attempts to match rows by some common field values, but it is not always possible. Example index = security sourcetype = linux_secure \"failed password\" NOT \"invalid user\" | timechart count as known_users | appendcols [ search index = security sourcetype = linux_secure \"failed password\" \"invalid user\" | timechart count as unknown_users ] # \"unknown_users\" is added as a new column to the outter search's results and matches the _time field appendpipe \u00b6 Append subpipeline search data as events to your search results. Appended search is NOT run until the command is reached . appendpipe is NOT an additional search , but merely a pipeline for the previous results. It acts on the data received and appended back new events while keeping the data before the pipe. It is useful to add summary to some results. Example index = network sourcetype = cisco_wsa_squid ( usage = Borderline OR usage = Violation ) | stats count by usage, cs_username | appendpipe [ stats sum ( count ) as count by usage | eval cs_username = \"TOTAL: \" .usage ] | appendpipe [ search cs_username = Total* | stats sum ( count ) as count | eval cs_username = \"GRAND TOTAL\" ] | sort usage, count addtotals \u00b6 Compute the SUM of ALL numeric fields for each event (table row) and creates a total column (default behavior). You can override the default behavior and let it calculate a sum for a column , see the example. Example # adds a column to calculate the totals of all numeric fields per row index = web sourcetype = access_combined | chart sum ( bytes ) over host by file | addtotals fieldname = \"Total by host\" # adds a row to calculate the totals of one column index = web sourcetype = access_combined | chart sum ( bytes ) as sum over host | addtotals col = true label = \"Total\" labelfield = sum bin \u00b6 Adjust and group numerical values into discrete sets (bins) so that all the items in a bin share the same value. bin overrides the field value on the field provided. Use a copied field if need the original values in subsequent pipes. Example index = sales sourcetype = vendor_sales | stats sum ( price ) as totalSales by product_name | bin totalSales span = 10 # span is the bin size to group on | stats list ( product_name ) by totalSales | eval totalSales = \" $ \" .totalSales dedup \u00b6 Remove duplicated events from results that share common values from the fields specified. Example index = security sourcetype = history* Address_Description = \"San Francisco\" | dedup Username # can be a single field or multiple fields | table Username First_Name Last_Name erex \u00b6 Like an auto field extractor, but had the same shortcomings as the regex field extractor through Splunk UI: only look for matches based on the samples provided. Better use rex command. Example index = security sourcetype = linux_secure \"port\" | erex ipaddress examples = \"74.125.19.106, 88.12.32.208\" | table ipaddress, src_port eval \u00b6 Calculate and manipulate field values in many powerful ways and also works with multi-value fields. Results can create new fields (case-sensitive) or override existing fields. Multiple field expressions can be calculated on the same eval command, and one is created/altered after another so the order of these expressions matters. Besides the many Functions it supports, eval supports operators include arithmetic, concatenation (using . ), and boolean. Example index = network sourcetype = cisco_wsa_squid | stats sum ( sc_bytes ) as Bytes by usage | eval bandwidth = round ( Bytes/1024/1024, 2 ) # eval with if conditions and cases | eval VendorTerritory = if ( VendorId< 4000 , \"North America\" , \"Other\" ) | eval httpCategory = case ( status> = 200 AND status< 300 , \"Success\" , status> = 300 AND status< 400 , \"Redirect\" , status> = 400 AND status< 500 , \"Client Error\" , status> = 500 , \"Server Error\" , true () , \"default catch all other cases\" ) # eval can be used as a Function with stats command to apply conditions index = web sourcetype = access_combined | stats count ( eval ( status< 300 )) as \"Success\" , ... Supported Functions see docs page eventstats \u00b6 Generates statistics for fields in searched events and save them as new fields in the results. Each event will have the SAME value for the statistics calculated. Example index = web sourcetype = access_combined action = remove | chart sum ( price ) as lostSales by product_name | eventstats avg ( lostSales ) as averageLoss | where lostSales > averageLoss | fields - averageLoss | sort -lostSales Supported Functions see docs page See also streamstats command. fieldformat \u00b6 Format values WITHOUT changing characteristics of underlying values. In other words, only a format is specified when the value is displayed as outputs and the same field can still participate calculations as if their values are not changed. Example index = web sourcetype = access_combined product_name = * action = purchase | stats sum ( price ) as total_list_price, sum ( sale_price ) as total_sale_price by product_name | fieldformat total_list_price = \" $ \" + tostring ( total_list_price ) Supported Functions see docs page . Specifically look for Conversion Functions and Text Functions. fields \u00b6 Include or exclude fields from search results to limit the fields to display and also make search run faster. Field extraction is one of the most costly parts of searching in Splunk. Eliminate unnecessary fields will improve search speed since field inclusion occurs BEFORE field extraction, while field exclusion happens AFTER field extraction. Internal fields like raw and _time will ALWAYS be extracted, but can also be removed from the search results using fields . Example index = web sourcetype = access_combined | fields status clientip # only include fields status and clientip index = web sourcetype = access_combined | fields - status clientip # to exclude fields status and clientip fillnull \u00b6 Replaces any null values in your events. It by default fills NULL with 0 and it can be set with value=\"something else\" Example index = sales sourcetype = vendor_sales | chart sum ( price ) over product_name by VendorCountry | fillnull foreach \u00b6 Run templated subsearches for each field in a specified list, and each row goes through the foreach command and subsearch calculation. Example index = web sourcetype = access_combined | timechart span = 1h sum ( bytes ) as totalBytes by host | foreach www* [ eval '<<FIELD>>' = round ( '<<FIELD>>' / ( 1024 *1024 ) ,2 )] # '<<FIELD>>' refers to the value of the column_name # assume 'host' gives values www1 www2 www3 join \u00b6 Combine results of subsearch with outter search using common fields . It requires both searches to share at least one field in common. Two types of join: inner join (default) - only return results from outter search that have matches in the subsearch outer/left join - returns all results from the outter search and those have matches in the subsearch; can be specified with argument Example index = \"security\" sourcetype = linux_secure src_ip = 10 .0.0.0/8 \"Failed\" | join src_ip [ search index = security sourcetype = winauthentication_security bcg_ip = * | dedup bcg_workstation | rename bcg_ip as src_ip | fields src_ip, bcg_workstation ] | stats count as access_attempts by user, dest, bcg_workstation | where access_attempts > 40 | sort - access_attempts See also union lookup \u00b6 Invoke field value lookups. Example # http_status is the name of the lookup definition # code is one of the columns in the csv, and we have status in our event # defualt all fields in lookup table are returned except the input fields # specify OUTPUT to choose the fields added from the lookup index = web sourcetype = access_combined NOT status = 200 | lookup http_status code as status, OUTPUT code as \"HTTP Code\" , description as \"HTTP Description\" makemv \u00b6 Create a multi-value field from an existing field and replace that field. It splits its values using some delimiter or regex . Example index = sales sourcetype = vendor_sales | eval account_codes = productId | makemv delim = \"-\" , account_codes | dedup product_name | table product_name, productId, account_codes # same results can be achieved by using regex and specifying capture groups | makemv tokenizer = \"([A-Z]{2}|[A-Z]{1}[0-9]{2})\" , account_codes multikv \u00b6 Extract field values from data formatted as large and single events of tabular data . Each row becomes an event, header becomes the field names, and tabular data becomes values of the fields. Example of some data event suitable: Name Age Occupation Josh 42 SoftwareEngineer Francine 35 CEO Samantha 22 ProjectManager Example index = main sourcetype = netstat | multikv fields LocalAddress State filter LISTEN | rex field = LocalAddress \":(?P<Listening_Port>\\d+) $ \" | dedup Listening_Port | table Listening_Port mvexpand \u00b6 Create separate event for each value contained in a multi-value field. It reates new events count as of the number of values in a multi-valued field and copy all other values to the new evnets. Best to remove unused fields before the mvexpand command. Example index = systems sourcetype = system_info | mvexpand ROOT_USERS | dedup SYSTE ROOT_USERS | stats list ( SYSTEM ) by ROOT_USERS rename \u00b6 Rename one or more fields. After the rename, subsequent commands must use the new names otherwise operation won't have any effects. Note that after rename if a field name contains spaces , you need to use single quotes on the field name to refer to it in subsequent commands. Example index = web sourcetype = access* status = 200 product_name = * | table JESSIONID, product_name, price | rename JESSIONID as \"User Session Id\" , product_name as \"Purchased Game\" , price as \"Purchase Price\" rex \u00b6 Allow use regex capture groups to extract values at search time. By default, it uses field=_raw if not provided a field name to read from. Example index = security sourcetype = linux_secure \"port\" | rex \"(?i) from (?P<ipaddress>[^ ]+)\" | table ipaddress, src_port index = network sourcetype = cisco_esa | where isnotnull ( mailfrom ) | rex field = mailfrom \"@(?P<mail_domain>\\S+)\" Better examples can be found at doc page search \u00b6 Add search terms further down the pipeline . In fact, the part before the first | is itself a search command. Example index = network sourcetype = cisco_wsa_squid usage = Violation | stats count ( usage ) as Visits by cs_username | search Visits > 1 sort \u00b6 Organize the results sorted by some fields. Example sourcetype = vendor_sales | table Vendor product_name sale_price | sort - sale_price Vendor # the '-' here affects all fields | sort -sale_price Vendor # sort-decending will only affect sale_price while Vendor is sorted ascending | sort -sale_price Vendor limit = 20 # will also limit the results for the first twenty in sorted order. spath \u00b6 Extracts info from semi-structured data (such as xml, json) and store in fields. It by default uses input=_raw , if not provided a field name to read from. Example index = systems sourcetype = system_info_xml | spath path = event.systeminfo.cpu.percent output = CPU # working with json, values in array specified with {} index = systems sourcetype = system_info_json | spath path = CPU_CORES {} .idle output = CPU_IDLE | stats avg ( CPU_IDLE ) by SYSTEM # spath Function index = hr sourcetype = HR_DB | eval RFID = spath ( Access, \"rfid.chip\" ) , Location = spath ( Access, \"rfid.location\" ) | table Name, Access, RFID, Location Better examples see doc page streamstats \u00b6 Aggregates statistics to your searched events as Splunk sees the events in time in a streaming manner. In other words, it calculates statistics for each event cumulatively at the time the event is seen. Three important arguments to know: current - setting to 'f' tells Splunk to only use the field values from previous events when performing statistical function window - setting to N tells Splunk to only calculate previous N events at a time; default 0 means uses all previous events time_window - setting to a time span to only calculate events happen in every that time span; with this function, events must be sorted by time Example index = web sourcetype = access_combined action = purchase status = 200 | stats sum ( price ) as orderAmount by JESSIONID | sort _time | streamstats avg ( orderAmount ) as averageOrder current = f window = 20 # for each event, calculate the average from the previous 20 events' orderAmount field table \u00b6 Specify fields kept in the results and retains the data in a tabular format. Fields appears in the order specified to the command. Example index = web sourcetype = access_combined | table status clientip transaction \u00b6 Transaction is any group of related events for a time span which can come from multiple applications or hosts. Events should be in a reversed chronological order before running this command. transaction command takes in one or multiple fields to make transactions, and creates two fields in the raw event: duration (time between the first and last event) and eventcount (number of events) transaction limits 1000 events per transaction by default and is an expensive command. Use it only when it has greater value than what you can do with stats command. Use keepevicted=true argument to keep incomplete transactions in results and closed_txn field will be added to indicate transactions incomplete. Example index = web sourcetype = access_combined | transaction clientip startswith = action = \"addtocart\" endswith = action = \"purchase\" maxspan = 1h maxpause = 30m # startswith and endswith should be valid search strings # compare the two searches # using transaction index = web sourcetype = access_combined | transaction JESSIONID | where duration > 0 | stats avg ( duration ) as avgd, avg ( eventcount ) as avgc | eval \"Average Time On Site\" = round ( avgd, 0 ) , \"Average Page Views\" = round ( avgc, 0 ) | table \"Average Time On Site\" , \"Average Page Views\" # using stats index = web sourcetype = access_combined | fields JESSIONID | stats range ( _time ) as duration, count as eventcount by JESSIONID | where duration > 0 | stats avg ( duration ) as avgd, avg ( eventcount ) as avgc | eval \"Average Time On Site\" = round ( avgd, 0 ) , \"Average Page Views\" = round ( avgc, 0 ) | table \"Average Time On Site\" , \"Average Page Views\" transaction is a very expensive operation. To build a transaction, all events data is searched and required, and is done on the Search Head. Some use cases can also be achieved by stats command. union \u00b6 Combine search results from two or more datasets into one. Datasets can be: saved searches, inputlookups, data models, subsearches. Example | union datamoel:Buttercup_Games_Online_Sales.successful_purchase, [ search index = sales sourcetype = vendor_sales ] | table sourcetype, product_name See also join untable \u00b6 Does the opposite of xyseries , for which it takes chartable and tabular data then format it similar to stats output. Good when need to further extract from, and manipulate values after using commands that result in tabular data. Syntax | untable row-field, label-field, data-field produces 3-column results Example index = sales sourcetype = vendor_sales ( VendorID > = 9000 ) | chart count as prod_count by product_name, VendorCountry limit = 5 useother = f | untable product_name, VendorCountry, prod_count | eventstats max ( prod_count ) as max by VendorCountry | where prod_count = max | stats list ( product_name ) , list ( prod_count ) by VendorCountry where \u00b6 Filter events to ONLY keep the results that evaluate as true . Try use search whenever you can rather than where as it is more efficient this way. Example index = network sourcetype = cisco_wsa_squid usage = Violation | stats count ( eval ( usage = \"Personal\" )) as Personal, count ( eval ( usage = \"Business\" )) as Business by username | where Personal > Business AND username! = \"lsagers\" | sort -Personal Supported Functions see docs page . where shared many of the eval Functions. xyseries \u00b6 Convert statistical results into a tabular format suitable for visualizations. Useful for additional process after initial statistical data is returned. Syntax | xyseries row-field, column-field, data-field produces n+1 column result, where n is number of distinct values in column-field. Example index = web sourcetype = access_combined | bin _time span = 1h | stats sum ( bytes ) as totalBytes by _time, host | eval MB = round ( totalBytes/ ( 1024 *1024 ) ,2 ) | xyseries _time, host, MB # transforms _time | host | MB ----- | ---- | -- 12 :00 | www1 | 12 12 :00 | www2 | 11 12 :00 | www3 | 7 ... | ... | ... # into _time | www1 | www2 | www3 ----- | ---- | ---- | ---- 12 :00 | 12 | 11 | 7 ... | ... | ... | ... See also untable command. Transforming Commands \u00b6 chart \u00b6 Organize the data as a regular or two-way chart table . To get a two-way table, you need to provide three fields used as table row value, table column value, and table data value. A common pattern is chart <function(data_field)> over <row_field> by <column_field> chart is limited to display 10 columns by default , others will show up as an \"other\" column in the chart visualization. The 'other' column can be turned off by passing an argument useother=f . Use limit=5 to control the max number of columns to display, or limit=0 to display all columns. Example index = web sourcetype = access_combined status> 299 | chart count over status by host # if more than one field is supplied to `by` clause without `over` clause # the first field is used as with a `over` clause # this has the same effect as above command index = web sourcetype = access_combined status> 299 | chart count by status, host Supported Functions see docs page fieldsummary \u00b6 Calculate summary statistics for fields in your events. For example, given a search index=web | fieldsummary it gives insights of the \"count, distinct_count, is_exact, min/max/mean/stdev, numberic_count, values\" for each field. If field names are provided, then only do summary for those fields provided. rare \u00b6 Shows the least common values of a field set. It is the opposite of top Command and accepts the same set of clauses. Example index = sales sourcetype = vendor_sales | rare Vendor limit = 5 countfield = \"Number of Sales\" showperc = False useother = True stats \u00b6 Produce statistics of our search results and need to use functions to produce stats. The eval Function is handy to apply quick and concise conditional filtering to limit the statistics calculated from desired data. Example index = sales sourcetype = vendor_sales | stats count as \"Total Sells By Vendors\" by product_name, categoryid # apply conditions using eval Function index = web sourcetype = access_combined | stats count ( eval ( status< 300 )) as \"Success\" , ... Supported Functions see docs page timechart \u00b6 Performs stats aggregations against time , and time is always the x-axis. Like chart , except only one value can be supplied to the by clause This command determines the time intervals from the time range selected, and it can be changed by using an argument span . Example index = web sourcetype = vendor_sales | timechart count by product_name span = 1m Supported Functions see docs page timewrap \u00b6 Compare the data from timechart further over an older time range. Specify a time period to apply to the result of a timechart command, then display series of data based on this time periods, with the X axis display the increments of this period and the Y axis display the aggregated values over that period. Example index = sales sourcetype = vendor_sales product_name = \"Dream Crusher\" | timechart span = 1d sum ( price ) by product_name | timewrap 7d | rename _time as Day | eval Day = strftime ( Day, \"%A\" ) top \u00b6 Finds the most common values of given field(s) in a result set and automatically returns count and percent columns. By default displays top 10 and the count can be set with limit=n ; while limit=0 yields all results. Top Command Clauses: limit=int countfield=string percentfield=string showcount=True/False showperc=True/False showother=True/False otherstr=string Example index = sales sourcetype = vendor_sales | top Vendor product_name limit = 5 # top command also supports results grouping by fields index = sales sourcetype = vendor_sales | top product_name by Vendor limit = 3 countfield = \"Number of Sales\" showperc = False Generating Commands \u00b6 Commands that generates events. datamodel \u00b6 Display the structure of data models and search against them. It accepts first argument as datamodel name and the second argument as the object name under than model. Add search to enable search on the data model data. Example | datamodel Buttercup_Games_Online_Sales http_request search | search http_request.action = purchase inputlookup \u00b6 Return values from a lookup table. Useful to use it in a subsearch to narrow down the set of events to search based on the values in the lookup table. Example index = sales sourcetype = vendor_sales [ | inputlookup API_Tokens | table VendorID ] # only take one column to use as filter; its values will be OR-ed together # can add 'NOT before the subsearch to exclude those vendor ids | top Vendor product_name limit = 5 makeresults \u00b6 Creates a defined number of search results, good for creating sample data for testing searches or building values to be used in searches. Example # must starts search with pipe | makeresults | eval tomorrow = relative_time ( now () , \"+1d\" ) , tomorrow = strftime ( tomorrow, \"%A\" ) , result = if ( tomorrow = \"Saturday\" OR tomorrow = \"Sunday\" , \"Huzzah!\" , \"Boo!\" ) , msg = printf ( \"Tomorrow is %s, %s\" , tomorrow, result ) tstats \u00b6 Get statistical info from tsidx files . Splunk will do full search if the search ran is outside of the summary range of the accelerated data model. Limit search to summary range with summariesonly argument. Example | tstats count as \"count\" from datamodel = linux_server_access where web_server_access.src_ip = 10 .* web_server_access.action = failure by web_server_access.src_ip, web_server_access.user summariesonly = t span = 1d | where count > 300 | sort -count Special Commands \u00b6 Splunk has commands to extract geographical info from data and display them in a good format. gauge \u00b6 Show a field value in a gauge. Example index = sales sourcetype = vendor_sales | stats sum ( sale ) as total | gauge total 0 3000 6000 7000 geom \u00b6 Adds fields with geographical data structures matching polygons on a choropleth map visulization . Example index = sales sourcetype = vendor_sales | stats count as Sales by VendorCountry | geom geo_countries featureIdField = VendorCountry # the field `VendorCountry` must map back a country name in the featureCollection geostats \u00b6 Aggregates geographical data for use on a map visualization . Example index = sales sourcetype = vendor_sales | geostats latfield = VendorLatitude longfield = VendorLongitude count by product_name globallimit = 4 iplocation \u00b6 Lookup IP address and add location information to events. Data like city, country, region, latitude, and longitude can be added to events that include external IP addresses. Not all location info might be available depends on the IP. Example index = web sourcetype = access_combined action = purchase status = 200 | iplocation clientip trendline \u00b6 Compute moving averages of field values, gives a good understanding of how the data is trending over time . Three trendtypes (used as Functions by trendline command): simple moving average ( sma ): compute the sum of data points over a period of time expoential moving average ( ema ): assigns a heavier weighting to more current data points weighted moving average ( wma ): assigns a heavier weighting to more current data points Example index = web sourcetype = access_combined | timechart sum ( price ) as sales | trendline wma2 ( sales ) as trend ema10 ( bars ) # '2' here means do calculation per 2 events # this number can be between 2 and 10000 SPL Best Practices \u00b6 'key!=value' vs 'NOT key=value' A small difference: key!=value only includes entries where key is not NULL, while NOT key=value includes entries even when the key does NOT exitst on those events entries. use the 'IN' operator Instead of doing (key=value1 OR key=value2 OR key=value3) , more viewable syntax: key IN (\"value1\", \"value2\", \"value3\") use 'index' 'source' 'host' early These fields are extracted when data was indexed and stored and won't take time to extract at search time. It is best to use these fields early in the search to limit the data to be further processed. use 'inclusion' over 'exclusion' Searching for exactly \"something\" is better than searching for \"NOT something\" specify time in the query when appropriate Use relative time earliest=-2h latest=-1h or absolute time earliest=01/08/2018:12:00:00 in the search. Use @ to round down time to the nearest 0 in the unit used earliest=-2@h time is the most efficient way to improve queries search time Splunk stores data in buckets (directories) containing raw data and indexing data, and buckets have maximum size and maximum time span. Three kinds of searchable buckets: hot, warm, cold. Access speed: hot (read/write, very fast, like memory) > warm (slow, read-only, slower medium) > cold (very slow, read-only, cheap and stable medium). When search is run, it go look for the right bucket to open, uncompress the raw data and search the contents inside. The order of effectiveness in filtering data: time > index > source > host > sourcetype create different indexes to group data Having specialized indexes helps make Splunk searches faster and more efficient. be careful when using wildcards Using wildcard at the beginning of a word cause Splunk to search all events which causes degradation in performance . Using wildcard in the middle of a string might cause inconsistent results due to the way Splunk indexes data that contains punctuation Best to make search as specific as possible, and only use wildcards at the end of a word , say status=failure instead of status=fail* save search as reports using fast mode Fast mode emphasis on performance and returns only essential data . Verbose mode emphasizes completeness and returns all fields and event data . Smart mode weights tradeoffs and returns the best results for the search being run. use 'fields' command to extract only the fields you need Doing so as early in the search as possible will let each indexer extract less fields, pack and return data back faster. consult Job Inspector to know the performance of the searches It tells you which phase of the search took the most time . Any search that has not expired can be inspected by this tool. Read more about Job Inspector avoid using subsearches when possible Compare the follow two queries, which returns the same results but the second one is significantly faster: index = security sourcetype = winauthentication_security ( EventCode = 4624 OR EventCode = 540 ) NOT [ search sourcetype = history_access | rename Username as User | fields User ] | stats count by User | table User index = security ( sourcetype = winauthentication_security ( EventCode = 4624 OR EventCode = 540 )) OR ( sourcetype = history_access ) | eval badge_access = if ( sourcetype = \"history_access\" , 1 , 0 ) | stats max ( badge_access ) as badged_in by Username | where badged_in = 0 | fields Username SPL Tricks \u00b6 let subsearch limit main search's time range Let the subsearch output a table with a single row and columns earliest and latest use Functions count and list in conjunction make readable results list returns a list of values of a field as a multi-value result (will be put in one cell in the result table), by default up to 100 values index = web sourcetype = access_combined action = purchase status = 200 | stats count by host, product_name | sort -count | stats list ( product_name ) as \"Product Name\" , list ( count ) as Count, sum ( count ) as total by host | sort -total | fields - total use Functions strftime, strptime, relative_time to extract/convert time # convert a time value into its timestamp representation index = manufacture sourcetype = 3dPrinterData | eval boot_ts = strptime ( boot_time, \"%b/%d/%y %H:%M:%S\" ) , days_since_boot = round (( now () -boot_ts ) /86400 ) | stats values ( days_since_boot ) as \"uptime_days\" by printer_name # extract parts from the time field sourcetype = foo | eval date_hour = strftime ( _time, \"%H\" ) | eval date_wday = strftime ( _time, \"%w\" ) | search date_hour> = 9 date_hour< = 18 date_wday> = 1 date_wday< = 5 # push time forward or backwards from a timestamp index = manufacture sourcetype = 3dPrinterData | eval boot_ts = strptime ( boot_time, \"%b/%d/%y %H:%M:%S\" ) , rt = relative_time ( boot_ts, \"+30d\" ) , reboot = strftime ( rt, \"%x\" ) | stats values ( reboot ) as \"day_to_reboot\" by printer_name A list of time format codes can be found here use Functions lower/upper, substr, replace, len, for text manipulation index = hr | eval Organization = lower ( Organization ) , Is_In_Marketing = if ( Organization == \"marketing\" , \"Yes\" , \"No\" ) | table Name, Organization, Is_In_Marketing index = hr | eval Group = substr ( Employee_ID, 1 , 2 ) , Location = substr ( Employee_ID, -2 ) | table Name, Group, Location index = hr | eval Employee_Details = replace ( Employee_ID, \"^([A-Z]+)_(\\d+)_([A-Z]+)\" , \"Employee #\\2 is a \\1 in \\3\" ) create empty column to help display data compared in bar chart index = security sourcetype = linux_secure \"fail*\" earliest = -31d@d latest = -1d@d | timechart count as daily_total | stats avg ( daily_total ) as DailyAvg | appendcols [ search index = security sourcetype = linux_secure \"fail*\" earliest = -1d@d latest = @d | stats count as Yesterday ] | eval Averages = \"\" | stats list ( DailyAvg ) as DailyAvg, list ( Yesterday ) as Yesterday by Averages swap row and column fields using untable and xyseries Commands index = web sourcetype = access_combined action = purchase | chart sum ( price ) by product_name, clientip limit = 0 | addtotals | sort 5 Total | fields - Total | untable product_name, clientip, count | xyseries clientip, product_name, count # it took these commands to swap row and column fields! | addtotals | sort 3 -Total | fields - Total aggregate for two different time ranges and compare on the same visualization index = security sourcetype = linux_secure \"failed password\" earliest = -30d@d latest = @h | eval Previous24h = relative_time ( now () , \"-24h@h\" ) , Series = if ( _time> = Previous24h, \"last24h\" , \"prior\" ) | timechart count span = 1h by Series | eval Hour = strftime ( _time, \"%H\" ) , currentHour = strftime ( now () , \"%H\" ) , offset = case ( Hour< = currentHour, Hour-currentHour, Hour>currentHour, ( Hour-24 ) -currentHour ) | stats avg ( prior ) as \"30 Day Average for Hour\" , sum ( last24 ) as \"Last 24 Hours\" by Hour, offset | sort offset | fields - offset | rename Hour as \"Hour of Day\" aggregate for certain time window each day You can do this with stats with more control and achieve what timechart provides and more! index = web sourcetype = access_combined action = purchase earliest = -3d@d latest = @d date_hour> = 0 AND date_hour< 6 | bin span = 1h _time | stats sum ( price ) as hourlySales by _time | eval Hour = strftime ( _time, \"%b %d, %I %p\" ) , \"Hourly Sales\" = \" $ \" .tostring ( hourlySales ) | table Hour, \"Hourly Sales\" Reports and Dashboards \u00b6 Reports \u00b6 Reports allow people to easily store and share search results and queries used to make the search. When a report is run, a fresh search is run. Save report results to speed up search see Accelerate Reports Doc Scheduled Reports and Alerts \u00b6 Scheduled Reports can do weekly/monthly reports and automatically send results via emails. Select a saved report and add a schedule on it. Only admin can set its priority . Alerts are based on searches that run on scheduled intervals or in real time (by admin). It is triggered when the results of a search meet defined conditions . Some alerts actions: create a log file with the events output to lookup , to apend or replace data in a lookup table send to a telemetry endpoint, call an endpoint trigger scripts, triggers a bash script stored on the machine send emails or Slack notifications use a webhook Visulizations \u00b6 Any searches that returns statistics information can be presented as visualizations , or charts. Charts can be based on numbers, time, and location . Save visualizations as a report or a dashboard panel. Creating customized visualizations see doc page Visulizations that are availble by default and are very intuitive (bar, line, area, pie, and table charts) are not explained here. Choropleth Maps \u00b6 Uses colored shadings to show differences in numbers over geographical locations . It requires a compressed Keyhole Markup Language (KMZ) file that defines region boundries. Splunk ships with two KMZ files, geo_us_states for US, and geo_countries for the countries of the world. Other KMZ files can be provided and used. geom is the command to use to show choropleth map. See geom command . Single Value \u00b6 Displays a single number with formatting options to add caption, color, unit , etc. You can use the timechart command to add a trend and a sparkline for that value. It can also display as gauges which can take forms of radial, filler, or marker. There are options to format ranges and color. gauge is the command to use to enable and pass in ranges. See gauge command . Dashboards \u00b6 Dashboard is a collection of reports or visualizations. Dashboarding Tricks \u00b6 let panels listen on variables Use $variable_name$ to refer to another variable that can be controlled by a dropdown or the drilldown 'earliest' and 'latest' from time picker For a time picker variable, the earliest and latest time can be accessed through $variable_name$.earliest and $variable_name$.latest Accelerations \u00b6 Splunk allows creation of summary of event data, as smaller segments of event data that only include those info needed to fullfill the search. Report acceleration \u00b6 Uses automatically created summaries, accelerates individual reports. acceleration summary basically are stored results populated every 10 minutes searches must be in Fast/Smart mode must include a transforming command, and having streaming command before it and non-streaming commands after also stored as file chunks alongside indexes buckets must re-build when associated knowledge objects change Summary indexing \u00b6 Uses manually created summaries, indexes separated from deployment. must use 'si'-prefixed commands such as sichart , sistats search a Summary Index by using index=<summary_index_group_name> | report=<summary_index_name> need to pay attention to avoid creating gaps and overlaps in the data Data model acceleration \u00b6 Accelerates all fields in a data model, easiest and most efficient option. adhoc acceleration - summary-creation happens automatically on data models that have not been accelerated; summary files created stays on the search head for the period that user is actively using the Pivot tool persistent acceleration - summary files created are stored alongside the data buckets and exits as long as the data model exists data models will become read-only after the acceleration requires searches use only streaming commands datamodel command allows display the structure of data models and search against them. Time-Series Index Files (tsidx files) are files created for data model acceleration. tsidx components: lexicon - an alphanumerically ordered list of terms found in data at index-time fields extracted at index-time showup as key-value pair in lexicon Splunk searches lexicon first and only open and read raw event data matching the terms using the pointers posting list - array of pointers that match each term to events in the raw data files building of tsidx files ALSO happen when a data model is accelerated for persistent acceleration, updated tsidx every 5 min and remove-outdated after 30 min tstats command is for getting statistical info from tsidx files","title":"Splunk Knowledge"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#splunk-brief","text":"","title":"Splunk Brief"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#what-splunk-is-good-for","text":"Monitoring, alerting, debugging issues and outages, generating reports and insights. Its workflow is roughly: Index Data -> Search & Investigate -> Add Knowledge -> Monitor & Alert -> Report & Analyze Splunk Data Sources can be from: Computers, network devices, virtual machines, Internet devices, commnuication devices, sensors, databases, logs, configurations, messages, call detail records, clickstream, alerts, metrics, scripts, changes, tickets, and more.","title":"What Splunk is good for"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#splunk-modules","text":"Indexer - Process incoming data and string results in indexes as events Create files organized in sets of directories by age When searching, Splunk only open the dirs that match the time frame of the search Search Heads - all users to search the data using Splunk Search Language Takes the search, divide and conquor and distribute work to indexers, and aggregate the results got back Also serve tools like dashboards, alerts, reports Forwarders - Splunk Enterprise component that consumes data and forward to the indexers for process and save Usually resides on the machines where the data origins and requires negligible resources Primary way data is supplied for indexing For a large scale of data sources, it is better to split the Splunk installation into multiple specialized instances : multiple Search Heads and Indexers (which can form clusters), and many Forwarders. Read More: https://splunkbase.splunk.com/ and http://dev.splunk.com/","title":"Splunk Modules"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#splunk-user-types","text":"Admin User - access to install apps , create knowledge objects for all users Power User - access to create and share knowledge objects for users of an app and do realtime searches User - only see their own knowledge objects and those shared with them knowledge objects created must be made share-in-app by Power User for it to be seen by other Users","title":"Splunk User Types"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#get-data-into-splunk","text":"Admin User upload data files through web UI Configure Splunk to monitor some files locally on the machine Event Logs, File System changes, Active Directory, Network Info (Main) Receive data from external Forwarders Read more: https://docs.splunk.com/Documentation/Splunk/latest/Data/Usingforwardingagents","title":"Get Data into Splunk"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#how-data-is-stored","text":"Splunk store data in indexes which held data in groups of data buckets .","title":"How Data is stored"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#bucketing","text":"Data are stored in Hot buckets as they arrive , then Warm buckets and time elapses and finally Cold buckets. Eventually they go to Frozen buckets for deletion or archiving . Each bucket contains: a journal.gz file, where Splunk stores raw event data , composed of many smaller and compressed slices (each about 128 kb) time-series index (.tsidx) files, serves as index keys to the journal file Splunk uses this to know which slice to open up and search for events created from raw events, where each line is tokenized with the token words written out to a lexicon which exists inside each tsidx file and each lexicon has a posting array which has the location of the events in the journal file","title":"Bucketing"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#bloom-filters","text":"Bloom Filters is created based on tsidx files when a bucket roll over from Hot to Warm it is a data structure for quick elimination of data that doesn't match a search allows Splunk to AVOID accessing disk unless really necessary when creating a Bloom Filter, each term inside a bucket's tsidx files' lexicon is run through a hashing algorithm resulting hash sets the bits in the Bloom filter to 0/1 When a search is run, it generates its own Bloom filter based on the search terms and compared with the buckets' Bloom Filters, resulting a fast filter on unrelevant buckets","title":"Bloom Filters"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#segmentation","text":"The process of tokenizing search terms at search-time. Two stages: splitting events up by finding major/minor breaker characters major breakers are used to isolate words, phrases, terms, and numerical data example major breakers: space \\n \\r \\t [] ! , minor breakers go through results in the first pass and break them up further in the second pass example minor breakers: / : . - $ The tokens become part of tsidx files lexicons at index-time and used to build bloom filters Read more from doc page","title":"Segmentation"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#inside-a-search","text":"Take this search as an example: index = security failed user = root | timechart count span = 1h | stats avg ( count ) as HourlyAverage Splunk extracts 'failed' and 'root' and creates a Bloom Filter from them Splunk then identify buckets in security index for the past 24h Splunk compares the search Bloom Filter with those from the buckets Splunk rules out tsidx files that don't have matches Splunk checks the tsidx files from the selected buckets for extracted terms Splunk uses the terms to go through each posting list to obtain seek address in journal file for raw events Splunk extracts search-time fields from the raw events Splunk filters the events that contains our search terms 'failed' and 'root' The remaining events are search results","title":"Inside a Search"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#inspect-a-search","text":"Within the Job Inspector: command.search.index gives the time to get location info from tsidx files command.search.rawdata tells the time to extract event data from the gzip journal files command.search.kv tells the time took to perform search-time field extractions command.search.filter shows the time took to filter out non-matching events command.search.log look for phrase \"base lispy\" that contains the Lispy Expression created from the query and is used to build Bloom Filter and locate terms in tsidx files","title":"Inspect a Search"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#search-and-report-app","text":"The Search & Report app allows you to search and analyze data through creating knowledge objects, reports, dashboards and more.","title":"Search and Report App"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#search-ui","text":"Events tab displays the events returned for your search and the fields extracted from the events. patterns tab allows you to see patterns in your data. If your search generates statistics or visuals, they will appear in Statistics and Visualization tabs. Commands that create statistics and visualizations are called transforming commands , they transform data into data tables. By default, a search is active & valid for 10 minutes , otherwise needs rerun. A shared search job is active for 7 days and its first-ran result visible by anyone you shared with. From the web UI, use Search & Reporting -> Search History to quickly view recent searches and reuse the searches if possible. Also from the web UI, use Activity -> Jobs to view recent search jobs (with data). Then further use Job -> inspect job to view the query performance.","title":"Search UI"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#search-fields","text":"Search fields are key=value string in the search query and will be automatically extracted and available for use by commands for creating insights. The key is case-sensitive but the value is not. Three fields ALWAYS available are hosts , source , and sourcetype . Interesting fields are those appear within at least 20% of the events From the extracted fields, a denotes a String value, # denotes a numeral value In the search, = != can be used on numeral and String values; > >= < <= can be used on numeral values only.","title":"Search Fields"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#tags","text":"Tags are Knowledge Objects allow you to designate descriptive names for key-value pairs. It is useful when you have logs that multiple fields may have the same value(s) you want to filter with. Then you can tag those fields with the same tag name and in future searches do filter with the tag name. Tags are value-specific and are case-sensitive. When you tag a key-value pair you are associating a tag with that pair. When the field appears with another value then it is not tagged with this tag name. Do NOT tag a field having infinite possible values . Steps: in a Splunk search, click on an event and see the list of key=vlaue pairs and in actions dropdown select Edit Tags to add new tags (comma separated). To search with tags, enter tag=<tag_name> . For example, index=security tag=SF matches all fields that are tagged with that value. To limit a tag to a field do tag::<field>=<tag_name> Read more about tags in doc page","title":"Tags"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#event-type","text":"Event Type allows saving a search's definition and use the definition in another search to highlight matching events. It is good for categorizing events based on search terms. The priority settings affects the display order of returned results. Steps: after a search, save as \"Event Type\" and use it in another search with eventtype . For example, index=web sourcetype=access_combined eventtype=purchase_strategy","title":"Event Type"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#knowledge-objects","text":"Knowledge Objects are like some tools to help discover and analyze data , which include data interpretation, classification, enrichment, normalization, and search-time-mapping of knowledge (Data Models). Saved Searches, Reports, Alerts, and Dashboards are all Knowledge Objects.","title":"Knowledge Objects"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#field-extractor","text":"The Field Extractor allows to use a GUI to extract fields that persist as Knowledge Objects and make them reusable in searches. It is useful if the information from events are badly structured (not in some key=value fashion), such as simple statements in a natural language but contains useful information in a specific pattern . Fields extracted are specific to a host, source, or sourcetype and are persistent . Two different ways a field extractor can use: regex and delimiters : regex : using regular expression, work well when having unstructured data and events to extract fields from delimiters : a delimiter such as comma, space, or any char. Use it for things like CSV files Three ways to access a field extractor: - Settings -> Fields - The field sidebar during a search - From an event's actions menu (easiest) Read more from the docs page for Splunk regex.","title":"Field Extractor"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#field-aliases","text":"Field Aliases give you a way to normalize data over multiple sources . For example, when aggregating results from multiple sources or indexes, but the data logged may have fields with different names such as url, uurl, dest_url, service_url, etc. which all refers to the same thing. Then this tool is handy. You can create one or more aliases to any extracted field and can apply them to lookup . Steps: Settings -> Fields -> Field aliases (add new) set destination app name (choose a meaningful name for the new field) apply to (sourcetype, source, or host) and its matching value Map fieldName=newFieldName Map multiple fields by adding another field","title":"Field Aliases"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#calculated-field","text":"Calculated Field saves frequently used calculation you would do in searches with complex eval statements and it is like a shorthand for a function made of SPL commands on extracted fields . For example, you might want to save something to do conversion between bytes to megabytes, or encoding some field to sha256, etc. Steps: Settings -> Fields -> Calculated fields (add new)","title":"Calculated Field"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#search-macros","text":"Macros are reusable search strings or portions of search strings. It is useful for frequent searches with complicated search syntax and can store entire search strings . You can pass arguments (fields or constants) to the search. Steps: Settings -> Advanced Search -> Search macros (add new) Then use it in the search by surrounding the macro name with backticks. For example: index=sales sourcetype=vendor_sales | stats sum(sale_price) as total_sales by Vendor | ``convertUSD`` Within a search, use ctrl-shift-E or cmd-shift-E to preview and expand the macro used without running it.","title":"Search Macros"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#datasets","text":"Datasets are small subsets of data for specific purposes, defined like tables with fields names as columns and fields values as cells.","title":"Datasets"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#lookups","text":"Lookups are in fact datasets that allow you to add useful values to your events not included in the indexed data. Field values from a Lookup are case sensitive by default Set it up in Splunk UI -> Settings -> Lookups -> Define a lookup table Use | inputlookup <lookup_definition_name> to verify lookup is setup correctly. Additionally, you can populate lookup table with search results or from external script or linux executable. Read more about external lookups and KVstore lookups Some tips for using lookups: in lookup table, keep the key field the first column put lookups at the end of the search if a lookup usage is very common in search, make it automatic use KV store or gzip CSVs for large lookup table only put data being used in lookup table the outputlookup can output results of search to a static lookup table","title":"Lookups"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#pivot","text":"Pivot allows users to get knowledge from the data without learning SPL in deep; a UI-based tool can help the users with creating useful searches. It is like a tool created by knowledgable Splunk users to serve the users don't know much about Splunk.","title":"Pivot"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#data-models","text":"Data Models are made of Datasets and are knowledge objects that provide the data structure that drives Pivots. It can also be used to significantly speed up searches, reports, and dashboard. Data Models must be created by Admins and Power Users and are hierarchically strucutred datasets. Starting from a Root Object (a search with index and sourcetype), then add Child Objects (a filter or constraint). Data Models are good for accelerating searches . Doing searches on data models are like searching on some cached events that gets updated regularly. Steps: Settings -> Data Models -> New Data Model Then search the datamodel with | datamodel keywords Read more from the doc page","title":"Data Models"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#splunk-search-language-spl","text":"","title":"Splunk Search Language (SPL)"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#spl-components","text":"Search Terms - defines what data to return i.e. host=xxx status>399 also includes Boolean operators ( NOT OR AND in its evaluation precedence) Commands - process search results and, create charts, compute statistics, and format results i.e. eval in eval foo=1 Functions - serves supplying additional conditions and algorithms to compute and evaluate the results in Commands i.e. avg in stats avg(latency) by host Arguments - variables we want to apply to the Functions i.e. latency in stats avg(latency) by host Clauses - how we want the results grouped i.e. by in stats avg(latency) by host Search Terms, Command names, Function names are NOT case-sensitive , with the exception to the command replace where search term must be an exact match.","title":"SPL Components"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#search-terms","text":"Keywords , any keywords can be searched on; \"AND\" boolean is implied for multiple keywords Booleans , AND OR NOT Phrases , keywords surrounded by \"\" Fields , key=value pair search Wildcards , used in keywords and fields to match any chars; inefficient if used at the beginning of a word Comparisons , = != < <= > >=","title":"Search Terms"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#subsearch","text":"Subsearch is a search enclosed in brackets that passes its results to its outer search as search terms . The return command can be used in subsearch to limit the first few number of values to return, i.e. | return src_ip . The field name is included by default; to omit returning values with the field name, use $ , i.e. | return 5 $src_ip Example index = security sourcetype = linux_secure \"accepted\" [ search index = security sourcetype = linux_secure \"failed password\" src_ip! = 10 .0.0.0/8 | stats count by src_ip | where count > 10 | fields src_ip ] # when subsearch returns, Splunk implicitly adds \"OR\" between each two results # then whole results is combined with the outter search with an implicit \"AND\" operator | dedup src_ip, user | table src_ip, user # subsearch with 'return' command index = sales sourcetype = vendor_sales [ search index = sales sourcetype = vendor_sales VendorID< = 2000 product_name = Mediocre* | top limit = 5 Vendor | return 5 Vendor ] | rare limit = 1 product_name by Vendor | fields - percent, count Some limitations of subsearches: default time limit of 60 seconds to execute right after time limit, result is finalized and returned to outer search default returns up to 10000 results if the outter search is executed real-time , its subsearch is executed for all-time best to use earliest and latest time modifiers in subsearch","title":"Subsearch"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#spl-commands","text":"Transforming vs. Streaming Commands transforming commands operate on the entire result set of data executes on the Search Head and waits for full set of results change event data into results once its complete example commands stats timechart chart top rare streaming commands has two types centralized (aka \"Stateful Streaming Commands\") executes on the Search Head apply transformations to each event returned by a search results order depends on the order when data came in example commands transaction streamstats distributable executes on Search head or Indexers order of events does not matter when preceded by commands that have to run on a Search Head, ALL will run on a Search Head example commands rename eval fields regex Read more from doc page","title":"SPL Commands"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#streaming-commands","text":"","title":"Streaming Commands"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#append","text":"Add results by appending subsearch results to the end of the main search results as additional rows . Does NOT work with read-time searches. It is useful to draw data on the same visualization by combining results from two searches. However, rows are NOT combined based on the values in some of the fields. Example index = security sourcetype = linux_secure \"failed password\" NOT \"invalid user\" | timechart count as known_users | append [ search index = security sourcetype = linux_secure \"failed password\" \"invalid user\" | timechart count as unknown_users ] # Final result data may look awkward as rows are NOT combined based on the values in some of the fields. # Fix it with the `first` Function - return the first value of a field by the order it appears in the results. | timechart first ( * ) as * # apply first Function on all fields and keep their original name","title":"append"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#appendcols","text":"Add results of a subsearch to the right of the main search's results. It also attempts to match rows by some common field values, but it is not always possible. Example index = security sourcetype = linux_secure \"failed password\" NOT \"invalid user\" | timechart count as known_users | appendcols [ search index = security sourcetype = linux_secure \"failed password\" \"invalid user\" | timechart count as unknown_users ] # \"unknown_users\" is added as a new column to the outter search's results and matches the _time field","title":"appendcols"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#appendpipe","text":"Append subpipeline search data as events to your search results. Appended search is NOT run until the command is reached . appendpipe is NOT an additional search , but merely a pipeline for the previous results. It acts on the data received and appended back new events while keeping the data before the pipe. It is useful to add summary to some results. Example index = network sourcetype = cisco_wsa_squid ( usage = Borderline OR usage = Violation ) | stats count by usage, cs_username | appendpipe [ stats sum ( count ) as count by usage | eval cs_username = \"TOTAL: \" .usage ] | appendpipe [ search cs_username = Total* | stats sum ( count ) as count | eval cs_username = \"GRAND TOTAL\" ] | sort usage, count","title":"appendpipe"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#addtotals","text":"Compute the SUM of ALL numeric fields for each event (table row) and creates a total column (default behavior). You can override the default behavior and let it calculate a sum for a column , see the example. Example # adds a column to calculate the totals of all numeric fields per row index = web sourcetype = access_combined | chart sum ( bytes ) over host by file | addtotals fieldname = \"Total by host\" # adds a row to calculate the totals of one column index = web sourcetype = access_combined | chart sum ( bytes ) as sum over host | addtotals col = true label = \"Total\" labelfield = sum","title":"addtotals"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#bin","text":"Adjust and group numerical values into discrete sets (bins) so that all the items in a bin share the same value. bin overrides the field value on the field provided. Use a copied field if need the original values in subsequent pipes. Example index = sales sourcetype = vendor_sales | stats sum ( price ) as totalSales by product_name | bin totalSales span = 10 # span is the bin size to group on | stats list ( product_name ) by totalSales | eval totalSales = \" $ \" .totalSales","title":"bin"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#dedup","text":"Remove duplicated events from results that share common values from the fields specified. Example index = security sourcetype = history* Address_Description = \"San Francisco\" | dedup Username # can be a single field or multiple fields | table Username First_Name Last_Name","title":"dedup"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#erex","text":"Like an auto field extractor, but had the same shortcomings as the regex field extractor through Splunk UI: only look for matches based on the samples provided. Better use rex command. Example index = security sourcetype = linux_secure \"port\" | erex ipaddress examples = \"74.125.19.106, 88.12.32.208\" | table ipaddress, src_port","title":"erex"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#eval","text":"Calculate and manipulate field values in many powerful ways and also works with multi-value fields. Results can create new fields (case-sensitive) or override existing fields. Multiple field expressions can be calculated on the same eval command, and one is created/altered after another so the order of these expressions matters. Besides the many Functions it supports, eval supports operators include arithmetic, concatenation (using . ), and boolean. Example index = network sourcetype = cisco_wsa_squid | stats sum ( sc_bytes ) as Bytes by usage | eval bandwidth = round ( Bytes/1024/1024, 2 ) # eval with if conditions and cases | eval VendorTerritory = if ( VendorId< 4000 , \"North America\" , \"Other\" ) | eval httpCategory = case ( status> = 200 AND status< 300 , \"Success\" , status> = 300 AND status< 400 , \"Redirect\" , status> = 400 AND status< 500 , \"Client Error\" , status> = 500 , \"Server Error\" , true () , \"default catch all other cases\" ) # eval can be used as a Function with stats command to apply conditions index = web sourcetype = access_combined | stats count ( eval ( status< 300 )) as \"Success\" , ... Supported Functions see docs page","title":"eval"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#eventstats","text":"Generates statistics for fields in searched events and save them as new fields in the results. Each event will have the SAME value for the statistics calculated. Example index = web sourcetype = access_combined action = remove | chart sum ( price ) as lostSales by product_name | eventstats avg ( lostSales ) as averageLoss | where lostSales > averageLoss | fields - averageLoss | sort -lostSales Supported Functions see docs page See also streamstats command.","title":"eventstats"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#fieldformat","text":"Format values WITHOUT changing characteristics of underlying values. In other words, only a format is specified when the value is displayed as outputs and the same field can still participate calculations as if their values are not changed. Example index = web sourcetype = access_combined product_name = * action = purchase | stats sum ( price ) as total_list_price, sum ( sale_price ) as total_sale_price by product_name | fieldformat total_list_price = \" $ \" + tostring ( total_list_price ) Supported Functions see docs page . Specifically look for Conversion Functions and Text Functions.","title":"fieldformat"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#fields","text":"Include or exclude fields from search results to limit the fields to display and also make search run faster. Field extraction is one of the most costly parts of searching in Splunk. Eliminate unnecessary fields will improve search speed since field inclusion occurs BEFORE field extraction, while field exclusion happens AFTER field extraction. Internal fields like raw and _time will ALWAYS be extracted, but can also be removed from the search results using fields . Example index = web sourcetype = access_combined | fields status clientip # only include fields status and clientip index = web sourcetype = access_combined | fields - status clientip # to exclude fields status and clientip","title":"fields"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#fillnull","text":"Replaces any null values in your events. It by default fills NULL with 0 and it can be set with value=\"something else\" Example index = sales sourcetype = vendor_sales | chart sum ( price ) over product_name by VendorCountry | fillnull","title":"fillnull"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#foreach","text":"Run templated subsearches for each field in a specified list, and each row goes through the foreach command and subsearch calculation. Example index = web sourcetype = access_combined | timechart span = 1h sum ( bytes ) as totalBytes by host | foreach www* [ eval '<<FIELD>>' = round ( '<<FIELD>>' / ( 1024 *1024 ) ,2 )] # '<<FIELD>>' refers to the value of the column_name # assume 'host' gives values www1 www2 www3","title":"foreach"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#join","text":"Combine results of subsearch with outter search using common fields . It requires both searches to share at least one field in common. Two types of join: inner join (default) - only return results from outter search that have matches in the subsearch outer/left join - returns all results from the outter search and those have matches in the subsearch; can be specified with argument Example index = \"security\" sourcetype = linux_secure src_ip = 10 .0.0.0/8 \"Failed\" | join src_ip [ search index = security sourcetype = winauthentication_security bcg_ip = * | dedup bcg_workstation | rename bcg_ip as src_ip | fields src_ip, bcg_workstation ] | stats count as access_attempts by user, dest, bcg_workstation | where access_attempts > 40 | sort - access_attempts See also union","title":"join"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#lookup","text":"Invoke field value lookups. Example # http_status is the name of the lookup definition # code is one of the columns in the csv, and we have status in our event # defualt all fields in lookup table are returned except the input fields # specify OUTPUT to choose the fields added from the lookup index = web sourcetype = access_combined NOT status = 200 | lookup http_status code as status, OUTPUT code as \"HTTP Code\" , description as \"HTTP Description\"","title":"lookup"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#makemv","text":"Create a multi-value field from an existing field and replace that field. It splits its values using some delimiter or regex . Example index = sales sourcetype = vendor_sales | eval account_codes = productId | makemv delim = \"-\" , account_codes | dedup product_name | table product_name, productId, account_codes # same results can be achieved by using regex and specifying capture groups | makemv tokenizer = \"([A-Z]{2}|[A-Z]{1}[0-9]{2})\" , account_codes","title":"makemv"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#multikv","text":"Extract field values from data formatted as large and single events of tabular data . Each row becomes an event, header becomes the field names, and tabular data becomes values of the fields. Example of some data event suitable: Name Age Occupation Josh 42 SoftwareEngineer Francine 35 CEO Samantha 22 ProjectManager Example index = main sourcetype = netstat | multikv fields LocalAddress State filter LISTEN | rex field = LocalAddress \":(?P<Listening_Port>\\d+) $ \" | dedup Listening_Port | table Listening_Port","title":"multikv"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#mvexpand","text":"Create separate event for each value contained in a multi-value field. It reates new events count as of the number of values in a multi-valued field and copy all other values to the new evnets. Best to remove unused fields before the mvexpand command. Example index = systems sourcetype = system_info | mvexpand ROOT_USERS | dedup SYSTE ROOT_USERS | stats list ( SYSTEM ) by ROOT_USERS","title":"mvexpand"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#rename","text":"Rename one or more fields. After the rename, subsequent commands must use the new names otherwise operation won't have any effects. Note that after rename if a field name contains spaces , you need to use single quotes on the field name to refer to it in subsequent commands. Example index = web sourcetype = access* status = 200 product_name = * | table JESSIONID, product_name, price | rename JESSIONID as \"User Session Id\" , product_name as \"Purchased Game\" , price as \"Purchase Price\"","title":"rename"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#rex","text":"Allow use regex capture groups to extract values at search time. By default, it uses field=_raw if not provided a field name to read from. Example index = security sourcetype = linux_secure \"port\" | rex \"(?i) from (?P<ipaddress>[^ ]+)\" | table ipaddress, src_port index = network sourcetype = cisco_esa | where isnotnull ( mailfrom ) | rex field = mailfrom \"@(?P<mail_domain>\\S+)\" Better examples can be found at doc page","title":"rex"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#search","text":"Add search terms further down the pipeline . In fact, the part before the first | is itself a search command. Example index = network sourcetype = cisco_wsa_squid usage = Violation | stats count ( usage ) as Visits by cs_username | search Visits > 1","title":"search"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#sort","text":"Organize the results sorted by some fields. Example sourcetype = vendor_sales | table Vendor product_name sale_price | sort - sale_price Vendor # the '-' here affects all fields | sort -sale_price Vendor # sort-decending will only affect sale_price while Vendor is sorted ascending | sort -sale_price Vendor limit = 20 # will also limit the results for the first twenty in sorted order.","title":"sort"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#spath","text":"Extracts info from semi-structured data (such as xml, json) and store in fields. It by default uses input=_raw , if not provided a field name to read from. Example index = systems sourcetype = system_info_xml | spath path = event.systeminfo.cpu.percent output = CPU # working with json, values in array specified with {} index = systems sourcetype = system_info_json | spath path = CPU_CORES {} .idle output = CPU_IDLE | stats avg ( CPU_IDLE ) by SYSTEM # spath Function index = hr sourcetype = HR_DB | eval RFID = spath ( Access, \"rfid.chip\" ) , Location = spath ( Access, \"rfid.location\" ) | table Name, Access, RFID, Location Better examples see doc page","title":"spath"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#streamstats","text":"Aggregates statistics to your searched events as Splunk sees the events in time in a streaming manner. In other words, it calculates statistics for each event cumulatively at the time the event is seen. Three important arguments to know: current - setting to 'f' tells Splunk to only use the field values from previous events when performing statistical function window - setting to N tells Splunk to only calculate previous N events at a time; default 0 means uses all previous events time_window - setting to a time span to only calculate events happen in every that time span; with this function, events must be sorted by time Example index = web sourcetype = access_combined action = purchase status = 200 | stats sum ( price ) as orderAmount by JESSIONID | sort _time | streamstats avg ( orderAmount ) as averageOrder current = f window = 20 # for each event, calculate the average from the previous 20 events' orderAmount field","title":"streamstats"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#table","text":"Specify fields kept in the results and retains the data in a tabular format. Fields appears in the order specified to the command. Example index = web sourcetype = access_combined | table status clientip","title":"table"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#transaction","text":"Transaction is any group of related events for a time span which can come from multiple applications or hosts. Events should be in a reversed chronological order before running this command. transaction command takes in one or multiple fields to make transactions, and creates two fields in the raw event: duration (time between the first and last event) and eventcount (number of events) transaction limits 1000 events per transaction by default and is an expensive command. Use it only when it has greater value than what you can do with stats command. Use keepevicted=true argument to keep incomplete transactions in results and closed_txn field will be added to indicate transactions incomplete. Example index = web sourcetype = access_combined | transaction clientip startswith = action = \"addtocart\" endswith = action = \"purchase\" maxspan = 1h maxpause = 30m # startswith and endswith should be valid search strings # compare the two searches # using transaction index = web sourcetype = access_combined | transaction JESSIONID | where duration > 0 | stats avg ( duration ) as avgd, avg ( eventcount ) as avgc | eval \"Average Time On Site\" = round ( avgd, 0 ) , \"Average Page Views\" = round ( avgc, 0 ) | table \"Average Time On Site\" , \"Average Page Views\" # using stats index = web sourcetype = access_combined | fields JESSIONID | stats range ( _time ) as duration, count as eventcount by JESSIONID | where duration > 0 | stats avg ( duration ) as avgd, avg ( eventcount ) as avgc | eval \"Average Time On Site\" = round ( avgd, 0 ) , \"Average Page Views\" = round ( avgc, 0 ) | table \"Average Time On Site\" , \"Average Page Views\" transaction is a very expensive operation. To build a transaction, all events data is searched and required, and is done on the Search Head. Some use cases can also be achieved by stats command.","title":"transaction"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#union","text":"Combine search results from two or more datasets into one. Datasets can be: saved searches, inputlookups, data models, subsearches. Example | union datamoel:Buttercup_Games_Online_Sales.successful_purchase, [ search index = sales sourcetype = vendor_sales ] | table sourcetype, product_name See also join","title":"union"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#untable","text":"Does the opposite of xyseries , for which it takes chartable and tabular data then format it similar to stats output. Good when need to further extract from, and manipulate values after using commands that result in tabular data. Syntax | untable row-field, label-field, data-field produces 3-column results Example index = sales sourcetype = vendor_sales ( VendorID > = 9000 ) | chart count as prod_count by product_name, VendorCountry limit = 5 useother = f | untable product_name, VendorCountry, prod_count | eventstats max ( prod_count ) as max by VendorCountry | where prod_count = max | stats list ( product_name ) , list ( prod_count ) by VendorCountry","title":"untable"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#where","text":"Filter events to ONLY keep the results that evaluate as true . Try use search whenever you can rather than where as it is more efficient this way. Example index = network sourcetype = cisco_wsa_squid usage = Violation | stats count ( eval ( usage = \"Personal\" )) as Personal, count ( eval ( usage = \"Business\" )) as Business by username | where Personal > Business AND username! = \"lsagers\" | sort -Personal Supported Functions see docs page . where shared many of the eval Functions.","title":"where"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#xyseries","text":"Convert statistical results into a tabular format suitable for visualizations. Useful for additional process after initial statistical data is returned. Syntax | xyseries row-field, column-field, data-field produces n+1 column result, where n is number of distinct values in column-field. Example index = web sourcetype = access_combined | bin _time span = 1h | stats sum ( bytes ) as totalBytes by _time, host | eval MB = round ( totalBytes/ ( 1024 *1024 ) ,2 ) | xyseries _time, host, MB # transforms _time | host | MB ----- | ---- | -- 12 :00 | www1 | 12 12 :00 | www2 | 11 12 :00 | www3 | 7 ... | ... | ... # into _time | www1 | www2 | www3 ----- | ---- | ---- | ---- 12 :00 | 12 | 11 | 7 ... | ... | ... | ... See also untable command.","title":"xyseries"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#transforming-commands","text":"","title":"Transforming Commands"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#chart","text":"Organize the data as a regular or two-way chart table . To get a two-way table, you need to provide three fields used as table row value, table column value, and table data value. A common pattern is chart <function(data_field)> over <row_field> by <column_field> chart is limited to display 10 columns by default , others will show up as an \"other\" column in the chart visualization. The 'other' column can be turned off by passing an argument useother=f . Use limit=5 to control the max number of columns to display, or limit=0 to display all columns. Example index = web sourcetype = access_combined status> 299 | chart count over status by host # if more than one field is supplied to `by` clause without `over` clause # the first field is used as with a `over` clause # this has the same effect as above command index = web sourcetype = access_combined status> 299 | chart count by status, host Supported Functions see docs page","title":"chart"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#fieldsummary","text":"Calculate summary statistics for fields in your events. For example, given a search index=web | fieldsummary it gives insights of the \"count, distinct_count, is_exact, min/max/mean/stdev, numberic_count, values\" for each field. If field names are provided, then only do summary for those fields provided.","title":"fieldsummary"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#rare","text":"Shows the least common values of a field set. It is the opposite of top Command and accepts the same set of clauses. Example index = sales sourcetype = vendor_sales | rare Vendor limit = 5 countfield = \"Number of Sales\" showperc = False useother = True","title":"rare"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#stats","text":"Produce statistics of our search results and need to use functions to produce stats. The eval Function is handy to apply quick and concise conditional filtering to limit the statistics calculated from desired data. Example index = sales sourcetype = vendor_sales | stats count as \"Total Sells By Vendors\" by product_name, categoryid # apply conditions using eval Function index = web sourcetype = access_combined | stats count ( eval ( status< 300 )) as \"Success\" , ... Supported Functions see docs page","title":"stats"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#timechart","text":"Performs stats aggregations against time , and time is always the x-axis. Like chart , except only one value can be supplied to the by clause This command determines the time intervals from the time range selected, and it can be changed by using an argument span . Example index = web sourcetype = vendor_sales | timechart count by product_name span = 1m Supported Functions see docs page","title":"timechart"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#timewrap","text":"Compare the data from timechart further over an older time range. Specify a time period to apply to the result of a timechart command, then display series of data based on this time periods, with the X axis display the increments of this period and the Y axis display the aggregated values over that period. Example index = sales sourcetype = vendor_sales product_name = \"Dream Crusher\" | timechart span = 1d sum ( price ) by product_name | timewrap 7d | rename _time as Day | eval Day = strftime ( Day, \"%A\" )","title":"timewrap"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#top","text":"Finds the most common values of given field(s) in a result set and automatically returns count and percent columns. By default displays top 10 and the count can be set with limit=n ; while limit=0 yields all results. Top Command Clauses: limit=int countfield=string percentfield=string showcount=True/False showperc=True/False showother=True/False otherstr=string Example index = sales sourcetype = vendor_sales | top Vendor product_name limit = 5 # top command also supports results grouping by fields index = sales sourcetype = vendor_sales | top product_name by Vendor limit = 3 countfield = \"Number of Sales\" showperc = False","title":"top"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#generating-commands","text":"Commands that generates events.","title":"Generating Commands"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#datamodel","text":"Display the structure of data models and search against them. It accepts first argument as datamodel name and the second argument as the object name under than model. Add search to enable search on the data model data. Example | datamodel Buttercup_Games_Online_Sales http_request search | search http_request.action = purchase","title":"datamodel"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#inputlookup","text":"Return values from a lookup table. Useful to use it in a subsearch to narrow down the set of events to search based on the values in the lookup table. Example index = sales sourcetype = vendor_sales [ | inputlookup API_Tokens | table VendorID ] # only take one column to use as filter; its values will be OR-ed together # can add 'NOT before the subsearch to exclude those vendor ids | top Vendor product_name limit = 5","title":"inputlookup"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#makeresults","text":"Creates a defined number of search results, good for creating sample data for testing searches or building values to be used in searches. Example # must starts search with pipe | makeresults | eval tomorrow = relative_time ( now () , \"+1d\" ) , tomorrow = strftime ( tomorrow, \"%A\" ) , result = if ( tomorrow = \"Saturday\" OR tomorrow = \"Sunday\" , \"Huzzah!\" , \"Boo!\" ) , msg = printf ( \"Tomorrow is %s, %s\" , tomorrow, result )","title":"makeresults"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#tstats","text":"Get statistical info from tsidx files . Splunk will do full search if the search ran is outside of the summary range of the accelerated data model. Limit search to summary range with summariesonly argument. Example | tstats count as \"count\" from datamodel = linux_server_access where web_server_access.src_ip = 10 .* web_server_access.action = failure by web_server_access.src_ip, web_server_access.user summariesonly = t span = 1d | where count > 300 | sort -count","title":"tstats"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#special-commands","text":"Splunk has commands to extract geographical info from data and display them in a good format.","title":"Special Commands"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#gauge","text":"Show a field value in a gauge. Example index = sales sourcetype = vendor_sales | stats sum ( sale ) as total | gauge total 0 3000 6000 7000","title":"gauge"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#geom","text":"Adds fields with geographical data structures matching polygons on a choropleth map visulization . Example index = sales sourcetype = vendor_sales | stats count as Sales by VendorCountry | geom geo_countries featureIdField = VendorCountry # the field `VendorCountry` must map back a country name in the featureCollection","title":"geom"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#geostats","text":"Aggregates geographical data for use on a map visualization . Example index = sales sourcetype = vendor_sales | geostats latfield = VendorLatitude longfield = VendorLongitude count by product_name globallimit = 4","title":"geostats"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#iplocation","text":"Lookup IP address and add location information to events. Data like city, country, region, latitude, and longitude can be added to events that include external IP addresses. Not all location info might be available depends on the IP. Example index = web sourcetype = access_combined action = purchase status = 200 | iplocation clientip","title":"iplocation"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#trendline","text":"Compute moving averages of field values, gives a good understanding of how the data is trending over time . Three trendtypes (used as Functions by trendline command): simple moving average ( sma ): compute the sum of data points over a period of time expoential moving average ( ema ): assigns a heavier weighting to more current data points weighted moving average ( wma ): assigns a heavier weighting to more current data points Example index = web sourcetype = access_combined | timechart sum ( price ) as sales | trendline wma2 ( sales ) as trend ema10 ( bars ) # '2' here means do calculation per 2 events # this number can be between 2 and 10000","title":"trendline"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#spl-best-practices","text":"'key!=value' vs 'NOT key=value' A small difference: key!=value only includes entries where key is not NULL, while NOT key=value includes entries even when the key does NOT exitst on those events entries. use the 'IN' operator Instead of doing (key=value1 OR key=value2 OR key=value3) , more viewable syntax: key IN (\"value1\", \"value2\", \"value3\") use 'index' 'source' 'host' early These fields are extracted when data was indexed and stored and won't take time to extract at search time. It is best to use these fields early in the search to limit the data to be further processed. use 'inclusion' over 'exclusion' Searching for exactly \"something\" is better than searching for \"NOT something\" specify time in the query when appropriate Use relative time earliest=-2h latest=-1h or absolute time earliest=01/08/2018:12:00:00 in the search. Use @ to round down time to the nearest 0 in the unit used earliest=-2@h time is the most efficient way to improve queries search time Splunk stores data in buckets (directories) containing raw data and indexing data, and buckets have maximum size and maximum time span. Three kinds of searchable buckets: hot, warm, cold. Access speed: hot (read/write, very fast, like memory) > warm (slow, read-only, slower medium) > cold (very slow, read-only, cheap and stable medium). When search is run, it go look for the right bucket to open, uncompress the raw data and search the contents inside. The order of effectiveness in filtering data: time > index > source > host > sourcetype create different indexes to group data Having specialized indexes helps make Splunk searches faster and more efficient. be careful when using wildcards Using wildcard at the beginning of a word cause Splunk to search all events which causes degradation in performance . Using wildcard in the middle of a string might cause inconsistent results due to the way Splunk indexes data that contains punctuation Best to make search as specific as possible, and only use wildcards at the end of a word , say status=failure instead of status=fail* save search as reports using fast mode Fast mode emphasis on performance and returns only essential data . Verbose mode emphasizes completeness and returns all fields and event data . Smart mode weights tradeoffs and returns the best results for the search being run. use 'fields' command to extract only the fields you need Doing so as early in the search as possible will let each indexer extract less fields, pack and return data back faster. consult Job Inspector to know the performance of the searches It tells you which phase of the search took the most time . Any search that has not expired can be inspected by this tool. Read more about Job Inspector avoid using subsearches when possible Compare the follow two queries, which returns the same results but the second one is significantly faster: index = security sourcetype = winauthentication_security ( EventCode = 4624 OR EventCode = 540 ) NOT [ search sourcetype = history_access | rename Username as User | fields User ] | stats count by User | table User index = security ( sourcetype = winauthentication_security ( EventCode = 4624 OR EventCode = 540 )) OR ( sourcetype = history_access ) | eval badge_access = if ( sourcetype = \"history_access\" , 1 , 0 ) | stats max ( badge_access ) as badged_in by Username | where badged_in = 0 | fields Username","title":"SPL Best Practices"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#spl-tricks","text":"let subsearch limit main search's time range Let the subsearch output a table with a single row and columns earliest and latest use Functions count and list in conjunction make readable results list returns a list of values of a field as a multi-value result (will be put in one cell in the result table), by default up to 100 values index = web sourcetype = access_combined action = purchase status = 200 | stats count by host, product_name | sort -count | stats list ( product_name ) as \"Product Name\" , list ( count ) as Count, sum ( count ) as total by host | sort -total | fields - total use Functions strftime, strptime, relative_time to extract/convert time # convert a time value into its timestamp representation index = manufacture sourcetype = 3dPrinterData | eval boot_ts = strptime ( boot_time, \"%b/%d/%y %H:%M:%S\" ) , days_since_boot = round (( now () -boot_ts ) /86400 ) | stats values ( days_since_boot ) as \"uptime_days\" by printer_name # extract parts from the time field sourcetype = foo | eval date_hour = strftime ( _time, \"%H\" ) | eval date_wday = strftime ( _time, \"%w\" ) | search date_hour> = 9 date_hour< = 18 date_wday> = 1 date_wday< = 5 # push time forward or backwards from a timestamp index = manufacture sourcetype = 3dPrinterData | eval boot_ts = strptime ( boot_time, \"%b/%d/%y %H:%M:%S\" ) , rt = relative_time ( boot_ts, \"+30d\" ) , reboot = strftime ( rt, \"%x\" ) | stats values ( reboot ) as \"day_to_reboot\" by printer_name A list of time format codes can be found here use Functions lower/upper, substr, replace, len, for text manipulation index = hr | eval Organization = lower ( Organization ) , Is_In_Marketing = if ( Organization == \"marketing\" , \"Yes\" , \"No\" ) | table Name, Organization, Is_In_Marketing index = hr | eval Group = substr ( Employee_ID, 1 , 2 ) , Location = substr ( Employee_ID, -2 ) | table Name, Group, Location index = hr | eval Employee_Details = replace ( Employee_ID, \"^([A-Z]+)_(\\d+)_([A-Z]+)\" , \"Employee #\\2 is a \\1 in \\3\" ) create empty column to help display data compared in bar chart index = security sourcetype = linux_secure \"fail*\" earliest = -31d@d latest = -1d@d | timechart count as daily_total | stats avg ( daily_total ) as DailyAvg | appendcols [ search index = security sourcetype = linux_secure \"fail*\" earliest = -1d@d latest = @d | stats count as Yesterday ] | eval Averages = \"\" | stats list ( DailyAvg ) as DailyAvg, list ( Yesterday ) as Yesterday by Averages swap row and column fields using untable and xyseries Commands index = web sourcetype = access_combined action = purchase | chart sum ( price ) by product_name, clientip limit = 0 | addtotals | sort 5 Total | fields - Total | untable product_name, clientip, count | xyseries clientip, product_name, count # it took these commands to swap row and column fields! | addtotals | sort 3 -Total | fields - Total aggregate for two different time ranges and compare on the same visualization index = security sourcetype = linux_secure \"failed password\" earliest = -30d@d latest = @h | eval Previous24h = relative_time ( now () , \"-24h@h\" ) , Series = if ( _time> = Previous24h, \"last24h\" , \"prior\" ) | timechart count span = 1h by Series | eval Hour = strftime ( _time, \"%H\" ) , currentHour = strftime ( now () , \"%H\" ) , offset = case ( Hour< = currentHour, Hour-currentHour, Hour>currentHour, ( Hour-24 ) -currentHour ) | stats avg ( prior ) as \"30 Day Average for Hour\" , sum ( last24 ) as \"Last 24 Hours\" by Hour, offset | sort offset | fields - offset | rename Hour as \"Hour of Day\" aggregate for certain time window each day You can do this with stats with more control and achieve what timechart provides and more! index = web sourcetype = access_combined action = purchase earliest = -3d@d latest = @d date_hour> = 0 AND date_hour< 6 | bin span = 1h _time | stats sum ( price ) as hourlySales by _time | eval Hour = strftime ( _time, \"%b %d, %I %p\" ) , \"Hourly Sales\" = \" $ \" .tostring ( hourlySales ) | table Hour, \"Hourly Sales\"","title":"SPL Tricks"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#reports-and-dashboards","text":"","title":"Reports and Dashboards"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#reports","text":"Reports allow people to easily store and share search results and queries used to make the search. When a report is run, a fresh search is run. Save report results to speed up search see Accelerate Reports Doc","title":"Reports"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#scheduled-reports-and-alerts","text":"Scheduled Reports can do weekly/monthly reports and automatically send results via emails. Select a saved report and add a schedule on it. Only admin can set its priority . Alerts are based on searches that run on scheduled intervals or in real time (by admin). It is triggered when the results of a search meet defined conditions . Some alerts actions: create a log file with the events output to lookup , to apend or replace data in a lookup table send to a telemetry endpoint, call an endpoint trigger scripts, triggers a bash script stored on the machine send emails or Slack notifications use a webhook","title":"Scheduled Reports and Alerts"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#visulizations","text":"Any searches that returns statistics information can be presented as visualizations , or charts. Charts can be based on numbers, time, and location . Save visualizations as a report or a dashboard panel. Creating customized visualizations see doc page Visulizations that are availble by default and are very intuitive (bar, line, area, pie, and table charts) are not explained here.","title":"Visulizations"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#choropleth-maps","text":"Uses colored shadings to show differences in numbers over geographical locations . It requires a compressed Keyhole Markup Language (KMZ) file that defines region boundries. Splunk ships with two KMZ files, geo_us_states for US, and geo_countries for the countries of the world. Other KMZ files can be provided and used. geom is the command to use to show choropleth map. See geom command .","title":"Choropleth Maps"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#single-value","text":"Displays a single number with formatting options to add caption, color, unit , etc. You can use the timechart command to add a trend and a sparkline for that value. It can also display as gauges which can take forms of radial, filler, or marker. There are options to format ranges and color. gauge is the command to use to enable and pass in ranges. See gauge command .","title":"Single Value"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#dashboards","text":"Dashboard is a collection of reports or visualizations.","title":"Dashboards"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#dashboarding-tricks","text":"let panels listen on variables Use $variable_name$ to refer to another variable that can be controlled by a dropdown or the drilldown 'earliest' and 'latest' from time picker For a time picker variable, the earliest and latest time can be accessed through $variable_name$.earliest and $variable_name$.latest","title":"Dashboarding Tricks"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#accelerations","text":"Splunk allows creation of summary of event data, as smaller segments of event data that only include those info needed to fullfill the search.","title":"Accelerations"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#report-acceleration","text":"Uses automatically created summaries, accelerates individual reports. acceleration summary basically are stored results populated every 10 minutes searches must be in Fast/Smart mode must include a transforming command, and having streaming command before it and non-streaming commands after also stored as file chunks alongside indexes buckets must re-build when associated knowledge objects change","title":"Report acceleration"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#summary-indexing","text":"Uses manually created summaries, indexes separated from deployment. must use 'si'-prefixed commands such as sichart , sistats search a Summary Index by using index=<summary_index_group_name> | report=<summary_index_name> need to pay attention to avoid creating gaps and overlaps in the data","title":"Summary indexing"},{"location":"Dev-Tools-Reference/Splunk/Splunk-References/#data-model-acceleration","text":"Accelerates all fields in a data model, easiest and most efficient option. adhoc acceleration - summary-creation happens automatically on data models that have not been accelerated; summary files created stays on the search head for the period that user is actively using the Pivot tool persistent acceleration - summary files created are stored alongside the data buckets and exits as long as the data model exists data models will become read-only after the acceleration requires searches use only streaming commands datamodel command allows display the structure of data models and search against them. Time-Series Index Files (tsidx files) are files created for data model acceleration. tsidx components: lexicon - an alphanumerically ordered list of terms found in data at index-time fields extracted at index-time showup as key-value pair in lexicon Splunk searches lexicon first and only open and read raw event data matching the terms using the pointers posting list - array of pointers that match each term to events in the raw data files building of tsidx files ALSO happen when a data model is accelerated for persistent acceleration, updated tsidx every 5 min and remove-outdated after 30 min tstats command is for getting statistical info from tsidx files","title":"Data model acceleration"},{"location":"Foundamental/Data_Structures_and_Algos/","text":"Some of the images references from this notes are referenced from geeksforgeeks.org and wikipedia.org . Data Structures Implementation \u00b6 These Data Structures are your friends: \u00b6 Array, Matrix, LinkedList, Double LinkedList Stack, Queue Binary Search Tree, Trie, Heap HashMap, HashSet Graph Adjacent List Stack \u00b6 Stack is a first-in-last-out data structure, good for hold state ( temporarily ) then process more recent elements. Use Stack also to turn recursive logic into iterative, mostly in DFS , where the call stack can be large for recursion, i.e. processing parenthesis errors. Use it also to solve cases when it is necessary to ( partially ) revert the order of some data sets. Stack is also useful for binary tree in-order traversal, as well as BST in-order traversal to find kth largest element. Monotonous Stack \u00b6 Monotonous Stack is not a new type of Stack, just that we force a property on top of its elements such that we only store elements coming in strict increasing or decreasing order; if a new element to be inserted breaks this rule, then we continue pop elements out until that property is maintained again. With this property, a Monotonous Stack can use O(n) to find the immediate elements (on left and right side) that are smaller than a chosen element, for any elements in the list. It is good for identify and calculate base on the local min/max. This type of property is especially useful for when need to calculate a sum/product of many continuous elements following an increasing or decreasing order. Queue \u00b6 Queue is a first-in-first-out data structure, often used in tree level-order traversal through BFS . Trees \u00b6 Tree is a data structure that each tree node may contain a value, and child nodes . Binary tree are the most used tree types. There are also n-ary tree like Trie. Use divide conquer on binary tree problems to calculate from small units and merge back up to final result. Binary Tree Divide Conquer Recursion often used for traversing binary trees public Result divideConquer () { if ( node == null ) { return ...; } Result left = divideConquer ( node . left ); Result right = divideConquer ( node . right ); Result result = merge ( left , right ); return result ; } BSTs \u00b6 Binary Search Trees (BST) are also called ordered or sorted binary tree. It is a binary tree data structure where for each root node, stores a key greater than all the keys in the node\u2019s left subtree and less than those in its right subtree. BST allow fast binary search to lookup value, of O(log(n)) . A BST's in-order expand the data into a sorted list. Since for the worst case the number of nodes of a tree can be equal to the hight of the tree which cause lookups take linear time O(n) , Balanced BSTs are often used. Traverse BST Use iteration with help of Stack to in-order traverse a BST: public Result problem () { List < Node > inorder = new ArrayList <> (); Stack < Node > stack = new Stack <> (); Node dummy = new Node (); dummy . right = root ; stack . push ( dummy ); while ( ! stack . isEmpty ()) { Node curr = stack . pop (); // safe to assume curr's left subtree are processed at this point if ( curr . right != null ) { // work on the right subtree curr = curr . right ; while ( curr != null ) { stack . push ( curr ); curr = curr . left ; // push left nodes all the way } } if ( ! stack . isEmpty ()) inorder . add ( stack . peek ()); // only take its value, it will be poped in next iteration } return inorder ; } Red-Black Tree \u00b6 A red-black tree is a kind of self-balancing binary search tree (BST) where each node has an extra bit , and that bit is often interpreted as the colour (red or black). These colours are used to ensure that the tree remains balanced during insertions and deletions. Most of the BST operations (e.g., search, max, min, insert, delete .. etc) take O(h) time where h is the height of the BST. The cost of these operations may become O(n) for a skewed Binary tree. If we make sure that the height of the tree remains O(log n) after every insertion and deletion, then we can guarantee an upper bound of O(log n) for all these operations. The height of a Red-Black tree is always O(log n) where n is the number of nodes in the tree. Rules That Every Red-Black Tree Follows: Every node has a colour either red or black. The root of the tree is always black . There are no two adjacent red nodes (A red node cannot have a red parent or red child). Every path from a node (including root) to any of its descendants NULL nodes has the same number of black nodes . All leaf nodes are black nodes. The balance of Red-Black tree is not perfect, compare to AVL Tree . But it does fewer rotations during insertion and deletion. It is good for use cases where many more insertions/deletions occur to the tree. Otherwise use AVL Tree. Red-Black Tree Properties: Black height of the red-black tree is the number of black nodes on a path from the root node to a leaf node. Leaf nodes are also counted as black nodes. So, a red-black tree of height h has black height >= h/2 . Height of a red-black tree with n nodes is h <= 2 log_2(n + 1) . All leaves (NIL) are black. The black depth of a node is defined as the number of black nodes from the root to that node i.e the number of black ancestors. Every red-black tree is a special case of a binary tree. Red-Black Tree Search Red-Black Tree Insertion Let x be the newly inserted node. Perform standard BST insertion and make the colour of newly inserted nodes as RED. If x is the root, change the colour of x as BLACK (Black height of complete tree increases by 1). Do the following if the color of x\u2019s parent is not BLACK and x is not the root. If x\u2019s uncle is RED (Grandparent must have been black from property 4) Change the colour of parent and uncle as BLACK. Colour of a grandparent as RED. Change x = x\u2019s grandparent, repeat steps 2 and 3 for new x. If x\u2019s uncle is BLACK, then there can be four configurations for x, x\u2019s parent (p) and x\u2019s grandparent (g) (This is similar to AVL Tree) Left Left Case (p is left child of g and x is left child of p) Left Right Case (p is left child of g and x is the right child of p) Right Right Case (Mirror of Left Left Case) Right Left Case (Mirror of Left Right Case) Red-Black Tree Deletion https://www.geeksforgeeks.org/red-black-tree-set-3-delete-2/?ref=lbp Red-Black tree re-balance strategies: 1. Recoloring; 2. Rotation . Recolor first, if not balanced then rotate. https://www.geeksforgeeks.org/red-black-tree-set-2-insert/?ref=lbp Java's TreeMap has a Red-Black tree implementation which allows it to do all basic operations in log(n) time while maintaining a sorted order of the elements. This makes it a decent alternative for PriorityQueue (Heap) for its remove(Object) method takes log(n) , if there are decent amount of operations to remove Objects in the middle of the heap, since the PriorityQueue implemention remove(Object) takes O(n) to find the element to remove, then O(1) to remove and O(log(n)) to maintain the heap state. AVL Tree \u00b6 AVL tree is a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees cannot be more than one for all nodes. AVL Tree Search The searching algorithm of an AVL tree is similar to that of a binary search tree, because of the property keys(left) < key(root) < keys(right) . AVL Tree Insertion Illustration of tree rotations which is critical for re-balancing the tree after each insertion/deletion operation. T1, T2 and T3 are subtrees of the tree rooted with y (on the left side) or x (on the right side) y x / \\ Right Rotation / \\ x T3 - - - - - - - > T1 y / \\ < - - - - - - - / \\ T1 T2 Left Rotation T2 T3 Keys in both of the above trees follow the following order keys(T1) < key(x) < keys(T2) < key(y) < keys(T3) So BST property is not violated anywhere. Let the newly inserted node be w Perform standard BST insert for w. Starting from w, travel up and find the first unbalanced node. Let z be the first unbalanced node, y be the child of z that comes on the path from w to z and x be the grandchild of z that comes on the path from w to z. Re-balance the tree by performing appropriate rotations on the subtree rooted with z. There can be 4 possible cases that needs to be handled as x, y and z can be arranged in 4 ways. Following are the possible 4 arrangements: y is left child of z and x is left child of y (Left Left Case) y is left child of z and x is right child of y (Left Right Case) y is right child of z and x is right child of y (Right Right Case) y is right child of z and x is left child of y (Right Left Case) We only need to re-balance the subtree rooted with z and the complete tree becomes balanced as the height of subtree rooted with z becomes same as it was before insertion. a) Left Left Case T1, T2, T3 and T4 are subtrees. z y / \\ / \\ y T4 Right Rotate (z) x z / \\ - - - - - - - - -> / \\ / \\ x T3 T1 T2 T3 T4 / \\ T1 T2 b) Left Right Case z z x / \\ / \\ / \\ y T4 Left Rotate (y) x T4 Right Rotate(z) y z / \\ - - - - - - - - -> / \\ - - - - - - - -> / \\ / \\ T1 x y T3 T1 T2 T3 T4 / \\ / \\ T2 T3 T1 T2 c) Right Right Case z y / \\ / \\ T1 y Left Rotate(z) z x / \\ - - - - - - - -> / \\ / \\ T2 x T1 T2 T3 T4 / \\ T3 T4 d) Right Left Case z z x / \\ / \\ / \\ T1 y Right Rotate (y) T1 x Left Rotate(z) z y / \\ - - - - - - - - -> / \\ - - - - - - - -> / \\ / \\ x T4 T2 y T1 T2 T3 T4 / \\ / \\ T2 T3 T3 T4 AVL Tree Deletion Very similar to the insertion operation. Perform standard BST delete for w. Starting from w, travel up and find the first unbalanced node. Let z be the first unbalanced node, y be the larger height child of z, and x be the larger height child of y. Note that the definitions of x and y are different from insertion here. Re-balance the tree by performing appropriate rotations on the subtree rooted with z. Like insertion, the rotations operations are the same After fixing z, we may have to fix ancestors of z as well https://www.geeksforgeeks.org/avl-tree-set-1-insertion/?ref=lbp Heap \u00b6 A heap is a tree-based data structure that is essentially a complete tree in the sense that new elements are always filling in the last level from left to right, then bobble up the tree. Heap is only \"partially sorted\", that it can only yield a min/max value of all elements in the tree, one at a time. It is useful for repeatedly removing objects from the heap to get ordered output . Heap can be easily implemented using an array. Each insert and remove operation takes O(log(n)). Thus for n elements it can take O(nlog(n)) sorting with a heap, which is how the Heap Sort got its name. Heapify an array using SiftDown approach (start from bottom of tree) takes O(n) to create the heap. A tricky case where heap can be used is, given a matrix of numbers, how can you visit the numbers from the outter loop and closing in until all are visited, while always visit the smallest number possible. It is also useful when dynamically calculate medians of a set of numbers that have numbers shift around. In Java you can use PriorityQueue or TreeSet . Signs for using Heap: Find the min or max from a given list of elements Find the kth largest/smallest When desires O(log(n)) for each operation Signs for not using Heap: Find the closest value to some value (balanced BST) Find the min/max from a given range (Segment Tree) Use O(n) to find kth largest (Quick Select) Trie \u00b6 Trie is a special n-ary Tree that is good for solving problems that require finding strings of prefix, or word prediction. Trie has great advantage over Maps when looking up a lots of words with common prefix; it requires much less Space . Trie is great for use with DFS for solving complex problems. Be sure to know how to implement a Trie. Signs for using a Trie: quick lookup if some word with certain prefix exists look for words in matrix of letters Trie Use as a helper class. public class Trie { boolean isWord ; Trie [] subTrie ; public Trie () { // do intialization if necessary this . isWord = false ; this . subTrie = new Trie [ 26 ] ; // 26 for letter range a-z } public void insert ( String word ) { // write your code here Trie curr = this ; for ( int i = 0 ; i < word . length (); ++ i ) { char c = word . charAt ( i ); Trie t = curr . subTrie [ c - 'a' ] ; if ( t == null ) { t = new Trie (); curr . subTrie [ c - 'a' ] = t ; } curr = t ; } curr . isWord = true ; } private Trie findTrieWithPrefix ( String prefix ) { Trie curr = this ; for ( int i = 0 ; i < prefix . length (); ++ i ) { char c = prefix . charAt ( i ); Trie t = curr . subTrie [ c - 'a' ] ; if ( t == null ) return null ; curr = t ; } return curr ; } public boolean search ( String word ) { Trie target = findTrieWithPrefix ( word ); return target == null ? false : target . isWord ; } public boolean startsWith ( String prefix ) { return findTrieWithPrefix ( prefix ) != null ; } } Segment Tree \u00b6 Segment Tree is a binary tree data structure that can be used to calulate the sums of elements within a given index range in O(log(n)) time, while also allowing the data to be updated in O(log(n)) time. It is mostly represented in array form (can also be built in pure tree structure with increased complexity), and maintain these properties: root node represents the total sum of data (entire interval) leaf nodes are the elements of the input array each internal node (non-leaf) represents the sum of all of its leaf nodes within its range each child node represents about half the parent node's range for each node at index i of the segment tree array, its left child is at 2*i + 1 , and right child at 2*i + 2 , and its parent at (i-1) / 2 . the size of the segment tree is bounded by 2 * 2^k - 1 , where k is the first number makes 2^k >= n . Segment Tree The construction, update, sumRange operations are all done in recursive mannor. class SegmentTree { private int [] segTree ; private int n ; public SegmentTree ( int [] nums ) { n = nums . length ; segTree = new int [ n * 4 ] ; // ensure enough capacity, proof https://stackoverflow.com/a/65300626/6037495 buildSegTree ( nums , 0 , 0 , n - 1 ); } private void buildSegTree ( int [] nums , int treeIndex , int lo , int hi ) { if ( lo == hi ) { segTree [ treeIndex ] = nums [ lo ] ; return ; } int mid = lo + ( hi - lo ) / 2 ; buildSegTree ( nums , 2 * treeIndex + 1 , lo , mid ); // fill left child buildSegTree ( nums , 2 * treeIndex + 2 , mid + 1 , hi ); // fill right child segTree [ treeIndex ] = segTree [ 2 * treeIndex + 1 ] + segTree [ 2 * treeIndex + 2 ] ; } public void update ( int index , int val ) { updateSegTree ( index , 0 , 0 , n - 1 , val ); } private void updateSegTree ( int numsIndex , int treeIndex , int lo , int hi , int val ) { if ( lo == hi ) { segTree [ treeIndex ] = val ; return ; } int mid = lo + ( hi - lo ) / 2 ; if ( mid < numsIndex ) { // look for numsIndex in the right half updateSegTree ( numsIndex , 2 * treeIndex + 2 , mid + 1 , hi , val ); } else { // look for numsIndex in the left half updateSegTree ( numsIndex , 2 * treeIndex + 1 , lo , mid , val ); } segTree [ treeIndex ] = segTree [ 2 * treeIndex + 1 ] + segTree [ 2 * treeIndex + 2 ] ; } public int sumRange ( int left , int right ) { return sumSegTree ( 0 , 0 , n - 1 , left , right ); } private int sumSegTree ( int treeIndex , int lo , int hi , int i , int j ) { if ( i > hi || j < lo ) return 0 ; if ( i <= lo && j >= hi ) return segTree [ treeIndex ] ; // this seg range [lo, hi] should all be used in the sum int mid = lo + ( hi - lo ) / 2 ; // half of [lo, hi] are not relevant, shrink [lo, hi] if ( i > mid ) { // use the right half, [i, j] unchanged return sumSegTree ( 2 * treeIndex + 2 , mid + 1 , hi , i , j ); } else if ( j <= mid ) { // use the left half, [i, j] unchanged return sumSegTree ( 2 * treeIndex + 1 , lo , mid , i , j ); } // now [i, j] contains mid of [lo, hi], tree them as two separate partitions int leftSum = sumSegTree ( 2 * treeIndex + 1 , lo , mid , i , mid ); int rightSum = sumSegTree ( 2 * treeIndex + 2 , mid + 1 , hi , mid + 1 , j ); return leftSum + rightSum ; } } Sorting \u00b6 Quick Sort \u00b6 Quick Sort is a divide-and-conquer algorithm, sorts with O(nlog(n)) time. It works by iterating log(n) times on the input array; in each iteration it chooses a pivot index which breaks the range into two partitions ; then use two pointers from two ends of the range and swap numbers to ensure that numbers smaller than the pivot number are move to the left partition and those greater are moved to the right partition. Quick Sort can be implemented in-place so it can sort in constant space . The choice of the pivot point can affect the effeciency of the algorithm depending on the data distribution. Quick Sort is NOT a stable sort , however, unlike merge sort, which could be a trade off. The Quick Select algorithm derived from Quck Sort borrows its partitioning thinking, which can be used to solve certain problems in O(log(n)) where sorting the list of items is not necessary. Merge Sort \u00b6 Merge Sort works by recursively divide the array into sub array of size 1, then on the way down the call stack, construct the merged and sorted subarrays. Merge Sort is a stable sort . But it takes both O(nlog(n)) time and O(n) space to complete. Count Sort \u00b6 Count Sort works when the data range is finite and value set is small. Then it can take O(n) to sort the data by counting the appearance of each value, and lay out the values by expanding the counts. Heap Sort \u00b6 Heap Sort uses a Heap data structure to assist the sort operation. It takes O(nlog(n)) time and O(n) space to complete. It is in theory slower than other O(nlog(n)) approaches, since it in fact O(2nlog(n)) for the extra step in building the heap. Dynamic Programming \u00b6 Dynamic Programming (DP) is a mathematical optimization method to break down a large and complicated problem into smaller and simpler sub-problems, then build each sub-problem upon results from previous ones. Signs for using DP: find number of ways to do something find the min/max of some value(s) that satisfy some condition find if some action can be completed Possibly not a DP solution: finding concrete set of solutions that satisfy some condition (DFS) given input is unsorted There are four fashions for Dynamic Programming thinking: Rolling Update on an array or matrix of numbers use previous result to build next result, how to get there how to initialize the state and starting point where does the final answer lies the key is to find what does the dp value represent, what are transition from state to the next state, and how to initialize the dp values before iterating (the extreme or end cases) Memorization of state when some state repeats within the sub-problems, calculate it once and save its result for use in future occurrances the core is dfs Strategy Game dp having a greedy smart opponent, whether you can win a game state transition involes considering my max gain and opponent's max gain Backsack dp use values as the dimension of the dp matrix The signs for dynamic programming problems: get the min/max , whether there exists some value, how many of something exists, that that satisfy some condition. DP often requires setting up a 1D array or 2D matrix to track the result of sub-problem as the two changing factors updates. It can grow to 3D or more depends on the number of changing factors to consider. Space optimization : usually 2D matrix is suffice, there may be opportunity to cut it down to 1D array since we mostly care about the previous row in a DP matrix. For problems solved intuitively using 1D array DP, you can cut it down to just 2-3 variables. There are no fixed way to solve anything with DP. The key is to think in DP. Rolling Update array/matrix Technique to optimize the rolling update numbers in array/matrix public void problem () { // size n matrix f [ i ] = Math . max ( f [ i - 1 ] , f [ i - 2 ] + A [ i ] ); // as size 2 matrix f [ i % 2 ] = Math . max ( f ( i - 1 ) % 2 , f [ ( i - 2 ) % 2 ) // or just use two intuitive variables } Memorization Some problems define relationships for a cell with its four neighbors and look for a maximum value that satisifies a condition . For this we can do memorization , pseudo code: dp = new int [ m ][ n ] for i = 1 -> m for j = 1 -> n search ( i , j ) search ( int x , int y , int [][] A ) if ( dp [ x ][ y ] == 0 ) return ; dp [ x ][ y ] = 1 for i = 1 -> 4 nx = x + dir [ 0 ] ny = y + dir [ 1 ] if ( A [ x ][ y ] < A [ nx ][ ny ] ) // depends on the required condition search ( nx , ny , A ) // ensure it is filled dp [ x ][ y ] = max ( dp [ x ][ y ] , dp [ nx ][ ny ] + 1 ) // depends on the condition The memorization is very useful when it is hard to determine the steps for state change, or defining the initial state. Its key is to take down large range and work on its smaller units and memorize and go back up, work on [0, n-1] at last. It often needs recursion to simplify the code. Greedy opponent A third type of DP problem involes thinking with a greedy opponent playing a game with you. You need to work from the end of the game to the beginning of the game to get the result, as it is easier to see the decision being made when close to end of game. The initial state are actually the state for end of the game , as we are working from the subset closing to the end of the game back to the beginning of the game to know whether we win or lose. Take Coins in a Line problem as an example: when facing no coin in the end, we get nothing, dp[n] = 0 index i means how many coins has been taken when facing last coin, we take it, dp[n-1] = values[n-1] when facing last two coins, we take both (greedy), dp[n-2] = values[n-2] + values[n-1] when facing last three coins, we take two, otherwise opponent will take two, dp[n-3] = values[n-3] + values[n-2] when facing four and more coins, things get interesting, as we can take one or two if only take one, you can assume the opponent is smart in maximizing his gain so he will choose wisely to take whether one or two thus our gain will be dp[i] = values[i] + min(dp[i+2], dp[i+3]) , the min function must be used here as the opponent may take one or two to minimize your gain if take two, for the same reason above our gain will be `dp[i] = values[i] + values[i+1] + min(dp[i+3], dp[i+4]) dp[0] will be the max gain we get from playing the game. to see whether we win, calculate coins sum and use sum - dp[0] < dp[0] Backsack A fourth type of DP problem is backsack problems . The key is to think about using range of values as one of the dimensions tracked with DP. Example problem: given a list of items with weights and values, maximize the total value put in a sack without going off the weight limit. Here the \"value range dimension\" is the max allowed weights. There are two factors: how many items we can choose from, and max weight we cannot go over. Then we work bottom up to figure out the max value at max allowed weights to choose from all items. Again, it is crucial to realize how dp matrix can be used and represent the steps. dp[i][j] will mean if I have first i items to choose from, and the max weight is j, what is the max gain. First get the solution out, then simplify the space complexity. The state transition involves thinking if take the current item or not: not take: dp[i][j] = dp[i-1][j] do take: dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-num]) in the condition when you can repeatedly take some num: dp[i][j] += dp[i-1][j - num * k] , where k = 0~v where k*v <= j basically we are trying to see, if we take one, two, or more of the same num, can be get more combinations DFS \u00b6 Depth-First-Search (DFS) is an algorithm good for searching within a tree, a graph, or backtrack iterating through elements in an array. DFS is mostly implemented recursively. Pay attention to when the order of elements affect the condition for going deeper, what does duplicate elements mean for the algorithm. Sometimes it helps to use Set or Map to memorize and to avoid revisiting the same path seen before. Signs for using DFS: Find all solutions that satisify some condition Binary Tree traversal Permutation problems (list order does not matter) Arrangement problems (list order matters) BFS \u00b6 Breath-First-Search (BFS) is an algorithm good for level-order traversing trees, topology sort, search in graphs. BFS can be implemented iteratively using a Queue. Consider use Set to track node visited, or Map to track distances between visited nodes. Some problems can be optimized using two BFS traversing from two ends to reduce number of nodes visited. Signs for using BFS: Topology sort Tree level order traversal Shortest path in graph Fewest moves to get to a state, given a state-change function When elements in a data structure are connected in some way Where there is a clear BFS solution, iteration may be more efficient than recursive DFS Special Algorithms \u00b6 Binary, Binary Search \u00b6 Binary search is an extremely useful algorithm to narrow down a large set of data and find the desired result that satisfy a certain condition. It usually can do O(logn) for an already sorted data set. Its key is to eliminate half of the data where the answer is not in. The the target could be a certain number, or a min/max value. Also note the binary search is NOT necessarily always must be applied on an array of numbers . It can be just a pair of bounds [a, b] , where your job is to find some value between [a, b] satisfy a condition, and maybe find the smallest/largest possible value. Binary thinking still applies . The Strategy: Find the solution range [a, b] guess from the middle of range, verify condition if condition not met, continue search while cut down range by half The Condition part of this problem usually can be the difficult part. It is not hard to realize the binary approach after all. This conditional verification can take O(n) to complete, thus brings overall to O(nlog(n)) . When working on some problem where the clear brute-force approach takes O(n^2) , try thinking in this direction see if you can come up with something. Signs for using binary search: search in ordered list when time complexity requires better than O(n) when a list can be partitioned into two parts, where answer lies in one part find a max/min value to satisfy a certain condition Binary Search Basic form, good for when we know there is only one index may satisfy the condition. public void binarySearch () { // check edge case when nums is empty int lo = 0 , hi = n - 1 ; while ( lo <= hi ) { int mid = lo + ( hi - lo ) / 2 ; if ( nums [ mid ] == target ) // condition might differ return mid ; else if ( nums [ mid ] < target ) { lo = mid + 1 ; } else { hi = mid - 1 ; } } return - 1 ; // not found } Another form, good for looking for a min/max index that satisfy the condition. public void binarySearch () { // check edge case when nums is empty int lo = 0 , hi = n - 1 ; while ( lo < hi ) { int mid = lo + ( hi - lo ) / 2 ; if ( nums [ mid ] >= target ) { hi = mid ; // now we are looking smallest i that nums[i] == target } else { lo = mid + 1 ; } } return nums [ hi ] == target ? hi : - 1 ; // not found } Yet another form which may work better, as binary always end up with two values, lo and hi; then use an additional condition to check which one to use. public int binarySearch () { // check edge case when nums is empty int lo = 0 , hi = n - 1 ; while ( lo < hi - 1 ) { int mid = lo + ( hi - lo ) / 2 ; if ( nums [ mid ] == target ) { return mid ; } else if ( nums [ mid ] < target ) { lo = mid ; } else { hi = mid ; } } if ( lo < target ) return lo ; return hi ; } Two Pointers \u00b6 Some common signs for two pointers: Sliding window, when need to inspect a range of numbers/characters i.e. inspect subarray/substring When the most optimal time complexity is O(n) When we want to use no extra space to solve Palindrome problems The two pointers can move in the same direction or the opposite direction Sliding Window \u00b6 Sliding Window Most problems follow this template public void problem () { for ( int lo = 0 , hi = 0 ; lo < n ; ++ lo ) { while ( hi < n && condition ) { ++ hi ; // some logic related to after hi is increased // like update a tracker variable } // nothing found, no need to move lo if ( above condition was still \"false\" ) break ; // some logic related to current lo value } } Topology Sort \u00b6 Classical problem where dependency relationships are clear. Topology sort sorts the Nodes within a Directed Acyclic Graph in an order that does not violate the edges relations. Topology sort L = Empty list that will contain the sorted elements S = Set of all nodes with no incoming edge while S is non-empty do remove a node n from S add n to tail of L for each node m with an edge e from n to m do remove edge e from the graph if m has no other incoming edges then insert m into S if graph has edges then return error (graph has at least one cycle) else return L (a topologically sorted order) Disjoint Sets for Union Find \u00b6 Union Find with Disjoint Sets is used to solve problems that require check if two elements are within the same sets union the two sets represented by the two elements convert all other operations to follow the above two rules It allows doing find and union operations in O(1) time on average. find - check if two elements are within the same set union - combine two sets Union Find Use as a helper class. public static class UnionFind { int [] parent ; public UnionFind ( int size ) { parent = new int [ size ] ; for ( int i = 1 ; i < size ; ++ i ) parent [ i ] = i ; // parent[i] == i means index i is its own root // other strategy: Arrays.fill(parent, -1) means -1 denotes a root index } public int find ( int i ) { if ( parent [ i ] == i ) return i ; // i's root is i parent [ i ] = find ( parent [ i ] ); // path compression return parent [ i ] ; } public void union ( int x , int y ) { int xRoot = find ( x ); int yRoot = find ( y ); if ( xRoot != yRoot ) { // let smaller index root point to higher index root parent [ Math . min ( xRoot , yRoot ) ] = Math . max ( xRoot , yRoot ); } } } Some tricks depends on the need of the problem: initialize the parent list: initialize all to '-1' to denode it is its own parent initialize all to its index to denode it is its own parent union, when two are disjoint, which should join which? min -> max max -> min smaller -> larger Kth Largest in N Arrays \u00b6 Sort the arrays, then use Heap; insert first column to a heap , then pop out a number and insert a new number ensuring it is the next smallest number (by tracking the x,y of the popped number); repeat k times to get the kth smallest. Time is mostly O(m * nlog(n)) for sorting and O(klog(min(k, m, n))) for Heap operations. Sweep line \u00b6 This type of problems have a common sign that you have to determine the max number of overlapping intervals at all time from the given intervals . To solve it in O(n) , first create a list to put all intervals numbers together (thus start and end form a line), then sort them base on the first number, then by whether it is the start or end. Then iterated from first to last, count the start number and reduce when end number is reached. The problem can get more complex, i.e. having additional parameter to consider and sort. But in the end it just adds more cases to consider, the core idea remains the same. Shortest Path \u00b6 Given a graph and a source vertex in the graph, find the shortest paths from the source to one or all vertices in the given graph. Dijkstra \u00b6 All nodes within the graph must be connected. Dijkstra's algorithm solves this problem in O(u * v). We keep a set to track visited vertices, and a another map to track the minimal distance from source to every other vertice. It works by starting from a node/vertice then update its immediate neighbors distance with the distance to current node + edge cost to that neighbor, then move on to the next unvisited node with minimal distance, then repeat until all nodes are visited. The algorithm: first make the adjacency matrix or adjacency list out of the graph. initialize a visitedSet to track visited vertices initialize minDistanceMap as empty and only keeps finite minDistances, assume unvisited vertices are Infinity distance if it is not in the Map set source minDistance to 0 by adding it to minDistanceMap while there is any vertice unvisited for each unvisited vertice with finite minDistance, choose one v that has the minimal distance set v as visited by adding it to the visitedSet remove v 's minDistance from minDistanceMap check v 's immediate neighbors u that are unvisited now v to u distance d is v 's minDistance + edge cost to u if u 's minDistance is Infinity, set u -> d in minDistanceMap if u 's minDistance is finite but larger than d , update this value in minDistanceMap Iterator implementation \u00b6 When implementing iterator for data structures that may contain nested structure, we need to avoid the hell of recursively getting the whole structure built at once either at the constructor or next() method. To keep it fast on average, we should move the buffering stage to the hasNext() method, and have next() invoke hasNext() before pulling next value. Meanwhile, keep a Stack and ensure its top value is ready to be accessed. If it is a nested structure, the process (flatten) it so it can be accessed directly next. Use the help of a temporary stack to pour the nested items on top of current stack in a first-in-first-out mannor. Big O Calculation \u00b6 For loops that go through all elements once takes O(n) , each dimention of nested loop increases O(n)'s power by one. Note that depends on the algorithm, mixed use of loops and recursion may also increase the O(n)'s power. For binary search when we can eliminate half of the results at each step, it takes O(log(n)) to narrow down to the result. For clear evidence of continuous time complexity of O(n + n/2 + n/4 + ... + 1) is roughly O(n) on average. Note that when processing strings or arrays, string copy or array copy may easily add O(n) to both time and space complexity.","title":"Data Structures and Algos"},{"location":"Foundamental/Data_Structures_and_Algos/#data-structures-implementation","text":"","title":"Data Structures Implementation"},{"location":"Foundamental/Data_Structures_and_Algos/#these-data-structures-are-your-friends","text":"Array, Matrix, LinkedList, Double LinkedList Stack, Queue Binary Search Tree, Trie, Heap HashMap, HashSet Graph Adjacent List","title":"These Data Structures are your friends:"},{"location":"Foundamental/Data_Structures_and_Algos/#stack","text":"Stack is a first-in-last-out data structure, good for hold state ( temporarily ) then process more recent elements. Use Stack also to turn recursive logic into iterative, mostly in DFS , where the call stack can be large for recursion, i.e. processing parenthesis errors. Use it also to solve cases when it is necessary to ( partially ) revert the order of some data sets. Stack is also useful for binary tree in-order traversal, as well as BST in-order traversal to find kth largest element.","title":"Stack"},{"location":"Foundamental/Data_Structures_and_Algos/#monotonous-stack","text":"Monotonous Stack is not a new type of Stack, just that we force a property on top of its elements such that we only store elements coming in strict increasing or decreasing order; if a new element to be inserted breaks this rule, then we continue pop elements out until that property is maintained again. With this property, a Monotonous Stack can use O(n) to find the immediate elements (on left and right side) that are smaller than a chosen element, for any elements in the list. It is good for identify and calculate base on the local min/max. This type of property is especially useful for when need to calculate a sum/product of many continuous elements following an increasing or decreasing order.","title":"Monotonous Stack"},{"location":"Foundamental/Data_Structures_and_Algos/#queue","text":"Queue is a first-in-first-out data structure, often used in tree level-order traversal through BFS .","title":"Queue"},{"location":"Foundamental/Data_Structures_and_Algos/#trees","text":"Tree is a data structure that each tree node may contain a value, and child nodes . Binary tree are the most used tree types. There are also n-ary tree like Trie. Use divide conquer on binary tree problems to calculate from small units and merge back up to final result. Binary Tree Divide Conquer Recursion often used for traversing binary trees public Result divideConquer () { if ( node == null ) { return ...; } Result left = divideConquer ( node . left ); Result right = divideConquer ( node . right ); Result result = merge ( left , right ); return result ; }","title":"Trees"},{"location":"Foundamental/Data_Structures_and_Algos/#bsts","text":"Binary Search Trees (BST) are also called ordered or sorted binary tree. It is a binary tree data structure where for each root node, stores a key greater than all the keys in the node\u2019s left subtree and less than those in its right subtree. BST allow fast binary search to lookup value, of O(log(n)) . A BST's in-order expand the data into a sorted list. Since for the worst case the number of nodes of a tree can be equal to the hight of the tree which cause lookups take linear time O(n) , Balanced BSTs are often used. Traverse BST Use iteration with help of Stack to in-order traverse a BST: public Result problem () { List < Node > inorder = new ArrayList <> (); Stack < Node > stack = new Stack <> (); Node dummy = new Node (); dummy . right = root ; stack . push ( dummy ); while ( ! stack . isEmpty ()) { Node curr = stack . pop (); // safe to assume curr's left subtree are processed at this point if ( curr . right != null ) { // work on the right subtree curr = curr . right ; while ( curr != null ) { stack . push ( curr ); curr = curr . left ; // push left nodes all the way } } if ( ! stack . isEmpty ()) inorder . add ( stack . peek ()); // only take its value, it will be poped in next iteration } return inorder ; }","title":"BSTs"},{"location":"Foundamental/Data_Structures_and_Algos/#red-black-tree","text":"A red-black tree is a kind of self-balancing binary search tree (BST) where each node has an extra bit , and that bit is often interpreted as the colour (red or black). These colours are used to ensure that the tree remains balanced during insertions and deletions. Most of the BST operations (e.g., search, max, min, insert, delete .. etc) take O(h) time where h is the height of the BST. The cost of these operations may become O(n) for a skewed Binary tree. If we make sure that the height of the tree remains O(log n) after every insertion and deletion, then we can guarantee an upper bound of O(log n) for all these operations. The height of a Red-Black tree is always O(log n) where n is the number of nodes in the tree. Rules That Every Red-Black Tree Follows: Every node has a colour either red or black. The root of the tree is always black . There are no two adjacent red nodes (A red node cannot have a red parent or red child). Every path from a node (including root) to any of its descendants NULL nodes has the same number of black nodes . All leaf nodes are black nodes. The balance of Red-Black tree is not perfect, compare to AVL Tree . But it does fewer rotations during insertion and deletion. It is good for use cases where many more insertions/deletions occur to the tree. Otherwise use AVL Tree. Red-Black Tree Properties: Black height of the red-black tree is the number of black nodes on a path from the root node to a leaf node. Leaf nodes are also counted as black nodes. So, a red-black tree of height h has black height >= h/2 . Height of a red-black tree with n nodes is h <= 2 log_2(n + 1) . All leaves (NIL) are black. The black depth of a node is defined as the number of black nodes from the root to that node i.e the number of black ancestors. Every red-black tree is a special case of a binary tree. Red-Black Tree Search Red-Black Tree Insertion Let x be the newly inserted node. Perform standard BST insertion and make the colour of newly inserted nodes as RED. If x is the root, change the colour of x as BLACK (Black height of complete tree increases by 1). Do the following if the color of x\u2019s parent is not BLACK and x is not the root. If x\u2019s uncle is RED (Grandparent must have been black from property 4) Change the colour of parent and uncle as BLACK. Colour of a grandparent as RED. Change x = x\u2019s grandparent, repeat steps 2 and 3 for new x. If x\u2019s uncle is BLACK, then there can be four configurations for x, x\u2019s parent (p) and x\u2019s grandparent (g) (This is similar to AVL Tree) Left Left Case (p is left child of g and x is left child of p) Left Right Case (p is left child of g and x is the right child of p) Right Right Case (Mirror of Left Left Case) Right Left Case (Mirror of Left Right Case) Red-Black Tree Deletion https://www.geeksforgeeks.org/red-black-tree-set-3-delete-2/?ref=lbp Red-Black tree re-balance strategies: 1. Recoloring; 2. Rotation . Recolor first, if not balanced then rotate. https://www.geeksforgeeks.org/red-black-tree-set-2-insert/?ref=lbp Java's TreeMap has a Red-Black tree implementation which allows it to do all basic operations in log(n) time while maintaining a sorted order of the elements. This makes it a decent alternative for PriorityQueue (Heap) for its remove(Object) method takes log(n) , if there are decent amount of operations to remove Objects in the middle of the heap, since the PriorityQueue implemention remove(Object) takes O(n) to find the element to remove, then O(1) to remove and O(log(n)) to maintain the heap state.","title":"Red-Black Tree"},{"location":"Foundamental/Data_Structures_and_Algos/#avl-tree","text":"AVL tree is a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees cannot be more than one for all nodes. AVL Tree Search The searching algorithm of an AVL tree is similar to that of a binary search tree, because of the property keys(left) < key(root) < keys(right) . AVL Tree Insertion Illustration of tree rotations which is critical for re-balancing the tree after each insertion/deletion operation. T1, T2 and T3 are subtrees of the tree rooted with y (on the left side) or x (on the right side) y x / \\ Right Rotation / \\ x T3 - - - - - - - > T1 y / \\ < - - - - - - - / \\ T1 T2 Left Rotation T2 T3 Keys in both of the above trees follow the following order keys(T1) < key(x) < keys(T2) < key(y) < keys(T3) So BST property is not violated anywhere. Let the newly inserted node be w Perform standard BST insert for w. Starting from w, travel up and find the first unbalanced node. Let z be the first unbalanced node, y be the child of z that comes on the path from w to z and x be the grandchild of z that comes on the path from w to z. Re-balance the tree by performing appropriate rotations on the subtree rooted with z. There can be 4 possible cases that needs to be handled as x, y and z can be arranged in 4 ways. Following are the possible 4 arrangements: y is left child of z and x is left child of y (Left Left Case) y is left child of z and x is right child of y (Left Right Case) y is right child of z and x is right child of y (Right Right Case) y is right child of z and x is left child of y (Right Left Case) We only need to re-balance the subtree rooted with z and the complete tree becomes balanced as the height of subtree rooted with z becomes same as it was before insertion. a) Left Left Case T1, T2, T3 and T4 are subtrees. z y / \\ / \\ y T4 Right Rotate (z) x z / \\ - - - - - - - - -> / \\ / \\ x T3 T1 T2 T3 T4 / \\ T1 T2 b) Left Right Case z z x / \\ / \\ / \\ y T4 Left Rotate (y) x T4 Right Rotate(z) y z / \\ - - - - - - - - -> / \\ - - - - - - - -> / \\ / \\ T1 x y T3 T1 T2 T3 T4 / \\ / \\ T2 T3 T1 T2 c) Right Right Case z y / \\ / \\ T1 y Left Rotate(z) z x / \\ - - - - - - - -> / \\ / \\ T2 x T1 T2 T3 T4 / \\ T3 T4 d) Right Left Case z z x / \\ / \\ / \\ T1 y Right Rotate (y) T1 x Left Rotate(z) z y / \\ - - - - - - - - -> / \\ - - - - - - - -> / \\ / \\ x T4 T2 y T1 T2 T3 T4 / \\ / \\ T2 T3 T3 T4 AVL Tree Deletion Very similar to the insertion operation. Perform standard BST delete for w. Starting from w, travel up and find the first unbalanced node. Let z be the first unbalanced node, y be the larger height child of z, and x be the larger height child of y. Note that the definitions of x and y are different from insertion here. Re-balance the tree by performing appropriate rotations on the subtree rooted with z. Like insertion, the rotations operations are the same After fixing z, we may have to fix ancestors of z as well https://www.geeksforgeeks.org/avl-tree-set-1-insertion/?ref=lbp","title":"AVL Tree"},{"location":"Foundamental/Data_Structures_and_Algos/#heap","text":"A heap is a tree-based data structure that is essentially a complete tree in the sense that new elements are always filling in the last level from left to right, then bobble up the tree. Heap is only \"partially sorted\", that it can only yield a min/max value of all elements in the tree, one at a time. It is useful for repeatedly removing objects from the heap to get ordered output . Heap can be easily implemented using an array. Each insert and remove operation takes O(log(n)). Thus for n elements it can take O(nlog(n)) sorting with a heap, which is how the Heap Sort got its name. Heapify an array using SiftDown approach (start from bottom of tree) takes O(n) to create the heap. A tricky case where heap can be used is, given a matrix of numbers, how can you visit the numbers from the outter loop and closing in until all are visited, while always visit the smallest number possible. It is also useful when dynamically calculate medians of a set of numbers that have numbers shift around. In Java you can use PriorityQueue or TreeSet . Signs for using Heap: Find the min or max from a given list of elements Find the kth largest/smallest When desires O(log(n)) for each operation Signs for not using Heap: Find the closest value to some value (balanced BST) Find the min/max from a given range (Segment Tree) Use O(n) to find kth largest (Quick Select)","title":"Heap"},{"location":"Foundamental/Data_Structures_and_Algos/#trie","text":"Trie is a special n-ary Tree that is good for solving problems that require finding strings of prefix, or word prediction. Trie has great advantage over Maps when looking up a lots of words with common prefix; it requires much less Space . Trie is great for use with DFS for solving complex problems. Be sure to know how to implement a Trie. Signs for using a Trie: quick lookup if some word with certain prefix exists look for words in matrix of letters Trie Use as a helper class. public class Trie { boolean isWord ; Trie [] subTrie ; public Trie () { // do intialization if necessary this . isWord = false ; this . subTrie = new Trie [ 26 ] ; // 26 for letter range a-z } public void insert ( String word ) { // write your code here Trie curr = this ; for ( int i = 0 ; i < word . length (); ++ i ) { char c = word . charAt ( i ); Trie t = curr . subTrie [ c - 'a' ] ; if ( t == null ) { t = new Trie (); curr . subTrie [ c - 'a' ] = t ; } curr = t ; } curr . isWord = true ; } private Trie findTrieWithPrefix ( String prefix ) { Trie curr = this ; for ( int i = 0 ; i < prefix . length (); ++ i ) { char c = prefix . charAt ( i ); Trie t = curr . subTrie [ c - 'a' ] ; if ( t == null ) return null ; curr = t ; } return curr ; } public boolean search ( String word ) { Trie target = findTrieWithPrefix ( word ); return target == null ? false : target . isWord ; } public boolean startsWith ( String prefix ) { return findTrieWithPrefix ( prefix ) != null ; } }","title":"Trie"},{"location":"Foundamental/Data_Structures_and_Algos/#segment-tree","text":"Segment Tree is a binary tree data structure that can be used to calulate the sums of elements within a given index range in O(log(n)) time, while also allowing the data to be updated in O(log(n)) time. It is mostly represented in array form (can also be built in pure tree structure with increased complexity), and maintain these properties: root node represents the total sum of data (entire interval) leaf nodes are the elements of the input array each internal node (non-leaf) represents the sum of all of its leaf nodes within its range each child node represents about half the parent node's range for each node at index i of the segment tree array, its left child is at 2*i + 1 , and right child at 2*i + 2 , and its parent at (i-1) / 2 . the size of the segment tree is bounded by 2 * 2^k - 1 , where k is the first number makes 2^k >= n . Segment Tree The construction, update, sumRange operations are all done in recursive mannor. class SegmentTree { private int [] segTree ; private int n ; public SegmentTree ( int [] nums ) { n = nums . length ; segTree = new int [ n * 4 ] ; // ensure enough capacity, proof https://stackoverflow.com/a/65300626/6037495 buildSegTree ( nums , 0 , 0 , n - 1 ); } private void buildSegTree ( int [] nums , int treeIndex , int lo , int hi ) { if ( lo == hi ) { segTree [ treeIndex ] = nums [ lo ] ; return ; } int mid = lo + ( hi - lo ) / 2 ; buildSegTree ( nums , 2 * treeIndex + 1 , lo , mid ); // fill left child buildSegTree ( nums , 2 * treeIndex + 2 , mid + 1 , hi ); // fill right child segTree [ treeIndex ] = segTree [ 2 * treeIndex + 1 ] + segTree [ 2 * treeIndex + 2 ] ; } public void update ( int index , int val ) { updateSegTree ( index , 0 , 0 , n - 1 , val ); } private void updateSegTree ( int numsIndex , int treeIndex , int lo , int hi , int val ) { if ( lo == hi ) { segTree [ treeIndex ] = val ; return ; } int mid = lo + ( hi - lo ) / 2 ; if ( mid < numsIndex ) { // look for numsIndex in the right half updateSegTree ( numsIndex , 2 * treeIndex + 2 , mid + 1 , hi , val ); } else { // look for numsIndex in the left half updateSegTree ( numsIndex , 2 * treeIndex + 1 , lo , mid , val ); } segTree [ treeIndex ] = segTree [ 2 * treeIndex + 1 ] + segTree [ 2 * treeIndex + 2 ] ; } public int sumRange ( int left , int right ) { return sumSegTree ( 0 , 0 , n - 1 , left , right ); } private int sumSegTree ( int treeIndex , int lo , int hi , int i , int j ) { if ( i > hi || j < lo ) return 0 ; if ( i <= lo && j >= hi ) return segTree [ treeIndex ] ; // this seg range [lo, hi] should all be used in the sum int mid = lo + ( hi - lo ) / 2 ; // half of [lo, hi] are not relevant, shrink [lo, hi] if ( i > mid ) { // use the right half, [i, j] unchanged return sumSegTree ( 2 * treeIndex + 2 , mid + 1 , hi , i , j ); } else if ( j <= mid ) { // use the left half, [i, j] unchanged return sumSegTree ( 2 * treeIndex + 1 , lo , mid , i , j ); } // now [i, j] contains mid of [lo, hi], tree them as two separate partitions int leftSum = sumSegTree ( 2 * treeIndex + 1 , lo , mid , i , mid ); int rightSum = sumSegTree ( 2 * treeIndex + 2 , mid + 1 , hi , mid + 1 , j ); return leftSum + rightSum ; } }","title":"Segment Tree"},{"location":"Foundamental/Data_Structures_and_Algos/#sorting","text":"","title":"Sorting"},{"location":"Foundamental/Data_Structures_and_Algos/#quick-sort","text":"Quick Sort is a divide-and-conquer algorithm, sorts with O(nlog(n)) time. It works by iterating log(n) times on the input array; in each iteration it chooses a pivot index which breaks the range into two partitions ; then use two pointers from two ends of the range and swap numbers to ensure that numbers smaller than the pivot number are move to the left partition and those greater are moved to the right partition. Quick Sort can be implemented in-place so it can sort in constant space . The choice of the pivot point can affect the effeciency of the algorithm depending on the data distribution. Quick Sort is NOT a stable sort , however, unlike merge sort, which could be a trade off. The Quick Select algorithm derived from Quck Sort borrows its partitioning thinking, which can be used to solve certain problems in O(log(n)) where sorting the list of items is not necessary.","title":"Quick Sort"},{"location":"Foundamental/Data_Structures_and_Algos/#merge-sort","text":"Merge Sort works by recursively divide the array into sub array of size 1, then on the way down the call stack, construct the merged and sorted subarrays. Merge Sort is a stable sort . But it takes both O(nlog(n)) time and O(n) space to complete.","title":"Merge Sort"},{"location":"Foundamental/Data_Structures_and_Algos/#count-sort","text":"Count Sort works when the data range is finite and value set is small. Then it can take O(n) to sort the data by counting the appearance of each value, and lay out the values by expanding the counts.","title":"Count Sort"},{"location":"Foundamental/Data_Structures_and_Algos/#heap-sort","text":"Heap Sort uses a Heap data structure to assist the sort operation. It takes O(nlog(n)) time and O(n) space to complete. It is in theory slower than other O(nlog(n)) approaches, since it in fact O(2nlog(n)) for the extra step in building the heap.","title":"Heap Sort"},{"location":"Foundamental/Data_Structures_and_Algos/#dynamic-programming","text":"Dynamic Programming (DP) is a mathematical optimization method to break down a large and complicated problem into smaller and simpler sub-problems, then build each sub-problem upon results from previous ones. Signs for using DP: find number of ways to do something find the min/max of some value(s) that satisfy some condition find if some action can be completed Possibly not a DP solution: finding concrete set of solutions that satisfy some condition (DFS) given input is unsorted There are four fashions for Dynamic Programming thinking: Rolling Update on an array or matrix of numbers use previous result to build next result, how to get there how to initialize the state and starting point where does the final answer lies the key is to find what does the dp value represent, what are transition from state to the next state, and how to initialize the dp values before iterating (the extreme or end cases) Memorization of state when some state repeats within the sub-problems, calculate it once and save its result for use in future occurrances the core is dfs Strategy Game dp having a greedy smart opponent, whether you can win a game state transition involes considering my max gain and opponent's max gain Backsack dp use values as the dimension of the dp matrix The signs for dynamic programming problems: get the min/max , whether there exists some value, how many of something exists, that that satisfy some condition. DP often requires setting up a 1D array or 2D matrix to track the result of sub-problem as the two changing factors updates. It can grow to 3D or more depends on the number of changing factors to consider. Space optimization : usually 2D matrix is suffice, there may be opportunity to cut it down to 1D array since we mostly care about the previous row in a DP matrix. For problems solved intuitively using 1D array DP, you can cut it down to just 2-3 variables. There are no fixed way to solve anything with DP. The key is to think in DP. Rolling Update array/matrix Technique to optimize the rolling update numbers in array/matrix public void problem () { // size n matrix f [ i ] = Math . max ( f [ i - 1 ] , f [ i - 2 ] + A [ i ] ); // as size 2 matrix f [ i % 2 ] = Math . max ( f ( i - 1 ) % 2 , f [ ( i - 2 ) % 2 ) // or just use two intuitive variables } Memorization Some problems define relationships for a cell with its four neighbors and look for a maximum value that satisifies a condition . For this we can do memorization , pseudo code: dp = new int [ m ][ n ] for i = 1 -> m for j = 1 -> n search ( i , j ) search ( int x , int y , int [][] A ) if ( dp [ x ][ y ] == 0 ) return ; dp [ x ][ y ] = 1 for i = 1 -> 4 nx = x + dir [ 0 ] ny = y + dir [ 1 ] if ( A [ x ][ y ] < A [ nx ][ ny ] ) // depends on the required condition search ( nx , ny , A ) // ensure it is filled dp [ x ][ y ] = max ( dp [ x ][ y ] , dp [ nx ][ ny ] + 1 ) // depends on the condition The memorization is very useful when it is hard to determine the steps for state change, or defining the initial state. Its key is to take down large range and work on its smaller units and memorize and go back up, work on [0, n-1] at last. It often needs recursion to simplify the code. Greedy opponent A third type of DP problem involes thinking with a greedy opponent playing a game with you. You need to work from the end of the game to the beginning of the game to get the result, as it is easier to see the decision being made when close to end of game. The initial state are actually the state for end of the game , as we are working from the subset closing to the end of the game back to the beginning of the game to know whether we win or lose. Take Coins in a Line problem as an example: when facing no coin in the end, we get nothing, dp[n] = 0 index i means how many coins has been taken when facing last coin, we take it, dp[n-1] = values[n-1] when facing last two coins, we take both (greedy), dp[n-2] = values[n-2] + values[n-1] when facing last three coins, we take two, otherwise opponent will take two, dp[n-3] = values[n-3] + values[n-2] when facing four and more coins, things get interesting, as we can take one or two if only take one, you can assume the opponent is smart in maximizing his gain so he will choose wisely to take whether one or two thus our gain will be dp[i] = values[i] + min(dp[i+2], dp[i+3]) , the min function must be used here as the opponent may take one or two to minimize your gain if take two, for the same reason above our gain will be `dp[i] = values[i] + values[i+1] + min(dp[i+3], dp[i+4]) dp[0] will be the max gain we get from playing the game. to see whether we win, calculate coins sum and use sum - dp[0] < dp[0] Backsack A fourth type of DP problem is backsack problems . The key is to think about using range of values as one of the dimensions tracked with DP. Example problem: given a list of items with weights and values, maximize the total value put in a sack without going off the weight limit. Here the \"value range dimension\" is the max allowed weights. There are two factors: how many items we can choose from, and max weight we cannot go over. Then we work bottom up to figure out the max value at max allowed weights to choose from all items. Again, it is crucial to realize how dp matrix can be used and represent the steps. dp[i][j] will mean if I have first i items to choose from, and the max weight is j, what is the max gain. First get the solution out, then simplify the space complexity. The state transition involves thinking if take the current item or not: not take: dp[i][j] = dp[i-1][j] do take: dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-num]) in the condition when you can repeatedly take some num: dp[i][j] += dp[i-1][j - num * k] , where k = 0~v where k*v <= j basically we are trying to see, if we take one, two, or more of the same num, can be get more combinations","title":"Dynamic Programming"},{"location":"Foundamental/Data_Structures_and_Algos/#dfs","text":"Depth-First-Search (DFS) is an algorithm good for searching within a tree, a graph, or backtrack iterating through elements in an array. DFS is mostly implemented recursively. Pay attention to when the order of elements affect the condition for going deeper, what does duplicate elements mean for the algorithm. Sometimes it helps to use Set or Map to memorize and to avoid revisiting the same path seen before. Signs for using DFS: Find all solutions that satisify some condition Binary Tree traversal Permutation problems (list order does not matter) Arrangement problems (list order matters)","title":"DFS"},{"location":"Foundamental/Data_Structures_and_Algos/#bfs","text":"Breath-First-Search (BFS) is an algorithm good for level-order traversing trees, topology sort, search in graphs. BFS can be implemented iteratively using a Queue. Consider use Set to track node visited, or Map to track distances between visited nodes. Some problems can be optimized using two BFS traversing from two ends to reduce number of nodes visited. Signs for using BFS: Topology sort Tree level order traversal Shortest path in graph Fewest moves to get to a state, given a state-change function When elements in a data structure are connected in some way Where there is a clear BFS solution, iteration may be more efficient than recursive DFS","title":"BFS"},{"location":"Foundamental/Data_Structures_and_Algos/#special-algorithms","text":"","title":"Special Algorithms"},{"location":"Foundamental/Data_Structures_and_Algos/#binary-binary-search","text":"Binary search is an extremely useful algorithm to narrow down a large set of data and find the desired result that satisfy a certain condition. It usually can do O(logn) for an already sorted data set. Its key is to eliminate half of the data where the answer is not in. The the target could be a certain number, or a min/max value. Also note the binary search is NOT necessarily always must be applied on an array of numbers . It can be just a pair of bounds [a, b] , where your job is to find some value between [a, b] satisfy a condition, and maybe find the smallest/largest possible value. Binary thinking still applies . The Strategy: Find the solution range [a, b] guess from the middle of range, verify condition if condition not met, continue search while cut down range by half The Condition part of this problem usually can be the difficult part. It is not hard to realize the binary approach after all. This conditional verification can take O(n) to complete, thus brings overall to O(nlog(n)) . When working on some problem where the clear brute-force approach takes O(n^2) , try thinking in this direction see if you can come up with something. Signs for using binary search: search in ordered list when time complexity requires better than O(n) when a list can be partitioned into two parts, where answer lies in one part find a max/min value to satisfy a certain condition Binary Search Basic form, good for when we know there is only one index may satisfy the condition. public void binarySearch () { // check edge case when nums is empty int lo = 0 , hi = n - 1 ; while ( lo <= hi ) { int mid = lo + ( hi - lo ) / 2 ; if ( nums [ mid ] == target ) // condition might differ return mid ; else if ( nums [ mid ] < target ) { lo = mid + 1 ; } else { hi = mid - 1 ; } } return - 1 ; // not found } Another form, good for looking for a min/max index that satisfy the condition. public void binarySearch () { // check edge case when nums is empty int lo = 0 , hi = n - 1 ; while ( lo < hi ) { int mid = lo + ( hi - lo ) / 2 ; if ( nums [ mid ] >= target ) { hi = mid ; // now we are looking smallest i that nums[i] == target } else { lo = mid + 1 ; } } return nums [ hi ] == target ? hi : - 1 ; // not found } Yet another form which may work better, as binary always end up with two values, lo and hi; then use an additional condition to check which one to use. public int binarySearch () { // check edge case when nums is empty int lo = 0 , hi = n - 1 ; while ( lo < hi - 1 ) { int mid = lo + ( hi - lo ) / 2 ; if ( nums [ mid ] == target ) { return mid ; } else if ( nums [ mid ] < target ) { lo = mid ; } else { hi = mid ; } } if ( lo < target ) return lo ; return hi ; }","title":"Binary, Binary Search"},{"location":"Foundamental/Data_Structures_and_Algos/#two-pointers","text":"Some common signs for two pointers: Sliding window, when need to inspect a range of numbers/characters i.e. inspect subarray/substring When the most optimal time complexity is O(n) When we want to use no extra space to solve Palindrome problems The two pointers can move in the same direction or the opposite direction","title":"Two Pointers"},{"location":"Foundamental/Data_Structures_and_Algos/#sliding-window","text":"Sliding Window Most problems follow this template public void problem () { for ( int lo = 0 , hi = 0 ; lo < n ; ++ lo ) { while ( hi < n && condition ) { ++ hi ; // some logic related to after hi is increased // like update a tracker variable } // nothing found, no need to move lo if ( above condition was still \"false\" ) break ; // some logic related to current lo value } }","title":"Sliding Window"},{"location":"Foundamental/Data_Structures_and_Algos/#topology-sort","text":"Classical problem where dependency relationships are clear. Topology sort sorts the Nodes within a Directed Acyclic Graph in an order that does not violate the edges relations. Topology sort L = Empty list that will contain the sorted elements S = Set of all nodes with no incoming edge while S is non-empty do remove a node n from S add n to tail of L for each node m with an edge e from n to m do remove edge e from the graph if m has no other incoming edges then insert m into S if graph has edges then return error (graph has at least one cycle) else return L (a topologically sorted order)","title":"Topology Sort"},{"location":"Foundamental/Data_Structures_and_Algos/#disjoint-sets-for-union-find","text":"Union Find with Disjoint Sets is used to solve problems that require check if two elements are within the same sets union the two sets represented by the two elements convert all other operations to follow the above two rules It allows doing find and union operations in O(1) time on average. find - check if two elements are within the same set union - combine two sets Union Find Use as a helper class. public static class UnionFind { int [] parent ; public UnionFind ( int size ) { parent = new int [ size ] ; for ( int i = 1 ; i < size ; ++ i ) parent [ i ] = i ; // parent[i] == i means index i is its own root // other strategy: Arrays.fill(parent, -1) means -1 denotes a root index } public int find ( int i ) { if ( parent [ i ] == i ) return i ; // i's root is i parent [ i ] = find ( parent [ i ] ); // path compression return parent [ i ] ; } public void union ( int x , int y ) { int xRoot = find ( x ); int yRoot = find ( y ); if ( xRoot != yRoot ) { // let smaller index root point to higher index root parent [ Math . min ( xRoot , yRoot ) ] = Math . max ( xRoot , yRoot ); } } } Some tricks depends on the need of the problem: initialize the parent list: initialize all to '-1' to denode it is its own parent initialize all to its index to denode it is its own parent union, when two are disjoint, which should join which? min -> max max -> min smaller -> larger","title":"Disjoint Sets for Union Find"},{"location":"Foundamental/Data_Structures_and_Algos/#kth-largest-in-n-arrays","text":"Sort the arrays, then use Heap; insert first column to a heap , then pop out a number and insert a new number ensuring it is the next smallest number (by tracking the x,y of the popped number); repeat k times to get the kth smallest. Time is mostly O(m * nlog(n)) for sorting and O(klog(min(k, m, n))) for Heap operations.","title":"Kth Largest in N Arrays"},{"location":"Foundamental/Data_Structures_and_Algos/#sweep-line","text":"This type of problems have a common sign that you have to determine the max number of overlapping intervals at all time from the given intervals . To solve it in O(n) , first create a list to put all intervals numbers together (thus start and end form a line), then sort them base on the first number, then by whether it is the start or end. Then iterated from first to last, count the start number and reduce when end number is reached. The problem can get more complex, i.e. having additional parameter to consider and sort. But in the end it just adds more cases to consider, the core idea remains the same.","title":"Sweep line"},{"location":"Foundamental/Data_Structures_and_Algos/#shortest-path","text":"Given a graph and a source vertex in the graph, find the shortest paths from the source to one or all vertices in the given graph.","title":"Shortest Path"},{"location":"Foundamental/Data_Structures_and_Algos/#dijkstra","text":"All nodes within the graph must be connected. Dijkstra's algorithm solves this problem in O(u * v). We keep a set to track visited vertices, and a another map to track the minimal distance from source to every other vertice. It works by starting from a node/vertice then update its immediate neighbors distance with the distance to current node + edge cost to that neighbor, then move on to the next unvisited node with minimal distance, then repeat until all nodes are visited. The algorithm: first make the adjacency matrix or adjacency list out of the graph. initialize a visitedSet to track visited vertices initialize minDistanceMap as empty and only keeps finite minDistances, assume unvisited vertices are Infinity distance if it is not in the Map set source minDistance to 0 by adding it to minDistanceMap while there is any vertice unvisited for each unvisited vertice with finite minDistance, choose one v that has the minimal distance set v as visited by adding it to the visitedSet remove v 's minDistance from minDistanceMap check v 's immediate neighbors u that are unvisited now v to u distance d is v 's minDistance + edge cost to u if u 's minDistance is Infinity, set u -> d in minDistanceMap if u 's minDistance is finite but larger than d , update this value in minDistanceMap","title":"Dijkstra"},{"location":"Foundamental/Data_Structures_and_Algos/#iterator-implementation","text":"When implementing iterator for data structures that may contain nested structure, we need to avoid the hell of recursively getting the whole structure built at once either at the constructor or next() method. To keep it fast on average, we should move the buffering stage to the hasNext() method, and have next() invoke hasNext() before pulling next value. Meanwhile, keep a Stack and ensure its top value is ready to be accessed. If it is a nested structure, the process (flatten) it so it can be accessed directly next. Use the help of a temporary stack to pour the nested items on top of current stack in a first-in-first-out mannor.","title":"Iterator implementation"},{"location":"Foundamental/Data_Structures_and_Algos/#big-o-calculation","text":"For loops that go through all elements once takes O(n) , each dimention of nested loop increases O(n)'s power by one. Note that depends on the algorithm, mixed use of loops and recursion may also increase the O(n)'s power. For binary search when we can eliminate half of the results at each step, it takes O(log(n)) to narrow down to the result. For clear evidence of continuous time complexity of O(n + n/2 + n/4 + ... + 1) is roughly O(n) on average. Note that when processing strings or arrays, string copy or array copy may easily add O(n) to both time and space complexity.","title":"Big O Calculation"},{"location":"Linux/Commands/Freq-Used-Shell-Cmds/","text":"Frequently Used sh/Bash Builtin Commands \u00b6 alias define shortcuts for long commands alias [-p] [<name>[=<value>] \u2026] without arguments or with the -p option, prints a list of aliases if arguments are supplied, an alias is defined for each name whose value is given caller helps debug shell script by displaying line number of execution caller [<expr>] without expr, caller displays the line number and source filename of the current subroutine call; good for debugging and printing stack trace if a non-negative integer is supplied as expr, caller displays the line number, subroutine name, and source file corresponding to that position in the current execution call stack. cd move to a different working directory cd [-L|[-P [-e]] [-@] [directory] change the current working directory to directory the value of the HOME shell variable is used if no directory supplied if directory is - , it is converted to $OLDPWD before the directory change is attempted -P will not resolve symbolic links -L will resolve symbolic links when changing directory (default) command runs command with arguments ignoring any shell function named command command [-pVv] <command> [arguments ...] only shell builtin commands or commands found by searching the PATH are executed. -p option means to use a default value for PATH that is guaranteed to find all of the standard utilities -v -V prints description of command echo output the args, separated by spaces, terminated with a newline echo [-neE] [<arg> ...] -n suppress the trailing newline for the print -e enables interpretation of the following backslash-escaped characters supported escape sequences: \\a \\b \\c \\e \\E \\f \\n \\r \\t \\v \\\\ \\0nnn \\xHH \\uHHHH \\UHHHHHHHH -E disables interpretation of backslash-escaped characters printf write the formatted arguments to STDOUT under the control of the format printf [-v <var>] <format> [<arguments>] -v causes the output to be assigned to the variable var rather than being printed to STDOUT special extensions in format : %b expand backslash escape sequences in the corresponding argument in the same way as echo -e %q output the corresponding argument in a format that can be reused as shell input %(<datefmt>)T output the date-time string resulting from using datefmt as a format string for strftime(3) The corresponding argument is an integer representing the number of seconds since the epoch Two special argument values may be used: -1 represents the current time, and -2 represents the time the shell was invoked read reads input from STDIN read [-ers] [-a <aname>] [-d <delim>] [-i <text>] [-n <nchars>] [-N <nchars>] [-p <prompt>] [-t <timeou>t] [-u <fd>] [<name> \u2026] one line is read from the STDIN, or from the file descriptor fd supplied as an argument to the -u option -a <aname> the words are assigned to sequential indices of the array variable aname, starting at 0 -d <delim> the first character of delim is used to terminate the input line, rather than newline. If delim is the empty string, read will terminate a line when it reads a NUL character. -p <prompt> display prompt , without a trailing newline, before attempting to read any input -r backslash does not act as an escape character -s silence mode (good for password prompt). If input is coming from a terminal, characters are not echoed -t <timeout> read to time out and return failure if a complete line of input (or a specified number of characters) is not read within timeout seconds -u <fd> read input from file descriptor fd source same as . , to run a script or file source filename A synonym for . read and execute commands from the filename argument in the current shell context. type shows how a term is interpreted by shell type [-afptP] [<name> \u2026] for each name , indicate how it would be interpreted if used as a command name -t type prints a single word which is one of 'alias', 'function', 'builtin', 'file' or 'keyword', if name is an alias, shell function, shell builtin, disk file, or shell reserved word, respectively -p returns the name of the disk file that would be executed, or nothing -P forces a path search for each name -a returns all of the places that contain an executable named file umask default new file permissions umask [-p] [-S] [mode] set the shell process\u2019s file creation mask to mode if mode begins with a digit, it is interpreted as an octal number; if not, it is interpreted as a symbolic mode mask similar to that accepted by the chmod command unset remove some env vars unset [-fnv] [name] remove each variable or function name -f refers function; -v refers variable -n means name will be a nameref attribute; only name is unset, not the variable it references For more info visit https://www.gnu.org/software/bash/manual/html_node/index.html Other General Linux Commands \u00b6 common commands halt(shutdown), reboot apt-get tool for handling packages using APT library apt-get [OPTIONS]... [update|upgrade|install|remove|purge|source|build-dep|download|check|clean|autoclean|autoremove] update : resynchronize package index files from their sources specified in /etc/apt/sources.list upgrade : install newest versions of all packages currently installed on the system from the sources in /etc/apt/sources.list install : get one+ package desired for install or upgrade remove : opposite of install purge : like remove but packages and configs are also removed source : causes to fetch source packages build-def : causes apt-get to install/remove packages to satisfy build dependencies for a source package. check : diagnostic tool; updates package cache and checks for broken dependencies download : download given binary package into curr dir clean : clean clears out local repo of retrieved package files. removes everything but the lock file from /var/cache/apt/archives/ and .../partial/ autoremove : remove packages that were auto installed to satisfy dependencies but no longer needed cat Concatenate FILEs to standard output. cat [OPTION]... FILE... -A : show all info, equivalent to -vET -n : number all output lines. -b number nonempty lines only -s : suppress repeated empty output lines -T : display TAB chars as ^I chmod change file mode bits chmod [OPTION]... MODE... FILE... -R : change files and dirs recursively MODE is of the form: [ugoa]*([-+=]([rwxXst]*|[ugo]))+|[-+=][0-7]+ chmod a+x file or chmod 755 file cp copy files/directories cp [OPTION]... SOURCE... DEST... -H : follow command-line symbolic links in SOURCE -s : make symbolic links instead of copying df report file system disk space usage df [OPTION]... [FILE]... -a : all, include pseudo, duplicate, inaccessible fs -B : print sizes by SIZE unit -h : size in human readable format -i : list inode info instead of block usage -l : listing only local fs -T : print fs type. -t=TYPE for limiting fs type to display du summarize disk usage of set of FILES, recursively for dirs du [OPTION]... [FILE]... -h : human readable -c : produce grand total --apparent-size : rather than disk usage, excludes sparse echo Echo the STRINGs to standard output. echo [SHORT-OPTION]... STRING... -n : do not output trailing newline -e/-E : enable/disable interpretation of backslash escapes hostname show or set the system's host name hostname [OPTIONS]... -d : display name of the DNS domain -I : display all network addresses of the host. locate to find files by name locate [OPTION]... PATTERN... -A : print entries that match all PATTERNs instead of any one -c : print number of matching entries -b : match only base name against patterns. -w is opposite -d : replace default database with DBPATH (: separated db file names) -e : print only existing files -i : ignore case when matching patterns -l : limit output entries -L : follow trailing symbolic links when checking file existence. -P is the opposite -q : write no error messages -r : [REGEXP] search for a basic regexp REGEXP, can be used multiple times ls show files in a directory ls [OPTION]... [FILE]... -a : show hidden(implied) files (starting with .) -c : sort by ctime newest first with -lt : sort by and show ctime (last modif.) with -l : show ctime and sort by name -h : with -l or -s: print human readable sizes -r : reverse order while sorting -R : list subdirs recursively -S : sort by file size, largest first --sort=WORD , WORD=[-U(none),-S,-t,-v,-X(extension)] mkdir create a directory mkdir [OPTION]... DIRECTORY... -p : make parent dirs as needed -v : print message for each created dir -m : set file mode (as in chmod) mv move file/directories to another location or rename file/directory mv [OPTION]... SOURCE... DEST -f : no prompt before overwriting; -i prompt before overwrite -n : do not overwrite existing file -u : move only when SOURCE file is newer than dest file (or missing) -v : explain what is done ping send ICMP ECHO_REQUEST to network hosts ping [OPTIONS]... destinationIP -b : allow pinging a broadcast address -c : COUNT stop after sending COUNT packets -D : print timestamp before each line -f : flood ping. check how many packets are being dropped -i : INTERVAL wait INTERVAL seconds b/w packets sending -w : DEADLINE a timeout for ping exists -W : TIMEOUT set time to wait for a resp -v : verbose output rm delete file/directories rm [OPTION]... {script} FILE... -f : ignore non-existent files and arguments -r :/-R remove dirs and sub-contents -d : remove empty directories -v : explain what is done sed stream editor for filtering and transforming text sed [OPTION]... [FILE]... -n : suppress auto printing of pattern space -e : SCRIPT add the script to the commands to exec -i : edit files in place -r : use extended regular expressions in the script sed commands: = print the current line number a TEXT append text i TEXT insert text c TEXT replace selected lines with text d delete pattern space /regexp/ match lines using this regexp tar work with tarballs archive file, ex. .tar .tar.gz .tar.bz2 tar [...] [OPTIONS] [PATHNAME...] -A : append tar files to an archive -c : create a new archive -d : run differences b/w archive and fs --delete : delete from archive -r : append files to the end of an archive -t : list contents of an archive -u : only append files newer than copy in archive -x : extract files from an archive -a : auto determine compression program by archive suffix -f : ARCHIVE use archive file or device ARCHIVE -h : follow symbolic links; archive and dump files linked -l : check links, print msg if not all links are dumped -s : handle sparse files efficiently -U : remove each file prior to extracting over it -v : print files processed -W : verify archive after writing it touch create a file or update file modification date touch [OPTION]... FILE... -c : do not create file -a/-m : change access/modification time only uname print system information uname [OPTION]... -a : print all info, in following order: kernel name, network node hostname, kernel release, kernel version, machine hardware name, processor type, hardware platform, OS zip/unzip compress/decompress zip files zip [OPTIONS]... ZIPPEDFILE FILE... -u : update existing entries for newer entries, or add new entries -f : like -u, but not adding new entries -d : delete entries in an existing archive -U : select entries and copy to a new archive -e : encrypt using a password -F : fix zip archive -i : include only specified files -o : output the archive modified file as a new archive -P : include relative file paths as part of the file names -r : traverse dir recursively","title":"Freq Used Shell Cmds"},{"location":"Linux/Commands/Freq-Used-Shell-Cmds/#frequently-used-shbash-builtin-commands","text":"alias define shortcuts for long commands alias [-p] [<name>[=<value>] \u2026] without arguments or with the -p option, prints a list of aliases if arguments are supplied, an alias is defined for each name whose value is given caller helps debug shell script by displaying line number of execution caller [<expr>] without expr, caller displays the line number and source filename of the current subroutine call; good for debugging and printing stack trace if a non-negative integer is supplied as expr, caller displays the line number, subroutine name, and source file corresponding to that position in the current execution call stack. cd move to a different working directory cd [-L|[-P [-e]] [-@] [directory] change the current working directory to directory the value of the HOME shell variable is used if no directory supplied if directory is - , it is converted to $OLDPWD before the directory change is attempted -P will not resolve symbolic links -L will resolve symbolic links when changing directory (default) command runs command with arguments ignoring any shell function named command command [-pVv] <command> [arguments ...] only shell builtin commands or commands found by searching the PATH are executed. -p option means to use a default value for PATH that is guaranteed to find all of the standard utilities -v -V prints description of command echo output the args, separated by spaces, terminated with a newline echo [-neE] [<arg> ...] -n suppress the trailing newline for the print -e enables interpretation of the following backslash-escaped characters supported escape sequences: \\a \\b \\c \\e \\E \\f \\n \\r \\t \\v \\\\ \\0nnn \\xHH \\uHHHH \\UHHHHHHHH -E disables interpretation of backslash-escaped characters printf write the formatted arguments to STDOUT under the control of the format printf [-v <var>] <format> [<arguments>] -v causes the output to be assigned to the variable var rather than being printed to STDOUT special extensions in format : %b expand backslash escape sequences in the corresponding argument in the same way as echo -e %q output the corresponding argument in a format that can be reused as shell input %(<datefmt>)T output the date-time string resulting from using datefmt as a format string for strftime(3) The corresponding argument is an integer representing the number of seconds since the epoch Two special argument values may be used: -1 represents the current time, and -2 represents the time the shell was invoked read reads input from STDIN read [-ers] [-a <aname>] [-d <delim>] [-i <text>] [-n <nchars>] [-N <nchars>] [-p <prompt>] [-t <timeou>t] [-u <fd>] [<name> \u2026] one line is read from the STDIN, or from the file descriptor fd supplied as an argument to the -u option -a <aname> the words are assigned to sequential indices of the array variable aname, starting at 0 -d <delim> the first character of delim is used to terminate the input line, rather than newline. If delim is the empty string, read will terminate a line when it reads a NUL character. -p <prompt> display prompt , without a trailing newline, before attempting to read any input -r backslash does not act as an escape character -s silence mode (good for password prompt). If input is coming from a terminal, characters are not echoed -t <timeout> read to time out and return failure if a complete line of input (or a specified number of characters) is not read within timeout seconds -u <fd> read input from file descriptor fd source same as . , to run a script or file source filename A synonym for . read and execute commands from the filename argument in the current shell context. type shows how a term is interpreted by shell type [-afptP] [<name> \u2026] for each name , indicate how it would be interpreted if used as a command name -t type prints a single word which is one of 'alias', 'function', 'builtin', 'file' or 'keyword', if name is an alias, shell function, shell builtin, disk file, or shell reserved word, respectively -p returns the name of the disk file that would be executed, or nothing -P forces a path search for each name -a returns all of the places that contain an executable named file umask default new file permissions umask [-p] [-S] [mode] set the shell process\u2019s file creation mask to mode if mode begins with a digit, it is interpreted as an octal number; if not, it is interpreted as a symbolic mode mask similar to that accepted by the chmod command unset remove some env vars unset [-fnv] [name] remove each variable or function name -f refers function; -v refers variable -n means name will be a nameref attribute; only name is unset, not the variable it references For more info visit https://www.gnu.org/software/bash/manual/html_node/index.html","title":"Frequently Used sh/Bash Builtin Commands"},{"location":"Linux/Commands/Freq-Used-Shell-Cmds/#other-general-linux-commands","text":"common commands halt(shutdown), reboot apt-get tool for handling packages using APT library apt-get [OPTIONS]... [update|upgrade|install|remove|purge|source|build-dep|download|check|clean|autoclean|autoremove] update : resynchronize package index files from their sources specified in /etc/apt/sources.list upgrade : install newest versions of all packages currently installed on the system from the sources in /etc/apt/sources.list install : get one+ package desired for install or upgrade remove : opposite of install purge : like remove but packages and configs are also removed source : causes to fetch source packages build-def : causes apt-get to install/remove packages to satisfy build dependencies for a source package. check : diagnostic tool; updates package cache and checks for broken dependencies download : download given binary package into curr dir clean : clean clears out local repo of retrieved package files. removes everything but the lock file from /var/cache/apt/archives/ and .../partial/ autoremove : remove packages that were auto installed to satisfy dependencies but no longer needed cat Concatenate FILEs to standard output. cat [OPTION]... FILE... -A : show all info, equivalent to -vET -n : number all output lines. -b number nonempty lines only -s : suppress repeated empty output lines -T : display TAB chars as ^I chmod change file mode bits chmod [OPTION]... MODE... FILE... -R : change files and dirs recursively MODE is of the form: [ugoa]*([-+=]([rwxXst]*|[ugo]))+|[-+=][0-7]+ chmod a+x file or chmod 755 file cp copy files/directories cp [OPTION]... SOURCE... DEST... -H : follow command-line symbolic links in SOURCE -s : make symbolic links instead of copying df report file system disk space usage df [OPTION]... [FILE]... -a : all, include pseudo, duplicate, inaccessible fs -B : print sizes by SIZE unit -h : size in human readable format -i : list inode info instead of block usage -l : listing only local fs -T : print fs type. -t=TYPE for limiting fs type to display du summarize disk usage of set of FILES, recursively for dirs du [OPTION]... [FILE]... -h : human readable -c : produce grand total --apparent-size : rather than disk usage, excludes sparse echo Echo the STRINGs to standard output. echo [SHORT-OPTION]... STRING... -n : do not output trailing newline -e/-E : enable/disable interpretation of backslash escapes hostname show or set the system's host name hostname [OPTIONS]... -d : display name of the DNS domain -I : display all network addresses of the host. locate to find files by name locate [OPTION]... PATTERN... -A : print entries that match all PATTERNs instead of any one -c : print number of matching entries -b : match only base name against patterns. -w is opposite -d : replace default database with DBPATH (: separated db file names) -e : print only existing files -i : ignore case when matching patterns -l : limit output entries -L : follow trailing symbolic links when checking file existence. -P is the opposite -q : write no error messages -r : [REGEXP] search for a basic regexp REGEXP, can be used multiple times ls show files in a directory ls [OPTION]... [FILE]... -a : show hidden(implied) files (starting with .) -c : sort by ctime newest first with -lt : sort by and show ctime (last modif.) with -l : show ctime and sort by name -h : with -l or -s: print human readable sizes -r : reverse order while sorting -R : list subdirs recursively -S : sort by file size, largest first --sort=WORD , WORD=[-U(none),-S,-t,-v,-X(extension)] mkdir create a directory mkdir [OPTION]... DIRECTORY... -p : make parent dirs as needed -v : print message for each created dir -m : set file mode (as in chmod) mv move file/directories to another location or rename file/directory mv [OPTION]... SOURCE... DEST -f : no prompt before overwriting; -i prompt before overwrite -n : do not overwrite existing file -u : move only when SOURCE file is newer than dest file (or missing) -v : explain what is done ping send ICMP ECHO_REQUEST to network hosts ping [OPTIONS]... destinationIP -b : allow pinging a broadcast address -c : COUNT stop after sending COUNT packets -D : print timestamp before each line -f : flood ping. check how many packets are being dropped -i : INTERVAL wait INTERVAL seconds b/w packets sending -w : DEADLINE a timeout for ping exists -W : TIMEOUT set time to wait for a resp -v : verbose output rm delete file/directories rm [OPTION]... {script} FILE... -f : ignore non-existent files and arguments -r :/-R remove dirs and sub-contents -d : remove empty directories -v : explain what is done sed stream editor for filtering and transforming text sed [OPTION]... [FILE]... -n : suppress auto printing of pattern space -e : SCRIPT add the script to the commands to exec -i : edit files in place -r : use extended regular expressions in the script sed commands: = print the current line number a TEXT append text i TEXT insert text c TEXT replace selected lines with text d delete pattern space /regexp/ match lines using this regexp tar work with tarballs archive file, ex. .tar .tar.gz .tar.bz2 tar [...] [OPTIONS] [PATHNAME...] -A : append tar files to an archive -c : create a new archive -d : run differences b/w archive and fs --delete : delete from archive -r : append files to the end of an archive -t : list contents of an archive -u : only append files newer than copy in archive -x : extract files from an archive -a : auto determine compression program by archive suffix -f : ARCHIVE use archive file or device ARCHIVE -h : follow symbolic links; archive and dump files linked -l : check links, print msg if not all links are dumped -s : handle sparse files efficiently -U : remove each file prior to extracting over it -v : print files processed -W : verify archive after writing it touch create a file or update file modification date touch [OPTION]... FILE... -c : do not create file -a/-m : change access/modification time only uname print system information uname [OPTION]... -a : print all info, in following order: kernel name, network node hostname, kernel release, kernel version, machine hardware name, processor type, hardware platform, OS zip/unzip compress/decompress zip files zip [OPTIONS]... ZIPPEDFILE FILE... -u : update existing entries for newer entries, or add new entries -f : like -u, but not adding new entries -d : delete entries in an existing archive -U : select entries and copy to a new archive -e : encrypt using a password -F : fix zip archive -i : include only specified files -o : output the archive modified file as a new archive -P : include relative file paths as part of the file names -r : traverse dir recursively","title":"Other General Linux Commands"},{"location":"Linux/Commands/Networking-Shell-Cmds/","text":"Linux Networking Related Commands/Knowledge \u00b6 To connect to Internet, proper settings of IP, Netmask, Gateway, DNS IP, and device name are all required. Linux based on CentOS \u00b6 Linux NIC Network Interface Card are named like modules in Linux, like eth0 for the first NIC, eth1 for the second NIC, and so on. Use dmesg to view specific device info. Like dmesg | grep -in eth Use lsmod to view whether the device is loaded by kernel. Like lsmod | grep 1000 Use modinfo to view details about this device, passing in the device number, like e1000 the filename portion is the NIC driver for current Kernel version. Use ifconfig to lookup the NIC number for a device. write driver for your NIC, see Book2 P113 Linux Networks config files /etc/services: sets port number for protocols /etc/protocols: defines protocols' related info /etc/init.d/networking restart: this is an important script to reset entire network to default ifup eth0 (ifdown eth0): a script to turn on or off one of the network device modify config file and important script commands to view results IP related /etc/sysconfig/network-scripts/ifcfg-eth0; /etc/init.d/network restart ifconfig; route -n DNS /etc/resolv.conf dig www.google.com host /etc/sysconfig/network hostname; ping hostname; reboot vim /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" <== device number, must match ifcfg-eth0 HWADDR=\"08:00:27:71:85:BD\" <== NIC address (MAC), can be omitted if having only one NIC NM_CONTROLLED=\"no\" <== uncontrolled by other software ONBOOT=\"yes\" <== startup on bootup BOOTPROTO=none <== can be ''dhcp'' or 'none' IPADDR=192.168.1.100 <== id address NETMASK=255.255.255.0 <== subnet mask GATEWAY=192.168.1.254 <== router # \u91cd\u70b9\u662f\u4e0a\u9762\u8fd9\u51e0\u4e2a\u8bbe\u5b9a\u9879\u76ee,\u5e95\u4e0b\u7684\u5219\u53ef\u4ee5\u7701\u7565\u7684\u5570! NETWORK=192.168.1.0 <== first IP of this network segment BROADCAST=192.168.1.255 <== broadcast IP MTU=1500 <== max transmition unit the file /etc/resolv.conf contains information about the IP address of DNS nameserver the command dig shows which DNS nameserver is used to reach an IP, which is part of a package called bind-utils change host name change /etc/sysconfig/network and /etc/hosts Book2 P123 ADSL configuration Book2 P124 Access Point When connecting to a router, in fact you are connecting to its Wireless Access Point(AP) When you are exposed under multiple AP range, you can detect many APs. Each AP has a SSID or ESSID for the client to identify the right AP. lsusb is part of usbutils package, that can show usb devices plugged-in Linux Networking Commands \u00b6 set on/off IP parameters ifconfig can directly change IP config for a NIC device. ifconfig [interface] [up|down|options] up, down: like the commands below mtu: change mtu value netmask: set subnet mask broadcast: set broadcast address ifup , ifdown can only turn on/off of the device, not changing its parameters set router parameters route is the command to use route [-nee] route add/del [-net|-host] [ip] netmask [mask] The command ip is like a combination of ifconfig and route host to host communication ping ping [options and parameters] IP -c numb: execute ping numb times -n: don't lookup hostname, use IP directly -s numb: the ICMP packet size is numb bytes -t numb: TTL's limit is numb -W numb: numb seconds to wait for respond -M [do|dont]: to test MTU size, do means send a DF(Don't Fragment) flag packet analyze nodes b/w host to host traceroute traceroute [options and parameters] IP -n: don't lookup hostname, use IP directly -U: use UDP port 33434 to test -I: use ICMP to test -T: use TCP port 80 to test -w numb: numb seconds to wait for respond -p port: use other port to test -i: only used when need to specify which AP to use view current machine's network stats netstat netstat -[rn] related to router netstat -[antulpc] related to network ports -r: show route table -n: don't lookup hostname, use IP directly -a: show all, including tcp/udp/unix socket -t show only TCP connections -u: show only UDP connections -l: show only listening services -p: show PID and Program filename -c: update after N seconds hostname, hostname lookup host can give you some host's IP. use host [-a] hostname [server] to get more info nslookup does similar thing. nslookup [-query=[type]] [hostname|IP] Remote control \u00b6 terminal and BBS: telnet telnet can combine with BBS and connect to a remote server. The downside is that it is not encrypted. telnet [host|IP [port]] FTP file transfer ftp , lftp , gftp can be used for this purpose. gftp has to be in X window mode ftp [host|IP] [port] to log in a system. use 'anonymous' to login anonymously. type help for useful commands. use lftp to call scripts and speed up the process of logging in. Book2 P173 Internet Surfing in Command Mode \u00b6 links links command allows you to view pure html docs. links URL/path_to_HTML there is no css styling nor img showing some freq-used shortcut: h: history of viewing g: goto URL d: download a URL link's target q: quit o: option, set preferences up/down arrow: move to next anchor left/right arrow: move back/forward in pages ENTER: like right-click can use links -dump url > index.html to download an entire page wget wget is used to fetch data from web servers. wget [option] URL options: --http-user=username --http-password=password --quiet can also set proxy at /etc/wgetrc Packet catching \u00b6 tcpdump allows you to intercept packet, analyze its contents. It provides so much information, that is categorized as a hecker software. Must be root to run it. Book2 P182 wireshark or burp is the equivalent in X window mode","title":"Freq Used Network Cmds"},{"location":"Linux/Commands/Networking-Shell-Cmds/#linux-networking-related-commandsknowledge","text":"To connect to Internet, proper settings of IP, Netmask, Gateway, DNS IP, and device name are all required.","title":"Linux Networking Related Commands/Knowledge"},{"location":"Linux/Commands/Networking-Shell-Cmds/#linux-based-on-centos","text":"Linux NIC Network Interface Card are named like modules in Linux, like eth0 for the first NIC, eth1 for the second NIC, and so on. Use dmesg to view specific device info. Like dmesg | grep -in eth Use lsmod to view whether the device is loaded by kernel. Like lsmod | grep 1000 Use modinfo to view details about this device, passing in the device number, like e1000 the filename portion is the NIC driver for current Kernel version. Use ifconfig to lookup the NIC number for a device. write driver for your NIC, see Book2 P113 Linux Networks config files /etc/services: sets port number for protocols /etc/protocols: defines protocols' related info /etc/init.d/networking restart: this is an important script to reset entire network to default ifup eth0 (ifdown eth0): a script to turn on or off one of the network device modify config file and important script commands to view results IP related /etc/sysconfig/network-scripts/ifcfg-eth0; /etc/init.d/network restart ifconfig; route -n DNS /etc/resolv.conf dig www.google.com host /etc/sysconfig/network hostname; ping hostname; reboot vim /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" <== device number, must match ifcfg-eth0 HWADDR=\"08:00:27:71:85:BD\" <== NIC address (MAC), can be omitted if having only one NIC NM_CONTROLLED=\"no\" <== uncontrolled by other software ONBOOT=\"yes\" <== startup on bootup BOOTPROTO=none <== can be ''dhcp'' or 'none' IPADDR=192.168.1.100 <== id address NETMASK=255.255.255.0 <== subnet mask GATEWAY=192.168.1.254 <== router # \u91cd\u70b9\u662f\u4e0a\u9762\u8fd9\u51e0\u4e2a\u8bbe\u5b9a\u9879\u76ee,\u5e95\u4e0b\u7684\u5219\u53ef\u4ee5\u7701\u7565\u7684\u5570! NETWORK=192.168.1.0 <== first IP of this network segment BROADCAST=192.168.1.255 <== broadcast IP MTU=1500 <== max transmition unit the file /etc/resolv.conf contains information about the IP address of DNS nameserver the command dig shows which DNS nameserver is used to reach an IP, which is part of a package called bind-utils change host name change /etc/sysconfig/network and /etc/hosts Book2 P123 ADSL configuration Book2 P124 Access Point When connecting to a router, in fact you are connecting to its Wireless Access Point(AP) When you are exposed under multiple AP range, you can detect many APs. Each AP has a SSID or ESSID for the client to identify the right AP. lsusb is part of usbutils package, that can show usb devices plugged-in","title":"Linux based on CentOS"},{"location":"Linux/Commands/Networking-Shell-Cmds/#linux-networking-commands","text":"set on/off IP parameters ifconfig can directly change IP config for a NIC device. ifconfig [interface] [up|down|options] up, down: like the commands below mtu: change mtu value netmask: set subnet mask broadcast: set broadcast address ifup , ifdown can only turn on/off of the device, not changing its parameters set router parameters route is the command to use route [-nee] route add/del [-net|-host] [ip] netmask [mask] The command ip is like a combination of ifconfig and route host to host communication ping ping [options and parameters] IP -c numb: execute ping numb times -n: don't lookup hostname, use IP directly -s numb: the ICMP packet size is numb bytes -t numb: TTL's limit is numb -W numb: numb seconds to wait for respond -M [do|dont]: to test MTU size, do means send a DF(Don't Fragment) flag packet analyze nodes b/w host to host traceroute traceroute [options and parameters] IP -n: don't lookup hostname, use IP directly -U: use UDP port 33434 to test -I: use ICMP to test -T: use TCP port 80 to test -w numb: numb seconds to wait for respond -p port: use other port to test -i: only used when need to specify which AP to use view current machine's network stats netstat netstat -[rn] related to router netstat -[antulpc] related to network ports -r: show route table -n: don't lookup hostname, use IP directly -a: show all, including tcp/udp/unix socket -t show only TCP connections -u: show only UDP connections -l: show only listening services -p: show PID and Program filename -c: update after N seconds hostname, hostname lookup host can give you some host's IP. use host [-a] hostname [server] to get more info nslookup does similar thing. nslookup [-query=[type]] [hostname|IP]","title":"Linux Networking Commands"},{"location":"Linux/Commands/Networking-Shell-Cmds/#remote-control","text":"terminal and BBS: telnet telnet can combine with BBS and connect to a remote server. The downside is that it is not encrypted. telnet [host|IP [port]] FTP file transfer ftp , lftp , gftp can be used for this purpose. gftp has to be in X window mode ftp [host|IP] [port] to log in a system. use 'anonymous' to login anonymously. type help for useful commands. use lftp to call scripts and speed up the process of logging in. Book2 P173","title":"Remote control"},{"location":"Linux/Commands/Networking-Shell-Cmds/#internet-surfing-in-command-mode","text":"links links command allows you to view pure html docs. links URL/path_to_HTML there is no css styling nor img showing some freq-used shortcut: h: history of viewing g: goto URL d: download a URL link's target q: quit o: option, set preferences up/down arrow: move to next anchor left/right arrow: move back/forward in pages ENTER: like right-click can use links -dump url > index.html to download an entire page wget wget is used to fetch data from web servers. wget [option] URL options: --http-user=username --http-password=password --quiet can also set proxy at /etc/wgetrc","title":"Internet Surfing in Command Mode"},{"location":"Linux/Commands/Networking-Shell-Cmds/#packet-catching","text":"tcpdump allows you to intercept packet, analyze its contents. It provides so much information, that is categorized as a hecker software. Must be root to run it. Book2 P182 wireshark or burp is the equivalent in X window mode","title":"Packet catching"},{"location":"Linux/Commands/Text-Manipulating-Cmds/","text":"Linux Text Manipulation Commands \u00b6 awk suitable for smaller data processing. Works like sed, line-by-line, but separate a line into parts to process. The default separation char is space or [tab] awk 'condition1{action1} condition2{action2} ...' filename can use $numb to access which part, starting from 1: last -n 5 | awk '{print $1 \"\\t\" $3}' $0 represents the entire line additionally, awk has internal variables accessible: NF how many parts on this line NR which line is current line FS current separation char cat /etc/passwd | awk 'BEGIN {FS=\":\"} $3 < 10 {print $1 \"\\t \" $3}' using conditions for different outputs i.e. awk 'NR==1{printf \"%10s %10s %10s %10s %10s\\n\",$1,$2,$3,$4,\"Total\" } NR>=2{total = $2 + $3 + $4; printf \"%10s %10d %10d %10d %10.2f\\n\", $1, $2, $3, $4, total}' col for simple process of a text file, like converting [tab] with spaces, etc. cut gets some part of info out of a line of text, like a log -d : [sparation char] -f : nth_part -c : get range of characters diff compare two pure-text files/dirs and output the differences. diff [-bBi] from-file to-file -b : ignore diff of one of more spaces, like \"about me\" and \"about me\" are the same -B : ignore empty lines -i : ignore capitalized differences diff can be used on directories to show difference in files grep supports regex, analyze a block of text and get the lines containing the match grep [-A] [-B] [--color=auto] 'search_regex' filename -A is display n lines after the result line -B : is display n lines before the result line -n : show line-number -v : reverse the condition Extended regex '|' means OR i.e: grep -v '^$' file | grep -v '^#' gives the same result as grep -v -E '^$|^#' file , to show lines without empty lines and commented lines grouping '()', egrep -n 'g(la|oo)d' file finds 'good' or 'glad' lines join, paste, expand join merge two files by comparing them and only put together similar parts/lines. Files should be sorted before doing join. paste is simpler, just connected two lines together with a [tab] expand converts [tab] as a number of spaces patch use diff to generate difference file, then apply difference file on the old file to patch updates. diff -Naur passwd.old passwd.new > passwd.patch patch -pN < patch_file apply patch patch -R -pN < patch_file restore old file from patch pr for processing pure text and format to be print-ready. printf format lines columns to be visually appealing sed useful for analyzing input, replace, delete, append, or extract text and lines sed [-nefr] ['action'] [filename] - -n : silent mode, only processed lines being output - -e [script]: have the script added to the command to be executed - -f [filename]: read script from a file - -r : let sed work with extended regex - -i : direct modify the file instead of output results - [action] : - in the form of [n1[,n2]]function ; function has: - a : insert a line after i.e. nl /etc/passwd | sed '2a drink tea' - c : replace lines b/w n1,n2 - d : delete matched line i.e. nl /etc/passwd | sed '2,5d' - i : insert a line before - p : print (stdout) selected lines of data/text i.e. nl /etc/passwd | sed -n '5,7p' is same as ln file | head -n 7 | tail -n 3 - s : find and replace inline! 1,20s/old_phrase/new_phrase/g here the phrase part supports regex! sort arranges text lines in the order we want. -f : ignore capitalized difference -b : ignore the space at the beginning -M : arrange using month -n : use number to arrange -r : reversed order -u : uniq lines only (filter out repeated lines) -t : separation char for columns (fields), default is [tab] -k [n]: use nth field to arrange split useful for splitting a large file into smaller ones according to size, or number of lines. tee redirects data as well as saving part of the data. last | tee last.list | cut -d \" \" -f1 tr deletes / replaces some text within a block of text last | tr '[a-z]' 'A-Z' will replace all lower case with upper wc shows text stats like number of characters, lines, english words. -l : lines -w : words -m : characters uniq shows only unique (non-repeated) lines only -c show count xargs provides pipe access to the commands that don't support pipes xclip copy STDOUT piped from other commands to the clipboard; MacOS equivalent is pbcopy","title":"Text Manipulating Cmds"},{"location":"Linux/Commands/Text-Manipulating-Cmds/#linux-text-manipulation-commands","text":"awk suitable for smaller data processing. Works like sed, line-by-line, but separate a line into parts to process. The default separation char is space or [tab] awk 'condition1{action1} condition2{action2} ...' filename can use $numb to access which part, starting from 1: last -n 5 | awk '{print $1 \"\\t\" $3}' $0 represents the entire line additionally, awk has internal variables accessible: NF how many parts on this line NR which line is current line FS current separation char cat /etc/passwd | awk 'BEGIN {FS=\":\"} $3 < 10 {print $1 \"\\t \" $3}' using conditions for different outputs i.e. awk 'NR==1{printf \"%10s %10s %10s %10s %10s\\n\",$1,$2,$3,$4,\"Total\" } NR>=2{total = $2 + $3 + $4; printf \"%10s %10d %10d %10d %10.2f\\n\", $1, $2, $3, $4, total}' col for simple process of a text file, like converting [tab] with spaces, etc. cut gets some part of info out of a line of text, like a log -d : [sparation char] -f : nth_part -c : get range of characters diff compare two pure-text files/dirs and output the differences. diff [-bBi] from-file to-file -b : ignore diff of one of more spaces, like \"about me\" and \"about me\" are the same -B : ignore empty lines -i : ignore capitalized differences diff can be used on directories to show difference in files grep supports regex, analyze a block of text and get the lines containing the match grep [-A] [-B] [--color=auto] 'search_regex' filename -A is display n lines after the result line -B : is display n lines before the result line -n : show line-number -v : reverse the condition Extended regex '|' means OR i.e: grep -v '^$' file | grep -v '^#' gives the same result as grep -v -E '^$|^#' file , to show lines without empty lines and commented lines grouping '()', egrep -n 'g(la|oo)d' file finds 'good' or 'glad' lines join, paste, expand join merge two files by comparing them and only put together similar parts/lines. Files should be sorted before doing join. paste is simpler, just connected two lines together with a [tab] expand converts [tab] as a number of spaces patch use diff to generate difference file, then apply difference file on the old file to patch updates. diff -Naur passwd.old passwd.new > passwd.patch patch -pN < patch_file apply patch patch -R -pN < patch_file restore old file from patch pr for processing pure text and format to be print-ready. printf format lines columns to be visually appealing sed useful for analyzing input, replace, delete, append, or extract text and lines sed [-nefr] ['action'] [filename] - -n : silent mode, only processed lines being output - -e [script]: have the script added to the command to be executed - -f [filename]: read script from a file - -r : let sed work with extended regex - -i : direct modify the file instead of output results - [action] : - in the form of [n1[,n2]]function ; function has: - a : insert a line after i.e. nl /etc/passwd | sed '2a drink tea' - c : replace lines b/w n1,n2 - d : delete matched line i.e. nl /etc/passwd | sed '2,5d' - i : insert a line before - p : print (stdout) selected lines of data/text i.e. nl /etc/passwd | sed -n '5,7p' is same as ln file | head -n 7 | tail -n 3 - s : find and replace inline! 1,20s/old_phrase/new_phrase/g here the phrase part supports regex! sort arranges text lines in the order we want. -f : ignore capitalized difference -b : ignore the space at the beginning -M : arrange using month -n : use number to arrange -r : reversed order -u : uniq lines only (filter out repeated lines) -t : separation char for columns (fields), default is [tab] -k [n]: use nth field to arrange split useful for splitting a large file into smaller ones according to size, or number of lines. tee redirects data as well as saving part of the data. last | tee last.list | cut -d \" \" -f1 tr deletes / replaces some text within a block of text last | tr '[a-z]' 'A-Z' will replace all lower case with upper wc shows text stats like number of characters, lines, english words. -l : lines -w : words -m : characters uniq shows only unique (non-repeated) lines only -c show count xargs provides pipe access to the commands that don't support pipes xclip copy STDOUT piped from other commands to the clipboard; MacOS equivalent is pbcopy","title":"Linux Text Manipulation Commands"},{"location":"Linux/Concepts/Linux_Foundation/","text":"This set of notes were taken from the Linux Foundation Course: Introduction to Linux (LFS101x) as well as some enrichments from linux.vbird.org About Linux \u00b6 Linux History \u00b6 Linux's inception was in 1991, created by Linus Torvalds and lead maintainer Greg Kroah-Hartman . Linux is initially developed on and for Intel x86-based personal computers. It has been subsequently ported to an astoundingly long list of other hardware platforms. In 1992, Linux was re-licensed using the General Public License (GPL) by GNU (a project of the Free Software Foundation or FSF, which promotes freely available software), which made it possible to build a worldwide community of developers. The Linux distributions created in the mid-90s provided the basis for fully free computing (in the sense of freedom, not zero cost) and became a driving force in the open source software movement. The success of Linux has catalyzed growth in the open source community , demonstrating the commercial efficacy of open source and inspiring countless new projects across all industries and levels of the technology stack. Today, Linux powers more than half of the servers on the Internet, the majority of smartphones, consumer products, automobiles, and all of the world\u2019s most powerful supercomputers. Linux Philosophy \u00b6 Linux borrows heavily from the well-established UNIX operating system. It was written to be a free and open source system to be used in place of UNIX, which at the time was designed for computers much more powerful than PCs and was quite expensive. Files are stored in a hierarchical filesystem, with the top node of the system being the root or simply \"/\". Whenever possible, Linux makes its components available via files or objects that look like files . Processes , devices , and network sockets are all represented by file-like objects , and can often be worked with using the same utilities used for regular files. Linux is a fully multitasking , multiuser operating system, with built-in networking and service processes known as daemons in the UNIX world. Linux stands for Linux is not UNIX . Linux Terminology \u00b6 kernel - brain of the Linux OS, controls hardware and let applications interacts with hardware distribution (Distro) - collection of programs combined with Linux kernel to make up a Linux-based OS boot loader - a program boots the OS , i.e. GRUB, ISOLINUX service - a program runs as a background process , i.e. httpd, nfsd, ntpd, ftpd, named filesystem - the method for storing and organizing files in Linux, i.e. ext3, ext4, FAT, XFS, Btrfs X Window System - provides standard toolkit and protocal to build graphical UI on all Linux Distro desktop environment - a graphical user interface on top of the OS, i.e. GNOME, KDE, Xfce, Fluxbox command line - interface for typing commands on top of OS shell - command line interpreter that interprets the command line input and instructs the OS to perform tasks, i.e. bash, tcsh, zsh Linux Distributions \u00b6 Linux is constantly evolving, both at the technical level (including kernel features) and at the distribution and interface level. A full Linux distribution consists of the kernel plus a number of other software tools for file-related operations, user management, and software package management. Linux distributions may be based on different kernel versions. Examples of other essential tools and ingredients provided by distributions include the C/C++ compiler, the gdb debugger, the core system libraries applications need to link with in order to run, the low-level interface for drawing graphics on the screen, as well as the higher-level desktop environment, and the system for installing and updating the various components, including the kernel itself. Three widely used Linux distributions (all distributions found here ): Red Hat Enterprise Linux (RHEL) Family - CentOS, Fedora, Oracle Linux SUSE Family - SLES, openSUSE Debian Family - Ubuntu, Linux Mint All major distributors provide update services for keeping your system primed with the latest security and bug fixes, and performance enhancements, as well as provide online support resources. RHEL \u00b6 RHEL is the most popular Linux distribution in enterprise environments. Some facts: Fedora is opensource version of RHEL, shipped with lots more software, and serves as an upstream testing platform for RHEL. CentOS is a close clone of RHEL now owned by Red Hat, while Oracle Linux is mostly a copy with some changes A heavily patched version 3.10 kernel is used in RHEL/CentOS 7, while version 4.18 is used in RHEL/CentOS 8. It supports hardware platforms such as Intel x86 , Arm , Itanium , PowerPC , and IBM System z . It uses the yum and dnf RPM-based yum package managers to install, update, and remove packages in the system. CentOS is a popular free alternative to RHEL and is often used by organizations that are comfortable operating without paid technical support. SUSE \u00b6 SUSE is an acronym for Software- und System-Entwicklung (Software and Systems Development). And SLES stands for SUSE Linux Enterprise Server . Some facts: SLES is upstream for openSUSE. Kernel version 4.12 is used in openSUSE Leap 15. It uses the RPM-based zypper package manager to install, update, and remove packages in the system. It includes the YaST (Yet Another Setup Tool) application for system administration purposes. SLES is widely used in retail and many other sectors. Debian \u00b6 Debian provides by far the largest and most complete software repository to its users of any Linux distribution, and has a strong focus on stability. Some facts: The Debian family is upstream for Ubuntu, and Ubuntu is upstream for Linux Mint and others. Debian is a pure open source community project not owned by any corporation. Kernel version 4.15 is used in Ubuntu 18.04 LTS. It uses the DPKG-based APT package manager (using apt , apt-get , apt-cache , etc.) to install, update, and remove packages in the system. Ubuntu has been widely used for cloud deployments . While Ubuntu is built on top of Debian and is GNOME-based under the hood, it differs visually from the interface on standard Debian, as well as other distributions. Ubuntu and Fedora are widely used by developers and are also popular in the educational realm. How Linux Works \u00b6 Boot Process \u00b6 The Linux boot process is the procedure for initializing the system, from pressing the power switch to a fully operational user interface. Power ON -> BIOS --> Master Boot Record (MBR) ---> Boot Loader ----> Kernel -----> Initial RAM disk ------> /sbin/init (parent process) -------> Command Shell using getty --------> X Windows System (GUI) BIOS \u00b6 BIOS stands for Basic Input/Output System. It runs and initializes the I/O hardware such as screen and keyboard, and tests the main memory, a process called POST (Power On Self Test). BIOS software is stored on a ROM chip on the motherboard. MBR and Boot Loader \u00b6 After POST, control is passed to the boot loader, usually stored on one of the hard disks either in the boot sector ( MBR ) or the EFI/UEFI partition ((Unified) Extensible Firmware Interface). Date, time, and other peripherals are loaded from the CMOS values (from a battery-powered memory store which allows the machine track date and time when powered off). MBR \u00b6 MBR is just 512 bytes in size which holds the boot loader. The boot loader examines the partition table and finds a bootable partition , then search for a second stage boot loader and loads it into RAM. EFI/UEFI \u00b6 EFI/UEFI boot method has firmware reads its Boot Manager data and determine the UEFI application to launch and the disk and partition to launch it from. Second stage boot loader \u00b6 The second stage boot loader resides under /boot . Common boot loaders: GRUB - GRand Unified Boot Loader, on most machines and Linux distributions ISOLINUX - for booting from removable media DAS U-Boot - for booting on embedded devices and appliances When booting Linux, the boot loader is responsible for loading and uncompress the kernel image and load the initial RAM disk , filesystem , or drivers into memory. Most boot loaders provide an UI for choosing boot options or other OS to boot into. Initial RAM Disk \u00b6 The initramfs filesystem image is a RAM-based filesystem which contains programs and binary files that perform all actions needed to provide kernel functionality, locating devices and drivers, mount the proper root filesystem , and check for filesystem errors. The mount program instructs the OS that a filesystem is ready for use, and associates it with a particular point in the overall hierarchy of the filesystem (the mount point ). Then initramfs is cleared from RAM and /sbin/init is executed, which handles the mounting and pivoting over to the final real root filesystem . It ten starts a number of text-mode login prompts (ttys) which allow you to type in username and password to get a command shell. Most distributions start six text terminals and one graphics terminal, and swith with CTRL-ALT + F1~F7 . The default command shell is bash (the GNU Bourne Again Shell). Linux Kernel \u00b6 When the kernel is loaded in RAM, it immediately initializes and configures the computer\u2019s memory and all the hardware attached to the system, and loads some necessary user space applications. /sbin/init is only ran after kernel set up all hardware and mounted the root filesystem. It is the origin of all non-kernel processes and is responsible for keeping the system running and for clean shutdowns . SysVinit \u00b6 In older distros, this process startup follows a System V UNIX convention (aka SysVinit ) where the system pass through a serial process of runlevels containing collections of scripts that start and stop services. Each runlevel supports a different mode of running the system . Within each runlevel, individual services can be set to run, or to be shut down if running. This startup method is slow and does NOT use the parallel processing benefit from multi-core processors. systemd \u00b6 Major recent distros have moved away from runlevels and use systemd and Upstart. Upstart was developed by Ubuntu in 2006, adopted in Fedora 9 in 2008 and RHEL 6. systemd was adopted by Fedora in 2011, by RHEL 7 and SUSE, and by Ubuntu 16.04. All distros now use systemd. Complicated startup shell scripts are replaced with simpler configuration files, which enumerate what has to be done before a service is started, how to execute service startup, and what conditions the service should indicate have been accomplished when startup is finished. /sbin/init just points to /lib/systemd/systemd . Starting, stopping, restarting a service $ sudo systemctl start|stop|restart nfs.service Enabling or disabling a system service from starting up at system boot $ sudo systemctl enable|disable nfs.service .service can be omitted. Linux Filesystem \u00b6 A filesystem is a method of storing/finding files on a hard disk (usually in a partition). A partition is a physically contiguous section of a disk. Partition is like a container in which a filesystem resides. By dividing the hard disk into partitions, data can be grouped and separated as needed. When a failure or mistake occurs, only the data in the affected partition will be damaged, while the data on the other partitions will likely survive. Different types of filesystems supported by Linux: Conventional disk filesystems: ext2, ext3, ext4 , XFS , Btrfs , JFS , NTFS, etc. Flash storage filesystems: ubifs, JFFS2, YAFFS, etc. Database filesystems Special purpose filesystems: procfs, sysfs, tmpfs, squashfs, debugfs, etc. other filesystems: ntfs, FAT, vfat, hfs, hfs+ It is often the case that more than one filesystem type is used on a machine, based on considerations such as the size of files, how often they are modified, what kind of hardware they sit on and what kind of access speed is needed, etc. Linux filesystem use a standard layout, Filesystem Hierarchy Standard (FHS), which uses / to separate paths and does not have drive letters. File names are case-sensitive. Multiple drives and/or partitions are mounted as directories in the single filesystem, called mount points . Mount points are usually empty. The mount command is used to attach a filesystem somewhere within the filesystem tree, i.e. sudo mount /dev/sda5 /home . umount does the opposite. fstab at /etc/fstab can be used to configure auto mount disks at system start up. df -Th can be used to display information about mounted filesystems, type, and usage statistics. In Linux, all open files are represented internally by what are called file descriptors . Simply put, these are represented by numbers starting at zero. stdin is file descriptor 0, stdout is file descriptor 1, and stderr is file descriptor 2. Typically, if other files are opened in addition to these three, which are opened by default, they will start at file descriptor 3 and increase from there. NFS \u00b6 A network filesystem ( NFS ) may have all its data on one machine or have it spread out on more than one network node. It is used to share data across physical systems which may be either in the same location or anywhere that can be reached by the Internet. NFS can be started as daemon with sudo systemctl start nfs . The text file /etc/exports configures the directories and permissions that a host is sharing with other systems over NFS. After updating the config file while nfs is running, use exportfs -av to notify NFS to re-apply the configuration. # example entry in /etc/exports /projects *.example.com ( rw ) # mount /projects using NFS with read and write permissions # and share within the example.com domain Client machine can mount the remote directory via NFS by mkdir -p /mnt/nfs/projects sudo mount <server_hostname/IP>:/projects /mnt/nfs/projects # let system boot auto mount the remote directory # add to /etc/fstab <server_hostname/IP>:/projects /mnt/nfs/projects nfs defaults 0 0 Directories under / \u00b6 directory function examples /bin (might link to /usr/bin) essential commands used to boot the system or in single-user mode, and required by all system users cat, cp, ls, mv, ps, rm /sbin (might link to /usr/sbin) essential binaries related to system administration fsck, ip /proc (a type of pseudo-filesystem) no permanent presence on the disk, contains virtual files (in memory) for constantly changing runtime system information system memory, devices mounted, hardware configs /dev contains device nodes, pseudo-file that is used by most hardware and software devices sda1 (first partition on the first hard disk), lp1 (second printer), random (source of rangom numbers), null (special file to safely dump unwanted data) /var contains files that are expected to change in size and content as the system is running log (system log files), lib (packages and database files), spool (print queue), tmp (temporary files), ftp (FTP service), www (HTTP web service) /var/cache program execution generated temp cache file /var/lib data file when program executes /var/lock lock files for programs to prevent simultaneous modification of files /var/log log files, including the login record for who used this system /var/mail personal mail /var/run storing PIDs after process started /var/spool stores some queue data, that is something queued up for process to use in order. Often deleted after use. /etc home for system configuration files or scripts, only for the superuser passwd, shadow, group (for managing user accounts), resolv.conf (DNS settings) /boot essential files needed to boot the system vmlinuz (compressed Linux kernel), initramfs/initrd (initial RAM filesystem), config (kernel config file), System.map (kernel symbol table), grub.conf (boot loader config) /lib and /lib64 (might link to /usr/lib) contains kernel modules and common code shared by applications and needed for them to run, mostly known as dynamically loaded libraries (aka Shared Objects) libncurses.so.5.9 /media, /run, /mnt either one can be used for mounting removable media onto the system NFS, loopback filesystems, USB drive /opt optional application software packages /sys virtual pseudo-filesystem giving information about the system and the hardware /srv site-specific data served up by the system /tmp temporary files; on some distributions erased across a reboot and/or may actually be a ramdisk in memory /usr stands for Unix Software Resource, for sharing in the Linux's multi-user setup applications, utilities and data, mostly static files /usr/bin This is the primary directory of executable commands on the system /usr/include Header files used to compile applications /usr/lib Libraries for programs in /usr/bin and /usr/sbin /usr/lib64 64-bit libraries for 64-bit programs in /usr/bin and /usr/sbin /usr/local Data and programs specific to the local machine. Subdirectories include bin, sbin, lib, share, include, etc. /usr/sbin Non-essential system binaries, such as system daemons /usr/share Shared data used by applications, generally architecture-independent /usr/src Source code, usually for the Linux kernel Some directories to consider for larger space allocation via partitioning: / /usr /home /var /tmp Swap Use basename on a file to get the file's name; use dirname on a file to get the full path to this file's belonging directory inode and block \u00b6 superblock : records this filesystem's information, including number of inode/block, used amount, remaining amount, filesystem format, etc. inode : records file-specific properties and records the block number of this record each record use one inode each 128 bytes for a large file, its inode records one block number for which that block records twelve additional direct block numbers, one redirect block number, and one triple-redirect block number. P247 Book. block : records the actual content of the file, may span to multiple blocks for larger files knowing an inode can know its block number. this way, data saved onto multiple continuous blocks can be read in sequence within a short amount of time, this is called localization to compensate possible large file system's performance, block group is used to divide the storage into block groups, for each having a separate inode/block/superblock system. data block stores file and data. block size: 1K, 2K or 4K small block size may cause larger file use more block and inodes large block size may create many blocks not fully utilized inode/block bitmaps records and track used and unused blocks and inodes which gives fast lookup and fast search for unused block/inode i.e. the process of reading a file at /etc/passwd : filesystem find / inode filesystem locate / block and look for etc inode find etc inode and check whether current user has rx access find etc block and look for passwd inode find passwd inode and check whether current user has r access read passwd block content Journaling filesystem : during sudden power outage during writing to the disk, disk data and real data can be inconsistent. To deal with this and prevent a whole scan of the filesystem, a journaling filesystem helps by: record each write to the filesystem in a log. preparation, writing, and completion are supposed to be recorded for each write. if anything happens, can quickly check the journal to find which file is wrong. This is available in ex3 filesystem on Linux. It can help servers recover faster from power outage. Commonly seen devices \u00b6 Device name in Linux IDE /dev/hd[a-d] SCSI/SATA/USB /dev/sd[a-p] ROM /dev/fd[0-1] printer /dev/lp[0-2] or /dev/usb/lp[0-15] mouse /dev/usb/mouse[0-15] or /dev/psaux CDROM/DVDROM /dev/cdrom current mouse /dev/mouse tape /dev/ht0 (IDE) or /dev/st0 (SCSI) Searching commands \u00b6 file command gives information on what kind of file it is which command gives exact path location of the command inspected whereis and locate can be used to find files. These two commands use database mapping to lookup and is therefore faster find can search files physically in the harddrive, can be slow and expensive find [PATH] [option] [action] some freq used options -mtime n : n is a number, means day. It makes a huge difference between adding [+] or [-] before the number: + means older than n days, - means within past n days, and neither, means exact n days ago. -newer : find /dir1 -newer /dir1/file finds files newer than /dir1/file -atime , -ctime similar as -mtime -perm : find files with/above/below certain access rights Comparing Files \u00b6 diff is used to compare files and directories . cmp can be used fro comparing binary files . You can compare three files at once using diff3 , which uses one file (second file argument) as the reference basis for the other two. Some common diff options diff Option Usage -c Provides a listing of differences that include three lines of context before and after the lines differing in content -r Used to recursively compare subdirectories , as well as the current directory -i Ignore the case of letters -w Ignore differences in spaces and tabs (white space) -q Be quiet: only report if files are different without listing the differences Many modifications to source code and configuration files are distributed utilizing patches with the patch program. A patch file contains the deltas (changes) required to update an older version of a file to the new one. Use ` diff -Nur originFile newFile > patchFile # create a patch file patch -p1 < patchFile # apply a patch file to an entire directory tree patch originFile patchFile # apply patch on one file In Linux, a file's extension does not categorize it, most applications directly examine a file's contents to see what kind of object it is rather than relying on an extension. Use file utility to assert the real nature of a file. Backing up Data \u00b6 While simple cp can help back up files or entire directory, rsync is more robust to synchronize directory trees , using the -r option, i.e. rsync -r sourceDir destinationDir . rsync checks if the file being copied already exists and skips copy if there is no change in size or modification time, therefore avoids unnecessary operations and saves time. Furthermore, rsync only copies the parts of files that actually changed and is very fast. rsync can also copy files from one machine to another in the form of user@host:filepath . A good combination of options rsync --progress -avrxH --delete sourceDir destinationDir Note that rsync could be destructive if not used properly, as a lot of files could be created at the target and it might use up all the space. Always use the -dry-run option to know what will be done before executing it. The Disk-to-Disk Copying program dd is very useful for making exact copies of raw disk space. Mostly used to backup a MBR, create a disk image, or install and OS, i.e. dd if=/dev/sda of=sda.mbr bs=512 count=1 Compressing Data \u00b6 File data is often compressed to save disk space and reduce the time it takes to transmit files over networks. Some good compression programs: gzip - the most frequently used Linux compression utility gzip to compress and gunzip or gzip -d to decompress compresses very well and is very fast, produces .gz files bzip2 - produces files significantly smaller than those produced by gzip but takes longer bzip2 to compress and bunzip2 or bzip2 -d to decompress produces .bz2 files xz - the most space-efficient compression utility used in Linux xz to compress and xz -d to decompress used to store archives of Linux kernel zip - often required to examine and decompress archives from other operating systems zip to compress and unzip to decompress tar - group files in an archive and then compress the whole archive at once tar czf to compress with gzip and gives xxx.tar.gz tar cjf to compress with bz2 and gives xxx.tar.bz2 tar cJf to compress with xz and gives xxx.tar.xz tar xf to decompress, no need to pass the option to tell it which format mostly used to archive files to a magnetic tape Generally, the more space-efficient techniques take longer . Decompression time does NOT vary as much across different methods. du can be used to check file sizes and total size for a directory. Use du -shc [list of files and dirs] to get a quick overview of selected files/dirs sizes. Use utilities such as zcat, zless, zdiff, zgrep to work directly with compressed files X Window System \u00b6 The X Window System (aka X ) is loaded as one of the final steps in the boot process. It can also be started from text-mode by the startx command, or commands to start the display manager gdm, lightgdm, kdm, xdm . A service called the Display Manager keeps track of the displays being provided and loads the X server. The display manager also handles graphical logins and starts the appropriate desktop environment after a user logs in. A desktop environment consists of a session manager, which starts and maintains the components of the graphical session, the window manager, which controls the placement and movement of windows, window title-bars, and controls, and a set of utilities. X is old software and has deficiencies in security . A newer system, Wayland , is superseding it and is used on Fedora, RHEL 8, and other Distros. For Distros using gnome-based X winodw manager, use gnome-tweak-tool to customize and remap keys. Package Management Systems \u00b6 A Package Management System distributes packages that each contains the files and other instructions needed to make one software component work well and cooperate with the other components that comprise the entire system. Two broad families of package managers: Debian and RPM . A package management system operates on two levels: low-level tool, such as dpkg, rpm , is responsible for unpacking individual packages, running scripts , getting the software installed correctly high-level tool, such as apt, yum, dnf, zypper , works with groups of packages , downloads packages from the vendor, and figures out dependencies apt stands for Advanced Packaging Tool, used on Debian-based systems yum stands for Yellowdog Updater Modified, is an open source tool for RPM-compatible Distros dnf aka Dandified YUM, is also RPM-based and used on Fedora and RHEL 8 systems zypper is RPM-based and used on openSUSE Operation RPM debian Install package rpm -i foo.rpm dpkg --install foo.deb Install package, dependencies yum install foo apt-get install foo Remove package rpm -e foo.rpm dpkg --remove foo.deb Remove package, dependencies yum remove foo apt-get autoremove foo Update package rpm -U foo.rpm dpkg --install foo.deb Update package, dependencies yum update foo apt-get install foo Update entire system yum update apt-get update && apt-get upgrade or apt-get dist-upgrade Show all installed packages rpm -qa or yum list installed dpkg --list Get information on package rpm -qil foo dpkg --listfiles foo Show packages named foo yum list \"foo\" apt-cache search foo Show all available packages yum list apt-cache dumpavail foo What package is <file> part of? rpm -qf <file> dpkg --search <file> Package documentation is directly pulled from the upstream source code and placed under the /usr/share/doc directory, grouped in subdirectories named after each package, perhaps including the version number in the name. Linux Documentation \u00b6 info pages \u00b6 info is the other form of documentation pages besides man . Navigation within info : Navigate within a Node Action Keys Next Line CTRL-n or arrow key Previous Line CTRL-p or arrow key Beginning of Line CTRL-a or key End of Line CTRL-e or key Forward 1 Character CTRL-f or arrow key Backward 1 Character CTRL-b or arrow key Forward 1 Word ALT-f or CTRL- arrow key Backward 1 Word ALT-b or CTRL- arrow key Beginning of Node ALT-< End of Node ALT-> End of Current Node e Quit q Selecting Nodes Action Keys Next Node n Previous Node p Up a Node u Last Node Viewed l Top Node t Directory Node d First Node < Last Node > Global Next Node ] Global Previous Node [ Searching within a Node Action Keys Search Forward s(string) or /(string) Search Backwards ?(string) Search Case-Sensitive S(string) Next word in Search n Next word Case-Sensitive N Interactively Search Forwards CTRL-s(string) Interactively Search Backwards CTRL-r(string) Index Search i(string) Next Index Search , Ask the man \u00b6 man is short for \"manual\". man pages are present on all Linux distributions and offer in-depth documentation about many programs and utilities, as well as other topics, including configuration files, and programming APIs for system calls, library routines, and the kernel. The man pages are divided into chapters numbered 1 through 9 . In some cases, a letter is appended to the chapter number to identify a specific topic. It is common to have multiple pages across multiple chapters with the same name , especially for names of library functions or system calls . The chapter number can be used to force man to display the page from a particular chapter , i.e. man 2 socket . Display all pages with -a option. Linux man pages online The man program searches, formats, and displays the information contained in the man page system. To list all pages on the topic, use -f option (same result as whatis ). To list all pages that discuss a specified topic, use the \u2013k option (same result as apropos ). Man page number \u00b6 Numb Meaning 1 shell executables or commands ( important ) 2 functions for kernels 3 library or libc functions 4 device manuals, often under /dev 5 setting file or format ( important ) 6 games 7 protocols 8 system administrator's commands ( important ) 9 kernel files navigation ( less ) \u00b6 Keys Functions [Space] next page [PageDown] next page [PageUp] next page [Home] first page [End] last page /string search string after current position ?string search string before current position n, N when searching, find next matching entry q quit search man pages \u00b6 To search for a specific man page, use man -f command_name To find any man page related to a term, use man -k searching_term , which will return all man-pages contain this phrase Info Page \u00b6 Info Page is a Linux specific feature that displays help doc like small paragraphs(pages), like a web-page. Use info command There are lots of information about the page displayed, including the progress of viewing the entire doc. Keys Functions [Space] next page [PageDown] next page [PageUp] next page [Home] first page [End] last page [b] move cursor to the first node in current screen [e] move cursor to the last node in current screen [n] next node [p] previous node [u] upper layer [s] or [/] search in current info page [h] show help [?] view commands [q] exit Additionally, /usr/share/doc/ usually contains many documentation docs GNU Info System \u00b6 This is the GNU project's standard documentation format , which it prefers as an alternative to man . The Info System is free-form, and its topics are connected using links . You can view help for a particular topic by typing info <topic name> , or view a top level index of available topics. The system then searches for the topic in all available info files. The topic which you view in an info page is called a node . You can move between nodes or view each node sequentially. Each node may contain menus and linked subtopics, aka items . Use n to go to next node, p for previous node, and u for moving one node up in the index. Items function like browser links and are identified by an asterisk ( ) at the beginning of the item name. Named items (outside a menu) are identified with double-colons * (::) at the end of the item name. Items can refer to other nodes within the file or to other files. --help option \u00b6 Most commands have an available short description which can be viewed using the --help or the -h option along with the command or application, which offers a quick reference and it displays information faster than the man or info pages. Process \u00b6 A process is simply an instance of one or more related tasks ( threads ) executing on your computer. A single command may start several processes simultaneously. Some processes are independent of each other and others are related . program : usually binary program, stored within physical media like hard-drives process : when a program is executed, executor's access and program data being loaded into the memory and OS gets assigned a PID fork and exec system fork a parent process as temporary process to execute the child program a PID is assigned and PPID is the parent's PID temporary process exec the child program and becomes the child process Processes use many system resources, such as memory, CPU cycles, and peripheral devices, such as network cards, hard drives, printers and displays. The OS (especially the kernel) is responsible for allocating a proper share of these resources to each process and ensuring overall optimized system utilization. Types \u00b6 Process Type Description Example Interactive Processes Need to be started by a user , either at a command line or through a graphical interface such as an icon or a menu selection. bash, firefox, top Batch Processes Automatic processes which are scheduled from and then disconnected from the terminal . These tasks are queued and work on a FIFO (First-In, First-Out) basis. updatedb, ldconfig Daemons Server processes that run continuously. Many are launched during system startup and then wait for a user or system request indicating that their service is required. httpd, sshd, libvirtd Threads Lightweight processes . These are tasks that run under the umbrella of a main process , sharing memory and other resources, but are scheduled and run by the system on an individual basis. An individual thread can end without terminating the whole process and a process can create new threads at any time. Many non-trivial programs are multi-threaded. firefox, gnome-terminal-server Kernel Threads Kernel tasks that users neither start nor terminate and have little control over. These may perform actions like moving a thread from one CPU to another, or making sure input/output operations to disk are completed. kthreadd, migration, ksoftirqd Scheduling \u00b6 The kernel scheduler constantly shifts processes on and off the CPU, sharing time according to relative priority , how much time is needed and how much has already been granted to a task. Some process states: running state means the process is either currently executing instructions on a CPU, or is waiting to be granted a share of time. All processes in this state reside on what is called a run queue . For machines with multi-core CPUs, there is a run queue on each core. sleep state means the process is waiting for something to happen before it can resume. It is said to be sitting on a wait queue. zombie state means when a child process is completed but its parent process has not asked about its state, then it still shows up in the system's list of processes but not really alive. The OS assigns each process an unique process ID ( PID ) to track process state, CPU usage, memory use, precisely where resources are located in memory, and other characteristics. You can terminate a process by issuing kill -SIGKILL <pid> , kill -9 <pid> , or kill -SIGTERM ID Type Description Process ID ( PID ) Unique Process ID number Parent Process ID ( PPID ) Process (Parent) that started this process. If the parent dies, the PPID will refer to an adoptive parent; on recent kernels, this is kthreadd which has PPID=2. Thread ID ( TID ) Thread ID number. This is the same as the PID for single-threaded processes. For a multi-threaded process, each thread shares the same PID, but has a unique TID. Users and Groups \u00b6 The OS identifies the user who starts a process by the Real User ID ( RUID ) assigned to the user. The user who determines the access rights for the users is identified by the Effective UID ( EUID ). EUID may not be the same as the RUID in some situations. Users can be categorized into various groups. Each group is identified by the Real Group ID ( RGID ). The access rights of the group are determined by the Effective Group ID ( EGID ). Priority and NICE \u00b6 The priority (PRI) for a process can be set by specifying a nice value , aka niceness (NI). The lower the nice value, the higher the priority. Higher priority processes grep preferential access to the CPU, therefore more CPU time. In Linux, a nice value of -20 represents the highest priority and +19 represents the lowest. This convention was adopted from UNIX. You can view the nice values using ps -lf and use renice +5/-5 <pid> to set the nice value. Parent process's nice value change also affects its child process's nice value. root can change all process NI , while a normal user can only adjust owning process NI within [0, 19] and can only adjust NI to a higher value , with command nice [-n numb] command or adjust existing process with renice [numb] PID . NI adjustments will be passed from parent process to child The load average is displayed using three numbers (i.e. 0.45, 0.17, and 0.12) with command w , interpreted as CPU utilization within last minute, 5 minutes before, and 15 minutes before. background process \u00b6 You can put a job in the background by suffixing & to the command, i.e. updatedb & . Use CTRL-Z to suspend a foreground job and bg to put it running in the background. Use fg to bring a process back to foreground, and jobs to see a list of background jobs ( -l option for showing PIDs). ps command \u00b6 For the BSD variation of ps command, use ps aux to display all processes of all users, and use ps axo <attributes> to specify a list of attributes to view. For the SystemV variation of ps command, options need the dash prefixes and are different. Several useful ps combination should be remembered: ps -l shows only your process related to this bash. Some columns explained: F represents process flags, means this process's access 4 means root 1 means forked but not exec S represents Status R: running S: sleep, idle, can be signaled to wakeup D: usually doing I/O, cannot be wakeup T: stop, might be under job control Z: zombie, process terminated but cannot be moved out of memory C represents CUP usage percentage PRI/NI is short for priority/nice, means the priority for CPU to execute it. Smaller number means higher priority ADDR/SZ/WCHAN related to memory, ADDR is a kernel function showing which part of memory; SZ means size; WCHAN means whether it is running ('-' means running) TTY : user's terminal from logged in TIME : CPU time used CMD : command ps aux shows all process. Some columns explained: USER the process belongs to PID that process has %CPU usage %MEM usage VSZ virtual memory usage (Kbytes) RSS physical memory usage (Kbytes) TTY from which terminal, if pts/n, means logged in from remote terminal STAT , status, shows the same as ps -l TIME , actual CPU usage in time unit COMMAND , which command triggered ps -axjf shows all processes in a tree view fashion pstree displays the processes running on the system in the form of a tree diagram showing the relationship between a process and its parent process and any other processes that it created, and threads displayed within {} . pstree [-A|U] [-up] -A : use ASCII char to represent tree -U : use UTF char to represent tree -p : show process PID -u : show process user top command \u00b6 top gives an over view of system performance live over time. The first line of the top output displays a quick summary of what is happening in the system: How long the system has been up How many users are logged on What is the load average load average of 1.00 per CPU indicates a fully subscribed system if greater than 1, the system is overloaded and processes are competing for CPU time if very high, it indicates the system may have a runaway process (non-responding state) The second line displays the total number of processes , the number of running, sleeping, stopped, and zombie processes. The third line indicates how the CPU time is being divided by displaying the percentage of CPU time used for each: us - CPU for user initiated processes sy - CPU for kernel processes ni - niceness, CPU for user jobs running at a lower priority id - idle CPU wa - waiting, CPU for jobs waiting for I/O hi - CPU for harware interrupts si - CPU for software interrupts st - steal time, used with virtual machines, which has some of its idle CPU time taken for other users The fourth and fifth lines indicate memory usage, which is divided in two categories and both displays total memory, used memory, and free space: Physical memory (RAM) on line 4. Swap space on line 5. Once the physical memory is exhausted, the system starts using swap space (temporary storage space on the hard drive) as an extended memory pool, and since accessing disk is much slower than accessing memory, this will negatively affect system performance. top [-d numb] | top [-bnp] -d : screen refresh rate at seconds -b : exec top in order, used with data redirection -n : used with -b, number of times top outputs -p : specify some PID for monitoring commands in top : ? : shows available commands P : arrange by CPU usage M : arrange by Memory usage N : arrange by PID T : arrange by CPU time k : send one PID a signal r : send one PID new nice value q : quit free command \u00b6 free [-b|-k|-m|-g] [-t] shows memory usage -b|-k|-m|-g , by default output shows in unit Kbytes, use this to override to bytes, Mbytes, Gbytes -t , shows physical and swap memory as well uname command \u00b6 uname [-asrmpi] checks system and core information -a : all system related information will be shown -s : system core name -r : system core version -m : system hardware architecture -p : CPU type -i : hardware platform netstat command \u00b6 netstat -[atunlp] can track network usage on a process level -a : show current system's all network, listening port, sockets -t : list tcp packet data -u : list udp packet data -n : show service by port number -l : list services being listened -p : show services with PID vmstat \u00b6 vmstat can track system resource changes -a [delay [total examine times]] shows active/inactive replace buffer/cache info -f show number of forks -s show memory changes -S <unit> use K/M replace bytes -d show number of disk read/write -p <partition> show a partition read/write stats categories shown: procs, memory, swap, io, system, cpu procs : the more of r and b, the busier the system r: process waiting to run b: un-wakeable processes memory : like shown by free swpd: virtual memory usage free: unused mem buff: buffer storage cache: high-speed cache swap : when si and so get larger, system is short of memory si: amount taken from disk so: amount written into swap io : when bi and bo get larger, system is doing lots of I/O bi: blocks read from disk bo: blocks written into disk system : when in and cs get larger, system communicates with external devices quite often in: processes interrupted per second cs: context-switch times per second cpu : us: non-core usage of CPU sy: core usage of CPU id: idle status wa: wait I/O CPU waste st: virtual machine CPU usage. fuser command \u00b6 fuser [-umv] [-k [i] [-signal]] file/dir can find out which process is using which file/directory, from the point of the file/directory -u : show both PID and process owner -m : increase priority of the file -v : show each file and process related -k : show the process using this file/dir, and signal kill to the process -i : use with -k, ask for decision before kill the process -<signal> : send a signal code What will be shown is USER PID ACCESS COMMAND the ACCESS represents: c : the process is under current directory e : can be executed f : is an opened file r : is the root directory F : the file is opened but pending complete m : sharable dynamical library lsof command \u00b6 lsof [-aUu] [+d] lists which process is using which files -a : show when all criteria satisfied -U : show only Unix like system's socket files -u username : list files opened by the user +d directory : list files opened under a directory pidof command \u00b6 pidof [-sx] program_name list the active PIDs of a program -s : show only one, not all of the PIDs -x : show also the program's possible parent PID (PPID) Process List \u00b6 Process list shows information about each process. By default, processes are ordered by highest CPU usage, with other information: PID - process id USER - process owner PR - priority NI - nice values VIRT - virtual memory RES - physical memory SHR - shared memory S - status %CPU - percentage of CPU used %MEM - percentage of memory used TIME+ - execution time COMMAND - command started the process top can be used interactively for monitoring and controlling processes Command Output t Display or hide summary information (rows 2 and 3) m Display or hide memory information (rows 4 and 5) A Sort the process list by top resource consumers r Renice (change the priority of) a specific processes k Kill a specific process f Enter the top configuration screen o Interactively select a new sort order in the process list Schedule Processes \u00b6 at and sleep \u00b6 Use at program to execute any non-interactive command at a specified future time for once. $ at now + 2 days at> cat file1.txt at> <EOT> ( CTRL-D ) job 1231 at xxxx-xx-xx xx:xx Use sleep to delay execution of a command for a specific period. sleep NUMBER [ SUFFIX ] # SUFFIX can be s(seconds, default if not provided), m(minutes), h(hours), d(days) cron \u00b6 cron is a time-based scheduling utility program. It can launch routine background jobs at specific times and/or days on an on-going basis . cron is configured at /etc/crontab (cron table) which contains the various shell commands that need to be run at the properly scheduled times. cron can be configured with the system-wide or the user-specific crontab. each line of crontab is composed of a CRON expression and a shell command. Use crontab -e to edit existing or add new jobs. # CRON expression MIN HOUR DOM MON DOW CMD # minute(0-59), hour(0-23), day of month(1-31), month(1-12), day of week(0-6), shell command System Services (Daemon) \u00b6 System service programs are called daemons. usually the service name with a suffix d Stand-alone Daemons starts without being managed by other programs. It Stays in the system memory once started, and uses resources. Fast responding to users. Super Daemon is a single daemon to start other daemons upon request from the client. The daemons started will be closed when the client session ends. i.e. telnet is a service managed by the super daemon Each service maps to an unique port and this mapping is in /etc/services file To starting up a daemon, it requires an executable, a configuration, and an environment. They are stored at: /etc/init.d/ : for starting up scripts /etc/sysconfig/ : for initialization environment config /etc/xinetd.conf , /etc/xinetd.d/ : super daemon config /etc/ : services' configuration files /var/lib/ : services' database files /var/run/ : all services' PID record service is a command (in fact, a script) to start, terminate, and monitor any services. service [service_name] (start|stop|restart|status|--status-all) Job Control \u00b6 foreground jobs are jobs actively prompting in the terminal and is interactable. Background jobs : the jobs running in the background without interaction with the user. Appending & to commands will be thrown to the background switching jobs: in the middle of running a command, press ctrl-z to pause it and throw it to the background use jobs command to check running/stopped jobs lists process recently put into the background, with (+) means next retrieving job using fg and (-) means the second latest job put into hte background jobs [-lrs] -l : show PID -r : show running only -s : show stopped only fg to bring back a job suspended. fg %<jobnumber> use it without jobnumber will bring back the one with (+) can also fg - to bring back the one with (-) bg can make a stopped job running in the background again bg %<jobnumber> will also append & to the job command kill can remove jobs or restart jobs kill -<signal> %<jobnumber> the <signal> can be a number or text: -l : list all kill signals -1 : reload configuration files -2 : like entering ctrl-c to interrupt a process -9 : forced stop -15 : normal termination -17 : like entering ctrl-z to stop a process kill -<signal> PID also works killall can work on all running processes of a command, useful if you don't want bother to lookup its PID killall [-iIe] [-signal] [command_name] -i : interactive -e : exact, means the command_name must match -I : command_name ignore cases Offline Jobs \u00b6 Notice the background from job control is not \"system background\", it is just a way to help you run and manage multiple things in the terminal. If there is need to run a job even after logged out of the system, then offline jobs may help. While at works for this case, nohup can also work! nohup <command> or nohup <command> & to run in the background Linux Users and Groups \u00b6 Linux is a multi-user operating system. To identify the current user, use whoami . To list the currently logged-on users, use who or users . who -a gives more detailed information. All Linux users are assigned a unique integer user ID (uid); normal users start with a uid of 1000 or greater. Use id to get information about current user, and id <username> can get information from other user. Linux uses groups for organizing users. Groups are collections of accounts with certain shared permissions , defined in the /etc/group file. Permissions on various files and directories can be modified at the group level . Users also have one or more group IDs (gid), including a default one which is the same as the user ID. Groups are used to establish a set of users who have common interests for the purposes of access rights, privileges, and security considerations. Only the root user can add and remove users and groups. Adding a new user is done with useradd and removing is done with userdel . i.e. sudo /usr/sbin/useradd bjmoose sets the home directory to /home/bjmoose , populates it with some basic files (copied from /etc/skel ) and adds a line to /etc/passwd such as: bjmoose:x:1002:1002::/home/bjmoose:/bin/bash . Removing a user with userdel will leave the user home directory, and is good for a temporary inactivation. Use userdel -r to remove the home directory too. Similiarly, add a new group with groupadd and remove with groupdel . To add a user to a new group, use usermod . i.e. usermod -aG <newgroup> <username> . To remove a user from a group, you must give the full list of groups except the one want to remove. i.e. usermod -G <groups>... <username> . To temporarily become the superuser for a series of commands, you can use su and then be prompted for the root password. To execute just one command with root privilege use sudo <command> . sudo access priviledge is granted per user and its configuration files are stored in the /etc/sudoers file and in the /etc/sudoers.d/ directory. By default, the sudoers.d directory is empty. File Ownership, Permission \u00b6 In Linux, every file is associated with a user who is the owner and a group for whom has the right to acess it in certain ways: read(r), write(w), execute(x) . For a file, execute(x) means whether it can be executed; for a directory it means whether a user can cd into this directory as working directory. Whether a user can delete a file depends on its access right on the current directory. Must be write(w) File permission [-][rwx][r-x][r--] 0 123 456 789 0 - file type 123 - owner access right 456 - group access right 789 - global access right file types: - regular file d directory l link b block device file, like a hard-drive; or c character device file, like a mouse or keyboard s socket, for network data p pipe, FIFO, allow many process read the same file chown is used to change user ownership (and group) of a file or directory, chgrp for changing group ownership. chmod is for changing the permissions on the file at user(u) group(g) others(o) levels. A single digit is sufficient to specify all three types permission bits for each entity: read(4), write(2), execute(1) which is the sum of those digits. umask \u00b6 umask can be used to disable certain rights for newly created files or directories. i.e. unmask 023 means new files created will NOT have w for groups and not have wx for world Hidden attributes \u00b6 Hidden attributes on a file are useful for security reasons. lsattr allows you to view the hidden attributes of a file chattr allows you to change the hidden attributes of a file -i means let a file be unchangable -a allows adding but not changing/deleting old portion of the file Clean shutdown \u00b6 use who to see who is using current system. use netstat -a to see Internet connections status use ps -aux to see running process in the background sync command will sync data into hard drives. It is best to remember to run this command before reboot or shutdown the system. shutdown or halt can done many things such as shutdown, reboot, or enter single-user mode set shutdown time, now or in the future set shutdown message to online users send warning info broadcast. Useful when need to notify others for important messages whether use fsck to check file system shutdown [-t seconds] [-arkhncfF] [time] [warning_info] usage below: Option Setting -t sec shutdown in some seconds -k send warning message without shutting down -r reboot after system services terminate -h shutdown after system services terminate -n shutdown without the init process -f reboot skipping fsck check -F reboot force fsck check -c cancel current shutdown directive Linux shell \u00b6 Startup file \u00b6 The command shell program uses one or more startup files to configure the user environment . Files in the /etc directory define global settings for all users , while initialization files in the user's home directory can include and/or override the global settings. Things can be configured: Customizing the prompt Defining command line aliases Setting the default text editor Setting the path for where to find executable programs Order of startup files evaluation (for user first logs onto the system): /etc/profile , then ~/.bash_profile or ~/.bash_login or ~/.profile . Every time you create a new shell, or terminal window, etc., you do NOT perform a full system login ; only a file named ~/.bashrc file is read and evaluated. PATH is a variable of an ordered list of directories (the path) which is scanned when a command is given to find the appropriate program or script to run. Use alias with no arguments will list currently defined aliases . unalias will remove an alias. Alias definition needs to be placed within either single or double quotes if it contains any spaces. i.e. alias ls='ls --color -l' Prompt Statement (the PS1 variable) is used to customize your prompt string in your terminal windows to display the information you want. Environment Variables \u00b6 Environment variables are quantities that have specific values which may be utilized by the command shell or other utilities and applications. Some are set by the system and others are set by the user, either at the command line or within startup and other scripts. An environment variable is actually just a character string that contains information used by one or more applications. Use set, env, export to view the values of currently set environment variables. Variables created within a script are only available to the current shell ; child processes (sub-shells) will NOT have access to values that have been set or modified. Allowing child processes to see the values requires use of the export command. You can also set environment variables to be fed as a one shot to a command as in: $ SDIRS=s_0* KROOT=/lib/modules/$(uname -r)/build make modules_install . Command History \u00b6 bash keeps track of previously entered commands and statements in a history buffer , stored in ~/.bash_history (each session saves the history in the very end). Recall previous commands using the arrow keys , search with CTRL-r , or use history to view all and use !<number> to re-execute a past command. Shell shortcuts \u00b6 Keyboard Shortcut Task CTRL-L Clears the screen CTRL-D Exits the current shell CTRL-Z Puts the current process into suspended background CTRL-C Kills the current process CTRL-H Works the same as backspace CTRL-A Goes to the beginning of the line CTRL-W Deletes the word before the cursor CTRL-U Deletes from beginning of line to cursor position CTRL-K Deletes from cursor position to end of line CTRL-E Goes to the end of the line Tab Auto-completes files, directories, and binaries Text Manipulation \u00b6 sed is abbreviation for stream editor and is a powerful text processing tool and is one of the oldest, earliest and most popular UNIX utilities. It is used to modify the contents of a file or input stream, usually placing the contents into a new file or output stream. sed can filter text , as well as perform substitutions in data streams. sed -e command <filename> - Specify editing commands at the command line, operate on file and put the output on standard out specify multiple -e command s to use perform multiple operations sed -f scriptfile <filename> - Specify a scriptfile containing sed commands, operate on file and put output on standard out Basic sed substitutions: Command Usage sed s/pattern/replace_string/ file Substitute first string occurrence in every line sed s/pattern/replace_string/g file Substitute all string occurrences in every line sed 1,3s/pattern/replace_string/g file Substitute all string occurrences in a range of lines sed -i s/pattern/replace_string/g file Save changes for string substitution in the same file awk is used to extract and then print specific contents of a file and is often used to construct reports. It got its name from the authors, Alfred Aho, Peter Weinberger, and Brian Kernighan. awk 'command' <filename> - Specify a command directly at the command line awk -f scriptfile <filename> - Specify a file that contains the script to be executed Basic awk usage: Command Usage awk '{ print $0 }' /etc/passwd Print entire file awk -F: '{ print $1 }' /etc/passwd Print first field (column) of every line, separated by a space awk -F: '{ print $1 $7 }' /etc/passwd Print first and seventh field of every line File Manipulation \u00b6 sort is used to rearrange the lines of a text file , in either ascending or descending order according to a sort key, or sort with respect to particular fields ( columns ) in a file Syntax Usage sort <filename> Sort the lines in the specified file, according to the characters at the beginning of each line cat file1 file2 | sort Combine the two files, then sort the lines and display the output on the terminal sort -r <filename> Sort the lines in reverse order sort -k 3 <filename> Sort the lines by the 3rd field on each line instead of the beginning sort -r <filename> Sort the lines then keep only unique lines, same as running uniq uniq removes duplicate consecutive lines in a text file and is useful for simplifying the text display. It requires duplicate entries be consecutive to be removed. Use uniq -c to only count the number of duplicate lines. paste can be used to combine file contents with respect to columns . paste -s causes it to combine data like you do cat file1 file2 > file3 join can be used when two files have shared column values that one can combine data based on that column, like you do in SQL statement. split is used to break up a file into equal-sized segments of new files for easier viewing and manipulation, by default 1000 lines per file segment. An optional prefix of the new files can be specified with split <file> <prefix> grep is extensively used as a primary text searching tool. It scans files for specified patterns and can be used with regular expressions. Command Usage grep [pattern] <filename> Search for a pattern in a file and print all matching lines grep -v [pattern] <filename> Print all lines that do not match the pattern grep -C 3 [pattern] <filename> Print context of lines (specified number of lines above and below the pattern) for matching the pattern strings book1.xls | grep my_string Take text input from pipe strings extracts printable character strings from binary files. tr is used to translate specified characters into other characters or to delete or keep some of them Command Usage tr a-z A-Z Convert lower case to upper case tr '{}' '()' < inputfile > outputfile Translate braces into parenthesis echo \"This is for testing\" | tr [:space:] '\\t' Translate white-space to tabs echo \"This is for testing\" | tr -s [:space:] Squeeze repetition of characters using -s echo \"the geek stuff\" | tr -d 't' Delete specified characters using -d option echo \"my username is 432234\" | tr -cd [:digit:] Complement the sets using -c option. Combined with -d, means only keep the characters in the set tr -cd [:print:] < file.txt Remove all non-printable character from a file tr -s '\\n' ' ' < file.txt Join all the lines in a file into a single line tee takes the output from any command, and, while sending it to standard output , it also saves to a file wc counts the number of lines ( -l option), words ( -w option), and characters ( -c option) in a file or list of files. cut is used for manipulating column-based files and is designed to extract specific columns using option -f <number> . Default separator is tab; use cut -d ';' to override that. Linux Networking \u00b6 Exchanging information across the network requires using streams of small packets , each of which contains a piece of the information going from one machine to another. These packets contain data buffers , together with headers which contain information about where the packet is going to and coming from, and where it fits in the sequence of packets that constitute the stream. A network requires the connection of many nodes. Data moves from source to destination by passing through a series of routers and potentially across multiple networks. IP Address \u00b6 Devices attached to a network must have at least one unique network address identifier known as the IP (Internet Protocol) address . The address is essential for routing packets of information through the network. IPv4 uses 32-bits for address and is older and by far the more widely used, while IPv6 uses 128-bits for addresses and is newer and designed to get past address pool limitations inherent in the older standard and furnish many more possible addresses. NAT (Network Address Translation) enables sharing one IP address among many locally connected computers, each of which has a unique address only seen on the local network. A 32-bit IPv4 address is divided into four 8-bit sections called octets , or bytes . Network addresses are divided into five classes : A, B, C, D and E. Classes A, B, C are classified into two parts: Network addresses (Net ID, for identify the network) and Host address (Host ID, for identify a host in the network). Class D is used for special multicast applications (information is broadcast to multiple computers simultaneously) and Class E is reserved for future use. Class A Address \u00b6 Class A addresses use the first octet as Net ID and use the other three as the Host ID. The first bit of the first octet is always set to zero , so you can use only 7-bits for unique network numbers, leaving a maximum of 126 Class A networks available (the addresses 0000000 and 1111111 are reserved). Each Class A network can have up to 16.7 million unique hosts on its network. The range of host address is from 1.0.0.0 to 127.255.255.255 . Class B Address \u00b6 Class B addresses use the first two octets of the IP address as their Net ID and the last two octets as the Host ID. The first two bits of the first octet are always set to binary 10 , so there are a maximum of 16384 (14-bits) Class B networks. The first octet of a Class B address has values from 128 to 191 . Each Class B network can support a maximum of 65,536 unique hosts on its network. The range of host address is from 128.0.0.0 to 191.255.255.255 . Class C Address \u00b6 Class C addresses use the first three octets of the IP address as their Net ID and the last octet as their Host ID. The first three bits of the first octet are set to binary 110 , so almost 2.1 million (21-bits) Class C networks are available. The first octet of a Class C address has values from 192 to 223 . These are most common for smaller networks which don't have many unique hosts. Each Class C network can support up to 256 (8-bits) unique hosts. The range of host address is from 192.0.0.0 to 223.255.255.255 . IP Address Allocation \u00b6 Typically, a range of IP addresses are requested from your Internet Service Provider (ISP) by your organization's network administrator. The class of IP address gieven depends on the size of your network and growth needs. If NAT is in operation, you only get one externally visible address. You can assign IP addresses to computers over a network either manually ( static address) or dynamically (can change when machine reboots) using Dynamic Host Configuration Protocol (DHCP). Name Resolution \u00b6 Name Resolution is used to convert numerical IP address values into a human-readable format known as the hostname . The special hostname localhost is associated with the IP address 127.0.0.1, and describes the machine you are currently on. Network Configuration \u00b6 Network configuration files are located in the /etc directory tree. Debian family distros store them under /etc/network , while Fedora and SUSE store under /etc/sysconfig/network . Network interfaces are a connection channel between a device and a network . Physically, network interfaces can proceed through a network interface card (NIC), or can be more abstractly implemented as software , and each can be activated or deactivated any time. Use ip or ifconfig utilities to view network interface information. Network utils \u00b6 ping is used to check whether or not a machine attached to the network can receive and send data ; i.e. it confirms that the remote host is online and is responding. One can use the route utility or the ip route command to view or change the IP routing table to add, delete, or modify specific (static) routes to specific hosts or networks. traceroute is used to inspect the route which the data packet takes to reach the destination host, which makes it quite useful for troubleshooting network delays and errors . By using traceroute, you can isolate connectivity issues between hops, which helps resolve them faster. Some other networking tools: Networking Tools Description ethtool Queries network interfaces and can also set various parameters such as the speed netstat Displays all active connections and routing tables. Useful for monitoring performance and troubleshooting nmap Scans open ports on a network. Important for security analysis tcpdump Dumps network traffic for analysis iptraf Monitors network traffic in text mode mtr Combines functionality of ping and traceroute and gives a continuously updated display dig Tests DNS workings. A good replacement for host and nslookup wget is a command line utility for handling large file downloads, recursive downloads, password-protected downloads, or multi-file downloads. curl can be used from the command line or a script to read information about a http call, or save the contents to a file. File Transfer Protocol (FTP) is a well-known and popular method for transferring files between computers using the Internet, built on a client-server model . All web browsers support FTP. Some cli FTP clients are ftp, sftp, ncftp, yafc . Secure Shell (SSH) is a cryptographic network protocol used for secure data communication (using ssh ) and remote services and other secure services between two devices on the network. Move files securely using Secure Copy ( scp ) between two networked hosts. scp uses the SSH protocol for transferring data. Linux Security \u00b6 User Accounts \u00b6 The Linux kernel allows properly authenticated users to access files and applications. Each user is identified by a unique integer ( UID ) and a separate database associates a username with each UID. Related tools are useradd userdel for creating and removing accounts. Upon account creation, new user information is added to the user database and the user's home directory must be created and populated with some essential files. For each user, the following seven fields are maintained in the /etc/passwd file: Field Name Details Remarks Username User login name Should be between 1 and 32 characters long Password User password (or the character x if the password is stored in the /etc/shadow file) in encrypted format Is never shown in Linux when it is being typed; this stops prying eyes User ID (UID) Every user must have a user id (UID) UID 0 is reserved for root user; UID's ranging from 1-99 are reserved for other predefined accounts; UID's ranging from 100-999 are reserved for system accounts and groups; Normal users have UID's of 1000 or greater Group ID (GID) The primary Group ID (GID); Group Identification Number stored in the /etc/group file Is covered in detail in the chapter on Processes User Info This field is optional and allows insertion of extra information about the user such as their name For example: Rufus T. Firefly Home Directory The absolute path location of user's home directory For example: /home/rtfirefly Shell The absolute location of a user's default shell For example: /bin/ba For a safe working environment, it is advised to grant the minimum privileges possible and necessary to accounts, and remove inactive accounts. The last utility can be used to identify potential inactive users. root is the most privileged account on a Linux/UNIX system. This account has the ability to carry out ALL facets of system administration, and utmost care must be taken when using this account. root privilege is required for performing administration tasks such as restarting most services, manually installing packages and managing parts of the filesystem that are outside the normal user\u2019s directories. SUID, SGID, SBIT \u00b6 SUID ( Set owner User ID upon execution - similar to the Windows \"run as\" feature) is a special kind of file permission given to a file. Use of SUID provides temporary permissions to a user to run a program with the permissions of the file owner (which may be root) instead of the permissions held by the user. i.e., I have x access to /usr/bin/passwd and passwd is owned by root . When I execute passwd I temporarily get root access so I can change /etc/shadow SUID can only be used on binary program, NOT on shell script, and NOT on directories. SGID can be used on binary program and directories, NOT on shell script. SBIT, Sticky Bit, only used on directories. When a user has wx access on a directory and creates a file under it, only this user or root can delete that file. How to set these bits: SUID: 4, SGID: 2, SBIT: 1 , i.e. chmod 4755 file_name sudo \u00b6 In Linux you can use either su (requires root password, can be root for as long as needed, limited logging trails) or sudo (requires the user's password, temporary access, more logging trails) to temporarily grant root access to a normal user. sudo has the ability to keep track of unsuccessful attempts at gaining root access (usually logged in /var/log/secure ). Users' authorization for using sudo is based on configuration information stored in the /etc/sudoers file and in the /etc/sudoers.d directory, which should be edited with command visudo for proper validations. sudo commands and any failures are logged in /var/log/auth.log under the Debian distribution family, and in /var/log/messages and/or /var/log/secure on other systems. A typical entry of the message for sudo contains: caller's username, terminal info, working dir, user account invoked, command & args. sudo inherits the PATH of the user, not the full root user. So the directories `/sbin and /usr/sbin are not searched when a user executes a command with sudo. It is best to add these two dirs to the user's .bashrc . passwords \u00b6 On modern systems, passwords are actually stored in an encrypted format in a secondary file named /etc/shadow . Only those with root access can read or modify this file. Most Linux distributions rely on a modern password encryption algorithm called SHA-512 (Secure Hashing Algorithm 512 bits) , developed by the U.S. National Security Agency (NSA) to encrypt passwords. SHA-512 is widely used by security applications and protocols such as TLS, SSL, PHP, SSH, S/MIME and IPSec and is one of the most tested hashing algorithms. Its CLI tool is sha512sum . chage can be used to configure the password expiry for users. Pluggable Authentication Modules (PAM) can be configured to automatically verify that a password created or modified using the passwd utility is sufficiently strong . You can secure the boot process with a secure password to prevent someone from bypassing the user authentication step (such as editing the bootloader configuration during boot). This can work in conjunction with password protection for the BIOS (such as botting from an alternative boot media and mount the harddrives and view the contents). You should NEVER edit /boot/grub/grub.cfg directly ; instead, you can modify the configuration files in /etc/grub.d and /etc/defaults/grub , and then run update-grub , or grub2-mkconfig and save the new configuration file. Physical Hardware Vulnerability \u00b6 Physical access to a system makes it possible for attackers to easily leverage several attack vectors, in a way that makes all operating system level recommendations irrelevant. Some possible attacks: Key logging Recording the real time activity of a computer user including the keys they press. The captured data can either be stored locally or transmitted to remote machines. Network sniffing Capturing and viewing the network packet level data on your network. Booting with a live or rescue disk Remounting and modifying disk content. The guidelines of enhancing security are: Lock down workstations and servers. Protect your network links such that it cannot be accessed by people you do not trust. Protect your keyboards where passwords are entered to ensure the keyboards cannot be tampered with. Ensure a password protects the BIOS in such a way that the system cannot be booted with a live or rescue DVD or USB key. Process Isolation \u00b6 Linux is considered to be more secure than many other operating systems because processes are naturally isolated from each other. One process normally cannot access the resources of another process, even when that process is running with the same user privileges. More recent additional security mechanisms that limit risks even further include: Control Groups (cgroups) - Allows system administrators to group processes and associate finite resources to each cgroup. Containers - Makes it possible to run multiple isolated Linux systems (containers) on a single system by relying on cgroups . Virtualization - Hardware is emulated in such a way that not only processes can be isolated, but entire systems are run simultaneously as isolated and insulated guests ( virtual machines ) on one physical host. Hardware Device Access \u00b6 Linux limits user access to non-networking hardware devices in a manner that is extremely similar to regular file access . Applications interact with devices by engaging the filesystem layer, which opens a device special file (aka device node) under /dev that corresponds to the device being accessed. Each device special file has standard owner, group and world permission fields. Security is naturally enforced just as it is when standard files are accessed. Linux System Troubleshoot \u00b6 Syslog files \u00b6 Syslog files log the timestamp, source IP, service name, actions from users It is useful in may ways: system side error debugging monitor service actions for abnormal activities fix network issues Some mostly accessed sys logs: /var/log/cron : for crontab /var/log/dmesg : core check on start up /var/log/lastlog : last logged in for each account /var/log/maillog : record SMTP provider's and POP3 provider's info and log /var/log/messages : all system error info will be here /var/log/secure : logs for any actions to do with passwords /var/log/wtmp , /var/log/faillog : records correct logged in users and failed log in attempts /var/log/httpd/ , /var/log/news/ , /var/log/samba/ : each service's own logs system services related to logs syslogd : for logging system and network info klog : for logging anything from core logrotate : for switching and getting rid of old large log files Other Misc. Linux Utilities \u00b6 Printing \u00b6 Printing itself requires software that converts information from the application you are using to a language your printer can understand. The Linux standard for printing software is the Common UNIX Printing System (CUPS) . CUPS uses a modular printing system which accommodates a wide variety of printers and also processes various data formats. It acts as a print server for both local and network printers. CUPS can be managed with the systemctl utility. The CUPS web interface is available on your browser at: http://localhost:631. How CUPS works \u00b6 The print scheduler reads server settings from several configuration files , commonly /etc/cups/cupsd.conf (system-wide settings, mostly related to network security, allow-listed devices), and /etc/cups/printers.conf (printer-specific settings). CUPS stores print requests as files under the /var/spool/cups directory and accessible before a doc is sent to a printer. Data files are prefixed with the letter d while control files are prefixed with the letter c. Data files are removed after a printer handles a job successfully. Log files are placed in /var/log/cups and are used by the scheduler to record activities that have taken place. CUPS uses filters to convert job file formats to printable formats . Printer drivers contain descriptions for currently connected and configured printers, and are usually stored under /etc/cups/ppd/ . The print data is then sent to the printer through a filter, and via a backend that helps to locate devices connected to the system. Print from CLI \u00b6 CUPS provides two command-line interfaces lp (System V, actually a front-end to lpr ) or lpr (BSD), useful in cases where printing operations must be automated . Some lp commands Command Usage lp <filename> To print the file to default printer lp -d printer <filename> To print to a specific printer (useful if multiple printers are available) program | lp or echo string | lp To print the output of a program lp -n number <filename> To print multiple copies lpoptions -d printer To set the default printer lpq -a To show the queue status lpadmin To configure printer queues lpstat -p -d To get a list of available printers, along with their status lpstat -a To check the status of all connected printers, including job numbers cancel job-id OR lprm job-id To cancel a print job lpmove job-id newprinter To move a print job to new printer Print formats \u00b6 PostScript is a standard page description language . It effectively manages scaling of fonts and vector graphics to provide quality printouts. It is purely a text format that contains the data fed to a PostScript interpreter. The format itself is a language that was developed by Adobe in the early 1980s to enable the transfer of data to printers. enscript is a tool that is used to convert a text file to PostScript and other formats. Postscript has been for the most part superseded by the PDF format (Portable Document Format). It can be converted from one to another format with tools like pdf2ps pdftops convert . Some other operations such as: Merging/splitting/rotating PDF documents Repairing corrupted PDF pages Pulling single pages from a file Encrypting and decrypting PDF files Adding, updating, and exporting a PDF\u2019s metadata Exporting bookmarks to a text file Filling out PDF forms can be done with tools like qpdf pdftk gs(ghostscript) . Some additional tools pdfinfo flpsed pdfmod provides basic information-fetching/editing capabilities. Tricks \u00b6 Calculator in Terminal bc can be a quick and light-weight calculator set scale = 4 to make division precision (number of digits after decimal point) quit to leave check filesystem space df gives the overall filesystem usage du evaluates filesystem usage of certain directory create partitions fdisk - use fdisk [-l] device_name shows the device's partitions. without -l will be interactive mode. (P264 for more info) df - use df pathname to find the name and usage of the hosting device It is best to do partition in single-user mode disk check fsck is a serious command to use when filesystem has problems actually calling e2fsck must be used when the partition inspected was unmounted badblocks can check whether the drive has broken sectors badblocks -[svw] device_name End of File [Ctrl]+[d] means End of File, End of Input. Can be used in the place of entering exit command Format a partition mkfs - to format and make a filesystem use mkfs [-t filesystem_format] device_name do mkfs[tab][tab] will give you a list of supported filesystem format mke2fs - a very detailed and sophisticated command can set filesystem label, block size, inode per N bytes, journal system configuration i.e. mke2fs -j -L \"vbird_logical\" -b 2048 -i 8192 /dev/hdc6 Linux X Window and Terminal Switching [Ctrl]+[Alt]+[F1]~[F6] are pre-loaded tty1 ~ tty6 Terminal workspaces [Ctrl]+[Alt]+[F7] switch back to X Window interface if started without X Window, can start it using command startx To change run levels, change /etc/inittab mount/unmount a partition Things to ensure before mounting single filesystem should not be mounted to different mounting points single directory should not be mounting multiple filesystems directories mouting filesystems should be originally empty mount mount -l shows mounted info mount -a mounts all unmounted filesystems mount [-t filesystem] [-L Label_name] [-o otheroptions] device_name mounting_point typical use of command mount -o remount,rw,auto / when root became read-only, use this to remount and make it writable again (saves a reboot) unmount unmount [-fn] device_name[or]mounting_point Mount at boot time Some limitations: root '/' must be the first to mount other mount point must be existing directory all mount points can be used only once all partition can be mounted only once /etc/fstab file contents listed in order: Device_label Mount_point filesystem parameters dump fsck device_label can be checked using dumpe2fs Softlink vs. hardlink use ln to make hard links use ln -s to make hard links hardlink to a file shares the original's inode hardlink has the same access rights of the original original inode exists as long as there is pointer to this inode content not lost if original file is deleted softlink is just a pointer to another file. can span to different filesystem can work on directory if original file deleted, content is lost and softlink become invalid troubleshoot file system errors Possible causes: abnormal shutdown, like sudden cut off of power frequent Harddisk access, over-heat, high-humidity If the error happens in partition of /dev/sda7 , then at boot time press ctrl-D to enter root password - then enter fsck /dev/sda7 to check for disk errors. If none found, enter Y to clear and reboot If root is broken, unplug the harddisk and connect to another working Linux machine do not mount that drive login as root, execute fsck /dev/sdb1 assume sdb1 is the broken disk the same thing can be done using a Linux bootable USB to rescue the disk use Single User Mode to reset forgotten root password reboot, when it is counting seconds, press any key to enter grub editor press [e] to enter grub editing mode move cursor to line starting with 'kernel', add 'single' at the end of line press [enter] to save press [b] to enter single user maintenance mode enter passwd and enter new root password twice","title":"Linux Foundation"},{"location":"Linux/Concepts/Linux_Foundation/#about-linux","text":"","title":"About Linux"},{"location":"Linux/Concepts/Linux_Foundation/#linux-history","text":"Linux's inception was in 1991, created by Linus Torvalds and lead maintainer Greg Kroah-Hartman . Linux is initially developed on and for Intel x86-based personal computers. It has been subsequently ported to an astoundingly long list of other hardware platforms. In 1992, Linux was re-licensed using the General Public License (GPL) by GNU (a project of the Free Software Foundation or FSF, which promotes freely available software), which made it possible to build a worldwide community of developers. The Linux distributions created in the mid-90s provided the basis for fully free computing (in the sense of freedom, not zero cost) and became a driving force in the open source software movement. The success of Linux has catalyzed growth in the open source community , demonstrating the commercial efficacy of open source and inspiring countless new projects across all industries and levels of the technology stack. Today, Linux powers more than half of the servers on the Internet, the majority of smartphones, consumer products, automobiles, and all of the world\u2019s most powerful supercomputers.","title":"Linux History"},{"location":"Linux/Concepts/Linux_Foundation/#linux-philosophy","text":"Linux borrows heavily from the well-established UNIX operating system. It was written to be a free and open source system to be used in place of UNIX, which at the time was designed for computers much more powerful than PCs and was quite expensive. Files are stored in a hierarchical filesystem, with the top node of the system being the root or simply \"/\". Whenever possible, Linux makes its components available via files or objects that look like files . Processes , devices , and network sockets are all represented by file-like objects , and can often be worked with using the same utilities used for regular files. Linux is a fully multitasking , multiuser operating system, with built-in networking and service processes known as daemons in the UNIX world. Linux stands for Linux is not UNIX .","title":"Linux Philosophy"},{"location":"Linux/Concepts/Linux_Foundation/#linux-terminology","text":"kernel - brain of the Linux OS, controls hardware and let applications interacts with hardware distribution (Distro) - collection of programs combined with Linux kernel to make up a Linux-based OS boot loader - a program boots the OS , i.e. GRUB, ISOLINUX service - a program runs as a background process , i.e. httpd, nfsd, ntpd, ftpd, named filesystem - the method for storing and organizing files in Linux, i.e. ext3, ext4, FAT, XFS, Btrfs X Window System - provides standard toolkit and protocal to build graphical UI on all Linux Distro desktop environment - a graphical user interface on top of the OS, i.e. GNOME, KDE, Xfce, Fluxbox command line - interface for typing commands on top of OS shell - command line interpreter that interprets the command line input and instructs the OS to perform tasks, i.e. bash, tcsh, zsh","title":"Linux Terminology"},{"location":"Linux/Concepts/Linux_Foundation/#linux-distributions","text":"Linux is constantly evolving, both at the technical level (including kernel features) and at the distribution and interface level. A full Linux distribution consists of the kernel plus a number of other software tools for file-related operations, user management, and software package management. Linux distributions may be based on different kernel versions. Examples of other essential tools and ingredients provided by distributions include the C/C++ compiler, the gdb debugger, the core system libraries applications need to link with in order to run, the low-level interface for drawing graphics on the screen, as well as the higher-level desktop environment, and the system for installing and updating the various components, including the kernel itself. Three widely used Linux distributions (all distributions found here ): Red Hat Enterprise Linux (RHEL) Family - CentOS, Fedora, Oracle Linux SUSE Family - SLES, openSUSE Debian Family - Ubuntu, Linux Mint All major distributors provide update services for keeping your system primed with the latest security and bug fixes, and performance enhancements, as well as provide online support resources.","title":"Linux Distributions"},{"location":"Linux/Concepts/Linux_Foundation/#rhel","text":"RHEL is the most popular Linux distribution in enterprise environments. Some facts: Fedora is opensource version of RHEL, shipped with lots more software, and serves as an upstream testing platform for RHEL. CentOS is a close clone of RHEL now owned by Red Hat, while Oracle Linux is mostly a copy with some changes A heavily patched version 3.10 kernel is used in RHEL/CentOS 7, while version 4.18 is used in RHEL/CentOS 8. It supports hardware platforms such as Intel x86 , Arm , Itanium , PowerPC , and IBM System z . It uses the yum and dnf RPM-based yum package managers to install, update, and remove packages in the system. CentOS is a popular free alternative to RHEL and is often used by organizations that are comfortable operating without paid technical support.","title":"RHEL"},{"location":"Linux/Concepts/Linux_Foundation/#suse","text":"SUSE is an acronym for Software- und System-Entwicklung (Software and Systems Development). And SLES stands for SUSE Linux Enterprise Server . Some facts: SLES is upstream for openSUSE. Kernel version 4.12 is used in openSUSE Leap 15. It uses the RPM-based zypper package manager to install, update, and remove packages in the system. It includes the YaST (Yet Another Setup Tool) application for system administration purposes. SLES is widely used in retail and many other sectors.","title":"SUSE"},{"location":"Linux/Concepts/Linux_Foundation/#debian","text":"Debian provides by far the largest and most complete software repository to its users of any Linux distribution, and has a strong focus on stability. Some facts: The Debian family is upstream for Ubuntu, and Ubuntu is upstream for Linux Mint and others. Debian is a pure open source community project not owned by any corporation. Kernel version 4.15 is used in Ubuntu 18.04 LTS. It uses the DPKG-based APT package manager (using apt , apt-get , apt-cache , etc.) to install, update, and remove packages in the system. Ubuntu has been widely used for cloud deployments . While Ubuntu is built on top of Debian and is GNOME-based under the hood, it differs visually from the interface on standard Debian, as well as other distributions. Ubuntu and Fedora are widely used by developers and are also popular in the educational realm.","title":"Debian"},{"location":"Linux/Concepts/Linux_Foundation/#how-linux-works","text":"","title":"How Linux Works"},{"location":"Linux/Concepts/Linux_Foundation/#boot-process","text":"The Linux boot process is the procedure for initializing the system, from pressing the power switch to a fully operational user interface. Power ON -> BIOS --> Master Boot Record (MBR) ---> Boot Loader ----> Kernel -----> Initial RAM disk ------> /sbin/init (parent process) -------> Command Shell using getty --------> X Windows System (GUI)","title":"Boot Process"},{"location":"Linux/Concepts/Linux_Foundation/#bios","text":"BIOS stands for Basic Input/Output System. It runs and initializes the I/O hardware such as screen and keyboard, and tests the main memory, a process called POST (Power On Self Test). BIOS software is stored on a ROM chip on the motherboard.","title":"BIOS"},{"location":"Linux/Concepts/Linux_Foundation/#mbr-and-boot-loader","text":"After POST, control is passed to the boot loader, usually stored on one of the hard disks either in the boot sector ( MBR ) or the EFI/UEFI partition ((Unified) Extensible Firmware Interface). Date, time, and other peripherals are loaded from the CMOS values (from a battery-powered memory store which allows the machine track date and time when powered off).","title":"MBR and Boot Loader"},{"location":"Linux/Concepts/Linux_Foundation/#mbr","text":"MBR is just 512 bytes in size which holds the boot loader. The boot loader examines the partition table and finds a bootable partition , then search for a second stage boot loader and loads it into RAM.","title":"MBR"},{"location":"Linux/Concepts/Linux_Foundation/#efiuefi","text":"EFI/UEFI boot method has firmware reads its Boot Manager data and determine the UEFI application to launch and the disk and partition to launch it from.","title":"EFI/UEFI"},{"location":"Linux/Concepts/Linux_Foundation/#second-stage-boot-loader","text":"The second stage boot loader resides under /boot . Common boot loaders: GRUB - GRand Unified Boot Loader, on most machines and Linux distributions ISOLINUX - for booting from removable media DAS U-Boot - for booting on embedded devices and appliances When booting Linux, the boot loader is responsible for loading and uncompress the kernel image and load the initial RAM disk , filesystem , or drivers into memory. Most boot loaders provide an UI for choosing boot options or other OS to boot into.","title":"Second stage boot loader"},{"location":"Linux/Concepts/Linux_Foundation/#initial-ram-disk","text":"The initramfs filesystem image is a RAM-based filesystem which contains programs and binary files that perform all actions needed to provide kernel functionality, locating devices and drivers, mount the proper root filesystem , and check for filesystem errors. The mount program instructs the OS that a filesystem is ready for use, and associates it with a particular point in the overall hierarchy of the filesystem (the mount point ). Then initramfs is cleared from RAM and /sbin/init is executed, which handles the mounting and pivoting over to the final real root filesystem . It ten starts a number of text-mode login prompts (ttys) which allow you to type in username and password to get a command shell. Most distributions start six text terminals and one graphics terminal, and swith with CTRL-ALT + F1~F7 . The default command shell is bash (the GNU Bourne Again Shell).","title":"Initial RAM Disk"},{"location":"Linux/Concepts/Linux_Foundation/#linux-kernel","text":"When the kernel is loaded in RAM, it immediately initializes and configures the computer\u2019s memory and all the hardware attached to the system, and loads some necessary user space applications. /sbin/init is only ran after kernel set up all hardware and mounted the root filesystem. It is the origin of all non-kernel processes and is responsible for keeping the system running and for clean shutdowns .","title":"Linux Kernel"},{"location":"Linux/Concepts/Linux_Foundation/#sysvinit","text":"In older distros, this process startup follows a System V UNIX convention (aka SysVinit ) where the system pass through a serial process of runlevels containing collections of scripts that start and stop services. Each runlevel supports a different mode of running the system . Within each runlevel, individual services can be set to run, or to be shut down if running. This startup method is slow and does NOT use the parallel processing benefit from multi-core processors.","title":"SysVinit"},{"location":"Linux/Concepts/Linux_Foundation/#systemd","text":"Major recent distros have moved away from runlevels and use systemd and Upstart. Upstart was developed by Ubuntu in 2006, adopted in Fedora 9 in 2008 and RHEL 6. systemd was adopted by Fedora in 2011, by RHEL 7 and SUSE, and by Ubuntu 16.04. All distros now use systemd. Complicated startup shell scripts are replaced with simpler configuration files, which enumerate what has to be done before a service is started, how to execute service startup, and what conditions the service should indicate have been accomplished when startup is finished. /sbin/init just points to /lib/systemd/systemd . Starting, stopping, restarting a service $ sudo systemctl start|stop|restart nfs.service Enabling or disabling a system service from starting up at system boot $ sudo systemctl enable|disable nfs.service .service can be omitted.","title":"systemd"},{"location":"Linux/Concepts/Linux_Foundation/#linux-filesystem","text":"A filesystem is a method of storing/finding files on a hard disk (usually in a partition). A partition is a physically contiguous section of a disk. Partition is like a container in which a filesystem resides. By dividing the hard disk into partitions, data can be grouped and separated as needed. When a failure or mistake occurs, only the data in the affected partition will be damaged, while the data on the other partitions will likely survive. Different types of filesystems supported by Linux: Conventional disk filesystems: ext2, ext3, ext4 , XFS , Btrfs , JFS , NTFS, etc. Flash storage filesystems: ubifs, JFFS2, YAFFS, etc. Database filesystems Special purpose filesystems: procfs, sysfs, tmpfs, squashfs, debugfs, etc. other filesystems: ntfs, FAT, vfat, hfs, hfs+ It is often the case that more than one filesystem type is used on a machine, based on considerations such as the size of files, how often they are modified, what kind of hardware they sit on and what kind of access speed is needed, etc. Linux filesystem use a standard layout, Filesystem Hierarchy Standard (FHS), which uses / to separate paths and does not have drive letters. File names are case-sensitive. Multiple drives and/or partitions are mounted as directories in the single filesystem, called mount points . Mount points are usually empty. The mount command is used to attach a filesystem somewhere within the filesystem tree, i.e. sudo mount /dev/sda5 /home . umount does the opposite. fstab at /etc/fstab can be used to configure auto mount disks at system start up. df -Th can be used to display information about mounted filesystems, type, and usage statistics. In Linux, all open files are represented internally by what are called file descriptors . Simply put, these are represented by numbers starting at zero. stdin is file descriptor 0, stdout is file descriptor 1, and stderr is file descriptor 2. Typically, if other files are opened in addition to these three, which are opened by default, they will start at file descriptor 3 and increase from there.","title":"Linux Filesystem"},{"location":"Linux/Concepts/Linux_Foundation/#nfs","text":"A network filesystem ( NFS ) may have all its data on one machine or have it spread out on more than one network node. It is used to share data across physical systems which may be either in the same location or anywhere that can be reached by the Internet. NFS can be started as daemon with sudo systemctl start nfs . The text file /etc/exports configures the directories and permissions that a host is sharing with other systems over NFS. After updating the config file while nfs is running, use exportfs -av to notify NFS to re-apply the configuration. # example entry in /etc/exports /projects *.example.com ( rw ) # mount /projects using NFS with read and write permissions # and share within the example.com domain Client machine can mount the remote directory via NFS by mkdir -p /mnt/nfs/projects sudo mount <server_hostname/IP>:/projects /mnt/nfs/projects # let system boot auto mount the remote directory # add to /etc/fstab <server_hostname/IP>:/projects /mnt/nfs/projects nfs defaults 0 0","title":"NFS"},{"location":"Linux/Concepts/Linux_Foundation/#directories-under","text":"directory function examples /bin (might link to /usr/bin) essential commands used to boot the system or in single-user mode, and required by all system users cat, cp, ls, mv, ps, rm /sbin (might link to /usr/sbin) essential binaries related to system administration fsck, ip /proc (a type of pseudo-filesystem) no permanent presence on the disk, contains virtual files (in memory) for constantly changing runtime system information system memory, devices mounted, hardware configs /dev contains device nodes, pseudo-file that is used by most hardware and software devices sda1 (first partition on the first hard disk), lp1 (second printer), random (source of rangom numbers), null (special file to safely dump unwanted data) /var contains files that are expected to change in size and content as the system is running log (system log files), lib (packages and database files), spool (print queue), tmp (temporary files), ftp (FTP service), www (HTTP web service) /var/cache program execution generated temp cache file /var/lib data file when program executes /var/lock lock files for programs to prevent simultaneous modification of files /var/log log files, including the login record for who used this system /var/mail personal mail /var/run storing PIDs after process started /var/spool stores some queue data, that is something queued up for process to use in order. Often deleted after use. /etc home for system configuration files or scripts, only for the superuser passwd, shadow, group (for managing user accounts), resolv.conf (DNS settings) /boot essential files needed to boot the system vmlinuz (compressed Linux kernel), initramfs/initrd (initial RAM filesystem), config (kernel config file), System.map (kernel symbol table), grub.conf (boot loader config) /lib and /lib64 (might link to /usr/lib) contains kernel modules and common code shared by applications and needed for them to run, mostly known as dynamically loaded libraries (aka Shared Objects) libncurses.so.5.9 /media, /run, /mnt either one can be used for mounting removable media onto the system NFS, loopback filesystems, USB drive /opt optional application software packages /sys virtual pseudo-filesystem giving information about the system and the hardware /srv site-specific data served up by the system /tmp temporary files; on some distributions erased across a reboot and/or may actually be a ramdisk in memory /usr stands for Unix Software Resource, for sharing in the Linux's multi-user setup applications, utilities and data, mostly static files /usr/bin This is the primary directory of executable commands on the system /usr/include Header files used to compile applications /usr/lib Libraries for programs in /usr/bin and /usr/sbin /usr/lib64 64-bit libraries for 64-bit programs in /usr/bin and /usr/sbin /usr/local Data and programs specific to the local machine. Subdirectories include bin, sbin, lib, share, include, etc. /usr/sbin Non-essential system binaries, such as system daemons /usr/share Shared data used by applications, generally architecture-independent /usr/src Source code, usually for the Linux kernel Some directories to consider for larger space allocation via partitioning: / /usr /home /var /tmp Swap Use basename on a file to get the file's name; use dirname on a file to get the full path to this file's belonging directory","title":"Directories under /"},{"location":"Linux/Concepts/Linux_Foundation/#inode-and-block","text":"superblock : records this filesystem's information, including number of inode/block, used amount, remaining amount, filesystem format, etc. inode : records file-specific properties and records the block number of this record each record use one inode each 128 bytes for a large file, its inode records one block number for which that block records twelve additional direct block numbers, one redirect block number, and one triple-redirect block number. P247 Book. block : records the actual content of the file, may span to multiple blocks for larger files knowing an inode can know its block number. this way, data saved onto multiple continuous blocks can be read in sequence within a short amount of time, this is called localization to compensate possible large file system's performance, block group is used to divide the storage into block groups, for each having a separate inode/block/superblock system. data block stores file and data. block size: 1K, 2K or 4K small block size may cause larger file use more block and inodes large block size may create many blocks not fully utilized inode/block bitmaps records and track used and unused blocks and inodes which gives fast lookup and fast search for unused block/inode i.e. the process of reading a file at /etc/passwd : filesystem find / inode filesystem locate / block and look for etc inode find etc inode and check whether current user has rx access find etc block and look for passwd inode find passwd inode and check whether current user has r access read passwd block content Journaling filesystem : during sudden power outage during writing to the disk, disk data and real data can be inconsistent. To deal with this and prevent a whole scan of the filesystem, a journaling filesystem helps by: record each write to the filesystem in a log. preparation, writing, and completion are supposed to be recorded for each write. if anything happens, can quickly check the journal to find which file is wrong. This is available in ex3 filesystem on Linux. It can help servers recover faster from power outage.","title":"inode and block"},{"location":"Linux/Concepts/Linux_Foundation/#commonly-seen-devices","text":"Device name in Linux IDE /dev/hd[a-d] SCSI/SATA/USB /dev/sd[a-p] ROM /dev/fd[0-1] printer /dev/lp[0-2] or /dev/usb/lp[0-15] mouse /dev/usb/mouse[0-15] or /dev/psaux CDROM/DVDROM /dev/cdrom current mouse /dev/mouse tape /dev/ht0 (IDE) or /dev/st0 (SCSI)","title":"Commonly seen devices"},{"location":"Linux/Concepts/Linux_Foundation/#searching-commands","text":"file command gives information on what kind of file it is which command gives exact path location of the command inspected whereis and locate can be used to find files. These two commands use database mapping to lookup and is therefore faster find can search files physically in the harddrive, can be slow and expensive find [PATH] [option] [action] some freq used options -mtime n : n is a number, means day. It makes a huge difference between adding [+] or [-] before the number: + means older than n days, - means within past n days, and neither, means exact n days ago. -newer : find /dir1 -newer /dir1/file finds files newer than /dir1/file -atime , -ctime similar as -mtime -perm : find files with/above/below certain access rights","title":"Searching commands"},{"location":"Linux/Concepts/Linux_Foundation/#comparing-files","text":"diff is used to compare files and directories . cmp can be used fro comparing binary files . You can compare three files at once using diff3 , which uses one file (second file argument) as the reference basis for the other two. Some common diff options diff Option Usage -c Provides a listing of differences that include three lines of context before and after the lines differing in content -r Used to recursively compare subdirectories , as well as the current directory -i Ignore the case of letters -w Ignore differences in spaces and tabs (white space) -q Be quiet: only report if files are different without listing the differences Many modifications to source code and configuration files are distributed utilizing patches with the patch program. A patch file contains the deltas (changes) required to update an older version of a file to the new one. Use ` diff -Nur originFile newFile > patchFile # create a patch file patch -p1 < patchFile # apply a patch file to an entire directory tree patch originFile patchFile # apply patch on one file In Linux, a file's extension does not categorize it, most applications directly examine a file's contents to see what kind of object it is rather than relying on an extension. Use file utility to assert the real nature of a file.","title":"Comparing Files"},{"location":"Linux/Concepts/Linux_Foundation/#backing-up-data","text":"While simple cp can help back up files or entire directory, rsync is more robust to synchronize directory trees , using the -r option, i.e. rsync -r sourceDir destinationDir . rsync checks if the file being copied already exists and skips copy if there is no change in size or modification time, therefore avoids unnecessary operations and saves time. Furthermore, rsync only copies the parts of files that actually changed and is very fast. rsync can also copy files from one machine to another in the form of user@host:filepath . A good combination of options rsync --progress -avrxH --delete sourceDir destinationDir Note that rsync could be destructive if not used properly, as a lot of files could be created at the target and it might use up all the space. Always use the -dry-run option to know what will be done before executing it. The Disk-to-Disk Copying program dd is very useful for making exact copies of raw disk space. Mostly used to backup a MBR, create a disk image, or install and OS, i.e. dd if=/dev/sda of=sda.mbr bs=512 count=1","title":"Backing up Data"},{"location":"Linux/Concepts/Linux_Foundation/#compressing-data","text":"File data is often compressed to save disk space and reduce the time it takes to transmit files over networks. Some good compression programs: gzip - the most frequently used Linux compression utility gzip to compress and gunzip or gzip -d to decompress compresses very well and is very fast, produces .gz files bzip2 - produces files significantly smaller than those produced by gzip but takes longer bzip2 to compress and bunzip2 or bzip2 -d to decompress produces .bz2 files xz - the most space-efficient compression utility used in Linux xz to compress and xz -d to decompress used to store archives of Linux kernel zip - often required to examine and decompress archives from other operating systems zip to compress and unzip to decompress tar - group files in an archive and then compress the whole archive at once tar czf to compress with gzip and gives xxx.tar.gz tar cjf to compress with bz2 and gives xxx.tar.bz2 tar cJf to compress with xz and gives xxx.tar.xz tar xf to decompress, no need to pass the option to tell it which format mostly used to archive files to a magnetic tape Generally, the more space-efficient techniques take longer . Decompression time does NOT vary as much across different methods. du can be used to check file sizes and total size for a directory. Use du -shc [list of files and dirs] to get a quick overview of selected files/dirs sizes. Use utilities such as zcat, zless, zdiff, zgrep to work directly with compressed files","title":"Compressing Data"},{"location":"Linux/Concepts/Linux_Foundation/#x-window-system","text":"The X Window System (aka X ) is loaded as one of the final steps in the boot process. It can also be started from text-mode by the startx command, or commands to start the display manager gdm, lightgdm, kdm, xdm . A service called the Display Manager keeps track of the displays being provided and loads the X server. The display manager also handles graphical logins and starts the appropriate desktop environment after a user logs in. A desktop environment consists of a session manager, which starts and maintains the components of the graphical session, the window manager, which controls the placement and movement of windows, window title-bars, and controls, and a set of utilities. X is old software and has deficiencies in security . A newer system, Wayland , is superseding it and is used on Fedora, RHEL 8, and other Distros. For Distros using gnome-based X winodw manager, use gnome-tweak-tool to customize and remap keys.","title":"X Window System"},{"location":"Linux/Concepts/Linux_Foundation/#package-management-systems","text":"A Package Management System distributes packages that each contains the files and other instructions needed to make one software component work well and cooperate with the other components that comprise the entire system. Two broad families of package managers: Debian and RPM . A package management system operates on two levels: low-level tool, such as dpkg, rpm , is responsible for unpacking individual packages, running scripts , getting the software installed correctly high-level tool, such as apt, yum, dnf, zypper , works with groups of packages , downloads packages from the vendor, and figures out dependencies apt stands for Advanced Packaging Tool, used on Debian-based systems yum stands for Yellowdog Updater Modified, is an open source tool for RPM-compatible Distros dnf aka Dandified YUM, is also RPM-based and used on Fedora and RHEL 8 systems zypper is RPM-based and used on openSUSE Operation RPM debian Install package rpm -i foo.rpm dpkg --install foo.deb Install package, dependencies yum install foo apt-get install foo Remove package rpm -e foo.rpm dpkg --remove foo.deb Remove package, dependencies yum remove foo apt-get autoremove foo Update package rpm -U foo.rpm dpkg --install foo.deb Update package, dependencies yum update foo apt-get install foo Update entire system yum update apt-get update && apt-get upgrade or apt-get dist-upgrade Show all installed packages rpm -qa or yum list installed dpkg --list Get information on package rpm -qil foo dpkg --listfiles foo Show packages named foo yum list \"foo\" apt-cache search foo Show all available packages yum list apt-cache dumpavail foo What package is <file> part of? rpm -qf <file> dpkg --search <file> Package documentation is directly pulled from the upstream source code and placed under the /usr/share/doc directory, grouped in subdirectories named after each package, perhaps including the version number in the name.","title":"Package Management Systems"},{"location":"Linux/Concepts/Linux_Foundation/#linux-documentation","text":"","title":"Linux Documentation"},{"location":"Linux/Concepts/Linux_Foundation/#info-pages","text":"info is the other form of documentation pages besides man . Navigation within info : Navigate within a Node Action Keys Next Line CTRL-n or arrow key Previous Line CTRL-p or arrow key Beginning of Line CTRL-a or key End of Line CTRL-e or key Forward 1 Character CTRL-f or arrow key Backward 1 Character CTRL-b or arrow key Forward 1 Word ALT-f or CTRL- arrow key Backward 1 Word ALT-b or CTRL- arrow key Beginning of Node ALT-< End of Node ALT-> End of Current Node e Quit q Selecting Nodes Action Keys Next Node n Previous Node p Up a Node u Last Node Viewed l Top Node t Directory Node d First Node < Last Node > Global Next Node ] Global Previous Node [ Searching within a Node Action Keys Search Forward s(string) or /(string) Search Backwards ?(string) Search Case-Sensitive S(string) Next word in Search n Next word Case-Sensitive N Interactively Search Forwards CTRL-s(string) Interactively Search Backwards CTRL-r(string) Index Search i(string) Next Index Search ,","title":"info pages"},{"location":"Linux/Concepts/Linux_Foundation/#ask-the-man","text":"man is short for \"manual\". man pages are present on all Linux distributions and offer in-depth documentation about many programs and utilities, as well as other topics, including configuration files, and programming APIs for system calls, library routines, and the kernel. The man pages are divided into chapters numbered 1 through 9 . In some cases, a letter is appended to the chapter number to identify a specific topic. It is common to have multiple pages across multiple chapters with the same name , especially for names of library functions or system calls . The chapter number can be used to force man to display the page from a particular chapter , i.e. man 2 socket . Display all pages with -a option. Linux man pages online The man program searches, formats, and displays the information contained in the man page system. To list all pages on the topic, use -f option (same result as whatis ). To list all pages that discuss a specified topic, use the \u2013k option (same result as apropos ).","title":"Ask the man"},{"location":"Linux/Concepts/Linux_Foundation/#man-page-number","text":"Numb Meaning 1 shell executables or commands ( important ) 2 functions for kernels 3 library or libc functions 4 device manuals, often under /dev 5 setting file or format ( important ) 6 games 7 protocols 8 system administrator's commands ( important ) 9 kernel files","title":"Man page number"},{"location":"Linux/Concepts/Linux_Foundation/#navigation-less","text":"Keys Functions [Space] next page [PageDown] next page [PageUp] next page [Home] first page [End] last page /string search string after current position ?string search string before current position n, N when searching, find next matching entry q quit","title":"navigation (less)"},{"location":"Linux/Concepts/Linux_Foundation/#search-man-pages","text":"To search for a specific man page, use man -f command_name To find any man page related to a term, use man -k searching_term , which will return all man-pages contain this phrase","title":"search man pages"},{"location":"Linux/Concepts/Linux_Foundation/#info-page","text":"Info Page is a Linux specific feature that displays help doc like small paragraphs(pages), like a web-page. Use info command There are lots of information about the page displayed, including the progress of viewing the entire doc. Keys Functions [Space] next page [PageDown] next page [PageUp] next page [Home] first page [End] last page [b] move cursor to the first node in current screen [e] move cursor to the last node in current screen [n] next node [p] previous node [u] upper layer [s] or [/] search in current info page [h] show help [?] view commands [q] exit Additionally, /usr/share/doc/ usually contains many documentation docs","title":"Info Page"},{"location":"Linux/Concepts/Linux_Foundation/#gnu-info-system","text":"This is the GNU project's standard documentation format , which it prefers as an alternative to man . The Info System is free-form, and its topics are connected using links . You can view help for a particular topic by typing info <topic name> , or view a top level index of available topics. The system then searches for the topic in all available info files. The topic which you view in an info page is called a node . You can move between nodes or view each node sequentially. Each node may contain menus and linked subtopics, aka items . Use n to go to next node, p for previous node, and u for moving one node up in the index. Items function like browser links and are identified by an asterisk ( ) at the beginning of the item name. Named items (outside a menu) are identified with double-colons * (::) at the end of the item name. Items can refer to other nodes within the file or to other files.","title":"GNU Info System"},{"location":"Linux/Concepts/Linux_Foundation/#-help-option","text":"Most commands have an available short description which can be viewed using the --help or the -h option along with the command or application, which offers a quick reference and it displays information faster than the man or info pages.","title":"--help option"},{"location":"Linux/Concepts/Linux_Foundation/#process","text":"A process is simply an instance of one or more related tasks ( threads ) executing on your computer. A single command may start several processes simultaneously. Some processes are independent of each other and others are related . program : usually binary program, stored within physical media like hard-drives process : when a program is executed, executor's access and program data being loaded into the memory and OS gets assigned a PID fork and exec system fork a parent process as temporary process to execute the child program a PID is assigned and PPID is the parent's PID temporary process exec the child program and becomes the child process Processes use many system resources, such as memory, CPU cycles, and peripheral devices, such as network cards, hard drives, printers and displays. The OS (especially the kernel) is responsible for allocating a proper share of these resources to each process and ensuring overall optimized system utilization.","title":"Process"},{"location":"Linux/Concepts/Linux_Foundation/#types","text":"Process Type Description Example Interactive Processes Need to be started by a user , either at a command line or through a graphical interface such as an icon or a menu selection. bash, firefox, top Batch Processes Automatic processes which are scheduled from and then disconnected from the terminal . These tasks are queued and work on a FIFO (First-In, First-Out) basis. updatedb, ldconfig Daemons Server processes that run continuously. Many are launched during system startup and then wait for a user or system request indicating that their service is required. httpd, sshd, libvirtd Threads Lightweight processes . These are tasks that run under the umbrella of a main process , sharing memory and other resources, but are scheduled and run by the system on an individual basis. An individual thread can end without terminating the whole process and a process can create new threads at any time. Many non-trivial programs are multi-threaded. firefox, gnome-terminal-server Kernel Threads Kernel tasks that users neither start nor terminate and have little control over. These may perform actions like moving a thread from one CPU to another, or making sure input/output operations to disk are completed. kthreadd, migration, ksoftirqd","title":"Types"},{"location":"Linux/Concepts/Linux_Foundation/#scheduling","text":"The kernel scheduler constantly shifts processes on and off the CPU, sharing time according to relative priority , how much time is needed and how much has already been granted to a task. Some process states: running state means the process is either currently executing instructions on a CPU, or is waiting to be granted a share of time. All processes in this state reside on what is called a run queue . For machines with multi-core CPUs, there is a run queue on each core. sleep state means the process is waiting for something to happen before it can resume. It is said to be sitting on a wait queue. zombie state means when a child process is completed but its parent process has not asked about its state, then it still shows up in the system's list of processes but not really alive. The OS assigns each process an unique process ID ( PID ) to track process state, CPU usage, memory use, precisely where resources are located in memory, and other characteristics. You can terminate a process by issuing kill -SIGKILL <pid> , kill -9 <pid> , or kill -SIGTERM ID Type Description Process ID ( PID ) Unique Process ID number Parent Process ID ( PPID ) Process (Parent) that started this process. If the parent dies, the PPID will refer to an adoptive parent; on recent kernels, this is kthreadd which has PPID=2. Thread ID ( TID ) Thread ID number. This is the same as the PID for single-threaded processes. For a multi-threaded process, each thread shares the same PID, but has a unique TID.","title":"Scheduling"},{"location":"Linux/Concepts/Linux_Foundation/#users-and-groups","text":"The OS identifies the user who starts a process by the Real User ID ( RUID ) assigned to the user. The user who determines the access rights for the users is identified by the Effective UID ( EUID ). EUID may not be the same as the RUID in some situations. Users can be categorized into various groups. Each group is identified by the Real Group ID ( RGID ). The access rights of the group are determined by the Effective Group ID ( EGID ).","title":"Users and Groups"},{"location":"Linux/Concepts/Linux_Foundation/#priority-and-nice","text":"The priority (PRI) for a process can be set by specifying a nice value , aka niceness (NI). The lower the nice value, the higher the priority. Higher priority processes grep preferential access to the CPU, therefore more CPU time. In Linux, a nice value of -20 represents the highest priority and +19 represents the lowest. This convention was adopted from UNIX. You can view the nice values using ps -lf and use renice +5/-5 <pid> to set the nice value. Parent process's nice value change also affects its child process's nice value. root can change all process NI , while a normal user can only adjust owning process NI within [0, 19] and can only adjust NI to a higher value , with command nice [-n numb] command or adjust existing process with renice [numb] PID . NI adjustments will be passed from parent process to child The load average is displayed using three numbers (i.e. 0.45, 0.17, and 0.12) with command w , interpreted as CPU utilization within last minute, 5 minutes before, and 15 minutes before.","title":"Priority and NICE"},{"location":"Linux/Concepts/Linux_Foundation/#background-process","text":"You can put a job in the background by suffixing & to the command, i.e. updatedb & . Use CTRL-Z to suspend a foreground job and bg to put it running in the background. Use fg to bring a process back to foreground, and jobs to see a list of background jobs ( -l option for showing PIDs).","title":"background process"},{"location":"Linux/Concepts/Linux_Foundation/#ps-command","text":"For the BSD variation of ps command, use ps aux to display all processes of all users, and use ps axo <attributes> to specify a list of attributes to view. For the SystemV variation of ps command, options need the dash prefixes and are different. Several useful ps combination should be remembered: ps -l shows only your process related to this bash. Some columns explained: F represents process flags, means this process's access 4 means root 1 means forked but not exec S represents Status R: running S: sleep, idle, can be signaled to wakeup D: usually doing I/O, cannot be wakeup T: stop, might be under job control Z: zombie, process terminated but cannot be moved out of memory C represents CUP usage percentage PRI/NI is short for priority/nice, means the priority for CPU to execute it. Smaller number means higher priority ADDR/SZ/WCHAN related to memory, ADDR is a kernel function showing which part of memory; SZ means size; WCHAN means whether it is running ('-' means running) TTY : user's terminal from logged in TIME : CPU time used CMD : command ps aux shows all process. Some columns explained: USER the process belongs to PID that process has %CPU usage %MEM usage VSZ virtual memory usage (Kbytes) RSS physical memory usage (Kbytes) TTY from which terminal, if pts/n, means logged in from remote terminal STAT , status, shows the same as ps -l TIME , actual CPU usage in time unit COMMAND , which command triggered ps -axjf shows all processes in a tree view fashion pstree displays the processes running on the system in the form of a tree diagram showing the relationship between a process and its parent process and any other processes that it created, and threads displayed within {} . pstree [-A|U] [-up] -A : use ASCII char to represent tree -U : use UTF char to represent tree -p : show process PID -u : show process user","title":"ps command"},{"location":"Linux/Concepts/Linux_Foundation/#top-command","text":"top gives an over view of system performance live over time. The first line of the top output displays a quick summary of what is happening in the system: How long the system has been up How many users are logged on What is the load average load average of 1.00 per CPU indicates a fully subscribed system if greater than 1, the system is overloaded and processes are competing for CPU time if very high, it indicates the system may have a runaway process (non-responding state) The second line displays the total number of processes , the number of running, sleeping, stopped, and zombie processes. The third line indicates how the CPU time is being divided by displaying the percentage of CPU time used for each: us - CPU for user initiated processes sy - CPU for kernel processes ni - niceness, CPU for user jobs running at a lower priority id - idle CPU wa - waiting, CPU for jobs waiting for I/O hi - CPU for harware interrupts si - CPU for software interrupts st - steal time, used with virtual machines, which has some of its idle CPU time taken for other users The fourth and fifth lines indicate memory usage, which is divided in two categories and both displays total memory, used memory, and free space: Physical memory (RAM) on line 4. Swap space on line 5. Once the physical memory is exhausted, the system starts using swap space (temporary storage space on the hard drive) as an extended memory pool, and since accessing disk is much slower than accessing memory, this will negatively affect system performance. top [-d numb] | top [-bnp] -d : screen refresh rate at seconds -b : exec top in order, used with data redirection -n : used with -b, number of times top outputs -p : specify some PID for monitoring commands in top : ? : shows available commands P : arrange by CPU usage M : arrange by Memory usage N : arrange by PID T : arrange by CPU time k : send one PID a signal r : send one PID new nice value q : quit","title":"top command"},{"location":"Linux/Concepts/Linux_Foundation/#free-command","text":"free [-b|-k|-m|-g] [-t] shows memory usage -b|-k|-m|-g , by default output shows in unit Kbytes, use this to override to bytes, Mbytes, Gbytes -t , shows physical and swap memory as well","title":"free command"},{"location":"Linux/Concepts/Linux_Foundation/#uname-command","text":"uname [-asrmpi] checks system and core information -a : all system related information will be shown -s : system core name -r : system core version -m : system hardware architecture -p : CPU type -i : hardware platform","title":"uname command"},{"location":"Linux/Concepts/Linux_Foundation/#netstat-command","text":"netstat -[atunlp] can track network usage on a process level -a : show current system's all network, listening port, sockets -t : list tcp packet data -u : list udp packet data -n : show service by port number -l : list services being listened -p : show services with PID","title":"netstat command"},{"location":"Linux/Concepts/Linux_Foundation/#vmstat","text":"vmstat can track system resource changes -a [delay [total examine times]] shows active/inactive replace buffer/cache info -f show number of forks -s show memory changes -S <unit> use K/M replace bytes -d show number of disk read/write -p <partition> show a partition read/write stats categories shown: procs, memory, swap, io, system, cpu procs : the more of r and b, the busier the system r: process waiting to run b: un-wakeable processes memory : like shown by free swpd: virtual memory usage free: unused mem buff: buffer storage cache: high-speed cache swap : when si and so get larger, system is short of memory si: amount taken from disk so: amount written into swap io : when bi and bo get larger, system is doing lots of I/O bi: blocks read from disk bo: blocks written into disk system : when in and cs get larger, system communicates with external devices quite often in: processes interrupted per second cs: context-switch times per second cpu : us: non-core usage of CPU sy: core usage of CPU id: idle status wa: wait I/O CPU waste st: virtual machine CPU usage.","title":"vmstat"},{"location":"Linux/Concepts/Linux_Foundation/#fuser-command","text":"fuser [-umv] [-k [i] [-signal]] file/dir can find out which process is using which file/directory, from the point of the file/directory -u : show both PID and process owner -m : increase priority of the file -v : show each file and process related -k : show the process using this file/dir, and signal kill to the process -i : use with -k, ask for decision before kill the process -<signal> : send a signal code What will be shown is USER PID ACCESS COMMAND the ACCESS represents: c : the process is under current directory e : can be executed f : is an opened file r : is the root directory F : the file is opened but pending complete m : sharable dynamical library","title":"fuser command"},{"location":"Linux/Concepts/Linux_Foundation/#lsof-command","text":"lsof [-aUu] [+d] lists which process is using which files -a : show when all criteria satisfied -U : show only Unix like system's socket files -u username : list files opened by the user +d directory : list files opened under a directory","title":"lsof command"},{"location":"Linux/Concepts/Linux_Foundation/#pidof-command","text":"pidof [-sx] program_name list the active PIDs of a program -s : show only one, not all of the PIDs -x : show also the program's possible parent PID (PPID)","title":"pidof command"},{"location":"Linux/Concepts/Linux_Foundation/#process-list","text":"Process list shows information about each process. By default, processes are ordered by highest CPU usage, with other information: PID - process id USER - process owner PR - priority NI - nice values VIRT - virtual memory RES - physical memory SHR - shared memory S - status %CPU - percentage of CPU used %MEM - percentage of memory used TIME+ - execution time COMMAND - command started the process top can be used interactively for monitoring and controlling processes Command Output t Display or hide summary information (rows 2 and 3) m Display or hide memory information (rows 4 and 5) A Sort the process list by top resource consumers r Renice (change the priority of) a specific processes k Kill a specific process f Enter the top configuration screen o Interactively select a new sort order in the process list","title":"Process List"},{"location":"Linux/Concepts/Linux_Foundation/#schedule-processes","text":"","title":"Schedule Processes"},{"location":"Linux/Concepts/Linux_Foundation/#at-and-sleep","text":"Use at program to execute any non-interactive command at a specified future time for once. $ at now + 2 days at> cat file1.txt at> <EOT> ( CTRL-D ) job 1231 at xxxx-xx-xx xx:xx Use sleep to delay execution of a command for a specific period. sleep NUMBER [ SUFFIX ] # SUFFIX can be s(seconds, default if not provided), m(minutes), h(hours), d(days)","title":"at and sleep"},{"location":"Linux/Concepts/Linux_Foundation/#cron","text":"cron is a time-based scheduling utility program. It can launch routine background jobs at specific times and/or days on an on-going basis . cron is configured at /etc/crontab (cron table) which contains the various shell commands that need to be run at the properly scheduled times. cron can be configured with the system-wide or the user-specific crontab. each line of crontab is composed of a CRON expression and a shell command. Use crontab -e to edit existing or add new jobs. # CRON expression MIN HOUR DOM MON DOW CMD # minute(0-59), hour(0-23), day of month(1-31), month(1-12), day of week(0-6), shell command","title":"cron"},{"location":"Linux/Concepts/Linux_Foundation/#system-services-daemon","text":"System service programs are called daemons. usually the service name with a suffix d Stand-alone Daemons starts without being managed by other programs. It Stays in the system memory once started, and uses resources. Fast responding to users. Super Daemon is a single daemon to start other daemons upon request from the client. The daemons started will be closed when the client session ends. i.e. telnet is a service managed by the super daemon Each service maps to an unique port and this mapping is in /etc/services file To starting up a daemon, it requires an executable, a configuration, and an environment. They are stored at: /etc/init.d/ : for starting up scripts /etc/sysconfig/ : for initialization environment config /etc/xinetd.conf , /etc/xinetd.d/ : super daemon config /etc/ : services' configuration files /var/lib/ : services' database files /var/run/ : all services' PID record service is a command (in fact, a script) to start, terminate, and monitor any services. service [service_name] (start|stop|restart|status|--status-all)","title":"System Services (Daemon)"},{"location":"Linux/Concepts/Linux_Foundation/#job-control","text":"foreground jobs are jobs actively prompting in the terminal and is interactable. Background jobs : the jobs running in the background without interaction with the user. Appending & to commands will be thrown to the background switching jobs: in the middle of running a command, press ctrl-z to pause it and throw it to the background use jobs command to check running/stopped jobs lists process recently put into the background, with (+) means next retrieving job using fg and (-) means the second latest job put into hte background jobs [-lrs] -l : show PID -r : show running only -s : show stopped only fg to bring back a job suspended. fg %<jobnumber> use it without jobnumber will bring back the one with (+) can also fg - to bring back the one with (-) bg can make a stopped job running in the background again bg %<jobnumber> will also append & to the job command kill can remove jobs or restart jobs kill -<signal> %<jobnumber> the <signal> can be a number or text: -l : list all kill signals -1 : reload configuration files -2 : like entering ctrl-c to interrupt a process -9 : forced stop -15 : normal termination -17 : like entering ctrl-z to stop a process kill -<signal> PID also works killall can work on all running processes of a command, useful if you don't want bother to lookup its PID killall [-iIe] [-signal] [command_name] -i : interactive -e : exact, means the command_name must match -I : command_name ignore cases","title":"Job Control"},{"location":"Linux/Concepts/Linux_Foundation/#offline-jobs","text":"Notice the background from job control is not \"system background\", it is just a way to help you run and manage multiple things in the terminal. If there is need to run a job even after logged out of the system, then offline jobs may help. While at works for this case, nohup can also work! nohup <command> or nohup <command> & to run in the background","title":"Offline Jobs"},{"location":"Linux/Concepts/Linux_Foundation/#linux-users-and-groups","text":"Linux is a multi-user operating system. To identify the current user, use whoami . To list the currently logged-on users, use who or users . who -a gives more detailed information. All Linux users are assigned a unique integer user ID (uid); normal users start with a uid of 1000 or greater. Use id to get information about current user, and id <username> can get information from other user. Linux uses groups for organizing users. Groups are collections of accounts with certain shared permissions , defined in the /etc/group file. Permissions on various files and directories can be modified at the group level . Users also have one or more group IDs (gid), including a default one which is the same as the user ID. Groups are used to establish a set of users who have common interests for the purposes of access rights, privileges, and security considerations. Only the root user can add and remove users and groups. Adding a new user is done with useradd and removing is done with userdel . i.e. sudo /usr/sbin/useradd bjmoose sets the home directory to /home/bjmoose , populates it with some basic files (copied from /etc/skel ) and adds a line to /etc/passwd such as: bjmoose:x:1002:1002::/home/bjmoose:/bin/bash . Removing a user with userdel will leave the user home directory, and is good for a temporary inactivation. Use userdel -r to remove the home directory too. Similiarly, add a new group with groupadd and remove with groupdel . To add a user to a new group, use usermod . i.e. usermod -aG <newgroup> <username> . To remove a user from a group, you must give the full list of groups except the one want to remove. i.e. usermod -G <groups>... <username> . To temporarily become the superuser for a series of commands, you can use su and then be prompted for the root password. To execute just one command with root privilege use sudo <command> . sudo access priviledge is granted per user and its configuration files are stored in the /etc/sudoers file and in the /etc/sudoers.d/ directory. By default, the sudoers.d directory is empty.","title":"Linux Users and Groups"},{"location":"Linux/Concepts/Linux_Foundation/#file-ownership-permission","text":"In Linux, every file is associated with a user who is the owner and a group for whom has the right to acess it in certain ways: read(r), write(w), execute(x) . For a file, execute(x) means whether it can be executed; for a directory it means whether a user can cd into this directory as working directory. Whether a user can delete a file depends on its access right on the current directory. Must be write(w) File permission [-][rwx][r-x][r--] 0 123 456 789 0 - file type 123 - owner access right 456 - group access right 789 - global access right file types: - regular file d directory l link b block device file, like a hard-drive; or c character device file, like a mouse or keyboard s socket, for network data p pipe, FIFO, allow many process read the same file chown is used to change user ownership (and group) of a file or directory, chgrp for changing group ownership. chmod is for changing the permissions on the file at user(u) group(g) others(o) levels. A single digit is sufficient to specify all three types permission bits for each entity: read(4), write(2), execute(1) which is the sum of those digits.","title":"File Ownership, Permission"},{"location":"Linux/Concepts/Linux_Foundation/#umask","text":"umask can be used to disable certain rights for newly created files or directories. i.e. unmask 023 means new files created will NOT have w for groups and not have wx for world","title":"umask"},{"location":"Linux/Concepts/Linux_Foundation/#hidden-attributes","text":"Hidden attributes on a file are useful for security reasons. lsattr allows you to view the hidden attributes of a file chattr allows you to change the hidden attributes of a file -i means let a file be unchangable -a allows adding but not changing/deleting old portion of the file","title":"Hidden attributes"},{"location":"Linux/Concepts/Linux_Foundation/#clean-shutdown","text":"use who to see who is using current system. use netstat -a to see Internet connections status use ps -aux to see running process in the background sync command will sync data into hard drives. It is best to remember to run this command before reboot or shutdown the system. shutdown or halt can done many things such as shutdown, reboot, or enter single-user mode set shutdown time, now or in the future set shutdown message to online users send warning info broadcast. Useful when need to notify others for important messages whether use fsck to check file system shutdown [-t seconds] [-arkhncfF] [time] [warning_info] usage below: Option Setting -t sec shutdown in some seconds -k send warning message without shutting down -r reboot after system services terminate -h shutdown after system services terminate -n shutdown without the init process -f reboot skipping fsck check -F reboot force fsck check -c cancel current shutdown directive","title":"Clean shutdown"},{"location":"Linux/Concepts/Linux_Foundation/#linux-shell","text":"","title":"Linux shell"},{"location":"Linux/Concepts/Linux_Foundation/#startup-file","text":"The command shell program uses one or more startup files to configure the user environment . Files in the /etc directory define global settings for all users , while initialization files in the user's home directory can include and/or override the global settings. Things can be configured: Customizing the prompt Defining command line aliases Setting the default text editor Setting the path for where to find executable programs Order of startup files evaluation (for user first logs onto the system): /etc/profile , then ~/.bash_profile or ~/.bash_login or ~/.profile . Every time you create a new shell, or terminal window, etc., you do NOT perform a full system login ; only a file named ~/.bashrc file is read and evaluated. PATH is a variable of an ordered list of directories (the path) which is scanned when a command is given to find the appropriate program or script to run. Use alias with no arguments will list currently defined aliases . unalias will remove an alias. Alias definition needs to be placed within either single or double quotes if it contains any spaces. i.e. alias ls='ls --color -l' Prompt Statement (the PS1 variable) is used to customize your prompt string in your terminal windows to display the information you want.","title":"Startup file"},{"location":"Linux/Concepts/Linux_Foundation/#environment-variables","text":"Environment variables are quantities that have specific values which may be utilized by the command shell or other utilities and applications. Some are set by the system and others are set by the user, either at the command line or within startup and other scripts. An environment variable is actually just a character string that contains information used by one or more applications. Use set, env, export to view the values of currently set environment variables. Variables created within a script are only available to the current shell ; child processes (sub-shells) will NOT have access to values that have been set or modified. Allowing child processes to see the values requires use of the export command. You can also set environment variables to be fed as a one shot to a command as in: $ SDIRS=s_0* KROOT=/lib/modules/$(uname -r)/build make modules_install .","title":"Environment Variables"},{"location":"Linux/Concepts/Linux_Foundation/#command-history","text":"bash keeps track of previously entered commands and statements in a history buffer , stored in ~/.bash_history (each session saves the history in the very end). Recall previous commands using the arrow keys , search with CTRL-r , or use history to view all and use !<number> to re-execute a past command.","title":"Command History"},{"location":"Linux/Concepts/Linux_Foundation/#shell-shortcuts","text":"Keyboard Shortcut Task CTRL-L Clears the screen CTRL-D Exits the current shell CTRL-Z Puts the current process into suspended background CTRL-C Kills the current process CTRL-H Works the same as backspace CTRL-A Goes to the beginning of the line CTRL-W Deletes the word before the cursor CTRL-U Deletes from beginning of line to cursor position CTRL-K Deletes from cursor position to end of line CTRL-E Goes to the end of the line Tab Auto-completes files, directories, and binaries","title":"Shell shortcuts"},{"location":"Linux/Concepts/Linux_Foundation/#text-manipulation","text":"sed is abbreviation for stream editor and is a powerful text processing tool and is one of the oldest, earliest and most popular UNIX utilities. It is used to modify the contents of a file or input stream, usually placing the contents into a new file or output stream. sed can filter text , as well as perform substitutions in data streams. sed -e command <filename> - Specify editing commands at the command line, operate on file and put the output on standard out specify multiple -e command s to use perform multiple operations sed -f scriptfile <filename> - Specify a scriptfile containing sed commands, operate on file and put output on standard out Basic sed substitutions: Command Usage sed s/pattern/replace_string/ file Substitute first string occurrence in every line sed s/pattern/replace_string/g file Substitute all string occurrences in every line sed 1,3s/pattern/replace_string/g file Substitute all string occurrences in a range of lines sed -i s/pattern/replace_string/g file Save changes for string substitution in the same file awk is used to extract and then print specific contents of a file and is often used to construct reports. It got its name from the authors, Alfred Aho, Peter Weinberger, and Brian Kernighan. awk 'command' <filename> - Specify a command directly at the command line awk -f scriptfile <filename> - Specify a file that contains the script to be executed Basic awk usage: Command Usage awk '{ print $0 }' /etc/passwd Print entire file awk -F: '{ print $1 }' /etc/passwd Print first field (column) of every line, separated by a space awk -F: '{ print $1 $7 }' /etc/passwd Print first and seventh field of every line","title":"Text Manipulation"},{"location":"Linux/Concepts/Linux_Foundation/#file-manipulation","text":"sort is used to rearrange the lines of a text file , in either ascending or descending order according to a sort key, or sort with respect to particular fields ( columns ) in a file Syntax Usage sort <filename> Sort the lines in the specified file, according to the characters at the beginning of each line cat file1 file2 | sort Combine the two files, then sort the lines and display the output on the terminal sort -r <filename> Sort the lines in reverse order sort -k 3 <filename> Sort the lines by the 3rd field on each line instead of the beginning sort -r <filename> Sort the lines then keep only unique lines, same as running uniq uniq removes duplicate consecutive lines in a text file and is useful for simplifying the text display. It requires duplicate entries be consecutive to be removed. Use uniq -c to only count the number of duplicate lines. paste can be used to combine file contents with respect to columns . paste -s causes it to combine data like you do cat file1 file2 > file3 join can be used when two files have shared column values that one can combine data based on that column, like you do in SQL statement. split is used to break up a file into equal-sized segments of new files for easier viewing and manipulation, by default 1000 lines per file segment. An optional prefix of the new files can be specified with split <file> <prefix> grep is extensively used as a primary text searching tool. It scans files for specified patterns and can be used with regular expressions. Command Usage grep [pattern] <filename> Search for a pattern in a file and print all matching lines grep -v [pattern] <filename> Print all lines that do not match the pattern grep -C 3 [pattern] <filename> Print context of lines (specified number of lines above and below the pattern) for matching the pattern strings book1.xls | grep my_string Take text input from pipe strings extracts printable character strings from binary files. tr is used to translate specified characters into other characters or to delete or keep some of them Command Usage tr a-z A-Z Convert lower case to upper case tr '{}' '()' < inputfile > outputfile Translate braces into parenthesis echo \"This is for testing\" | tr [:space:] '\\t' Translate white-space to tabs echo \"This is for testing\" | tr -s [:space:] Squeeze repetition of characters using -s echo \"the geek stuff\" | tr -d 't' Delete specified characters using -d option echo \"my username is 432234\" | tr -cd [:digit:] Complement the sets using -c option. Combined with -d, means only keep the characters in the set tr -cd [:print:] < file.txt Remove all non-printable character from a file tr -s '\\n' ' ' < file.txt Join all the lines in a file into a single line tee takes the output from any command, and, while sending it to standard output , it also saves to a file wc counts the number of lines ( -l option), words ( -w option), and characters ( -c option) in a file or list of files. cut is used for manipulating column-based files and is designed to extract specific columns using option -f <number> . Default separator is tab; use cut -d ';' to override that.","title":"File Manipulation"},{"location":"Linux/Concepts/Linux_Foundation/#linux-networking","text":"Exchanging information across the network requires using streams of small packets , each of which contains a piece of the information going from one machine to another. These packets contain data buffers , together with headers which contain information about where the packet is going to and coming from, and where it fits in the sequence of packets that constitute the stream. A network requires the connection of many nodes. Data moves from source to destination by passing through a series of routers and potentially across multiple networks.","title":"Linux Networking"},{"location":"Linux/Concepts/Linux_Foundation/#ip-address","text":"Devices attached to a network must have at least one unique network address identifier known as the IP (Internet Protocol) address . The address is essential for routing packets of information through the network. IPv4 uses 32-bits for address and is older and by far the more widely used, while IPv6 uses 128-bits for addresses and is newer and designed to get past address pool limitations inherent in the older standard and furnish many more possible addresses. NAT (Network Address Translation) enables sharing one IP address among many locally connected computers, each of which has a unique address only seen on the local network. A 32-bit IPv4 address is divided into four 8-bit sections called octets , or bytes . Network addresses are divided into five classes : A, B, C, D and E. Classes A, B, C are classified into two parts: Network addresses (Net ID, for identify the network) and Host address (Host ID, for identify a host in the network). Class D is used for special multicast applications (information is broadcast to multiple computers simultaneously) and Class E is reserved for future use.","title":"IP Address"},{"location":"Linux/Concepts/Linux_Foundation/#class-a-address","text":"Class A addresses use the first octet as Net ID and use the other three as the Host ID. The first bit of the first octet is always set to zero , so you can use only 7-bits for unique network numbers, leaving a maximum of 126 Class A networks available (the addresses 0000000 and 1111111 are reserved). Each Class A network can have up to 16.7 million unique hosts on its network. The range of host address is from 1.0.0.0 to 127.255.255.255 .","title":"Class A Address"},{"location":"Linux/Concepts/Linux_Foundation/#class-b-address","text":"Class B addresses use the first two octets of the IP address as their Net ID and the last two octets as the Host ID. The first two bits of the first octet are always set to binary 10 , so there are a maximum of 16384 (14-bits) Class B networks. The first octet of a Class B address has values from 128 to 191 . Each Class B network can support a maximum of 65,536 unique hosts on its network. The range of host address is from 128.0.0.0 to 191.255.255.255 .","title":"Class B Address"},{"location":"Linux/Concepts/Linux_Foundation/#class-c-address","text":"Class C addresses use the first three octets of the IP address as their Net ID and the last octet as their Host ID. The first three bits of the first octet are set to binary 110 , so almost 2.1 million (21-bits) Class C networks are available. The first octet of a Class C address has values from 192 to 223 . These are most common for smaller networks which don't have many unique hosts. Each Class C network can support up to 256 (8-bits) unique hosts. The range of host address is from 192.0.0.0 to 223.255.255.255 .","title":"Class C Address"},{"location":"Linux/Concepts/Linux_Foundation/#ip-address-allocation","text":"Typically, a range of IP addresses are requested from your Internet Service Provider (ISP) by your organization's network administrator. The class of IP address gieven depends on the size of your network and growth needs. If NAT is in operation, you only get one externally visible address. You can assign IP addresses to computers over a network either manually ( static address) or dynamically (can change when machine reboots) using Dynamic Host Configuration Protocol (DHCP).","title":"IP Address Allocation"},{"location":"Linux/Concepts/Linux_Foundation/#name-resolution","text":"Name Resolution is used to convert numerical IP address values into a human-readable format known as the hostname . The special hostname localhost is associated with the IP address 127.0.0.1, and describes the machine you are currently on.","title":"Name Resolution"},{"location":"Linux/Concepts/Linux_Foundation/#network-configuration","text":"Network configuration files are located in the /etc directory tree. Debian family distros store them under /etc/network , while Fedora and SUSE store under /etc/sysconfig/network . Network interfaces are a connection channel between a device and a network . Physically, network interfaces can proceed through a network interface card (NIC), or can be more abstractly implemented as software , and each can be activated or deactivated any time. Use ip or ifconfig utilities to view network interface information.","title":"Network Configuration"},{"location":"Linux/Concepts/Linux_Foundation/#network-utils","text":"ping is used to check whether or not a machine attached to the network can receive and send data ; i.e. it confirms that the remote host is online and is responding. One can use the route utility or the ip route command to view or change the IP routing table to add, delete, or modify specific (static) routes to specific hosts or networks. traceroute is used to inspect the route which the data packet takes to reach the destination host, which makes it quite useful for troubleshooting network delays and errors . By using traceroute, you can isolate connectivity issues between hops, which helps resolve them faster. Some other networking tools: Networking Tools Description ethtool Queries network interfaces and can also set various parameters such as the speed netstat Displays all active connections and routing tables. Useful for monitoring performance and troubleshooting nmap Scans open ports on a network. Important for security analysis tcpdump Dumps network traffic for analysis iptraf Monitors network traffic in text mode mtr Combines functionality of ping and traceroute and gives a continuously updated display dig Tests DNS workings. A good replacement for host and nslookup wget is a command line utility for handling large file downloads, recursive downloads, password-protected downloads, or multi-file downloads. curl can be used from the command line or a script to read information about a http call, or save the contents to a file. File Transfer Protocol (FTP) is a well-known and popular method for transferring files between computers using the Internet, built on a client-server model . All web browsers support FTP. Some cli FTP clients are ftp, sftp, ncftp, yafc . Secure Shell (SSH) is a cryptographic network protocol used for secure data communication (using ssh ) and remote services and other secure services between two devices on the network. Move files securely using Secure Copy ( scp ) between two networked hosts. scp uses the SSH protocol for transferring data.","title":"Network utils"},{"location":"Linux/Concepts/Linux_Foundation/#linux-security","text":"","title":"Linux Security"},{"location":"Linux/Concepts/Linux_Foundation/#user-accounts","text":"The Linux kernel allows properly authenticated users to access files and applications. Each user is identified by a unique integer ( UID ) and a separate database associates a username with each UID. Related tools are useradd userdel for creating and removing accounts. Upon account creation, new user information is added to the user database and the user's home directory must be created and populated with some essential files. For each user, the following seven fields are maintained in the /etc/passwd file: Field Name Details Remarks Username User login name Should be between 1 and 32 characters long Password User password (or the character x if the password is stored in the /etc/shadow file) in encrypted format Is never shown in Linux when it is being typed; this stops prying eyes User ID (UID) Every user must have a user id (UID) UID 0 is reserved for root user; UID's ranging from 1-99 are reserved for other predefined accounts; UID's ranging from 100-999 are reserved for system accounts and groups; Normal users have UID's of 1000 or greater Group ID (GID) The primary Group ID (GID); Group Identification Number stored in the /etc/group file Is covered in detail in the chapter on Processes User Info This field is optional and allows insertion of extra information about the user such as their name For example: Rufus T. Firefly Home Directory The absolute path location of user's home directory For example: /home/rtfirefly Shell The absolute location of a user's default shell For example: /bin/ba For a safe working environment, it is advised to grant the minimum privileges possible and necessary to accounts, and remove inactive accounts. The last utility can be used to identify potential inactive users. root is the most privileged account on a Linux/UNIX system. This account has the ability to carry out ALL facets of system administration, and utmost care must be taken when using this account. root privilege is required for performing administration tasks such as restarting most services, manually installing packages and managing parts of the filesystem that are outside the normal user\u2019s directories.","title":"User Accounts"},{"location":"Linux/Concepts/Linux_Foundation/#suid-sgid-sbit","text":"SUID ( Set owner User ID upon execution - similar to the Windows \"run as\" feature) is a special kind of file permission given to a file. Use of SUID provides temporary permissions to a user to run a program with the permissions of the file owner (which may be root) instead of the permissions held by the user. i.e., I have x access to /usr/bin/passwd and passwd is owned by root . When I execute passwd I temporarily get root access so I can change /etc/shadow SUID can only be used on binary program, NOT on shell script, and NOT on directories. SGID can be used on binary program and directories, NOT on shell script. SBIT, Sticky Bit, only used on directories. When a user has wx access on a directory and creates a file under it, only this user or root can delete that file. How to set these bits: SUID: 4, SGID: 2, SBIT: 1 , i.e. chmod 4755 file_name","title":"SUID, SGID, SBIT"},{"location":"Linux/Concepts/Linux_Foundation/#sudo","text":"In Linux you can use either su (requires root password, can be root for as long as needed, limited logging trails) or sudo (requires the user's password, temporary access, more logging trails) to temporarily grant root access to a normal user. sudo has the ability to keep track of unsuccessful attempts at gaining root access (usually logged in /var/log/secure ). Users' authorization for using sudo is based on configuration information stored in the /etc/sudoers file and in the /etc/sudoers.d directory, which should be edited with command visudo for proper validations. sudo commands and any failures are logged in /var/log/auth.log under the Debian distribution family, and in /var/log/messages and/or /var/log/secure on other systems. A typical entry of the message for sudo contains: caller's username, terminal info, working dir, user account invoked, command & args. sudo inherits the PATH of the user, not the full root user. So the directories `/sbin and /usr/sbin are not searched when a user executes a command with sudo. It is best to add these two dirs to the user's .bashrc .","title":"sudo"},{"location":"Linux/Concepts/Linux_Foundation/#passwords","text":"On modern systems, passwords are actually stored in an encrypted format in a secondary file named /etc/shadow . Only those with root access can read or modify this file. Most Linux distributions rely on a modern password encryption algorithm called SHA-512 (Secure Hashing Algorithm 512 bits) , developed by the U.S. National Security Agency (NSA) to encrypt passwords. SHA-512 is widely used by security applications and protocols such as TLS, SSL, PHP, SSH, S/MIME and IPSec and is one of the most tested hashing algorithms. Its CLI tool is sha512sum . chage can be used to configure the password expiry for users. Pluggable Authentication Modules (PAM) can be configured to automatically verify that a password created or modified using the passwd utility is sufficiently strong . You can secure the boot process with a secure password to prevent someone from bypassing the user authentication step (such as editing the bootloader configuration during boot). This can work in conjunction with password protection for the BIOS (such as botting from an alternative boot media and mount the harddrives and view the contents). You should NEVER edit /boot/grub/grub.cfg directly ; instead, you can modify the configuration files in /etc/grub.d and /etc/defaults/grub , and then run update-grub , or grub2-mkconfig and save the new configuration file.","title":"passwords"},{"location":"Linux/Concepts/Linux_Foundation/#physical-hardware-vulnerability","text":"Physical access to a system makes it possible for attackers to easily leverage several attack vectors, in a way that makes all operating system level recommendations irrelevant. Some possible attacks: Key logging Recording the real time activity of a computer user including the keys they press. The captured data can either be stored locally or transmitted to remote machines. Network sniffing Capturing and viewing the network packet level data on your network. Booting with a live or rescue disk Remounting and modifying disk content. The guidelines of enhancing security are: Lock down workstations and servers. Protect your network links such that it cannot be accessed by people you do not trust. Protect your keyboards where passwords are entered to ensure the keyboards cannot be tampered with. Ensure a password protects the BIOS in such a way that the system cannot be booted with a live or rescue DVD or USB key.","title":"Physical Hardware Vulnerability"},{"location":"Linux/Concepts/Linux_Foundation/#process-isolation","text":"Linux is considered to be more secure than many other operating systems because processes are naturally isolated from each other. One process normally cannot access the resources of another process, even when that process is running with the same user privileges. More recent additional security mechanisms that limit risks even further include: Control Groups (cgroups) - Allows system administrators to group processes and associate finite resources to each cgroup. Containers - Makes it possible to run multiple isolated Linux systems (containers) on a single system by relying on cgroups . Virtualization - Hardware is emulated in such a way that not only processes can be isolated, but entire systems are run simultaneously as isolated and insulated guests ( virtual machines ) on one physical host.","title":"Process Isolation"},{"location":"Linux/Concepts/Linux_Foundation/#hardware-device-access","text":"Linux limits user access to non-networking hardware devices in a manner that is extremely similar to regular file access . Applications interact with devices by engaging the filesystem layer, which opens a device special file (aka device node) under /dev that corresponds to the device being accessed. Each device special file has standard owner, group and world permission fields. Security is naturally enforced just as it is when standard files are accessed.","title":"Hardware Device Access"},{"location":"Linux/Concepts/Linux_Foundation/#linux-system-troubleshoot","text":"","title":"Linux System Troubleshoot"},{"location":"Linux/Concepts/Linux_Foundation/#syslog-files","text":"Syslog files log the timestamp, source IP, service name, actions from users It is useful in may ways: system side error debugging monitor service actions for abnormal activities fix network issues Some mostly accessed sys logs: /var/log/cron : for crontab /var/log/dmesg : core check on start up /var/log/lastlog : last logged in for each account /var/log/maillog : record SMTP provider's and POP3 provider's info and log /var/log/messages : all system error info will be here /var/log/secure : logs for any actions to do with passwords /var/log/wtmp , /var/log/faillog : records correct logged in users and failed log in attempts /var/log/httpd/ , /var/log/news/ , /var/log/samba/ : each service's own logs system services related to logs syslogd : for logging system and network info klog : for logging anything from core logrotate : for switching and getting rid of old large log files","title":"Syslog files"},{"location":"Linux/Concepts/Linux_Foundation/#other-misc-linux-utilities","text":"","title":"Other Misc. Linux Utilities"},{"location":"Linux/Concepts/Linux_Foundation/#printing","text":"Printing itself requires software that converts information from the application you are using to a language your printer can understand. The Linux standard for printing software is the Common UNIX Printing System (CUPS) . CUPS uses a modular printing system which accommodates a wide variety of printers and also processes various data formats. It acts as a print server for both local and network printers. CUPS can be managed with the systemctl utility. The CUPS web interface is available on your browser at: http://localhost:631.","title":"Printing"},{"location":"Linux/Concepts/Linux_Foundation/#how-cups-works","text":"The print scheduler reads server settings from several configuration files , commonly /etc/cups/cupsd.conf (system-wide settings, mostly related to network security, allow-listed devices), and /etc/cups/printers.conf (printer-specific settings). CUPS stores print requests as files under the /var/spool/cups directory and accessible before a doc is sent to a printer. Data files are prefixed with the letter d while control files are prefixed with the letter c. Data files are removed after a printer handles a job successfully. Log files are placed in /var/log/cups and are used by the scheduler to record activities that have taken place. CUPS uses filters to convert job file formats to printable formats . Printer drivers contain descriptions for currently connected and configured printers, and are usually stored under /etc/cups/ppd/ . The print data is then sent to the printer through a filter, and via a backend that helps to locate devices connected to the system.","title":"How CUPS works"},{"location":"Linux/Concepts/Linux_Foundation/#print-from-cli","text":"CUPS provides two command-line interfaces lp (System V, actually a front-end to lpr ) or lpr (BSD), useful in cases where printing operations must be automated . Some lp commands Command Usage lp <filename> To print the file to default printer lp -d printer <filename> To print to a specific printer (useful if multiple printers are available) program | lp or echo string | lp To print the output of a program lp -n number <filename> To print multiple copies lpoptions -d printer To set the default printer lpq -a To show the queue status lpadmin To configure printer queues lpstat -p -d To get a list of available printers, along with their status lpstat -a To check the status of all connected printers, including job numbers cancel job-id OR lprm job-id To cancel a print job lpmove job-id newprinter To move a print job to new printer","title":"Print from CLI"},{"location":"Linux/Concepts/Linux_Foundation/#print-formats","text":"PostScript is a standard page description language . It effectively manages scaling of fonts and vector graphics to provide quality printouts. It is purely a text format that contains the data fed to a PostScript interpreter. The format itself is a language that was developed by Adobe in the early 1980s to enable the transfer of data to printers. enscript is a tool that is used to convert a text file to PostScript and other formats. Postscript has been for the most part superseded by the PDF format (Portable Document Format). It can be converted from one to another format with tools like pdf2ps pdftops convert . Some other operations such as: Merging/splitting/rotating PDF documents Repairing corrupted PDF pages Pulling single pages from a file Encrypting and decrypting PDF files Adding, updating, and exporting a PDF\u2019s metadata Exporting bookmarks to a text file Filling out PDF forms can be done with tools like qpdf pdftk gs(ghostscript) . Some additional tools pdfinfo flpsed pdfmod provides basic information-fetching/editing capabilities.","title":"Print formats"},{"location":"Linux/Concepts/Linux_Foundation/#tricks","text":"Calculator in Terminal bc can be a quick and light-weight calculator set scale = 4 to make division precision (number of digits after decimal point) quit to leave check filesystem space df gives the overall filesystem usage du evaluates filesystem usage of certain directory create partitions fdisk - use fdisk [-l] device_name shows the device's partitions. without -l will be interactive mode. (P264 for more info) df - use df pathname to find the name and usage of the hosting device It is best to do partition in single-user mode disk check fsck is a serious command to use when filesystem has problems actually calling e2fsck must be used when the partition inspected was unmounted badblocks can check whether the drive has broken sectors badblocks -[svw] device_name End of File [Ctrl]+[d] means End of File, End of Input. Can be used in the place of entering exit command Format a partition mkfs - to format and make a filesystem use mkfs [-t filesystem_format] device_name do mkfs[tab][tab] will give you a list of supported filesystem format mke2fs - a very detailed and sophisticated command can set filesystem label, block size, inode per N bytes, journal system configuration i.e. mke2fs -j -L \"vbird_logical\" -b 2048 -i 8192 /dev/hdc6 Linux X Window and Terminal Switching [Ctrl]+[Alt]+[F1]~[F6] are pre-loaded tty1 ~ tty6 Terminal workspaces [Ctrl]+[Alt]+[F7] switch back to X Window interface if started without X Window, can start it using command startx To change run levels, change /etc/inittab mount/unmount a partition Things to ensure before mounting single filesystem should not be mounted to different mounting points single directory should not be mounting multiple filesystems directories mouting filesystems should be originally empty mount mount -l shows mounted info mount -a mounts all unmounted filesystems mount [-t filesystem] [-L Label_name] [-o otheroptions] device_name mounting_point typical use of command mount -o remount,rw,auto / when root became read-only, use this to remount and make it writable again (saves a reboot) unmount unmount [-fn] device_name[or]mounting_point Mount at boot time Some limitations: root '/' must be the first to mount other mount point must be existing directory all mount points can be used only once all partition can be mounted only once /etc/fstab file contents listed in order: Device_label Mount_point filesystem parameters dump fsck device_label can be checked using dumpe2fs Softlink vs. hardlink use ln to make hard links use ln -s to make hard links hardlink to a file shares the original's inode hardlink has the same access rights of the original original inode exists as long as there is pointer to this inode content not lost if original file is deleted softlink is just a pointer to another file. can span to different filesystem can work on directory if original file deleted, content is lost and softlink become invalid troubleshoot file system errors Possible causes: abnormal shutdown, like sudden cut off of power frequent Harddisk access, over-heat, high-humidity If the error happens in partition of /dev/sda7 , then at boot time press ctrl-D to enter root password - then enter fsck /dev/sda7 to check for disk errors. If none found, enter Y to clear and reboot If root is broken, unplug the harddisk and connect to another working Linux machine do not mount that drive login as root, execute fsck /dev/sdb1 assume sdb1 is the broken disk the same thing can be done using a Linux bootable USB to rescue the disk use Single User Mode to reset forgotten root password reboot, when it is counting seconds, press any key to enter grub editor press [e] to enter grub editing mode move cursor to line starting with 'kernel', add 'single' at the end of line press [enter] to save press [b] to enter single user maintenance mode enter passwd and enter new root password twice","title":"Tricks"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/","text":"This set of notes were taken from the Linux Foundation Course: Linux Networking and Administration (LFS211) . Networking Concepts \u00b6 The OSI Model \u00b6 The Open System Interconnection (OSI) model was created to standardize the language used to describe networking protocols. OSI defines the manner in which systems communicate with one another using abstraction layers . Each layer communicates with the layer directly above and below. Not all layers are used at all times. Most networking stacks have layer 1-4 and 7. The layer 5-6 is often combined into layer 4 or 7. Physical Layer \u00b6 The Physical Layer is the lowest possible layer and deals with the actual physical transfer of information; it deals with transferring bits over a physical medium such as: Electric pulses over copper cables Laser pulses over fiber optic cables Frequency modulations over radio waves Scraps of paper over carrier pigeons Some protocols, hardware types and standards defined for this layer: IEEE 802.3: Copper or fiber connections (examples and additional information about various cables and connection is available on Wikipedia, in the \"10BASE2\" article) IEEE 802.11: Wireless (Wi-Fi) connections Bluetooth: Wireless connections USB: Copper connections RS232: Copper serial connections A Frame is a unit of data collected from the physical layer interface. Data Link Layer \u00b6 The Data Link Layer accepts data from the hardware in Layer 1 and prepends an address to all the inbound packets it accepts. The address number is 6 bytes, 3 bytes for the manufacturer and 3 random bytes assigned to the adapter. This 6 byte address is known as the MAC (Media Access Control) Address . The Data Link layer deals with transferring data between network nodes using the MAC address. This layer is where the IP address and MAC address are associated using the ARP protocol . A broadcast is initiated to find the MAC address of the IP address that is required. Quite often a broadcast is initiated requesting MAC addresses as the associations time out and are deleted. The ARP frame usually has the data of Who has 192.168.0.2? Tell 192.168.0.7 . Since ARP uses broadcasts, the packets are confined to a single network segment. A network segment may also be called a broadcast domain or collision domain . A collision domain may be expanded with the use of a bridge . Bridges will work but all nodes will hear and examine all traffic on the wire. Adding bridges increases the number of nodes on the wire. Common Data Link protocols or EtherType include: ARP: ID=x0806, Address Resolution Protocol RARP: ID=x0825, Reverse Address Resolution Protocol IPv4: ID=x0800, Internet Protocol version 4 PPPoE: ID=x8863, Point to Point Protocol over Ethernet (discovery) STP: Spanning Tree Protocol, does not have an ID as it uses an Ethernet II frame with a LLC (Logical Link Control) header. Network Layer \u00b6 The Network Layer deals with routing and forwarding packets, and may also have Quality of Service component. The Data Link layer is confined to a single network segment and is used by Network Layer for local delivery. Some of the common Layer 3 protocols are: IPv4, Internet Protocol version 4 IPv6, Internet Protocol version 6 OSPF, Open Shortest Path First IGRP, Interior Gateway Routing Protocol ICMP, Internet Control Message Protocol The layer 3 is tasked with packet delivery to the next hop. Each packet is sent/forwarded on its own merits and run in a connectionless environment. In many cases the final destination is not adjacent to this machine so the packets are routed based on the local routing table information. Internet Protocol \u00b6 The addressing function examines the address on the incoming packet: If the address indicates the datagram is for the local system then the headers are removed and the datagram is passed up to the next layer in the protocol stack. If the address indicates the datagram is for another machine then it is passed to the next system in the direction of the final destination. The fragmentation component will split and re-assemble the packets if the path to the next system uses a smaller transmission unit size. IP Address \u00b6 IP addresses have two parts: Network Part - indicated by the value of the netmask Host Part - the remaining bits after the host network component is removed They are distinguished by using a netmask , a bitmask defining which part of an IP address is the network part. IP networks can be broken into smaller pieces by using a subnet . As an example: parts address IP Addr Dec 10 20 0 100 Netmask Dec 255 255 0 0 IP Addr Bin 00001010 00010100 00000000 01100100 Netmask Bin 11111111 11111111 00000000 00000000 Subnet Bin 11111111 Network Part Bin 00001010 00010100 Subnet Part Bin 00000000 Host Part Bin 00000000 01100100 Originally, the IPv4 addresses were broken into the following three classes: Class A: 0.0.0.0/255.0.0.0 Class B: 128.0.0.0/255.255.0.0 Class C: 192.0.0.0/255.255.255.0 Original classes of networks and subnets did not scale well. Networks which did not fit in a class B were often given a class A. This led to IP addresses going to waste and the creation of CIDR (Classless Inter-Domain Routing) which uses a numbered bitmask instead of the class bitmask . Example CIDR Network Netmask: parts address CIDR /18 Dec 255 255 192 0 CIDR /18 Bin 11111111 11111111 11000000 00000000 IP Routing \u00b6 Routing begins when the adapter collects a Frame of data and passes it up the stack. In the first stage the Data Link (MAC) is examined to see if it matches the local machine\u2019s hardware address. When a match is made, the packet is examined for a match of the Destination IP Address. If a match of IP addresses is made, then the packet\u2019s destination is the local machine. The packet is then passed up to Layer 4 (Transport), for additional processing. If there is no IP address match then the datagram is forwarded to the next hop based on the local routing tables. The MAC address is updated to the next hop machine and the packet is passed along. Notice that the MAC address is modified to the next hop and the IP address is the final destination and is not usually modified. Network routers inspect packet headers, and make routing decisions based on the destination address. There are multiple routes the packets can take to reach the final destination and the routers involved make routing decisions based on path data for most direct path or fastest path. IPv4 and IPv6 \u00b6 IPv4 was the first major Internet Protocol version and most used protocol. Its 32bit address allows 4,294,967,296 possible addresses, which has exhausted on Jan. 31, 2011. Some solutions for mitigating the problem are: The move from Classed networks to Classless Inter-Domain Routing (CIDR) The invention of Network Address Translation (NAT) The move to IPv6 IPv6 is the successor to IPv4; it has 128bit address allows for 3.4 x 10^38 possible addresses. It was designed to deal with IPv4 exhaustion and other shortcomings: Expanded addresses capabilities Header format simplification Improved support for extensions and options Flow labeling capabilities IP Management Tools \u00b6 ip is part of the iproute2 package, the default tool for many distributions that manages layer 2 and 3 settings. NetworkManager is a daemon with D-bus for communication to applications and a robust API available to inspect network settings and operations. It has command line interface ( nmcli ), curses-based interface ( nmtui ) and graphical interface ( nm-connection-editor ) ifconfig is part of the net-tools package, not recommened as some new constructs created by ip are not visible to ifconfig . Network Types \u00b6 Local Area Network (LAN) form a network on its own Smaller, locally connected network. Connected at layer 2 by the same series of switches or hubs. Node to node communication happens at layer 3 using the same network. Layer 2 to layer 3 association may use layer 2 broadcasts and the ARP and RARP protocols to associate MAC addresses with IP addresses. Virtual Local Area Network (VLAN) is a method for combining two or more separated LANs to appear as the same LAN, or securing several LANs from each other on the same set of switches. VLANs are defined to the switch, and must configure the trunk port for switch-to-switch communication. A network bridge or repeater accepts packets on one side of the bridge and passes them through to the other side of the bridge, and it is bi-directional. It can be a hardware bridge or software bridge. It is a combination of two or more networks at layer 2. Bridged networks communicate as a single network . It is generally used to increase the length of the network connections or increase the number of connections by joining two LANs. Software bridges are present in Kernel and mostly noticeable when implementing VMs, containers or network namespaces. Several methods for configuring software bridges: iproute2, systemd-networkd, nmcli, and VM software Wide Area Networks (WAN) are the components that make the internet work. Typically a WAN is comprised of many individual LAN\u2019s connected together. The layer 2 (MAC address) contains the address of the \"gateway\" node. Once the \"gateway\" node receives the packet it determines if the packet is local or needs to be \"forwarded\" to the next \"gateway\". Transport Layer \u00b6 The Transport Layer is responsible for the end-to-end communication protocols. Data is properly multiplexed by defining source and destination port numbers. This layer deals with reliability by adding check sums, doing request repeats, and avoiding congestion. Common protocols in the Transport Layer include: Transmission Control Protocol (TCP), the main component of the TCP/IP (Internet Protocol Suite) stack. User Datagram Protocol (UDP), another popular component of the Internet Protocol Suite stack. Stream Control Transmission Protocol (SCTP) Transport Layer uses port numbers to allow for connection multiplexing. The port numbers are usually used in pairs, servers have fixed ports that they listen on and clients use a random port number for port number. The ports are classed in three different ways: Well-Known Ports 0-1023 - assigned by the Internet Assigned Numbers Authority (IANA), and usually require super-user privilege to be bound. Some of the well-known ports are: 22 TCP: SSH; 25 TCP: SMTP; 80 TCP: HTTP; 443 TCP: HTTPS . Registered Ports 1024-49151 - also assigned by the IANA. They can be bound on most systems by non-super-user privilege. Dynamic or Ephemeral Ports 49152-65535 - used as source ports for the client-side of a TCP or UDP connection. Can also be used for a temporary or non-root service. TCP, UDP, SCTP \u00b6 TCP is useful when data integrity, ordered delivery and reliability are important. UDP is useful when transmission speed is important and the integrity of the data isn\u2019t as important, or is managed by a higher layer. SCTP is an evolving protocol designed for efficient robust communication. Some of the features are still being sorted such as using SCTP through a NAT firewall. Characteristics TCP UDP SCTP Connection-oriented Yes No Yes Reliable Yes No Yes Ordered delivery Yes No Yes Checksums Yes Optional Yes Flow control Yes No Yes Congestion avoidance Yes No Yes NAT friendly Yes Yes Not yet ECC Yes Yes No Header size 20-60 bytes 8 bytes 12 bytes Session Layer \u00b6 Session Layer is used for establishing, managing, synchronizing and termination of application connections between local and remote application. If an established connection is lost or disrupted, this layer may try to recover the connection. If a connection is not used for a long time, the session layer may close and reopen it. There are two types of sessions: connection-mode service and connectionless-mode sessions. Session options: Simplex or duplex communication Transport Layer reliability Checkpoints of data units Session services may be involved in: Authentication for Transport Layer Setup and encryption initialization Support for steaming media Support for smtp,http and https protocols SOCKS proxy, Secure Sockets, TLS The Session Layer creates a semi-permanent connection which is then used for communications, many of the RPC-type protocols depend on this layer: NetBIOS: Network Basic Input Output System RPC: Remote Procedure Call PPTP: Point to Point Tunneling Protocol Presentation Layer \u00b6 The Presentation Layer is commonly rolled up into a different layer, Layer 5 and/or Layer 7. Some of the protocols that function at this level include: AFP, Apple filing protocol NCP, Netware Core protocol x25 PAD, x25 Packet Assembler/Disassembler Some of the services that may be available at the Presentation level are: Data Conversion (EBCDIC to ASCII) Compression Encryption/Decryption Serialization Application Layer \u00b6 The Application Layer is the top of the stack and deals with the protocols which make a global communications network function. Common protocols which exist in the Application Layer are: HTTP: Hypertext Transfer Protocol SMTP: Simple Mail Transfer Protocol DNS: Domain Name System FTP: File Transfer Protocol DHCP: Dynamic Host Configuration Protocol Manage System Services with systemd \u00b6 Common actions performed from systemctl utility: # Start the service systemctl start httpd.service # Stop the service systemctl stop httpd.service # Enable the service to auto start on system boot systemctl enable httpd.service # Disable or prohibit a service from auto starting systemctl disable httpd.service # Obtain the status of a service systemctl status httpd.service # reload systemd confs after making changes systemctl daemon-reload # restart a service after the reload (.service can be omitted) systemctl restart httpd # Run the service in foreground for debugging /usr/sbin/httpd -f /etc/httpd/httpd.conf # List the services (units) currently loaded systemctl # List the sockets in use by systemd launched services systemctl list-sockets # List the timers currently active systemctl list-timers # Set specific unit runtime values if supported; this will set properties used for resource control if enabled systemctl set-property foobar.service CPUWeight = 200 MemoryMax = 2G IPAccounting = yes # Display the status, state, configuration file(s) and last few log messages for this service systemctl status httpd.service # Find overridden configuration files and display the differences systemd-delta Some transient system services are not used enough to keep a daemon running full time. The xinet daemon was created to manage these transient daemons. Its default configuration file is /etc/xinetd.conf , with additional per-service files in /etc/xinetd.d/ . The sequence that systemd processes the configuration files is predictable and extensible. The common scan sequence is: The vendor or package supplied unit files in one of the following directories: /usr/lib/systemd/system/<service>.service or /lib/systemd/system/<service>.service Optional or dynamically created unit files in /run/systemd/system/ directory Optional user unit override files in /etc/systemd/system/ directory Optional user drop-in files in /etc/systemd/system/<service>.d (most popular drop-in directory) <service>.d might need to be manually created It is common practice to copy the vendor unit file into the /etc/systemd/system/ directory and make appropriate customizations. Often times it is desirable to add or change features by program or script control, the drop-in files are convenient for this. One item of caution, if one is changing a previously defined function (like ExecStart) it must be undefined first then added back in. With systemd , additional features and capabilities can be easily added. As an example, cgroups controls can be added to our service by adding a Slice directive in the Service block. Network Configuration \u00b6 Layer 2 Configuration \u00b6 There are two methods for examining/changing additional parameters for Layer 2: Kernel module tools modinfo and modprobe using /etc/modprobe.d/ , with suffix in .conf or .local udev using the drop-in directory /etc/udev/rules.d/ The udev (user device facility) is used to manage network hardware interfaces for the Linux kernel. Sometimes changes are needed to rename interfaces or change configurations to match hardware MAC addresses. When rules are processed, all the files in the rules directories are combined and sorted in a lexical order. The udevadm command is used for control, query and debugging udev configuration files. You can see an example rule to rename a network interface: cat /etc/udev/rules.d/70-persistent-net.rules SUBSYSTEM == \"net\" ,ACTION == \"add\" ,ATTR { address }== \"52:54:42:42:00:01\" ,NAME = \"net%n\" The operations required are: List optional parameters Set optional parameters Verify additional parameters Block or allow hardware modules from loading Sometimes it becomes necessary to inhibit the loading of a hardware module. The syntax is blacklist MODULENAME Example of changing layer 2 configuration: # change the maximum transmission unit (MTU): ip link set mtu 1492 dev eth0 # change the link speed: ethtool -s eth0 speed 1000 duplex full # check the link status ip -s link ip -s link show dev eth0 # check the network interface driver ethtool -i eth0 # check driver module info to understand optional parameters to the module modinfo e1000 udevadm info -a /sys/class/net/ens9 # add parameters in /etc/modprobe.d/mynic.conf: options e1000 Speed = 100 Duplex = 0 AutoNeg = 0 Layer 3 Configuration \u00b6 Examples of changing layer 3 configuration # manually set a network (Layer 3) address: ip addr add 10 .0.2.25/255.255.255.0 dev eth0 # manually set or change the default route: ip route add default via 10 .0.2.2 route add default gw 10 .0.2.2 # add the address of a DNS server, use: echo \"nameserver 4.2.2.1\" >> /etc/resolv.conf # manually request a DHCP configuration, use the dhclient command: dhclient eth0 Boot Time Configuration \u00b6 Network settings are stored in configuration files which allow for persistent configuration across reboots. Network-Manager is an example of a common configuration tool with several interfaces ( nmcli ). The systemd-network service relies on text file configuration and has no text or GUI menu application ( networkctl ). netplan is a a Ubuntu-specific tool that creates a network configuration at runtime from a pre-defined yaml file. It can dynamically create either a Network Manager or a systemd-networkd based configuration. Network Manager \u00b6 Network Manager is used by Ubuntu, CenOS, SUSE, and Debian, which provides: GUI tool ( nm-connection-editor ) applet ( nm-applet ) text interface ( nmtui ) CLI interface ( nmcli ) The current release of Network Manager will automatically configure and start a network interface with a Dynamic Host Configuration Protocol (DHCP) if there is no network configuration file for the adapter. If there is a configuration file, Network Manager will set the device to an \"unmanaged\" state and initialize the adapter with the attributes of the configuration file. Network Manager can manage Hardware: Bluetooth, DSL/PPPoE, Ethernet, InfiniBand, Mobile Broadband, Wi-Fi Virtual: Bond, Bridge, IP Tunnel, MACsec, Team, Vlan, Wireguard The network configuration files on an Ubuntu system typically reside in the /etc/network directory, with the interface configuration file being /etc/network/interfaces . The hostname config file is /etc/hostname . The network configuration files on a CentOS system are located in the /etc/sysconfig/network-scripts directory and match the ifcfg-<interface> pattern. The DNS client settings are managed by editing the /etc/resolv.conf file. Beware that some of the network adapters may be excluded from Network Manager and will have to be re-added to allow Network Manager to control the interfaces. VPN \u00b6 Virtual Private Networks (VPNs) provide a secure connection for remote users through unsecured networks. Data is encrypted to avoid unwanted exposure. Initialization of the connection usually has multi-factor authentication for additional security. There are VPN types provided by many different protocols: Secure Transport Layer (SSL/TLS) Internet Protocol Security (IPSEC) Datagram Transport Layer (DTLS) Secure Shell (SSH) VPN. One of the more popular VPN tools is OpenVPN , which provides SSL/TLS-based VPN connectivity. OpenVPN is a single binary for both the server and the client, and is a command-line tool. Network Troubleshoot and Monitor \u00b6 Client side Troubleshoot \u00b6 Some common networking issues found at the client side include DNS issues, Firewall settings, or incorrect network settings (routes, netmasks). The basics of network troubleshooting usually deal with connectivity testing. You can use the tools ping, traceroute, and nmap Use ping for checking connectivity. ping uses the ICMP protocol. Test the IP address to your network adapter, gateway, and DNS. Use the DNS name for domain name resolution. Use traceroute or mtr which shows the connectivity path to the destination. mtr show statistics of the connection and packets drops/failures. Use nmap which scans the server to see if the required ports are available. Use telnet to test plain-text protocols, such as http. i.e. telnet example.com 80 . Use openssl to test SSL or TLS protocols. i.e. openssl s_client -connect www.google.com:443 Use arp to check link-layer connectivity. Advanced troubleshooting involves using tcpdump and wireshark . The command line-based tcpdump truncates packets by default and generates pcap files. wireshark uses the graphical interface to capture packets. It can capture and analyze packets in real time. It is useful to analyze pcap files, but you may not want wireshark installed on the system you are troubleshooting. To capture packets with tcpdump for use with wireshark, do sudo tcpdump -i eth0 -s 65535 -w capture.pcap port 22 Server side Troubleshoot \u00b6 Common server problems include broken DNS, overzealous firewall rules, incorrect network settings, and the daemon not listening on the right interface/port. Some protocols break when return traffic comes back from a different IP address. Verify that your egress route is correct. Some access control systems require that Reverse DNS be properly set up. Perform basic server troubleshooting: test the network connectivity from the server's point of view. One of the first steps in troubleshooting a server-side daemon should be to check the log files, and verify the daemon/service is running. Use netstat to list the ports that daemons listen on, i.e. sudo netstat -taupe | grep httpd The ss command is another socket statistics utility. It may be a replacement to netstat although it is missing some socket types. Usage: sudo ss -ltp | grep httpd . Server side firewall configuration needs to take into account that it allows certain inbound and outbound traffic. Also check the settings of tools such as TCP wrappers ( /etc/hosts.allow and /etc/hosts.deny ). Consult man 5 hosts_access for additional details. For advanced server troubleshooting, the /proc filesystem has settings that affect the network stack: /proc/sys/net/ipv4/ip_forward - Allows for network traffic to be forwarded from one interface to another. /proc/sys/net/ipv4/conf/*/accept_redirects - Accepting Internet Control Message Protocol (ICMP) redirects from a router to find better routes. This setting has the potential to be exploited by a malicious party to redirect your traffic. /proc/sys/net/ipv4/icmp_echo_ignore_all - Changing this setting will affect the host's visibility to ICMP ping packets. /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts - This setting will change the host's visibility to broadcast ICMP ping packets. /proc/net/arp - Contains the current arp table. These settings are not persistent across reboots. To persistently enable changes you must use the sysctl command with its configuration file /etc/sysctl.conf . The syntax for /etc/sysctl.conf matches the path for the file in /proc/sys with the . character instead of / . netcat is a TCP and UDP sender and listener. To test a network connection, use netcat to open a listening port on one system with netcat -l 4242 , cause it to listen on all adapters port 4242 for traffic. On another machine, use netcat <listener_ip_address> 4242 to open a connection to the listener, then send input which should appear on the listener side, the communication is bi-directional. netcat can save many hours of frustration by proving TCP and UDP traffic can transverse the network. Network Monitoring \u00b6 The iptraf tool is a real-time network traffic analyzer. It recognizes protocols: IP, TCP, UDP, ICMP, IGMP, IGP, IGRP, OSPF, ARP, RARP . snort is a network intrusion detection system. In addition to being a network monitor, it can help pinpoint unwanted traffic inside of a network. ntop is an application and web app for monitoring network usage. It can pinpoint bandwidth use, display network statistics, and more. tcpdump has been around for a long time, it is text-based, small and efficient. wireshark , graphical tracer with protocol decode that allows the user to view more or less data depending on the requirement. It can read tcpdump output files, which allows the collection of trace data with the efficient tcpdump on prod system and then use wireshark to display the information for analysis. Remote Access \u00b6 Cryptography \u00b6 Cryptography is about securing communications. Symmetric encryption uses a single secret shared key , which both parties must have to communicate. Plain text encrypted with a symmetric encryption method can easily be turned back into plain text using the same key. Example Caesar cipher One benefit of symmetric encryption is that it is less computationally-intensive than asymmetric encryption. One downside of symmetric encryption is that a secure shared key exchange is difficult to attain. Asymmetric encryption uses mathematically-related public and private keys to communicate. Plain text encrypted with an asymmetric public key can only be decrypted by using the corresponding private key , not the public key. Asymmetric encryption has no key-exchange problem and uses the published public key to send secure messages. However, it is more computationally-intensive than symmetric encryption, and needs proper verification on the shared public key. By using the asymmetric encryption to pass the symmetric key, you can overcome the problems associated with both. Party One creates a session key using a symmetric algorithm. Party One then encrypts the session key, using the public key of Party Two, and sends the encrypted session key to Party Two. Party Two uses their private key to decrypt the session key. Both parties now communicate using the symmetric session key for encryption. Secure Shell \u00b6 telnet is one of the earlier protocols developed for the Internet back in 1969 and was not built with security in mind, the protocol is sent over the wire in plain text. Remote Shell rsh was originally written for the BSD (Berkeley Software Distribution) system in 1983. rsh is a similar system to telnet and is an insecure protocol, which is not encrypted and sends data in clear text. The Secure Shell ( ssh ) protocol was developed to overcome the security concerns of telnet and rsh . OpenSSH is the most widely used version of SSH and is built upon the concepts of symmetric and asymmetric encryption. OpenSSH Architecture Layers: transport layer - deals with the initial key-exchange and setting up a symmetric-key session and establish the connection layer. user auth layer - deals with authenticating and authorizing the user accounts. connection layer - deals with the communication once the session is set up. The OpenSSH host-wide client configuration is /etc/ssh/ssh_config . The per-user client configuration is each user's $HOME/.ssh/config . SSH uses a key-based authentication. Syntax can be found with man 5 ssh_config Other protocols can be tunneled over SSH. The X11 protocol support is part of the OpenSSH client. You can also manually open a connection for any other protocol using the LocalForward and RemoteForward tokens in the OpenSSH client configuration. Client SSH config \u00b6 $HOME/.ssh/config can be set up with shortcuts to servers you frequently access. As an example, you can do ssh web rather than typing ssh webusr@www.example.com : Host web HostName www.example.com User webusr More advanced configuration: Host web KeepAlive yes IdentityFile ~/.ssh/web_id_rsa HostName www.example.com Port 2222 User webusr ForwardX11 no Host * Port 22 OpenSSH client key-based authentication provides a passwordless authentication for users. Private keys can be encrypted and password protected. ssh-agent program can cache decrypted private keys. ssh-copy-id program can copy your public key to a remote host. # To generate a user key for SSH authentication, use: $ ssh-keygen -f $HOME /.ssh/id_rsa -N 'supersecret' -t rsa # To start ssh-agent and use it to cache your private key, use: $ eval $( ssh-agent ) $ ssh-add $HOME /.ssh/id_rsa # To copy your public key to the remote system overthere for remote user joe, use: $ ssh-copy-id joe@overthere OpenSSH Tunnel \u00b6 The local tunnel ( ssh -L ) indicates which port is to be opened on the local host (4242) and the final destination to be (charlie:2200), and the connection to the final destination is made by machine (bob). The remote tunnel ( ssh -R ) requests machine (bob) to open a listening port (2424) to which any connection will be transferred to the destination, (charlie:2200). There is also dynamic port forwarding using ssh -B . Option -N sets the option to not execute a command on connection to the remote system, and option -f informs ssh to go into background just before command execution. Parallel SSH Commands \u00b6 The pssh package is available for execute the same command on many systems, which includes: pssh: parallel ssh pnuke: parallel process kill prsync: parallel copy program using rsync pscp: parallel copy using scp pslurp: parallel copy from hosts The pssh command and friends use the existing ssh configuration. It is best to configure aliases, keys, known hosts and authorized keys prior to attempting to use pssh. If there is a password or fingerprint prompt, the pssh command will FAIL. When using pssh, it is convenient to create a file with a list of the hosts you wish to access. The list can contain IP addresses or hostnames. $ cat ~/ips.txt 127 .0.0.1 192 .168.42.1 $ pssh -i -h ~/ips.txt date [ 1 ] 10 :07:35 [ SUCCESS ] 120 .0.0.1 Thu Sep 28 10 :07:35 CDT 2017 [ 2 ] 10 :07:35 [ SUCCESS ] 192 .168.42.1 Thu Sep 28 10 :07:35 CDT 2017 VNC Server \u00b6 The Virtual Network Computing (VNC) server allows for cross-platform, graphical remote access. The most common implementation is tigervnc client and server. The server component has Xvnc (the main server for VNC and X), vncserver (Perl script to control Xvnc), vncpasswd (set and change vnc-only password), and vncconfig (configure and control a running Xvnc). When the server starts, it uses the xstartup configuration from the users ~/.vnc directory. If the xstartup file is altered, the vncserver needs to be restarted. VNC is a display-based protocol, which makes it cross-platform. It also means that it is a relatively heavy protocol, as pixel updates have to be sent over-the-wire. The client, vncviewer , is usually packaged separately. It connects to the VNC server on the specified port or display number. Passwords are not sent in clear text. On its own, VNC is not secure after the authentication step. However, the protocol can be tunneled through SSH or VPN connections. X Window System \u00b6 The X Window system was developed as part of Project Athena at MIT in 1984. This simple network-transparent GUI system provides basic GUI primitives and is network transparent, allowing for ease of use. When it comes to X authentication, the client-server security is done using keys. To secure the X protocol, it must be tunneled with VPN or SSH. OpenSSH supports X11 tunneling via -X option. Domain Name Service \u00b6 Before DNS , there was ARPANET . The original solution to a name service was a flat text file called HOSTS.TXT. This file was hosted on a single machine, and, when you wanted to get a copy, you pulled it from this central server using File Transfer Protocol (FTP) or a similar protocol. A descendant of the HOSTS.TXT file is the /etc/hosts file. It has a very simple syntax: <IP ADDRESS> <HOSTNAME> [HOSTNAME or alias] ... . This hosts file usually takes precedence over other resolution methods. The Domain Name System (DNS) is a distributed, hierarchical database for converting DNS names into IP addresses. The DNS protocol runs in two modes: recursive with caching, authoritative When a network node makes a DNS query, it most often makes that query against a recursive, caching server. That recursive, caching server will then make a recursive query through the DNS database, until it comes to an authoritative server. The authoritative server will then send the answer for the query. The DNS database consists of a tree-like, key-value store. The database is broken into tree nodes called Domains. These domains are managed as part of a zone. Zones are the area of the namespace managed by authoritative server(s). DNS delegation is done on zone boundaries. The Caching Server most likely have already cached those frequently accessed top domains so this process would usually be fast. Query/Record Types \u00b6 A Record - Address Mapping Records, a 32bit IPv4 address. AAAA Record - IP Version 6 Address Records, a 128big IPv6 address. CNAME - Canonical Name Records, an alias to another name MX - Mail Exchanger Records, the message transfer agents (mail servers) for a domain. NS - Nameserver Records, delegate an authoritative DNS zone nameserver. PTR - Reverse-Lookup Pointer Records, pointer to a canonical name (IP address to name). SOA - Start of Authority Records, Start of Authority for a domain (domain and zone settings). TXT - Text Records, arbitrary human-readable text, or machine-readable data for specific purposes. More about DNS DNS Queries \u00b6 Forward DNS queries use A or AAAA record types and are most often used to turn a DNS name into an IP address. A Fully Qualified Domain Name (FQDN) is the full DNS address in the DNS database, and the most significant part is first. A reverse DNS query is used to turn an IP address into a DNS name. It uses a PTR record type and an arpa. domain in a DNS database. In an IP address, the most significant part is on the right; we have to translate an IP address to put it into the DNS database. i.e. 192.168.13.32 becomes 32.13.168.192.in-addr.arpa. ; 2001:500:88:200::10 becomes 0.1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.2.0.8.8.0.0.0.0.5.0.1.0.0.2.ip6.arpa. See popular DNS servers . DNS Client \u00b6 Network configuration is more dynamic and the nameserver entries need to be adjusted dynamically. In most distros, the nameserver information may be added to the interface configuration file, overwriting or modifying the /etc/resolv.conf when the interface is started. The DHCP Server often provides nameserver information as part of the information sent to the DHCP client, replacing the existing nameserver records. resolvconf service uses additional files like /etc/resolvconf.conf and a background service resolvconf.service to \"optimize\" the contents of /etc/resolv.conf. dnsmasq sets up in \"mini\" caching DNS server and may alter the resolver configuration to look at dnsmasq instead of the items listed in /etc/resolv.conf. systemd.resolved provides a DNS stub listener on IP address 127.0.0.53 on the loopback adapter and takes input from several files including: /etc/systemd/resolved.conf, /etc/systemd/network/*.network and any DNS information made available by other services like \"dnsmasq\". BIND \u00b6 BIND , Berkeley Internet Name Domain, is a widely-used, ISC standard DNS Internet software available for most UNIX-type systems. Its configuration file is /etc/named.conf or /etc/bind/named.conf , syntax specified in man 5 named.conf . On CenOS or SUSE distro, its package is bind and the service is named ; on Ubuntu, its package is bind9 and the service is also bind9 . Authoritative zones are defined and must contain an SOA (Start of Authority) record. Zone files should contain an NS (nameserver) record. Some of the syntax considerations include: The record format is: \"{name} {ttl} {class} {type} {data}\" Always use trailing \".\" on all fully-qualified domain names @ special character GENERATE syntax Comment marker - ; to end of line. The SOA (Start of Authority) is required for every zone. Special control fields include: Admin email Primary name server Serial number Timers (secondary server refresh settings) Refresh: How often to check for new serial from primary. Retry: How often to retry if no response from primary. Expire: How long to keep returning authoritative answers when we cannot reach the primary server. Negative TTL: How long to cache an NX domain answer. A DNS view (aka split horizon) will cause the DNS server to respond with different data depending on the match criteria, such as source IP address: it can provide different DNS answers to requests depending on selection criteria uses multiple zone files with different data for the same zone can provide DNS services inside a corporation using private or non-routable addresses.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b DNS Tools \u00b6 Tools for DNS testing are in three categories: server configuration testing, server information testing, and server update. DNS client tools formulate a standard DNS request and send it off to the default or named DNS server. dig (domain information groper) queries DNS for for domain name or ip address mapping, and output format resembles the records used by the DNS server. host is a simple interface for DNS queries, good for use in scripts nslookup queries DNS for domain name or IP address mapping and less verbose than dig nsupdate end updates to a name server and requires authentication and permission HTTP Servers \u00b6 Apache used to be the lead technology of all active web servers. The location of the Apache configuration files move from distribution to distribution, and so is its systemd.service name and package name. # On RedHat, CentOS, or Fedora: Package: httpd Service: httpd Primary configuration file: /etc/httpd/conf/httpd.conf # On OpenSUSE: Package: apache2 Service: apache2 Primary configuration file: /etc/apache2/httpd.conf # On Debian, Ubuntu, or Linux Mint: Package: apache2 Service: apache2 Primary configuration file: /etc/apache2/apache2.conf Apache allows include other files and directories from the primary configuration files, just like a drop-in configuration file. The default include directories are: # CentOS: /etc/httpd/conf.d/*.conf # OpenSUSE: /etc/apache2/conf.d/ /etc/apache2/* # Ubuntu: /etc/apache2/*-enabled /etc/apache2/*-available/ Other important files include the document root ( /var/www/html/, /srv/www/htdocs ), log file locations ( /var/log/httpd, /var/log/apache2 ), and module locations. Logs \u00b6 To create custom logs on an Apache server, you must first define a custom log format: LogFormat \"example-custom-fmt %h %l %u %t \"%r\" %>s %b\" example-custom-fmt Variable Explanation %h Remote host name %l Remote login name \u200b%u Remote user %t \u200bTime of request %r First line of request \u200b%s Status %b Size of response More tokens reference is here apache mod_log_config mod_userdir \u00b6 The mod_userdir module is used to allow all or some users to share a part of their home directory via the web server, even without access to the main document root. The URIs will look something like http://example.com/~user/index.html and will commonly be placed in the /home/user/public_html/ directory. IP/Port Virtual Hosts \u00b6 For multiple web sites using multiple addresses/ports, use VirtualHost stanzas, and a unique IP address and port pair. Ensure all of the IP addresses and ports are defined in a Listen directive, and add a stanza for each virtual host. i.e. Listen 192.168.42.11:4374 <VirtualHost 192.168.42.11:4374> ServerAdmin webmaster@host1.example.com DocumentRoot /www/docs/host1.example.com ServerName host1.example.com ErrorLog logs/host1.example.com-error_log CustomLog logs/host1.example.com-access_log common </VirtualHost> To enable a name-based virtual host for an IP/Port, create a VirtualHost stanza and modify the DocumentRoot and ServerName directives. Host names which are not defined in a VirtualHost stanza which match the IP address and port will be served by the first VirtualHost stanza. Name-based virtual hosts have some SSL limitations. Due to the way SSL works, the server has no way of knowing which host name is being sent by the client before the SSL session is started. The server cannot send the proper certificate back to the client without the proper host name. This has been worked around by using Server Name Indication (SNI). Access Control \u00b6 Password-protected directories are protected via SSL. Use htpasswd to create new passwords. See man 1 htpasswd . Per-User Access Control is activated using AllowOverride (.htaccess file). i.e. <Location /secure/> AuthType Basic AuthName \"Restricted Files\" AuthUserFile secure.passwords Require valid-user </Location> <Directory /var/www/html/get-only/> <LimitExcept GET> Require valid-user </LimitExcept> </Directory> Filesystem permissions must allow access for the user the Apache httpd daemon is running under ( apache on most systems). A properly configured SELinux system can increase the security of an Apache web server. There are some settings you may need to change: To allow Apache to make outbound network connections (not directly related to serving a request), modify the httpd_can_network_connect boolean. The default Document Root and CGI root have the proper SELinux context. If you serve files from outside those locations, you need to use the chcon command. To enable the userdir module, modify the httpd_read_user_content boolean. To serve files from an NFS filesystem, modify the httpd_use_nfs boolean. Secure Sockets Layer \u00b6 While HTTP is a clear text protocol, Secure Sockets Layer (SSL) is a port-based vhost. Types of SSL certificates include: Self-signed: for hobby or testing use. Certificate Authority Signed (Certificate Signing Request or CSR)\u200b. # generate a private key $ openssl genrsa -aes128 2048 > server.key # generate a CSR $ openssl req -new -key server.key -out server.csr # generate a self-signed certificate $ openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt # remove the encryption on private key (not recommended) $ openssl rsa -in server.key -out server.key.unlocked To install the keys, change the configuration file ( ssl.config ) or place them # On CentOS: /etc/pki/tls/certs/localhost.crt: Signed Certificate /etc/pki/tls/private/localhost.key: Private Key # On OpenSUSE: /etc/apache2/ssl.crt/server.crt: Signed Certificate /etc/apache2/ssl.key/server.key: Private Key # On Ubuntu: /etc/ssl/certs/ssl-cert-snakeoil.pem: Signed Certificate /etc/ssl/private/ssl-cert-snakeoil.key: Private Key Rewriting rules with mod_rewrite \u00b6 URL rewriting is a powerful way to modify the request that an Apache process receives. Modifications can include moved documents, forcing SSL, forcing login, human-readable URIs, and redirecting based on the browser. Rewrite rules can exist in the .htaccess files, the main configuration file , or <Directory> stanzas. The .htaccess files seem the most common, but the main configuration file may be more secure. The mod_rewrite uses PCRE (Perl Compatible Regular Expressions). This provides nearly unlimited conditions or filters to include, like: Time of day Environment variables URLs or query strings HTTP headers External scripts Use a rewrite map for advanced or dynamic rewrite rules, which is useful when rewrite rules are too complex or numerous.. Pre-compiled rewrite maps exist for specific purposes, i.e. WURFL . There are many different types of maps: txt: Flat text dbm: DBM hash (indexed) rnd: Randomized plain text prg: External program Some rewriting examples # turn on the rewrite engine RewriteEngine on # force a redirect to a new location for a document RewriteRule \u02c6/old.html$ new.html [ R ] # rewrite a URI based on the web browser (without a redirect), use: RewriteCond % { HTTP_USER_AGENT } .*Chrome.* RewriteRule \u02c6/foo.html$ /chrome.html [ L ] # create a human-readable URI from a CGI script (the PT or Passthrough flag: without it, the rewrite is treated as a file, not a URI) RewriteRule \u02c6/script/ ( .* ) /cgi-bin/script.cgi? $1 [ L,PT ] mod_alias \u00b6 The mod_alias maps URIs to filesystem paths outside of normal DocumentRoot . It also provides Alias and Redirect directives, Apache access control considerations, file permission considerations, and SELinux considerations. mod_alias is much simpler than mod_rewrite . A redirect directive sends a redirect that is similar to the [R] flag for mod_rewrite . AliasMatch allows use of regular expressions. Example: # serve the files located at /newdocroot/ under the URI /new/ (the <Directory> stanza for access control modifications) Alias /new/ /newdocroot/ <Directory /newdocroot/> Options Indexes FollowSymLinks AllowOverride None Require all granted </Directory> # serve the files located at /newdocroot/<SUBDIR> under the URI /new/<SUBDIR> with different permissions for each AliasMatch \\\u02c6 {} /new/ ( .* ) $ /newdocroot/ $1 <Directory /newdocroot/foo/> Options Indexes FollowSymLinks AllowOverride None Require all granted </Directory> <Directory /newdocroot/bar/> Options -Indexes FollowSymLinks Require all granted </Directory> There are two ways to run scripts outside your document root: Create an Alias and set the handler to cgi-script . Use ScriptAlias (does the above automatically). Because of the nature of Common Gateway Interface (CGI) scripts, they should not be directly served as plain files and should not be in your document root. Example ScriptAlias /newscripts/ /newscripts-docroot/ <Directory /newscripts-docroot/> Options Indexes FollowSymLinks AllowOverride None Require all granted </Directory> mod_status, mod_include, mod_perl \u00b6 mod_status provides an endpoint to show characteristics: Apache server status Human-readable page Machine-readable page Security considerations with access control and extended status mod_include provides filtering before a response is sent to the client. It is enabled with the +Includes directive. By default, this is enabled only for *.shtml files, but it can be enabled for all *.html files with the XBitHack directive, and execute permission should be set on the file. mod_perl is a Perl interpreter embedded in Apache. It can provide full access to the entire Apache API, sharing information between Apache processes. CGI scripts can run unmodified. However, some CPAN (Comprehensive Perl Archive Network) modules are not thread-safe, therefore they do not work with the worker MPM (Multi-Processing Module). Some Perl methods like die() and exit() can cause performance issues. Example: <Location /status> SetHandler server-status Require ip 10 .100.0.0/24 </Location> <Location /dynamic/> Options +Includes XBitHack on </Location> # An HTML file which had includes would contain: <!--#set var = \"dude\" value = \"yes\" --> <!--#echo var = \"dude\" --> <!--#include virtual = \"http://localhost/includes/foo.html\" --> Alias /perl /var/www/perl <Directory /var/www/perl> SetHandler perl-script PerlResponseHandler ModPerl::Registry PerlOptions +ParseHeaders Options +ExecCGI </Directory> Multi-Processing Modules (MPMs) \u00b6 The Prefork MPM is the default MPM, non-threaded, safe for non-thread-safe scripts and libraries. It uses more memory and resources to handle the same number of connections compared to others. The Worker MPM is a hybrid multi-process, multi-threaded model. Each sub-process spawns threads which handle the incoming connections and perform better from the lighter threads than processes. The Event MPM is based on the Worker MPM but it off-loads some of the request handling to shared processes and further boosts performance. Some Prefork configurations: StartServers - number of child server processes to create on startup MinSpareServers - min number of child server processes to keep on hand to serve incoming requests MaxSpareServers - max number of child server processes to keep on hand to serve incoming requests MaxRequestsPerChild - max number of requests a child server process serves MaxClients - max number of simultaneous connections to serve. May also have to increase the ServerLimit directive Some Worker configurations in addition to the Prefork: MinSpareThreads - min number of worker threads to keep on hand to serve incoming requests MaxSpareThreads - max number of worker threads to keep on hand to serve incoming requests ThreadsPerChild - number of worker threads in each child server process Load Test \u00b6 The best testing results come from using URIs which match the real-life workflow. Apachebench ( ab ) is the tool provided as a part of the Apache httpd server package. Httperf ( httperf ) can use log files as a source of URIs to load test. Load Balance \u00b6 Apache load balancer requires the modules mod_proxy (along with its complementary modules) and mod_proxy_balancer which have specialized modules for the balancing strategy: mod_libmethod_byrequests (request counting) mod_libmethod_traffic (weighted traffic counting) mod_libmethod_bybusyness (pending request counting) mod_libmethod_heartbeat (heartbeat traffic counting) A minimal configuration as an example: <VirtualHost *:80> ProxyRequests off ServerName www.somedomain.com <Proxy balancer://mycluster> BalancerMember ht\u200ctp://10.176.42.144:80 #minion1 BalancerMember ht\u200ctp://10.176.42.148:80 #minion2 BalancerMember ht\u200ctp://10.176.42.150:80 #minion3 Require all granted # all requests are allowed ProxySet lbmethod = byrequests # balancer setting, round-robin </Proxy> ProxyPass / balancer://mycluster/ </VirtualHost> Cacheing and Proxy \u00b6 Apache can act as a forward or reverse proxy that is managed by mod_proxy and mod_proxy_http . However, there are special-purpose tools which do caching much more efficiently: Varnish: A RAM and disk-based cache which uses the Linux kernel's swapping mechanism to speed up caching. Squid: A general-purpose caching proxy. # enable a reverse proxy to an internal server <Location /foo> ProxyPass ht\u200ctp://appserver1.example.com/foo </Location> # alternative syntax ProxyPass /foo ht\u200ctp://appserver1.example.com/foo C10k Problem and Speciality HTTP Servers \u00b6 The C10k Problem describes the issues in making a web server which can scale to ten thousand concurrent connections/requests. Specialty HTTP servers have been developed to deal with different issues. cherokee - innovative web-based configuration panel, very fast in serving dynamic and static content nginx - has a small resource footprint, scales well from small servers to high performance web servers lighttpd - power some high-profile sites, light-weight and scales well Email Servers \u00b6 Email programs and daemons have multiple roles and utilize various protocols. Mail User Agent (MUA) is the main role of email client for composing and reading emails. It uses Internet Message Access Protocol (IMAP) or Post Office Protocol (POP3). Mail Submission Program (MSP) is the role of email client responsible for sending mails through the Mail Transfer Agent (MTA) using SMTP. Mail Transfer Agent (MTA) is the main role of email server, responsible for starting the process of sending the message to the recipient by looking up the recipient and sending the message to their MTA using SMTP. Mail Delivery Agent (MDA) is the role of email server for receiving and storing email for future retrieval. MTA uses SMTP, Local Mail Transfer Protocl (LMTP) or other protocols to transfer message to MDA. SMTP is a TCP/IP protocol used as an Internet standard for electronic mail transmission. It uses a plain \"English\" syntax such as HELO, MAIL, RCPT, DATA, or QUIT . SMTP is easily tested using telnet . POP3 is one of the main protocols used by MUAs to fetch mail. By default, the protocol downloads the messages and deletes them from the server. It is simpler yet less flexible protocol. IMAP is the other main protocol used by MUA to fetch mail. Messages are managed on the server and left there. Copies are downloaded to the MUA. This protocol is more complex and more flexible than POP3. Email lifecycle \u00b6 The email life cycle looks like this: You compose an email using your MUA. Your MUA connects to your outbound MTA via SMTP, and sends the message to be delivered. Your outbound MTA connects to the inbound MTA of the recipient via SMTP, and sends the message along. (Note: This step can happen more than once). Once the message gets to the final destination MTA, it is delivered to the MDA. This can happen over SMTP, LMTP or other protocols. The MDA stores the message (on disk as a file, or in a database, etc). The recipient connects (via IMAP, POP3 or a similar protocol) to their email server, and fetches the message. The IMAP or POP daemon fetches the message out of the storage and sends it to the MUA. The message is then read by the recipient. Postfix \u00b6 Postfix has components that does MTA and MDA. In postfix /etc/postfix/main.cf contains location information for the alias database; /etc/aliases ( newaliases command) for system-wide redirection of mail, ~/.forward for user-configurable redirection of mail. /etc/aliases generally stores data in a .bdm or hash format, with format name : val1, val2, val3 ... The postconf command can be used to customize main.cf . Some usual candidates for customization in main.cf : The domain name to use for outbound mail (myorigin). The domains to receive mail for (mydestination). The clients to allow relaying of mail (mynetworks). The destinations to relay mail to (relay_domains). The delivery method, indirect or direct (relayhost). Trust hosts for blindly forward email (mynetworkds_style). IP addresses to listen on for incoming connections (inet_interfaces). Within Postfix there is a process called master that controls all other Postfix processes that are involved in moving mail. The defaults in the /etc/postfix/master.cf file are usually sufficient. SMTP is a clear-text protocol, security is a concern; postfix by default has no SSL/TLS encryption and no spam filtering. Postfix does not provide a Simple Authentication and Security Layer (SASL) mechanism itself and relies on proper set up. Postfix daemon support the Cyrus SASL or the Dovecot SASL mechanisms. smtpd_sasl_type = dovecot smtpd_sasl_path = private/auth smtpd_sasl_auth_enable = yes broken_sasl_auth_clients = yes Postfix SASL authentication with Dovecot can be enabled over a UNIX Domain Socket and over TCP. To enable UNIX accounts to authenticate over a UNIX Domain Socket, edit the /etc/dovecot/conf.d/10-master.conf file: service auth { unix_listener auth-userdb { } unix_listener /var/spool/postfix/private/auth { } } When testing plain-text SASL authentication, you will need to submit a base64 encoded username and password, i.e. echo -en \"\\0user\\0password\" | base64 , then test SMTP auth with AUTH PLAIN <hash_string> To enable TLS encryption: smtpd_tls_cert_file = /etc/postfix/server.pem smtpd_tls_key_file = $smtpd_tls_cert_file smtpd_tls_security_level = may smtpd_tls_auth_only = yes Some ways to monitor postfix: pflogsumm - a Perl script which translates Postfix log files into a human-readable summary. qshape - prints the domains and ages of items in the Postfix queue, useful for determining where emails are getting stuck in queues. The output of the command displays the distribution of the items in the Postfix queues by recipient or sender domain. mailgraph - creates RRD graphics of mail logs and allows monitor trends in your email server. SpamAssassin can be used as a milter (mail filter) or a standalone MDA. Other tools exist like Sender Policy Framework, DomainKeys. Dovecot \u00b6 Dovecot is an open-source IMAP/POP3 server. Dovecot is secure, easy to configure and is standards compliant. The doveconf utility parses the configuration file ( /etc/dovecot/dovecot.conf, /etc/dovecot/conf.d/ ) for both the Dovecot daemons and for debugging purposes. The common Dovecot setup binds to a specific IP address, defines the protocols to serve, applies password restrictions, accepts some clients with minor protocol issues, and points to user and password databases: listen = 10 .20.34.111 protocols = imap pop3 lmtp disable_plaintext_auth = yes imap_client_workarounds pop3_client_workarounds # SSL/TSL config in /etc/dovecot/conf.d/10-ssl.conf ssl = yes ssl_cert = < cert-file > # suggested permissions root:root 0444 ssl_key = < private-key-file > # suggested permissions root:root 0400 A working email client is the easiest way to troubleshoot and test an IMAP or POP3 server. The mutt email client works well. File Sharing \u00b6 The File Transfer Protocol (FTP) is one of the first protocols of the Internet. Data is sent in plain text. Clients have interactive or non-interactive modes. FTP Active transfer mode let server push data to the client on a random high-numbered port. This method is not compatible with firewalls or NAT networks. FTP Passive transfer mode let client request a passive connection and the server opens a random high-numbered port for data transfer. vsftpd \u00b6 Very Secure FTP Daemon ( vsftpd ) has enhanced security features than FTP server: Virtual IPs Virtual users Stand-alone daemon or inetd-ready Per-user configuration Per-source configuration Optional SSL integration Verify that anonymous access is enabled in vsftpd.conf : anonymous_enable=YES . To allow system users to create authenticated FTP sessions, you should make the following changes to vsftpd.conf : local_enable=YES write_enable=YES local_umask=022 vsftpd uses PAM by default for authentication ( /etc/pam.d/vsftpd ). System security is enforced by the files /etc/vsftpd/ftpusers, /etc/vsftpd/users_list . Account names listed in ftpusers are not allowed to login via FTP. Depending on the value of the userlist_deny setting in vsftpd.conf, the users in /etc/vsftpd/users_list act as either allow only the users in the list, or explicitly deny the listed users. rsync \u00b6 The rsync protocol was written as a replacement for rcp and uses an advanced algorithm to intelligently transfer files. Only the files or parts of files which have changed are copied. rsync uses Delta encoding and requires the source and destination to be specified (either or both may be remote). rsync protocol does not have in-transit security. However, rsync can be tunneled over the SSH protocol. rsync is mostly used over SSH, but starting the rsync daemon is as easy as rsync --daemon . When running as a daemon, rsync will read the /etc/rsyncd.conf configuration file. Each client that connects to the rsync daemon will force rsync to re-read the configuration. The /etc/rsyncd.conf file defines global options, as well as rsync modules which will be served by the rsync daemon. When running the rsync command, it will use any transparent remote shell. The default shell is ssh, which encrypts the traffic. The rsync protocol can be invoked with the rsync:// URI in the command or with rsyncd . i.e. rsync -av root@server:/etc/. /srv/backup/server-backup/etc/. backs up an entire directory. scp and sftp \u00b6 scp (Secure Copy) is non-interactive, while sftp (Simple File Transfer Protocol) is interactive way to transfer files and they both use your system or the user ssh client configuration files ( ~/.ssh/config ). WebDAV \u00b6 WebDAV is an extension to HTTP for read/write access to resources and is available on most operating systems. A command-line tool cadaver can be used to manipulate a WebDAV share. The Apache module mod_dav is one method to enable WebDAV and requires a defined lock file, which should be writable by the Apache user: DavLockDB davlockdb . WebDAV can be enabled in either a <Directory> or <Location> stanza. However, it is more secure to create an alias and use the directory options to increase security. Uploads, PUT, POST, and similar methods should not be allowed, except for by an authenticated user. BitTorrent \u00b6 BitTorrent is a protocol which allows for very efficient transfer of large files or directories using a distributed peer-to-peer architecture. A file served over BitTorrent is divided into small chunks and is distributed by anyone who is connected to the same tracker. This allows for much lesser resource use by the server hosting the original content, as the file needs to be downloaded only once. BitTorrent becomes more efficient the more concurrent clients are participating. Tools available are rtorrent as a CLI client and mktorrent as a CLI for creating .torrent files. Advanced Networking \u00b6 Routing \u00b6 Routing commands include route and ip route . Dynamic routing protocols include RIP, OSPF, BGP and IS-IS. To create a new run time route, do ip route add 10.1.11.0/24 via 10.30.0.101 dev eth2 To create a static rule which will survive a reboot, do: # On CentOS, edit the /etc/sysconfig/network-scripts/route-<INTERFACE> file and add a line like this: 10 .1.11.0/24 via 10 .30.0.101 dev eth2 # On Ubuntu, edit the /etc/network/interfaces file and add a line like this: ip route add -net 10 .1.11.0/24 gw 10 .30.0.101 dev eth2 # On OpenSUSE, edit the /etc/sysconfig/network/ifroute-<INTERFACE> file and add a line like this: 10 .1.11.0/24 10 .30.0.101 - eth2 # Network Manager systems can use: nmcli con modify \"connection-name\" ipv4.routes \"10.1.11.0/24 10.30.0.101 99\" The downside to static routes is inflexibility. Dynamic routing protocols are more efficient at detecting and fixing routing problems quickly. VLAN \u00b6 VLANs use functionality in the switches and routers. The most common use for VLANs is to bridge switches together. A VLAN is also a method for securing two or more LANs from each other on the same set of switches. Creating a trunk connection (802.1q is one such protocol between two switches essentially connects the networks together. VLANs can be created on the trunked together switches to then isolate specific ports on each switch to belong to a Virtual LAN. VLANs use optional functionality within the packet to identify which VLANs are being used. When VLANs are enabled, an additional header is added to the packet. This is the 802.1Q or dot1q header. The 802.1Q header contains: Tag Protocol ID (TPID), a 16bit identifier to distinguish between a VLAN tagged frame or it indicates the EtherType. The value of x8100 indicates an 802.1Q tagged frame. The next 16bits are the Tag Control Information (TCI), comprising of: Priority Code Point (PCP), a 3bit field that indicates the 802.1q priority class. See the IEEE P802.1Q webpage for more details. Drop Eligible Indicator (DEI). This 1bit flag indicates the frame may be dropped when network congestion occurs. The field may be used alone or in conjunction with the PCP. This field used to be the Canonical Format Indicator (CFI), and was used for compatibility between Ethernet and Token Ring frames. VLAN Identifier ID (VID), a 12bit field indicating to which VLAN the frame belongs to. DHCP Server \u00b6 The dhcpd daemon is configured with /etc/dhcp/dhcpd.conf and some options files: /etc/sysconfig/dhcpd (CentOS), /etc/default/isc-dhcp-server (Ubuntu), /etc/sysconfig/dhcpd (OpenSUSE) The dhcp server will only serve out addresses on an interface that it finds a subnet block defined in the /etc/dhcp/dhcpd.conf file. Additional or different daemon command line options may be passed to the daemon at start time by the systems' drop-in files. Global options are settings which should apply to all the hosts in a network. You can also define options on a per-network basis. A sample configuration would be: subnet 10.5.5.0 netmask 255.255.255.224 { range 10.5.5.26 10.5.5.30; option domain-name-servers ns1.internal.example.org; option domain-name \"internal.example.org\"; option routers 10.5.5.1; option broadcast-address 10.5.5.31; default-lease-time 600; max-lease-time 7200; } Network Time Protocol \u00b6 The security of many encryption systems is highly dependent on proper time. NTP time sources are divided up into strata . A strata 0 clock is a special purpose time device (atomic clock, GPS radio, etc). A strata 1 server is any NTP server connected directly to a strata 0 source (over serial or the like). A strata 2 server is any NTP server which references a strata 1 server using NTP. A strata 3 server is any NTP server which references a strata 2 server using NTP. NTP may function as a client, a server, or a peer: Client: Acquires time from a server or a peer. Server: Provides time to a client. Peers: Synchronize time between other peers, regardless of the defined servers. Some NTP applications implementations are ntp , chrony , or systemd-timesyncd . The ntpdc -c peers command can show the time difference between the local system and configured time servers. The timedatectl command is in many distributions and may be used to query and control the system time and date. To configure the ntpd server, allow clients to request time (restrict), set up access for peers, declare the local machine to be a time reference, regulate who can query the time server with ntpq and ntpdc commands and start the NTP daemon. A good NTP server is only as good as its time source. The NTP Pool Project was created to alleviate the load that was crippling the small number of NTP servers. To configure your NTP server to use the NTP pool, edit the /etc/ntp.conf file and add or edit the following settings: driftfile /var/lib/ntp/ntp.drift pool 0.pool.ntp.org pool 1.pool.ntp.org pool 2.pool.ntp.org pool 3.pool.ntp.org","title":"Linux Network Administration"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#networking-concepts","text":"","title":"Networking Concepts"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#the-osi-model","text":"The Open System Interconnection (OSI) model was created to standardize the language used to describe networking protocols. OSI defines the manner in which systems communicate with one another using abstraction layers . Each layer communicates with the layer directly above and below. Not all layers are used at all times. Most networking stacks have layer 1-4 and 7. The layer 5-6 is often combined into layer 4 or 7.","title":"The OSI Model"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#physical-layer","text":"The Physical Layer is the lowest possible layer and deals with the actual physical transfer of information; it deals with transferring bits over a physical medium such as: Electric pulses over copper cables Laser pulses over fiber optic cables Frequency modulations over radio waves Scraps of paper over carrier pigeons Some protocols, hardware types and standards defined for this layer: IEEE 802.3: Copper or fiber connections (examples and additional information about various cables and connection is available on Wikipedia, in the \"10BASE2\" article) IEEE 802.11: Wireless (Wi-Fi) connections Bluetooth: Wireless connections USB: Copper connections RS232: Copper serial connections A Frame is a unit of data collected from the physical layer interface.","title":"Physical Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#data-link-layer","text":"The Data Link Layer accepts data from the hardware in Layer 1 and prepends an address to all the inbound packets it accepts. The address number is 6 bytes, 3 bytes for the manufacturer and 3 random bytes assigned to the adapter. This 6 byte address is known as the MAC (Media Access Control) Address . The Data Link layer deals with transferring data between network nodes using the MAC address. This layer is where the IP address and MAC address are associated using the ARP protocol . A broadcast is initiated to find the MAC address of the IP address that is required. Quite often a broadcast is initiated requesting MAC addresses as the associations time out and are deleted. The ARP frame usually has the data of Who has 192.168.0.2? Tell 192.168.0.7 . Since ARP uses broadcasts, the packets are confined to a single network segment. A network segment may also be called a broadcast domain or collision domain . A collision domain may be expanded with the use of a bridge . Bridges will work but all nodes will hear and examine all traffic on the wire. Adding bridges increases the number of nodes on the wire. Common Data Link protocols or EtherType include: ARP: ID=x0806, Address Resolution Protocol RARP: ID=x0825, Reverse Address Resolution Protocol IPv4: ID=x0800, Internet Protocol version 4 PPPoE: ID=x8863, Point to Point Protocol over Ethernet (discovery) STP: Spanning Tree Protocol, does not have an ID as it uses an Ethernet II frame with a LLC (Logical Link Control) header.","title":"Data Link Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-layer","text":"The Network Layer deals with routing and forwarding packets, and may also have Quality of Service component. The Data Link layer is confined to a single network segment and is used by Network Layer for local delivery. Some of the common Layer 3 protocols are: IPv4, Internet Protocol version 4 IPv6, Internet Protocol version 6 OSPF, Open Shortest Path First IGRP, Interior Gateway Routing Protocol ICMP, Internet Control Message Protocol The layer 3 is tasked with packet delivery to the next hop. Each packet is sent/forwarded on its own merits and run in a connectionless environment. In many cases the final destination is not adjacent to this machine so the packets are routed based on the local routing table information.","title":"Network Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#internet-protocol","text":"The addressing function examines the address on the incoming packet: If the address indicates the datagram is for the local system then the headers are removed and the datagram is passed up to the next layer in the protocol stack. If the address indicates the datagram is for another machine then it is passed to the next system in the direction of the final destination. The fragmentation component will split and re-assemble the packets if the path to the next system uses a smaller transmission unit size.","title":"Internet Protocol"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#ip-address","text":"IP addresses have two parts: Network Part - indicated by the value of the netmask Host Part - the remaining bits after the host network component is removed They are distinguished by using a netmask , a bitmask defining which part of an IP address is the network part. IP networks can be broken into smaller pieces by using a subnet . As an example: parts address IP Addr Dec 10 20 0 100 Netmask Dec 255 255 0 0 IP Addr Bin 00001010 00010100 00000000 01100100 Netmask Bin 11111111 11111111 00000000 00000000 Subnet Bin 11111111 Network Part Bin 00001010 00010100 Subnet Part Bin 00000000 Host Part Bin 00000000 01100100 Originally, the IPv4 addresses were broken into the following three classes: Class A: 0.0.0.0/255.0.0.0 Class B: 128.0.0.0/255.255.0.0 Class C: 192.0.0.0/255.255.255.0 Original classes of networks and subnets did not scale well. Networks which did not fit in a class B were often given a class A. This led to IP addresses going to waste and the creation of CIDR (Classless Inter-Domain Routing) which uses a numbered bitmask instead of the class bitmask . Example CIDR Network Netmask: parts address CIDR /18 Dec 255 255 192 0 CIDR /18 Bin 11111111 11111111 11000000 00000000","title":"IP Address"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#ip-routing","text":"Routing begins when the adapter collects a Frame of data and passes it up the stack. In the first stage the Data Link (MAC) is examined to see if it matches the local machine\u2019s hardware address. When a match is made, the packet is examined for a match of the Destination IP Address. If a match of IP addresses is made, then the packet\u2019s destination is the local machine. The packet is then passed up to Layer 4 (Transport), for additional processing. If there is no IP address match then the datagram is forwarded to the next hop based on the local routing tables. The MAC address is updated to the next hop machine and the packet is passed along. Notice that the MAC address is modified to the next hop and the IP address is the final destination and is not usually modified. Network routers inspect packet headers, and make routing decisions based on the destination address. There are multiple routes the packets can take to reach the final destination and the routers involved make routing decisions based on path data for most direct path or fastest path.","title":"IP Routing"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#ipv4-and-ipv6","text":"IPv4 was the first major Internet Protocol version and most used protocol. Its 32bit address allows 4,294,967,296 possible addresses, which has exhausted on Jan. 31, 2011. Some solutions for mitigating the problem are: The move from Classed networks to Classless Inter-Domain Routing (CIDR) The invention of Network Address Translation (NAT) The move to IPv6 IPv6 is the successor to IPv4; it has 128bit address allows for 3.4 x 10^38 possible addresses. It was designed to deal with IPv4 exhaustion and other shortcomings: Expanded addresses capabilities Header format simplification Improved support for extensions and options Flow labeling capabilities","title":"IPv4 and IPv6"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#ip-management-tools","text":"ip is part of the iproute2 package, the default tool for many distributions that manages layer 2 and 3 settings. NetworkManager is a daemon with D-bus for communication to applications and a robust API available to inspect network settings and operations. It has command line interface ( nmcli ), curses-based interface ( nmtui ) and graphical interface ( nm-connection-editor ) ifconfig is part of the net-tools package, not recommened as some new constructs created by ip are not visible to ifconfig .","title":"IP Management Tools"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-types","text":"Local Area Network (LAN) form a network on its own Smaller, locally connected network. Connected at layer 2 by the same series of switches or hubs. Node to node communication happens at layer 3 using the same network. Layer 2 to layer 3 association may use layer 2 broadcasts and the ARP and RARP protocols to associate MAC addresses with IP addresses. Virtual Local Area Network (VLAN) is a method for combining two or more separated LANs to appear as the same LAN, or securing several LANs from each other on the same set of switches. VLANs are defined to the switch, and must configure the trunk port for switch-to-switch communication. A network bridge or repeater accepts packets on one side of the bridge and passes them through to the other side of the bridge, and it is bi-directional. It can be a hardware bridge or software bridge. It is a combination of two or more networks at layer 2. Bridged networks communicate as a single network . It is generally used to increase the length of the network connections or increase the number of connections by joining two LANs. Software bridges are present in Kernel and mostly noticeable when implementing VMs, containers or network namespaces. Several methods for configuring software bridges: iproute2, systemd-networkd, nmcli, and VM software Wide Area Networks (WAN) are the components that make the internet work. Typically a WAN is comprised of many individual LAN\u2019s connected together. The layer 2 (MAC address) contains the address of the \"gateway\" node. Once the \"gateway\" node receives the packet it determines if the packet is local or needs to be \"forwarded\" to the next \"gateway\".","title":"Network Types"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#transport-layer","text":"The Transport Layer is responsible for the end-to-end communication protocols. Data is properly multiplexed by defining source and destination port numbers. This layer deals with reliability by adding check sums, doing request repeats, and avoiding congestion. Common protocols in the Transport Layer include: Transmission Control Protocol (TCP), the main component of the TCP/IP (Internet Protocol Suite) stack. User Datagram Protocol (UDP), another popular component of the Internet Protocol Suite stack. Stream Control Transmission Protocol (SCTP) Transport Layer uses port numbers to allow for connection multiplexing. The port numbers are usually used in pairs, servers have fixed ports that they listen on and clients use a random port number for port number. The ports are classed in three different ways: Well-Known Ports 0-1023 - assigned by the Internet Assigned Numbers Authority (IANA), and usually require super-user privilege to be bound. Some of the well-known ports are: 22 TCP: SSH; 25 TCP: SMTP; 80 TCP: HTTP; 443 TCP: HTTPS . Registered Ports 1024-49151 - also assigned by the IANA. They can be bound on most systems by non-super-user privilege. Dynamic or Ephemeral Ports 49152-65535 - used as source ports for the client-side of a TCP or UDP connection. Can also be used for a temporary or non-root service.","title":"Transport Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#tcp-udp-sctp","text":"TCP is useful when data integrity, ordered delivery and reliability are important. UDP is useful when transmission speed is important and the integrity of the data isn\u2019t as important, or is managed by a higher layer. SCTP is an evolving protocol designed for efficient robust communication. Some of the features are still being sorted such as using SCTP through a NAT firewall. Characteristics TCP UDP SCTP Connection-oriented Yes No Yes Reliable Yes No Yes Ordered delivery Yes No Yes Checksums Yes Optional Yes Flow control Yes No Yes Congestion avoidance Yes No Yes NAT friendly Yes Yes Not yet ECC Yes Yes No Header size 20-60 bytes 8 bytes 12 bytes","title":"TCP, UDP, SCTP"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#session-layer","text":"Session Layer is used for establishing, managing, synchronizing and termination of application connections between local and remote application. If an established connection is lost or disrupted, this layer may try to recover the connection. If a connection is not used for a long time, the session layer may close and reopen it. There are two types of sessions: connection-mode service and connectionless-mode sessions. Session options: Simplex or duplex communication Transport Layer reliability Checkpoints of data units Session services may be involved in: Authentication for Transport Layer Setup and encryption initialization Support for steaming media Support for smtp,http and https protocols SOCKS proxy, Secure Sockets, TLS The Session Layer creates a semi-permanent connection which is then used for communications, many of the RPC-type protocols depend on this layer: NetBIOS: Network Basic Input Output System RPC: Remote Procedure Call PPTP: Point to Point Tunneling Protocol","title":"Session Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#presentation-layer","text":"The Presentation Layer is commonly rolled up into a different layer, Layer 5 and/or Layer 7. Some of the protocols that function at this level include: AFP, Apple filing protocol NCP, Netware Core protocol x25 PAD, x25 Packet Assembler/Disassembler Some of the services that may be available at the Presentation level are: Data Conversion (EBCDIC to ASCII) Compression Encryption/Decryption Serialization","title":"Presentation Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#application-layer","text":"The Application Layer is the top of the stack and deals with the protocols which make a global communications network function. Common protocols which exist in the Application Layer are: HTTP: Hypertext Transfer Protocol SMTP: Simple Mail Transfer Protocol DNS: Domain Name System FTP: File Transfer Protocol DHCP: Dynamic Host Configuration Protocol","title":"Application Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#manage-system-services-with-systemd","text":"Common actions performed from systemctl utility: # Start the service systemctl start httpd.service # Stop the service systemctl stop httpd.service # Enable the service to auto start on system boot systemctl enable httpd.service # Disable or prohibit a service from auto starting systemctl disable httpd.service # Obtain the status of a service systemctl status httpd.service # reload systemd confs after making changes systemctl daemon-reload # restart a service after the reload (.service can be omitted) systemctl restart httpd # Run the service in foreground for debugging /usr/sbin/httpd -f /etc/httpd/httpd.conf # List the services (units) currently loaded systemctl # List the sockets in use by systemd launched services systemctl list-sockets # List the timers currently active systemctl list-timers # Set specific unit runtime values if supported; this will set properties used for resource control if enabled systemctl set-property foobar.service CPUWeight = 200 MemoryMax = 2G IPAccounting = yes # Display the status, state, configuration file(s) and last few log messages for this service systemctl status httpd.service # Find overridden configuration files and display the differences systemd-delta Some transient system services are not used enough to keep a daemon running full time. The xinet daemon was created to manage these transient daemons. Its default configuration file is /etc/xinetd.conf , with additional per-service files in /etc/xinetd.d/ . The sequence that systemd processes the configuration files is predictable and extensible. The common scan sequence is: The vendor or package supplied unit files in one of the following directories: /usr/lib/systemd/system/<service>.service or /lib/systemd/system/<service>.service Optional or dynamically created unit files in /run/systemd/system/ directory Optional user unit override files in /etc/systemd/system/ directory Optional user drop-in files in /etc/systemd/system/<service>.d (most popular drop-in directory) <service>.d might need to be manually created It is common practice to copy the vendor unit file into the /etc/systemd/system/ directory and make appropriate customizations. Often times it is desirable to add or change features by program or script control, the drop-in files are convenient for this. One item of caution, if one is changing a previously defined function (like ExecStart) it must be undefined first then added back in. With systemd , additional features and capabilities can be easily added. As an example, cgroups controls can be added to our service by adding a Slice directive in the Service block.","title":"Manage System Services with systemd"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-configuration","text":"","title":"Network Configuration"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#layer-2-configuration","text":"There are two methods for examining/changing additional parameters for Layer 2: Kernel module tools modinfo and modprobe using /etc/modprobe.d/ , with suffix in .conf or .local udev using the drop-in directory /etc/udev/rules.d/ The udev (user device facility) is used to manage network hardware interfaces for the Linux kernel. Sometimes changes are needed to rename interfaces or change configurations to match hardware MAC addresses. When rules are processed, all the files in the rules directories are combined and sorted in a lexical order. The udevadm command is used for control, query and debugging udev configuration files. You can see an example rule to rename a network interface: cat /etc/udev/rules.d/70-persistent-net.rules SUBSYSTEM == \"net\" ,ACTION == \"add\" ,ATTR { address }== \"52:54:42:42:00:01\" ,NAME = \"net%n\" The operations required are: List optional parameters Set optional parameters Verify additional parameters Block or allow hardware modules from loading Sometimes it becomes necessary to inhibit the loading of a hardware module. The syntax is blacklist MODULENAME Example of changing layer 2 configuration: # change the maximum transmission unit (MTU): ip link set mtu 1492 dev eth0 # change the link speed: ethtool -s eth0 speed 1000 duplex full # check the link status ip -s link ip -s link show dev eth0 # check the network interface driver ethtool -i eth0 # check driver module info to understand optional parameters to the module modinfo e1000 udevadm info -a /sys/class/net/ens9 # add parameters in /etc/modprobe.d/mynic.conf: options e1000 Speed = 100 Duplex = 0 AutoNeg = 0","title":"Layer 2 Configuration"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#layer-3-configuration","text":"Examples of changing layer 3 configuration # manually set a network (Layer 3) address: ip addr add 10 .0.2.25/255.255.255.0 dev eth0 # manually set or change the default route: ip route add default via 10 .0.2.2 route add default gw 10 .0.2.2 # add the address of a DNS server, use: echo \"nameserver 4.2.2.1\" >> /etc/resolv.conf # manually request a DHCP configuration, use the dhclient command: dhclient eth0","title":"Layer 3 Configuration"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#boot-time-configuration","text":"Network settings are stored in configuration files which allow for persistent configuration across reboots. Network-Manager is an example of a common configuration tool with several interfaces ( nmcli ). The systemd-network service relies on text file configuration and has no text or GUI menu application ( networkctl ). netplan is a a Ubuntu-specific tool that creates a network configuration at runtime from a pre-defined yaml file. It can dynamically create either a Network Manager or a systemd-networkd based configuration.","title":"Boot Time Configuration"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-manager","text":"Network Manager is used by Ubuntu, CenOS, SUSE, and Debian, which provides: GUI tool ( nm-connection-editor ) applet ( nm-applet ) text interface ( nmtui ) CLI interface ( nmcli ) The current release of Network Manager will automatically configure and start a network interface with a Dynamic Host Configuration Protocol (DHCP) if there is no network configuration file for the adapter. If there is a configuration file, Network Manager will set the device to an \"unmanaged\" state and initialize the adapter with the attributes of the configuration file. Network Manager can manage Hardware: Bluetooth, DSL/PPPoE, Ethernet, InfiniBand, Mobile Broadband, Wi-Fi Virtual: Bond, Bridge, IP Tunnel, MACsec, Team, Vlan, Wireguard The network configuration files on an Ubuntu system typically reside in the /etc/network directory, with the interface configuration file being /etc/network/interfaces . The hostname config file is /etc/hostname . The network configuration files on a CentOS system are located in the /etc/sysconfig/network-scripts directory and match the ifcfg-<interface> pattern. The DNS client settings are managed by editing the /etc/resolv.conf file. Beware that some of the network adapters may be excluded from Network Manager and will have to be re-added to allow Network Manager to control the interfaces.","title":"Network Manager"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#vpn","text":"Virtual Private Networks (VPNs) provide a secure connection for remote users through unsecured networks. Data is encrypted to avoid unwanted exposure. Initialization of the connection usually has multi-factor authentication for additional security. There are VPN types provided by many different protocols: Secure Transport Layer (SSL/TLS) Internet Protocol Security (IPSEC) Datagram Transport Layer (DTLS) Secure Shell (SSH) VPN. One of the more popular VPN tools is OpenVPN , which provides SSL/TLS-based VPN connectivity. OpenVPN is a single binary for both the server and the client, and is a command-line tool.","title":"VPN"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-troubleshoot-and-monitor","text":"","title":"Network Troubleshoot and Monitor"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#client-side-troubleshoot","text":"Some common networking issues found at the client side include DNS issues, Firewall settings, or incorrect network settings (routes, netmasks). The basics of network troubleshooting usually deal with connectivity testing. You can use the tools ping, traceroute, and nmap Use ping for checking connectivity. ping uses the ICMP protocol. Test the IP address to your network adapter, gateway, and DNS. Use the DNS name for domain name resolution. Use traceroute or mtr which shows the connectivity path to the destination. mtr show statistics of the connection and packets drops/failures. Use nmap which scans the server to see if the required ports are available. Use telnet to test plain-text protocols, such as http. i.e. telnet example.com 80 . Use openssl to test SSL or TLS protocols. i.e. openssl s_client -connect www.google.com:443 Use arp to check link-layer connectivity. Advanced troubleshooting involves using tcpdump and wireshark . The command line-based tcpdump truncates packets by default and generates pcap files. wireshark uses the graphical interface to capture packets. It can capture and analyze packets in real time. It is useful to analyze pcap files, but you may not want wireshark installed on the system you are troubleshooting. To capture packets with tcpdump for use with wireshark, do sudo tcpdump -i eth0 -s 65535 -w capture.pcap port 22","title":"Client side Troubleshoot"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#server-side-troubleshoot","text":"Common server problems include broken DNS, overzealous firewall rules, incorrect network settings, and the daemon not listening on the right interface/port. Some protocols break when return traffic comes back from a different IP address. Verify that your egress route is correct. Some access control systems require that Reverse DNS be properly set up. Perform basic server troubleshooting: test the network connectivity from the server's point of view. One of the first steps in troubleshooting a server-side daemon should be to check the log files, and verify the daemon/service is running. Use netstat to list the ports that daemons listen on, i.e. sudo netstat -taupe | grep httpd The ss command is another socket statistics utility. It may be a replacement to netstat although it is missing some socket types. Usage: sudo ss -ltp | grep httpd . Server side firewall configuration needs to take into account that it allows certain inbound and outbound traffic. Also check the settings of tools such as TCP wrappers ( /etc/hosts.allow and /etc/hosts.deny ). Consult man 5 hosts_access for additional details. For advanced server troubleshooting, the /proc filesystem has settings that affect the network stack: /proc/sys/net/ipv4/ip_forward - Allows for network traffic to be forwarded from one interface to another. /proc/sys/net/ipv4/conf/*/accept_redirects - Accepting Internet Control Message Protocol (ICMP) redirects from a router to find better routes. This setting has the potential to be exploited by a malicious party to redirect your traffic. /proc/sys/net/ipv4/icmp_echo_ignore_all - Changing this setting will affect the host's visibility to ICMP ping packets. /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts - This setting will change the host's visibility to broadcast ICMP ping packets. /proc/net/arp - Contains the current arp table. These settings are not persistent across reboots. To persistently enable changes you must use the sysctl command with its configuration file /etc/sysctl.conf . The syntax for /etc/sysctl.conf matches the path for the file in /proc/sys with the . character instead of / . netcat is a TCP and UDP sender and listener. To test a network connection, use netcat to open a listening port on one system with netcat -l 4242 , cause it to listen on all adapters port 4242 for traffic. On another machine, use netcat <listener_ip_address> 4242 to open a connection to the listener, then send input which should appear on the listener side, the communication is bi-directional. netcat can save many hours of frustration by proving TCP and UDP traffic can transverse the network.","title":"Server side Troubleshoot"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-monitoring","text":"The iptraf tool is a real-time network traffic analyzer. It recognizes protocols: IP, TCP, UDP, ICMP, IGMP, IGP, IGRP, OSPF, ARP, RARP . snort is a network intrusion detection system. In addition to being a network monitor, it can help pinpoint unwanted traffic inside of a network. ntop is an application and web app for monitoring network usage. It can pinpoint bandwidth use, display network statistics, and more. tcpdump has been around for a long time, it is text-based, small and efficient. wireshark , graphical tracer with protocol decode that allows the user to view more or less data depending on the requirement. It can read tcpdump output files, which allows the collection of trace data with the efficient tcpdump on prod system and then use wireshark to display the information for analysis.","title":"Network Monitoring"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#remote-access","text":"","title":"Remote Access"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#cryptography","text":"Cryptography is about securing communications. Symmetric encryption uses a single secret shared key , which both parties must have to communicate. Plain text encrypted with a symmetric encryption method can easily be turned back into plain text using the same key. Example Caesar cipher One benefit of symmetric encryption is that it is less computationally-intensive than asymmetric encryption. One downside of symmetric encryption is that a secure shared key exchange is difficult to attain. Asymmetric encryption uses mathematically-related public and private keys to communicate. Plain text encrypted with an asymmetric public key can only be decrypted by using the corresponding private key , not the public key. Asymmetric encryption has no key-exchange problem and uses the published public key to send secure messages. However, it is more computationally-intensive than symmetric encryption, and needs proper verification on the shared public key. By using the asymmetric encryption to pass the symmetric key, you can overcome the problems associated with both. Party One creates a session key using a symmetric algorithm. Party One then encrypts the session key, using the public key of Party Two, and sends the encrypted session key to Party Two. Party Two uses their private key to decrypt the session key. Both parties now communicate using the symmetric session key for encryption.","title":"Cryptography"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#secure-shell","text":"telnet is one of the earlier protocols developed for the Internet back in 1969 and was not built with security in mind, the protocol is sent over the wire in plain text. Remote Shell rsh was originally written for the BSD (Berkeley Software Distribution) system in 1983. rsh is a similar system to telnet and is an insecure protocol, which is not encrypted and sends data in clear text. The Secure Shell ( ssh ) protocol was developed to overcome the security concerns of telnet and rsh . OpenSSH is the most widely used version of SSH and is built upon the concepts of symmetric and asymmetric encryption. OpenSSH Architecture Layers: transport layer - deals with the initial key-exchange and setting up a symmetric-key session and establish the connection layer. user auth layer - deals with authenticating and authorizing the user accounts. connection layer - deals with the communication once the session is set up. The OpenSSH host-wide client configuration is /etc/ssh/ssh_config . The per-user client configuration is each user's $HOME/.ssh/config . SSH uses a key-based authentication. Syntax can be found with man 5 ssh_config Other protocols can be tunneled over SSH. The X11 protocol support is part of the OpenSSH client. You can also manually open a connection for any other protocol using the LocalForward and RemoteForward tokens in the OpenSSH client configuration.","title":"Secure Shell"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#client-ssh-config","text":"$HOME/.ssh/config can be set up with shortcuts to servers you frequently access. As an example, you can do ssh web rather than typing ssh webusr@www.example.com : Host web HostName www.example.com User webusr More advanced configuration: Host web KeepAlive yes IdentityFile ~/.ssh/web_id_rsa HostName www.example.com Port 2222 User webusr ForwardX11 no Host * Port 22 OpenSSH client key-based authentication provides a passwordless authentication for users. Private keys can be encrypted and password protected. ssh-agent program can cache decrypted private keys. ssh-copy-id program can copy your public key to a remote host. # To generate a user key for SSH authentication, use: $ ssh-keygen -f $HOME /.ssh/id_rsa -N 'supersecret' -t rsa # To start ssh-agent and use it to cache your private key, use: $ eval $( ssh-agent ) $ ssh-add $HOME /.ssh/id_rsa # To copy your public key to the remote system overthere for remote user joe, use: $ ssh-copy-id joe@overthere","title":"Client SSH config"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#openssh-tunnel","text":"The local tunnel ( ssh -L ) indicates which port is to be opened on the local host (4242) and the final destination to be (charlie:2200), and the connection to the final destination is made by machine (bob). The remote tunnel ( ssh -R ) requests machine (bob) to open a listening port (2424) to which any connection will be transferred to the destination, (charlie:2200). There is also dynamic port forwarding using ssh -B . Option -N sets the option to not execute a command on connection to the remote system, and option -f informs ssh to go into background just before command execution.","title":"OpenSSH Tunnel"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#parallel-ssh-commands","text":"The pssh package is available for execute the same command on many systems, which includes: pssh: parallel ssh pnuke: parallel process kill prsync: parallel copy program using rsync pscp: parallel copy using scp pslurp: parallel copy from hosts The pssh command and friends use the existing ssh configuration. It is best to configure aliases, keys, known hosts and authorized keys prior to attempting to use pssh. If there is a password or fingerprint prompt, the pssh command will FAIL. When using pssh, it is convenient to create a file with a list of the hosts you wish to access. The list can contain IP addresses or hostnames. $ cat ~/ips.txt 127 .0.0.1 192 .168.42.1 $ pssh -i -h ~/ips.txt date [ 1 ] 10 :07:35 [ SUCCESS ] 120 .0.0.1 Thu Sep 28 10 :07:35 CDT 2017 [ 2 ] 10 :07:35 [ SUCCESS ] 192 .168.42.1 Thu Sep 28 10 :07:35 CDT 2017","title":"Parallel SSH Commands"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#vnc-server","text":"The Virtual Network Computing (VNC) server allows for cross-platform, graphical remote access. The most common implementation is tigervnc client and server. The server component has Xvnc (the main server for VNC and X), vncserver (Perl script to control Xvnc), vncpasswd (set and change vnc-only password), and vncconfig (configure and control a running Xvnc). When the server starts, it uses the xstartup configuration from the users ~/.vnc directory. If the xstartup file is altered, the vncserver needs to be restarted. VNC is a display-based protocol, which makes it cross-platform. It also means that it is a relatively heavy protocol, as pixel updates have to be sent over-the-wire. The client, vncviewer , is usually packaged separately. It connects to the VNC server on the specified port or display number. Passwords are not sent in clear text. On its own, VNC is not secure after the authentication step. However, the protocol can be tunneled through SSH or VPN connections.","title":"VNC Server"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#x-window-system","text":"The X Window system was developed as part of Project Athena at MIT in 1984. This simple network-transparent GUI system provides basic GUI primitives and is network transparent, allowing for ease of use. When it comes to X authentication, the client-server security is done using keys. To secure the X protocol, it must be tunneled with VPN or SSH. OpenSSH supports X11 tunneling via -X option.","title":"X Window System"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#domain-name-service","text":"Before DNS , there was ARPANET . The original solution to a name service was a flat text file called HOSTS.TXT. This file was hosted on a single machine, and, when you wanted to get a copy, you pulled it from this central server using File Transfer Protocol (FTP) or a similar protocol. A descendant of the HOSTS.TXT file is the /etc/hosts file. It has a very simple syntax: <IP ADDRESS> <HOSTNAME> [HOSTNAME or alias] ... . This hosts file usually takes precedence over other resolution methods. The Domain Name System (DNS) is a distributed, hierarchical database for converting DNS names into IP addresses. The DNS protocol runs in two modes: recursive with caching, authoritative When a network node makes a DNS query, it most often makes that query against a recursive, caching server. That recursive, caching server will then make a recursive query through the DNS database, until it comes to an authoritative server. The authoritative server will then send the answer for the query. The DNS database consists of a tree-like, key-value store. The database is broken into tree nodes called Domains. These domains are managed as part of a zone. Zones are the area of the namespace managed by authoritative server(s). DNS delegation is done on zone boundaries. The Caching Server most likely have already cached those frequently accessed top domains so this process would usually be fast.","title":"Domain Name Service"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#queryrecord-types","text":"A Record - Address Mapping Records, a 32bit IPv4 address. AAAA Record - IP Version 6 Address Records, a 128big IPv6 address. CNAME - Canonical Name Records, an alias to another name MX - Mail Exchanger Records, the message transfer agents (mail servers) for a domain. NS - Nameserver Records, delegate an authoritative DNS zone nameserver. PTR - Reverse-Lookup Pointer Records, pointer to a canonical name (IP address to name). SOA - Start of Authority Records, Start of Authority for a domain (domain and zone settings). TXT - Text Records, arbitrary human-readable text, or machine-readable data for specific purposes. More about DNS","title":"Query/Record Types"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#dns-queries","text":"Forward DNS queries use A or AAAA record types and are most often used to turn a DNS name into an IP address. A Fully Qualified Domain Name (FQDN) is the full DNS address in the DNS database, and the most significant part is first. A reverse DNS query is used to turn an IP address into a DNS name. It uses a PTR record type and an arpa. domain in a DNS database. In an IP address, the most significant part is on the right; we have to translate an IP address to put it into the DNS database. i.e. 192.168.13.32 becomes 32.13.168.192.in-addr.arpa. ; 2001:500:88:200::10 becomes 0.1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.2.0.8.8.0.0.0.0.5.0.1.0.0.2.ip6.arpa. See popular DNS servers .","title":"DNS Queries"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#dns-client","text":"Network configuration is more dynamic and the nameserver entries need to be adjusted dynamically. In most distros, the nameserver information may be added to the interface configuration file, overwriting or modifying the /etc/resolv.conf when the interface is started. The DHCP Server often provides nameserver information as part of the information sent to the DHCP client, replacing the existing nameserver records. resolvconf service uses additional files like /etc/resolvconf.conf and a background service resolvconf.service to \"optimize\" the contents of /etc/resolv.conf. dnsmasq sets up in \"mini\" caching DNS server and may alter the resolver configuration to look at dnsmasq instead of the items listed in /etc/resolv.conf. systemd.resolved provides a DNS stub listener on IP address 127.0.0.53 on the loopback adapter and takes input from several files including: /etc/systemd/resolved.conf, /etc/systemd/network/*.network and any DNS information made available by other services like \"dnsmasq\".","title":"DNS Client"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#bind","text":"BIND , Berkeley Internet Name Domain, is a widely-used, ISC standard DNS Internet software available for most UNIX-type systems. Its configuration file is /etc/named.conf or /etc/bind/named.conf , syntax specified in man 5 named.conf . On CenOS or SUSE distro, its package is bind and the service is named ; on Ubuntu, its package is bind9 and the service is also bind9 . Authoritative zones are defined and must contain an SOA (Start of Authority) record. Zone files should contain an NS (nameserver) record. Some of the syntax considerations include: The record format is: \"{name} {ttl} {class} {type} {data}\" Always use trailing \".\" on all fully-qualified domain names @ special character GENERATE syntax Comment marker - ; to end of line. The SOA (Start of Authority) is required for every zone. Special control fields include: Admin email Primary name server Serial number Timers (secondary server refresh settings) Refresh: How often to check for new serial from primary. Retry: How often to retry if no response from primary. Expire: How long to keep returning authoritative answers when we cannot reach the primary server. Negative TTL: How long to cache an NX domain answer. A DNS view (aka split horizon) will cause the DNS server to respond with different data depending on the match criteria, such as source IP address: it can provide different DNS answers to requests depending on selection criteria uses multiple zone files with different data for the same zone can provide DNS services inside a corporation using private or non-routable addresses.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b","title":"BIND"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#dns-tools","text":"Tools for DNS testing are in three categories: server configuration testing, server information testing, and server update. DNS client tools formulate a standard DNS request and send it off to the default or named DNS server. dig (domain information groper) queries DNS for for domain name or ip address mapping, and output format resembles the records used by the DNS server. host is a simple interface for DNS queries, good for use in scripts nslookup queries DNS for domain name or IP address mapping and less verbose than dig nsupdate end updates to a name server and requires authentication and permission","title":"DNS Tools"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#http-servers","text":"Apache used to be the lead technology of all active web servers. The location of the Apache configuration files move from distribution to distribution, and so is its systemd.service name and package name. # On RedHat, CentOS, or Fedora: Package: httpd Service: httpd Primary configuration file: /etc/httpd/conf/httpd.conf # On OpenSUSE: Package: apache2 Service: apache2 Primary configuration file: /etc/apache2/httpd.conf # On Debian, Ubuntu, or Linux Mint: Package: apache2 Service: apache2 Primary configuration file: /etc/apache2/apache2.conf Apache allows include other files and directories from the primary configuration files, just like a drop-in configuration file. The default include directories are: # CentOS: /etc/httpd/conf.d/*.conf # OpenSUSE: /etc/apache2/conf.d/ /etc/apache2/* # Ubuntu: /etc/apache2/*-enabled /etc/apache2/*-available/ Other important files include the document root ( /var/www/html/, /srv/www/htdocs ), log file locations ( /var/log/httpd, /var/log/apache2 ), and module locations.","title":"HTTP Servers"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#logs","text":"To create custom logs on an Apache server, you must first define a custom log format: LogFormat \"example-custom-fmt %h %l %u %t \"%r\" %>s %b\" example-custom-fmt Variable Explanation %h Remote host name %l Remote login name \u200b%u Remote user %t \u200bTime of request %r First line of request \u200b%s Status %b Size of response More tokens reference is here apache mod_log_config","title":"Logs"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#mod_userdir","text":"The mod_userdir module is used to allow all or some users to share a part of their home directory via the web server, even without access to the main document root. The URIs will look something like http://example.com/~user/index.html and will commonly be placed in the /home/user/public_html/ directory.","title":"mod_userdir"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#ipport-virtual-hosts","text":"For multiple web sites using multiple addresses/ports, use VirtualHost stanzas, and a unique IP address and port pair. Ensure all of the IP addresses and ports are defined in a Listen directive, and add a stanza for each virtual host. i.e. Listen 192.168.42.11:4374 <VirtualHost 192.168.42.11:4374> ServerAdmin webmaster@host1.example.com DocumentRoot /www/docs/host1.example.com ServerName host1.example.com ErrorLog logs/host1.example.com-error_log CustomLog logs/host1.example.com-access_log common </VirtualHost> To enable a name-based virtual host for an IP/Port, create a VirtualHost stanza and modify the DocumentRoot and ServerName directives. Host names which are not defined in a VirtualHost stanza which match the IP address and port will be served by the first VirtualHost stanza. Name-based virtual hosts have some SSL limitations. Due to the way SSL works, the server has no way of knowing which host name is being sent by the client before the SSL session is started. The server cannot send the proper certificate back to the client without the proper host name. This has been worked around by using Server Name Indication (SNI).","title":"IP/Port Virtual Hosts"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#access-control","text":"Password-protected directories are protected via SSL. Use htpasswd to create new passwords. See man 1 htpasswd . Per-User Access Control is activated using AllowOverride (.htaccess file). i.e. <Location /secure/> AuthType Basic AuthName \"Restricted Files\" AuthUserFile secure.passwords Require valid-user </Location> <Directory /var/www/html/get-only/> <LimitExcept GET> Require valid-user </LimitExcept> </Directory> Filesystem permissions must allow access for the user the Apache httpd daemon is running under ( apache on most systems). A properly configured SELinux system can increase the security of an Apache web server. There are some settings you may need to change: To allow Apache to make outbound network connections (not directly related to serving a request), modify the httpd_can_network_connect boolean. The default Document Root and CGI root have the proper SELinux context. If you serve files from outside those locations, you need to use the chcon command. To enable the userdir module, modify the httpd_read_user_content boolean. To serve files from an NFS filesystem, modify the httpd_use_nfs boolean.","title":"Access Control"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#secure-sockets-layer","text":"While HTTP is a clear text protocol, Secure Sockets Layer (SSL) is a port-based vhost. Types of SSL certificates include: Self-signed: for hobby or testing use. Certificate Authority Signed (Certificate Signing Request or CSR)\u200b. # generate a private key $ openssl genrsa -aes128 2048 > server.key # generate a CSR $ openssl req -new -key server.key -out server.csr # generate a self-signed certificate $ openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt # remove the encryption on private key (not recommended) $ openssl rsa -in server.key -out server.key.unlocked To install the keys, change the configuration file ( ssl.config ) or place them # On CentOS: /etc/pki/tls/certs/localhost.crt: Signed Certificate /etc/pki/tls/private/localhost.key: Private Key # On OpenSUSE: /etc/apache2/ssl.crt/server.crt: Signed Certificate /etc/apache2/ssl.key/server.key: Private Key # On Ubuntu: /etc/ssl/certs/ssl-cert-snakeoil.pem: Signed Certificate /etc/ssl/private/ssl-cert-snakeoil.key: Private Key","title":"Secure Sockets Layer"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#rewriting-rules-with-mod_rewrite","text":"URL rewriting is a powerful way to modify the request that an Apache process receives. Modifications can include moved documents, forcing SSL, forcing login, human-readable URIs, and redirecting based on the browser. Rewrite rules can exist in the .htaccess files, the main configuration file , or <Directory> stanzas. The .htaccess files seem the most common, but the main configuration file may be more secure. The mod_rewrite uses PCRE (Perl Compatible Regular Expressions). This provides nearly unlimited conditions or filters to include, like: Time of day Environment variables URLs or query strings HTTP headers External scripts Use a rewrite map for advanced or dynamic rewrite rules, which is useful when rewrite rules are too complex or numerous.. Pre-compiled rewrite maps exist for specific purposes, i.e. WURFL . There are many different types of maps: txt: Flat text dbm: DBM hash (indexed) rnd: Randomized plain text prg: External program Some rewriting examples # turn on the rewrite engine RewriteEngine on # force a redirect to a new location for a document RewriteRule \u02c6/old.html$ new.html [ R ] # rewrite a URI based on the web browser (without a redirect), use: RewriteCond % { HTTP_USER_AGENT } .*Chrome.* RewriteRule \u02c6/foo.html$ /chrome.html [ L ] # create a human-readable URI from a CGI script (the PT or Passthrough flag: without it, the rewrite is treated as a file, not a URI) RewriteRule \u02c6/script/ ( .* ) /cgi-bin/script.cgi? $1 [ L,PT ]","title":"Rewriting rules with mod_rewrite"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#mod_alias","text":"The mod_alias maps URIs to filesystem paths outside of normal DocumentRoot . It also provides Alias and Redirect directives, Apache access control considerations, file permission considerations, and SELinux considerations. mod_alias is much simpler than mod_rewrite . A redirect directive sends a redirect that is similar to the [R] flag for mod_rewrite . AliasMatch allows use of regular expressions. Example: # serve the files located at /newdocroot/ under the URI /new/ (the <Directory> stanza for access control modifications) Alias /new/ /newdocroot/ <Directory /newdocroot/> Options Indexes FollowSymLinks AllowOverride None Require all granted </Directory> # serve the files located at /newdocroot/<SUBDIR> under the URI /new/<SUBDIR> with different permissions for each AliasMatch \\\u02c6 {} /new/ ( .* ) $ /newdocroot/ $1 <Directory /newdocroot/foo/> Options Indexes FollowSymLinks AllowOverride None Require all granted </Directory> <Directory /newdocroot/bar/> Options -Indexes FollowSymLinks Require all granted </Directory> There are two ways to run scripts outside your document root: Create an Alias and set the handler to cgi-script . Use ScriptAlias (does the above automatically). Because of the nature of Common Gateway Interface (CGI) scripts, they should not be directly served as plain files and should not be in your document root. Example ScriptAlias /newscripts/ /newscripts-docroot/ <Directory /newscripts-docroot/> Options Indexes FollowSymLinks AllowOverride None Require all granted </Directory>","title":"mod_alias"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#mod_status-mod_include-mod_perl","text":"mod_status provides an endpoint to show characteristics: Apache server status Human-readable page Machine-readable page Security considerations with access control and extended status mod_include provides filtering before a response is sent to the client. It is enabled with the +Includes directive. By default, this is enabled only for *.shtml files, but it can be enabled for all *.html files with the XBitHack directive, and execute permission should be set on the file. mod_perl is a Perl interpreter embedded in Apache. It can provide full access to the entire Apache API, sharing information between Apache processes. CGI scripts can run unmodified. However, some CPAN (Comprehensive Perl Archive Network) modules are not thread-safe, therefore they do not work with the worker MPM (Multi-Processing Module). Some Perl methods like die() and exit() can cause performance issues. Example: <Location /status> SetHandler server-status Require ip 10 .100.0.0/24 </Location> <Location /dynamic/> Options +Includes XBitHack on </Location> # An HTML file which had includes would contain: <!--#set var = \"dude\" value = \"yes\" --> <!--#echo var = \"dude\" --> <!--#include virtual = \"http://localhost/includes/foo.html\" --> Alias /perl /var/www/perl <Directory /var/www/perl> SetHandler perl-script PerlResponseHandler ModPerl::Registry PerlOptions +ParseHeaders Options +ExecCGI </Directory>","title":"mod_status, mod_include, mod_perl"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#multi-processing-modules-mpms","text":"The Prefork MPM is the default MPM, non-threaded, safe for non-thread-safe scripts and libraries. It uses more memory and resources to handle the same number of connections compared to others. The Worker MPM is a hybrid multi-process, multi-threaded model. Each sub-process spawns threads which handle the incoming connections and perform better from the lighter threads than processes. The Event MPM is based on the Worker MPM but it off-loads some of the request handling to shared processes and further boosts performance. Some Prefork configurations: StartServers - number of child server processes to create on startup MinSpareServers - min number of child server processes to keep on hand to serve incoming requests MaxSpareServers - max number of child server processes to keep on hand to serve incoming requests MaxRequestsPerChild - max number of requests a child server process serves MaxClients - max number of simultaneous connections to serve. May also have to increase the ServerLimit directive Some Worker configurations in addition to the Prefork: MinSpareThreads - min number of worker threads to keep on hand to serve incoming requests MaxSpareThreads - max number of worker threads to keep on hand to serve incoming requests ThreadsPerChild - number of worker threads in each child server process","title":"Multi-Processing Modules (MPMs)"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#load-test","text":"The best testing results come from using URIs which match the real-life workflow. Apachebench ( ab ) is the tool provided as a part of the Apache httpd server package. Httperf ( httperf ) can use log files as a source of URIs to load test.","title":"Load Test"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#load-balance","text":"Apache load balancer requires the modules mod_proxy (along with its complementary modules) and mod_proxy_balancer which have specialized modules for the balancing strategy: mod_libmethod_byrequests (request counting) mod_libmethod_traffic (weighted traffic counting) mod_libmethod_bybusyness (pending request counting) mod_libmethod_heartbeat (heartbeat traffic counting) A minimal configuration as an example: <VirtualHost *:80> ProxyRequests off ServerName www.somedomain.com <Proxy balancer://mycluster> BalancerMember ht\u200ctp://10.176.42.144:80 #minion1 BalancerMember ht\u200ctp://10.176.42.148:80 #minion2 BalancerMember ht\u200ctp://10.176.42.150:80 #minion3 Require all granted # all requests are allowed ProxySet lbmethod = byrequests # balancer setting, round-robin </Proxy> ProxyPass / balancer://mycluster/ </VirtualHost>","title":"Load Balance"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#cacheing-and-proxy","text":"Apache can act as a forward or reverse proxy that is managed by mod_proxy and mod_proxy_http . However, there are special-purpose tools which do caching much more efficiently: Varnish: A RAM and disk-based cache which uses the Linux kernel's swapping mechanism to speed up caching. Squid: A general-purpose caching proxy. # enable a reverse proxy to an internal server <Location /foo> ProxyPass ht\u200ctp://appserver1.example.com/foo </Location> # alternative syntax ProxyPass /foo ht\u200ctp://appserver1.example.com/foo","title":"Cacheing and Proxy"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#c10k-problem-and-speciality-http-servers","text":"The C10k Problem describes the issues in making a web server which can scale to ten thousand concurrent connections/requests. Specialty HTTP servers have been developed to deal with different issues. cherokee - innovative web-based configuration panel, very fast in serving dynamic and static content nginx - has a small resource footprint, scales well from small servers to high performance web servers lighttpd - power some high-profile sites, light-weight and scales well","title":"C10k Problem and Speciality HTTP Servers"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#email-servers","text":"Email programs and daemons have multiple roles and utilize various protocols. Mail User Agent (MUA) is the main role of email client for composing and reading emails. It uses Internet Message Access Protocol (IMAP) or Post Office Protocol (POP3). Mail Submission Program (MSP) is the role of email client responsible for sending mails through the Mail Transfer Agent (MTA) using SMTP. Mail Transfer Agent (MTA) is the main role of email server, responsible for starting the process of sending the message to the recipient by looking up the recipient and sending the message to their MTA using SMTP. Mail Delivery Agent (MDA) is the role of email server for receiving and storing email for future retrieval. MTA uses SMTP, Local Mail Transfer Protocl (LMTP) or other protocols to transfer message to MDA. SMTP is a TCP/IP protocol used as an Internet standard for electronic mail transmission. It uses a plain \"English\" syntax such as HELO, MAIL, RCPT, DATA, or QUIT . SMTP is easily tested using telnet . POP3 is one of the main protocols used by MUAs to fetch mail. By default, the protocol downloads the messages and deletes them from the server. It is simpler yet less flexible protocol. IMAP is the other main protocol used by MUA to fetch mail. Messages are managed on the server and left there. Copies are downloaded to the MUA. This protocol is more complex and more flexible than POP3.","title":"Email Servers"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#email-lifecycle","text":"The email life cycle looks like this: You compose an email using your MUA. Your MUA connects to your outbound MTA via SMTP, and sends the message to be delivered. Your outbound MTA connects to the inbound MTA of the recipient via SMTP, and sends the message along. (Note: This step can happen more than once). Once the message gets to the final destination MTA, it is delivered to the MDA. This can happen over SMTP, LMTP or other protocols. The MDA stores the message (on disk as a file, or in a database, etc). The recipient connects (via IMAP, POP3 or a similar protocol) to their email server, and fetches the message. The IMAP or POP daemon fetches the message out of the storage and sends it to the MUA. The message is then read by the recipient.","title":"Email lifecycle"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#postfix","text":"Postfix has components that does MTA and MDA. In postfix /etc/postfix/main.cf contains location information for the alias database; /etc/aliases ( newaliases command) for system-wide redirection of mail, ~/.forward for user-configurable redirection of mail. /etc/aliases generally stores data in a .bdm or hash format, with format name : val1, val2, val3 ... The postconf command can be used to customize main.cf . Some usual candidates for customization in main.cf : The domain name to use for outbound mail (myorigin). The domains to receive mail for (mydestination). The clients to allow relaying of mail (mynetworks). The destinations to relay mail to (relay_domains). The delivery method, indirect or direct (relayhost). Trust hosts for blindly forward email (mynetworkds_style). IP addresses to listen on for incoming connections (inet_interfaces). Within Postfix there is a process called master that controls all other Postfix processes that are involved in moving mail. The defaults in the /etc/postfix/master.cf file are usually sufficient. SMTP is a clear-text protocol, security is a concern; postfix by default has no SSL/TLS encryption and no spam filtering. Postfix does not provide a Simple Authentication and Security Layer (SASL) mechanism itself and relies on proper set up. Postfix daemon support the Cyrus SASL or the Dovecot SASL mechanisms. smtpd_sasl_type = dovecot smtpd_sasl_path = private/auth smtpd_sasl_auth_enable = yes broken_sasl_auth_clients = yes Postfix SASL authentication with Dovecot can be enabled over a UNIX Domain Socket and over TCP. To enable UNIX accounts to authenticate over a UNIX Domain Socket, edit the /etc/dovecot/conf.d/10-master.conf file: service auth { unix_listener auth-userdb { } unix_listener /var/spool/postfix/private/auth { } } When testing plain-text SASL authentication, you will need to submit a base64 encoded username and password, i.e. echo -en \"\\0user\\0password\" | base64 , then test SMTP auth with AUTH PLAIN <hash_string> To enable TLS encryption: smtpd_tls_cert_file = /etc/postfix/server.pem smtpd_tls_key_file = $smtpd_tls_cert_file smtpd_tls_security_level = may smtpd_tls_auth_only = yes Some ways to monitor postfix: pflogsumm - a Perl script which translates Postfix log files into a human-readable summary. qshape - prints the domains and ages of items in the Postfix queue, useful for determining where emails are getting stuck in queues. The output of the command displays the distribution of the items in the Postfix queues by recipient or sender domain. mailgraph - creates RRD graphics of mail logs and allows monitor trends in your email server. SpamAssassin can be used as a milter (mail filter) or a standalone MDA. Other tools exist like Sender Policy Framework, DomainKeys.","title":"Postfix"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#dovecot","text":"Dovecot is an open-source IMAP/POP3 server. Dovecot is secure, easy to configure and is standards compliant. The doveconf utility parses the configuration file ( /etc/dovecot/dovecot.conf, /etc/dovecot/conf.d/ ) for both the Dovecot daemons and for debugging purposes. The common Dovecot setup binds to a specific IP address, defines the protocols to serve, applies password restrictions, accepts some clients with minor protocol issues, and points to user and password databases: listen = 10 .20.34.111 protocols = imap pop3 lmtp disable_plaintext_auth = yes imap_client_workarounds pop3_client_workarounds # SSL/TSL config in /etc/dovecot/conf.d/10-ssl.conf ssl = yes ssl_cert = < cert-file > # suggested permissions root:root 0444 ssl_key = < private-key-file > # suggested permissions root:root 0400 A working email client is the easiest way to troubleshoot and test an IMAP or POP3 server. The mutt email client works well.","title":"Dovecot"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#file-sharing","text":"The File Transfer Protocol (FTP) is one of the first protocols of the Internet. Data is sent in plain text. Clients have interactive or non-interactive modes. FTP Active transfer mode let server push data to the client on a random high-numbered port. This method is not compatible with firewalls or NAT networks. FTP Passive transfer mode let client request a passive connection and the server opens a random high-numbered port for data transfer.","title":"File Sharing"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#vsftpd","text":"Very Secure FTP Daemon ( vsftpd ) has enhanced security features than FTP server: Virtual IPs Virtual users Stand-alone daemon or inetd-ready Per-user configuration Per-source configuration Optional SSL integration Verify that anonymous access is enabled in vsftpd.conf : anonymous_enable=YES . To allow system users to create authenticated FTP sessions, you should make the following changes to vsftpd.conf : local_enable=YES write_enable=YES local_umask=022 vsftpd uses PAM by default for authentication ( /etc/pam.d/vsftpd ). System security is enforced by the files /etc/vsftpd/ftpusers, /etc/vsftpd/users_list . Account names listed in ftpusers are not allowed to login via FTP. Depending on the value of the userlist_deny setting in vsftpd.conf, the users in /etc/vsftpd/users_list act as either allow only the users in the list, or explicitly deny the listed users.","title":"vsftpd"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#rsync","text":"The rsync protocol was written as a replacement for rcp and uses an advanced algorithm to intelligently transfer files. Only the files or parts of files which have changed are copied. rsync uses Delta encoding and requires the source and destination to be specified (either or both may be remote). rsync protocol does not have in-transit security. However, rsync can be tunneled over the SSH protocol. rsync is mostly used over SSH, but starting the rsync daemon is as easy as rsync --daemon . When running as a daemon, rsync will read the /etc/rsyncd.conf configuration file. Each client that connects to the rsync daemon will force rsync to re-read the configuration. The /etc/rsyncd.conf file defines global options, as well as rsync modules which will be served by the rsync daemon. When running the rsync command, it will use any transparent remote shell. The default shell is ssh, which encrypts the traffic. The rsync protocol can be invoked with the rsync:// URI in the command or with rsyncd . i.e. rsync -av root@server:/etc/. /srv/backup/server-backup/etc/. backs up an entire directory.","title":"rsync"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#scp-and-sftp","text":"scp (Secure Copy) is non-interactive, while sftp (Simple File Transfer Protocol) is interactive way to transfer files and they both use your system or the user ssh client configuration files ( ~/.ssh/config ).","title":"scp and sftp"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#webdav","text":"WebDAV is an extension to HTTP for read/write access to resources and is available on most operating systems. A command-line tool cadaver can be used to manipulate a WebDAV share. The Apache module mod_dav is one method to enable WebDAV and requires a defined lock file, which should be writable by the Apache user: DavLockDB davlockdb . WebDAV can be enabled in either a <Directory> or <Location> stanza. However, it is more secure to create an alias and use the directory options to increase security. Uploads, PUT, POST, and similar methods should not be allowed, except for by an authenticated user.","title":"WebDAV"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#bittorrent","text":"BitTorrent is a protocol which allows for very efficient transfer of large files or directories using a distributed peer-to-peer architecture. A file served over BitTorrent is divided into small chunks and is distributed by anyone who is connected to the same tracker. This allows for much lesser resource use by the server hosting the original content, as the file needs to be downloaded only once. BitTorrent becomes more efficient the more concurrent clients are participating. Tools available are rtorrent as a CLI client and mktorrent as a CLI for creating .torrent files.","title":"BitTorrent"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#advanced-networking","text":"","title":"Advanced Networking"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#routing","text":"Routing commands include route and ip route . Dynamic routing protocols include RIP, OSPF, BGP and IS-IS. To create a new run time route, do ip route add 10.1.11.0/24 via 10.30.0.101 dev eth2 To create a static rule which will survive a reboot, do: # On CentOS, edit the /etc/sysconfig/network-scripts/route-<INTERFACE> file and add a line like this: 10 .1.11.0/24 via 10 .30.0.101 dev eth2 # On Ubuntu, edit the /etc/network/interfaces file and add a line like this: ip route add -net 10 .1.11.0/24 gw 10 .30.0.101 dev eth2 # On OpenSUSE, edit the /etc/sysconfig/network/ifroute-<INTERFACE> file and add a line like this: 10 .1.11.0/24 10 .30.0.101 - eth2 # Network Manager systems can use: nmcli con modify \"connection-name\" ipv4.routes \"10.1.11.0/24 10.30.0.101 99\" The downside to static routes is inflexibility. Dynamic routing protocols are more efficient at detecting and fixing routing problems quickly.","title":"Routing"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#vlan","text":"VLANs use functionality in the switches and routers. The most common use for VLANs is to bridge switches together. A VLAN is also a method for securing two or more LANs from each other on the same set of switches. Creating a trunk connection (802.1q is one such protocol between two switches essentially connects the networks together. VLANs can be created on the trunked together switches to then isolate specific ports on each switch to belong to a Virtual LAN. VLANs use optional functionality within the packet to identify which VLANs are being used. When VLANs are enabled, an additional header is added to the packet. This is the 802.1Q or dot1q header. The 802.1Q header contains: Tag Protocol ID (TPID), a 16bit identifier to distinguish between a VLAN tagged frame or it indicates the EtherType. The value of x8100 indicates an 802.1Q tagged frame. The next 16bits are the Tag Control Information (TCI), comprising of: Priority Code Point (PCP), a 3bit field that indicates the 802.1q priority class. See the IEEE P802.1Q webpage for more details. Drop Eligible Indicator (DEI). This 1bit flag indicates the frame may be dropped when network congestion occurs. The field may be used alone or in conjunction with the PCP. This field used to be the Canonical Format Indicator (CFI), and was used for compatibility between Ethernet and Token Ring frames. VLAN Identifier ID (VID), a 12bit field indicating to which VLAN the frame belongs to.","title":"VLAN"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#dhcp-server","text":"The dhcpd daemon is configured with /etc/dhcp/dhcpd.conf and some options files: /etc/sysconfig/dhcpd (CentOS), /etc/default/isc-dhcp-server (Ubuntu), /etc/sysconfig/dhcpd (OpenSUSE) The dhcp server will only serve out addresses on an interface that it finds a subnet block defined in the /etc/dhcp/dhcpd.conf file. Additional or different daemon command line options may be passed to the daemon at start time by the systems' drop-in files. Global options are settings which should apply to all the hosts in a network. You can also define options on a per-network basis. A sample configuration would be: subnet 10.5.5.0 netmask 255.255.255.224 { range 10.5.5.26 10.5.5.30; option domain-name-servers ns1.internal.example.org; option domain-name \"internal.example.org\"; option routers 10.5.5.1; option broadcast-address 10.5.5.31; default-lease-time 600; max-lease-time 7200; }","title":"DHCP Server"},{"location":"Linux/Concepts/Linux_Foundation_Network_Admin/#network-time-protocol","text":"The security of many encryption systems is highly dependent on proper time. NTP time sources are divided up into strata . A strata 0 clock is a special purpose time device (atomic clock, GPS radio, etc). A strata 1 server is any NTP server connected directly to a strata 0 source (over serial or the like). A strata 2 server is any NTP server which references a strata 1 server using NTP. A strata 3 server is any NTP server which references a strata 2 server using NTP. NTP may function as a client, a server, or a peer: Client: Acquires time from a server or a peer. Server: Provides time to a client. Peers: Synchronize time between other peers, regardless of the defined servers. Some NTP applications implementations are ntp , chrony , or systemd-timesyncd . The ntpdc -c peers command can show the time difference between the local system and configured time servers. The timedatectl command is in many distributions and may be used to query and control the system time and date. To configure the ntpd server, allow clients to request time (restrict), set up access for peers, declare the local machine to be a time reference, regulate who can query the time server with ntpq and ntpdc commands and start the NTP daemon. A good NTP server is only as good as its time source. The NTP Pool Project was created to alleviate the load that was crippling the small number of NTP servers. To configure your NTP server to use the NTP pool, edit the /etc/ntp.conf file and add or edit the following settings: driftfile /var/lib/ntp/ntp.drift pool 0.pool.ntp.org pool 1.pool.ntp.org pool 2.pool.ntp.org pool 3.pool.ntp.org","title":"Network Time Protocol"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/","text":"This set of notes were taken from the Linux Foundation Course: Essentials of Linux System Administration (LFS201) . Linux Filesystem Tree Layout \u00b6 Linux's One Big Filesystem - diagrammed as an inverted tree with root directory / at top of tree. The large logical filesystem may have many distinct filesystems mounted at various points appear as subdirectories. Data Distinctions Sharable - data can be shared between hosts Non-sharable - data that must be specific to a particular host Static - data such as binaries, libraries, documentation, that does not change without system admin's actions Variable - data that change without a system admin's help The Filesystem Hierarchy Standard (FHS) document specifies the main directories that need to be present on a Linux host. /bin \u00b6 Contains executable programs and scripts needed by both system administrators and unprivileged users, which are required when no other filesystems have yet been mounted (single user or recovery mode). Required programs which must exist in /bin/ include: cat, chgrp, chmod, chown, cp, date, dd, df, dmesg, echo, false, hostname, kill, ln, login, ls, mkdir, mknod, more, mount, mv, ps, pwd, rm, rmdir, sed, sh, stty, su, sync, true, umount and uname /boot \u00b6 Essential files for booting the system. Stores data used before the kernel begins executing user-mode programs vmlinz - the compressed Linux kernel initramfs or initrd - initial RAM filesystem, mounted before the real root fs is available config - configure the kernel compilation System.map - Kernel symbol table, used for debugging /dev \u00b6 Contains special device files (also known as device nodes) which represent devices built into or connected to the system. Such device files represent character (byte-stream) and block I/O devices. Network devices do not have device nodes in Linux, and are instead referenced by name, such as eth1 or wlan0. All modern Linux distributions use the udev system, which creates nodes in /dev only as needed when devices are found. On ancient systems (or embedded devices, it can be created by MAKEDEV or mknod at install or at any other time, as needed. /etc \u00b6 Contains machine-local configuration files and some startup scripts. Files and directories which may be found in this directory include: csh.login, exports, fstab, ftpusers, gateways, gettydefs, group, host.conf, hosts.allow, hosts.deny, hosts,equiv, hosts.lpd, inetd.conf, inittab, issue, ld.so.conf, motd, mtab, mtools.conf, networks, passwd, printcap, profile, protocols, resolv.conf, rpc, securetty, services, shells, syslog.conf /etc/skel - Contains skeleton files used to populate newly created home directories. /etc/systemd - Contains or points to configuration scripts for starting, stopping system services when using systemd. /etc/init.d - Contains startup and shut down scripts when using System V initialization. /etc/default/ - Contains configurations for many default actions, like cron, useradd /opt \u00b6 This directory is designed for software packages that wish to keep all or most of their files in one isolated place, rather than scatter them all over the system in directories shared by other software. For example, if dolphy_app were the name of a package which resided under /opt, then all of its files should reside in directories under /opt/dolphy_app, including /opt/dolphy_app/bin for binaries and /opt/dolphy_app/man for any man pages. This can make both installing and uninstalling software relatively easy, as everything is in one convenient isolated location in a predictable and structured manner. The directories /opt/bin, /opt/doc, /opt/include, /opt/info, /opt/lib, and /opt/man are reserved for local system administrator use. Packages may provide files which are linked or copied to these reserved directories, but the packages must also be able to function without the programs being in these special directories. Most systems do not populate these directories. /proc \u00b6 Mount point for a pseudo-filesystem, where all information resides only in memory. The kernel exposes some important data structures through /proc entries Important pseudo-files, including /proc/interrupts, /proc/meminfo, /proc/mounts, and /proc/partitions , provide an up-to-the-moment glimpse of the system's hardware. Others, like /proc/filesystems and the /proc/sys/ directory, provide system configuration information and interfaces. The process directories contain information about each running process on the system. /sys \u00b6 Mount point for the sysfs pseudo-filesystem where all information resides only in memory. It contains information about devices and drivers, kernel modules, system configuration structures, etc. sysfs is used both to gather information about the system, and modify its behavior while running. Almost all pseudo-files in /sys contain only one line, or value. /sbin \u00b6 Contains binaries essential for booting, restoring, recovering, and/or repairing in addition to those binaries in the /bin directory. The following programs should be included in this directory (if their subsystems are installed): fdisk, fsck, getty, halt, ifconfig, init, mkfs, mkswap, reboot, route, swapon, swapoff, update /usr \u00b6 Can be thought of as a secondary hierarchy used for files which are not needed for system booting. This directory is typically read-only data. It contains binaries which are not needed in single user mode. /var \u00b6 Contains variable (or volatile) data files that change frequently during system operation. These include: Log files Spool directories and files Administrative data files Transient and temporary files, such as cache contents. For security reasons, it is often considered a good idea to mount /var as a separate filesystem. Furthermore, if the directory gets filled up, it should not lock up the system. /run \u00b6 The purpose of /run is to store transient files: those that contain runtime information, which may need to be written early in system startup, and which do not need to be preserved when rebooting. Generally, /run is implemented as an empty mount point, with a tmpfs ram disk (like /dev/shm ) mounted there at runtime. Thus, this is a pseudo-filesystem existing only in memory. Some existing locations, such as /var/run and /var/lock , will be now just symbolic links to directories under /run. Processes \u00b6 Definition \u00b6 A program is a set of instructions, along with any internal or external data used while carrying the instructions out. People often distinguish between programs, which are compiled into a binary executable form; and scripts, which need to be run by an interpreter such as bash, Python or Perl. A process is an instance of a program in execution. Every process has a pid (Process ID), a ppid (Parent Process ID), and a pgid (Process Group ID). Every process has program code, data, variables, file descriptors, and an environment. For historical reasons, the largest PID has been limited to a 16-bit number, or 32768 . It is possible to alter this value by changing /proc/sys/kernel/pid_max . Eventually when process id reaches the max, it will start again at PID = 300 . A program may be composed of multiple simultaneous threads (multithreading), each of which is considered as its own process. Process Attributes \u00b6 At any given moment, the process may take a snapshot of itself by trapping the state of its CPU registers, where it is executing in the program, what is in the process' memory, and other information. This is the context (state) of the process, which is critical to the kernel's ability to do context switching . Every process has permissions based on which user has called it to execute. It may also have permissions based on who owns its program file. Programs which are marked with an \"s\" execute bit have a different \"effective\" user id than their \"real\" user id. These programs are referred to as setuid programs . They run with the user-id of the user who owns the program, where a non-setuid program runs with the permissions of the user who starts it. Note that setuid programs owned by root can be a security problem . Also Note that setuid bit only works on binary executables , not on scripts, as a security measure. Every process has resources such as allocated memory, file handles, etc. When a process is started, it is isolated in its own user space to protect it from other processes. Processes do not have direct access to hardware. Hardware is managed by the kernel, so a process must use system calls to indirectly access hardware. System calls are the fundamental interface between an application and the kernel. Control process with ulimit \u00b6 ulimit is a built-in bash command that displays or resets a number of resource limits associated with processes running under a shell. The changes only affect the current shell . To make changes that are effective for all logged-in users, you need to modify /etc/security/limits.conf A system administrator may need to change some of these values in either direction: To restrict capabilities so an individual user and/or process cannot exhaust system resources, such as memory, cpu time or the maximum number of processes on the system. To expand capabilities so a process does not run into resource limits; for example, a server handling many clients may find that the default of 1024 open files makes its work impossible to perform. Creating Processes \u00b6 An average Linux system is always creating new processes. This is often called forking ; the original parent process keeps running , while the new child process starts. Often, rather than just a fork, one follows it with an exec , where the parent process terminates, and the child process inherits the process ID of the parent . Take sshd daemon as an example: it is started when the init process executes the sshd init script, then the daemon process listens for ssh requests from remote users.When a request is received, sshd creates a new copy of itself to service the request. Each remote user gets their own copy of the sshd daemon running to service their remote login. The sshd process will start the login program to validate the remote user. If the authentication succeeds, the login process will fork off a shell (say bash) to interpret the user commands, and so on. systemd-based systems run a special process named kthreadd with pid=2 whose job is to adopt orphaned children , who will then show ppid=2. What happens when a user executes a command in a command shell interpreter, such as bash? A new process is created (forked from the user's login shell). A wait system call puts the parent shell process to sleep. The command is loaded onto the child process's space via the exec system call. In other words, the code for the command replaces the bash program in the child process's memory space. The command completes executing, and the child process dies via the exit system call. The parent shell is re-awakened by the death of the child process and proceeds to issue a new shell prompt. The parent shell then waits for the next command request from the user, at which time the cycle will be repeated. If a command is issued for background processing (by adding an ampersand & at the end of the command line), the parent shell skips the wait request and is free to issue a new shell prompt immediately, allowing the background process to execute in parallel . Otherwise, for foreground requests, the shell waits until the child process has completed or is stopped via a signal. Use jobs -l to view background processes. Use kill -15 %<job_number> to send a SIG_TERM to that process. Use fg %<job_number to bring a background process to the foreground. Use bg %<job_number> after CTRL+Z to resume a stopped process. Some shell commands (such as echo and kill) are built into the shell itself, and do not involve loading of program files. For these commands, neither a fork nor an exec is issued for the execution. Process States \u00b6 Processes can be in one of several possible states. The scheduler manages all of the processes. The process state is reported by the process listing. Running - process is executing on a CPU or sitting in the run queue waiting for a new time slice Sleeping / Waiting - process is waiting on a request (usually I/O) Stopped - process has been suspended, commonly when the program's memory, CPU registers, or other attributes are being examined, after which can be resumed. Zombie / Defunct- process terminates and no other process or parent process requests for its exit state. if the parent of a process dies, it is adopted by init (PID=1) or kthreadd (PID=2) Execution Modes \u00b6 A process may be executing in either user mode or system mode (kernel mode). What instructions can be executed depends on the mode and is enforced at the hardware, not software, level. The mode is not a state of the system; it is a state of the processor, as in a multi-core or multi-CPU system each unit can be in its own individual state. User Mode \u00b6 Each process executing in user mode has its own memory space , parts of which may be shared with other processes; except for the shared memory segments, a user process is not able to read or write into or from the memory space of any other process. Even a process run by the root user or as a setuid program runs in user mode, except when jumping into a system call, and has only limited ability to access hardware. System (Kernel) Mode \u00b6 CPU has full access to all hardware on the system , including peripherals, memory, disks, etc. If an application needs access to these resources, it must issue a system call, which causes a context switch from user mode to kernel mode. This procedure must be followed when reading and writing from files, creating a new process, etc. Application code never runs in kernel mode, only the system call itself which is kernel code. When the system call is complete, a return value is produced and the process returns to user mode with the inverse context switch. Daemons \u00b6 A daemon process is a background process whose sole purpose is to provide some specific service to users of the system. They can be quite efficient because they only operate when needed. Many daemons are started at boot time. Daemon names often (but not always) end with d, e.g. httpd and systemd-udevd. Daemons may respond to external events (systemd-udevd) or elapsed time (crond). Daemons generally have no controlling terminal and no standard input/output - devices. Daemons sometimes provide better security control. Some examples include xinetd, httpd, lpd, and vsftpd. nice value set priorities \u00b6 Process priority can be controlled through the nice and renice commands. renice is used to raise or lower the nice value of an already running process. Since the early days of UNIX, the idea has been that a nice process lowers its priority to yield to others. Thus, the higher the niceness is, the lower the priority. The niceness value can range from -20 (the highest priority) to +19 (the lowest priority). Note that increasing the niceness of a process does not mean it won't run; it may even get all the CPU time if there is nothing else with which to compete. By default, only a superuser can decrease the niceness. After a non-privileged user has increased the nice value, only a superuser can lower it back. It is possible to give normal users the ability to decrease their niceness within a predetermined range, by editing /etc/security/limits.conf . Static and Shared Libraries \u00b6 static library - The code for the library functions is inserted in the program at compile time, and does not change thereafter, even if the library is updated. shared library - The code for the library functions is loaded into the program at run time, and if the library is changed later, the running program runs with the new library modifications. Using shared libraries is more efficient because they can be used by many applications at once; memory usage, executable sizes, and application load time are reduced. Shared Libraries are also called Dynamic Link Libraries (DLLs). Under Linux, shared libraries are (and must be) carefully versioned to avoid DLL Hell. ldd can be used to ascertain what shared libraries an executable requires. It shows the .so name of the library and what file it actually points to. ldconfig is generally run at boot time (but can be run anytime), and uses /etc/ld.so.conf , which lists the directories that will be searched for shared libraries. ldconfig must be run as root, and shared libraries should only be stored in system directories when they are stable and useful. The linker also first search any directories specified in the environment variable LD_LIBRARY_PATH , a colon separated list of directories. Signals \u00b6 Signals are one of the oldest methods of Inter-Process Communication (IPC) and are used to notify processes about asynchronous events (or exceptions). Signals can only be sent between processes owned by the same user or from a process owned by the superuser to any process. When a process receives a signal, what it does will depend on the way the program is written, or just respond according to system defaults. SIGKILL(#9) and SIGSTOP(#19) cannot be handled, and will always terminate the program. Use kill -l to view list of signals. man 7 signal will give further documentation. Generally, signals are used to handle two things: Exceptions detected by hardware (such as an illegal memory reference) Exceptions generated by the environment (such as the premature death of a process from the user's terminal) Since a process cannot send a signal directly to another process, it must ask the kernel to send the signal. We use kill to send signals (by either number or name) to a process. killall kills all processes with a given name. pkill is similar to kill but uses name instead of pid. Note that POSIX says one should use signal names, not numbers, which are allowed to be completely implementation dependent. $ kill 1234 $ kill -9 1234 $ kill -SIGTERM 1234 $ killall bash $ killall -9 bash $ killall -SIGKILL bash $ pkill -HUP rsyslogd Signal Value Default Action POSIX? Meaning SIGHUP 1 Terminate Yes Hangup detected on controlling terminal or death of controlling process SIGINT 2 Terminate Yes Interrupt from keyboard SIGQUIT 3 Core dump Yes Quit from keyboard SIGILL 4 Core dump Yes Illegal instruction SIGTRAP 5 Core dump No Trace/breakpoint trap for debugging SIGABTR SIGIOT 6 Core dump Yes Abnormal termination SIGBUS 7 Core dump Yes Bus error SIGFPE 8 Core dump Yes Floating point exception SIGKILL 9 Terminate Yes Kill signal (cannot be caught or ignored) SIGUSR1 10 Terminate Yes User-defined signal 1 SIGSEGV 11 Core dump Yes Invalid memory reference SIGUSR2 12 Terminate Yes User-defined signal 2 SIGPIPE 13 Terminate Yes Broken pipe: write to pipe with no readers SIGALRM 14 Terminate Yes Timer signal from alarm SIGTERM 15 Terminate Yes Process termination SIGSTKFLT 16 Terminate No Stack fault on math co-processor SIGCHLD 17 Ignore Yes Child stopped or terminated SIGCONT 18 Continue Yes Continue if stopped SIGSTOP 19 Stop Yes Stop process (can not be caught or ignored) SIGTSTP 20 Stop Yes Stop types at tty SIGTTIN 21 Stop Yes Background process requires tty input SIGTTOU 22 Stop Yes Background process requires tty output SIGURG 23 Ignore No Urgent condition on socket (4.2 BSD) SIGXCPU 24 Core dump Yes CPU time limit exceeded (4.2 BSD) SIGXFSZ 25 Core dump Yes File size limit exceeded (4.2 BSD) SIGVTALRM 26 Terminate No Virtual alarm clock (4.2 BSD) SIGPROF 27 Terminate No Profile alarm clock (4.2 BSD) SIGWINCH 28 Ignore No Window resize signal (4.3 BSD, Sun) SIGIO SIGPOLL 29 Terminate No I/O now possible (4.2 BSD) (System V) SIGPWR 30 Terminate No Power Failure (System V) SIGSYS SIGUNUSED 31 Terminate No Bad System Called. Unused signal Package Management Systems \u00b6 Use of Packages and Package Management System helps keep track of files and metadata in an automated, predictable and reliable way, so that system administrators can automate the process of installing, upgrading, configuring and removing software packages and scale to thousands of systems without requiring manual work on each individual system. Automation: No need for manual installs and upgrades. Scalability: Install packages on one system, or 10,000 systems. Repeatability and predictability. Security and auditing. A given package may contain executable files, data files, documentation, installation scripts and configuration files. Also included are metadata attributes such as version numbers, checksums, vendor information, dependencies, descriptions, etc. Upon installation, all that information is stored locally into an internal database, which can be conveniently queried for version status and update information. Package Types \u00b6 Binary Packages Binary packages contain files ready for deployment, including executable files and libraries. These are architecture dependent. Source Packages Source packages are used to generate binary packages; you should always be able to rebuild a binary package from the source package. One source package can be used for multiple architectures. Architecture-independent Architecture-independent packages contain files and scripts that run under script interpreters, as well as documentation and configuration files. Meta-packages Meta-packages are groups of associated packages that collect everything needed to install a relatively large subsystem, such as a desktop environment, or an office suite, etc. Packaing Tool Levels \u00b6 Low Level Utilities This simply installs or removes a single package, or a list of packages, each one of which is individually and specifically named. Dependencies are not fully handled, only warned about or produce an error: If another package needs to be installed, first installation will fail. If the package is needed by another package, removal will fail. The rpm and dpkg utilities play this role for the packaging systems that use them. High Level Utilities If another package or group of packages needs to be installed before software can be installed, such needs will be satisfied. If removing a package interferes with another installed package, the administrator will be given the choice of either aborting, or removing all affected software. The dnf and zypper utilities (and the older yum ) take care of the dependency resolution for rpm systems, and apt-get and apt-cache and other utilities take care of it for dpkg systems. Package Sources \u00b6 Every distribution has one or more package repositories where system utilities go to obtain software and to update with new versions. It is the job of the distribution to make sure all packages in the repositories play well with each other. There are always other external repositories which can be added to the standard distribution-supported list. i.e. EPEL (Extra Packages for Enterprise Linux) fit well with RHEL since their source is Fedora and the maintainers are close to Red Hat. Building your own package allows you to control exactly what goes in the software and exactly how it is installed. Creating needed symbolic links Creating directories as needed Setting permissions Anything that can be scripted. Source control Management System \u00b6 Git \u00b6 Git has two important data structures: an object database and a directory cache . The object database contains objects of three varieties: Blobs: Chunks of binary data containing file contents Trees: Sets of blobs including file names and attributes, giving the directory structure Commits: Changesets describing tree snapshots. The directory cache captures the state of the directory tree. RPM \u00b6 RPM stood for Redhat Package Manager. All files related to a specific task or a subsystem are packaged into a single file, which also contains information about how and where to install and uninstall the files. RPM allows builders to keep the changes necessary for building on Linux separate from the original source. This capability facilitates incorporating new versions of the code, because build-related changes are all in one place. It also facilitates building versions of Linux for different architectures. rpm does not retrieve packages over the network unless given a specific URL to draw from. It installs from the local machine using absolute or relative paths. The standard naming convention for a binary RPM package is: <name>-<version>-<release>.<distro>.<architecture>.rpm sed-4.5-2.e18.x86_64.rpm /var/lib/rpm is the default system directory which holds RPM database files in the form of Berkeley DB hash files. The database files should not be manually modified; updates should be done only through the use of the rpm program. One can use --dbpath to specify another location for database, and --rebuilddb to rebuild the database indices from the installed package headers. Helper programs and scripts used by RPM reside in /usr/lib/rpm . You can create an rpmrc file to specify default settings for rpm. By default, rpm looks for it in: /usr/lib/rpm/rpmrc /etc/rpmrc ~/.rpmrc specified by --rcfile Queries \u00b6 Run rpm inquiries with the -q option, which can be combined with numerous other query options: -f: allows you to determine which package a file came from -l: lists the contents of a specific package -a: all the packages installed on the system -i: information about the package -p: run the query against a package file instead of the package database --requires: return a list of prerequisites for a package --whatprovides: show what installed package provides a particular requisite package i.e. to list files within a package, do rpm -qilp <package_rpm> Verifying Packages \u00b6 Run rpm verify inquiries with the -V option to verify whether the files from a particular package are consistent with the system\u2019s RPM database. In the output (you only see output if there is a problem) each of the characters denotes the result of a comparison of attribute(s) of the file to the value of those attribute(s) recorded in the database. A single \u201d.\u201d (period) means the test passed, while a single \u201d?\u201d (question mark) indicates the test could not be performed (e.g. file permissions prevent reading). Otherwise, the character denotes the failure of the corresponding --verify test. S: filesize differs M: mode differs (permissions and file type) 5: MD5 sum differs D: device major/minor number mismatch L: readLink path mismatch U: user ownership differs G: group ownership differs T: mTime differs Installing package \u00b6 The command to install a package sudo rpm -ivh <package_name>... , or sudo rpm -Uvh <package_name>... for upgrading if package is already installed. sudo rpm -U --oldpackage <package_name>... for downgrading. Tasks performed during installation: Performs dependency checks Performs conflict checks Executes commands required before installation Deals intelligently with configuration files if doing an update, config files from original installation will be kept with .rpmsave extension Unpacks files from packages and installs them with correct attributes Executes commands required after installation Updates the system RPM database The -e <package_name> option causes rpm to uninstall (erase) a package. It would fail with an error message if the package you are attempting to uninstall is required by other packages on the system. A successful uninstall produces no output. --test option can be used for a dry-run. -vv is for more verbose information. Command sudo rpm -Fvh <package_name>... will attempt to freshen the packages, only when the older versions of the packages were installed, then upgrade will happen. Good for applying lots of patches at once. When upgrading Linux kernel, it is recommended to use -i instead of -U which will remove the older kernel and it is irreversible. DPKG \u00b6 DPKG (Debian Package) is the packaging system used to install, remove, and manage software packages under Debian Linux and other distributions derived from it. Package files have a .deb suffix and the DPKG database resides in the /var/lib/dpkg directory. The standard naming convention for a binary package is: <name>_<version>-<revision_number>_<architecture>.deb logrotate_3.14.0-4ubuntu3_amd64.deb For historical reasons, the 64-bit x86 platform is called amd64 rather than x86_64 In the Debian packaging system, a source package consists of at least three files: An upstream tarball, ending with .tar.gz. This is the unmodified source as it comes from the package maintainers. A description file, ending with .dsc, containing the package name and other metadata, such as architecture and dependencies. A second tarball that contains any patches to the upstream source, and additional files created for the package, and ends with a name .debian.tar.gz or .diff.gz, depending on distribution. Queries \u00b6 -l: List all packages installed -L: List files installed in the wget package -s: Show information about an installed package -I: Show information about a package file -c: List files in a package file -S: Show what package owns /etc/init/networking.conf -V: Verify the installed package's integrity Installing package \u00b6 The command to install or upgrade a package sudo dpkg -i package.deb . To remove all of an installed package except for its configurtion files, use sudo dpkg -r package . To complete remove a package include the configuration files, use sudo dpkg -P package . DNF and YUM \u00b6 The higher-level package management systems (such as dnf, yum, apt and zypper ) work with databases of available software and incorporate the tools needed to find, install, update, and uninstall software in a highly intelligent fashion. Use both local and remote repositories as a source to install and update binary, as well as source software packages. Automate the install, upgrade, and removal of software packages. Resolve dependencies automatically. Save time in search and download packages comparing to doing so manually. Configuration files are located in /etc/yum.repos.d directory and have a .repo extension. You can toggle use of a particular repo on or off by changing the value of enabled, or using the --disablerepo somerepo and --enablerepo somerepo options. [repo-name] name=Description of the repository baseurl=ht\u200ctp://somesystem.com/path/to/repo enabled=1 gpgcheck=1 dnf replaced yum during the RHEL/CentOS 7 to 8 transition. dnf is backwards compatible - almost all common yum commands still work. Queries \u00b6 dnf info package-name: Displays information about a package dnf list [installed | updates | available ]: Lists packages installed, available, or updates dnf search [keyword]: Find package by name or information in its metadata dnf grouplist: Shows information about package groups installed, available and updates dnf groupinfo packagegroup: Shows information about a package group dnf provides /path/to/file: Shows the owner of the package for file Installing package \u00b6 sudo dnf install package: Installs a package from a repository; also resolves and installs dependencies sudo dnf localinstall package-file: Installs a package from a local rpm file sudo dnf groupinstall 'group-name': Installs a specific software group from a repository; also resolves and installs dependencies for each package in the group sudo dnf remove package: Removes a package from the system sudo dnf update [package]: Updates a package from a repository (if no package listed, updates all packages) During installation (or update), if a package has a configuration file which is updated, it will rename the old configuration file with a .rpmsave extension. If the old configuration file will still work with the new software, it will name the new configuration file with a .rpmnew extension. some other useful actions to perform: - sudo dnf list \"dnf-plugin*\": Lists additional dnf plugins - sudo dnf repolist: Shows a list of enabled repositories - sudo dnf shell: Provides an interactive shell in which to run multiple dnf commands (the second form executes the commands in file.txt) - sudo dnf install --downloadonly package: Downloads the packages for you (it stores them in the /var/cache/dnf directory) - sudo dnf history: Views the history of dnf commands on the system, and with the correction options, even undoes or redoes previous commands - sudo dnf clean [packages|metadata|expire-cache|rpmdb|plugins|all]: Cleans up locally stored files and metadata under /var/cache/dnf. This saves space and does house cleaning of obsolete data Similar to dnf , zypper is the command line tool for installing and managing .rpm packages in SUSE Linux and openSUSE. APT \u00b6 For use on Debian-based systems, the APT (Advanced Packaging Tool) set of programs provides a higher level of intelligent services for using the underlying dpkg program, and plays the same role as dnf on Red Hat-based systems. The main utilities are apt-get and apt-cache . It can automatically resolve dependencies when installing, updating and removing packages. It accesses external software repositories, synchronizing with them and retrieving and installing software as needed. The APT system works with Debian packages whose files have a .deb extension. Read more about it from Debian packages webpage and the Ubuntu packages webpage Queries \u00b6 Queries are done using the apt-cache or apt-file utilities. You may have to install apt-file first, and update its database, as in: $ sudo apt-get install apt-file $ sudo apt-file update apt-cache search apache2: Searches the repository for a package named apache2 apt-cache show apache2: Displays basic information about the apache2 package apt-cache showpkg apache2: Displays detailed information about the apache2 package apt-cache depends apache2: Lists all dependent packages for apache2 apt-file search apache2.conf: Searches the repository for a file named apache2.conf apt-file list apache2: Lists all files in the apache2 package Installing package \u00b6 Used to install new packages or update a package which is already installed: sudo apt-get install [package]: Used to install new packages or update a package which is already installed sudo apt-get remove [package]: Used to remove a package from the system (this does not remove the configuration files) sudo apt-get --purge remove [package]: Used to remove a package and its configuration files from a system sudo apt-get update: Used to synchronize the package index files with their sources. The indexes of available packages are fetched from the location(s) specified in /etc/apt/sources.list sudo apt-get upgrade: Apply all available updates to packages already installed sudo apt-get autoremove: Gets rid of any packages not needed anymore, such as older Linux kernel versions sudo apt-get clean: Cleans out cache files and any archived package files that have been installed Linux System Monitoring \u00b6 Linux distributions come with many standard performance and profiling tools which make use of mounted pseudo-filesystems, especially /proc and secondarily /sys . Some frequently used utilities: Process and Load Monitoring Utility Purpose Package top Process activity, dynamically updated procps uptime How long the system is running and the average load procps ps Detailed information about processes procps pstree A tree of processes and their connections psmisc (or pstree) mpstat Multiple processor usage sysstat iostat CPU utilization and I/O statistics sysstat sar Display and collect information about system activity sysstat numastat Information about NUMA (Non-Uniform Memory Architecture) numactl strace Information about all system calls a process makes strace Memory Monitoring Utility Purpose Package free Brief summary of memory usage procps vmstat Detailed virtual memory statistics and block I/O, dynamically updated procps pmap Process memory map procps I/O Monitoring Utility Purpose Package iostat CPU utilization and I/O statistics sysstat sar Display and collect information about system activity sysstat vmstat Detailed virtual memory statistics and block I/O, dynamically updated procps Network Monitoring Utility Purpose Package netstat detailed networking statistics netstat iptraf Gather information on network interfaces iptraf tcpdump Detailed analysis of network packets and traffic tcpdump wireshark Detailed network traffic analysis wireshark sar \u00b6 sar stands for the Systems Activity Reporter. It is an all-purpose tool for gathering system activity and performance data and creating reports that are readable by humans. sar backend is sadc (system activity data collector), which is usually cron-based /etc/cron.d/sysstat and accumulates the statistics (logs in /var/log/sa ). Option Meaning -A Almost all information -b I/O and transfer rate statistics (similar to iostat) -B Paging statistics including page faults -d Block device activity (similar to iostat -x) -n Network statistics -P Per CPU statistics (as in sar -P ALL 3) -q Queue lengths (run queue, processes and threads) -r Memory utilization statistics -S Swap utilization statistics -u CPU utilization (default) -v Statistics about inodes and files and file handles -w Context switching statistics -W Swapping statistics, pages in and out per second -f Extract information from specified file, created by the -o option -o Save readings in the file specified, to be read in later with the -f option Log files \u00b6 System log files are under /var/log , controlled by the syslogd (usually rsyslogd on modern systems) daemon. Important system messages are located at /var/log/messages for RHEL and /var/log/syslog for Debian. boot.log holds system boot messages, and secure holds security-related messages. You can view new messages continuously as new lines appear with sudo tail -f /var/log/messages or view kernel-related messages with dmesg -w . logrotate program is run periodically and keeps four previous copies (by default) of the log files (optionally compressed) and is controlled by /etc/logrotate.conf . stress and stress-ng \u00b6 stress is a C language program written by Amos Waterland and is designed to place a configurable amount of stress by generating various kinds of workloads on the system. Install stress-ng (enhanced version of stress): $ git clone git://kernel.ubuntu.com/cking/stress-ng.git $ cd stress-ng $ make $ sudo make install # fork 8 CPU-intensive processes via sqrt() calculation # fork 4 IO-intensive processes via sync() # fork 6 mem-intensive processes via malloc() and each allocate 256MB by default (override with --vm-bytes 128M) # run stress test for 20 seconds $ stress-ng -c 8 -i 4 -m 6 -t 20s Process Monitoring \u00b6 ps \u00b6 ps displays characteristics and statistics associated with processes, all of which are garnered from the /proc directory associated with the process. ps has existed in all UNIX-like operating system variants and the options rule is a little different: UNIX options, which must be preceded by -, and which may be grouped. BSD options, which must not be preceded by -, and which may be grouped. GNU long options, each of which must be preceded by --. Some common choices of options are: $ ps aux # show all processes $ ps -elf # show process and parent process id, nice value $ ps -eL # show shorter summary about pid and commands $ ps -C \"bash\" # show process using that command $ ps -o pid,user,uid,priority,cputime,pmem,size,command # costomize the outputs pstree \u00b6 pstree gives a visual description of the process ancestry and multi-threaded applications. i.e. pstree -aAp 2408 checks process tree of process with PID 2408, which shows the child processes spawned by that process. Another way to see that is doing ls -l /proc/2408/task Use -p to show process IDs, use -H [pid] to highlight [pid] and its ancestors. top \u00b6 top is used to display processes with highest CPU usage. Processes are initially sorted by CPU usage. If not run in secure mode (top s) user can signal processes: Press the k key Give a PID when prompted Give a signal number when prompted Memory Montioring \u00b6 One can use free -m to see a brief summary of memory usage: total, used, free, shared, buff/cached, available. The pseudofile /proc/meminfo contains a wealth of information about how memory is being used. The /proc/sys/vm directory contains many tunable knobs to control the Virtual Memory system, which can be changed either by directly writing to the entry, or using the sysctl utility. The primary (inter-related) tasks are: Controlling flushing parameters; i.e., how many pages are allowed to be dirty and how often they are flushed out to disk. Controlling swap behavior; i.e., how much pages that reflect file contents are allowed to remain in memory, as opposed to those that need to be swapped out as they have no other backing store. Controlling how much memory overcommission is allowed, since many programs never need the full amount of memory they request, particularly because of copy on write (COW) techniques. The usual best practice is to adjust one thing at a time and look for effects. vmstat \u00b6 vmstat is a multi-purpose tool that displays information about memory, paging, I/O, processor activity and processes. i.e. vmstat [options] [delay] [count] If delay is given in seconds, the report is repeated at that interval count times; if count is not given, vmstat will keep reporting statistics forever until killed by a signal. If the option -S m is given, memory statistics will be in MB instead of KB. With the -a option, vmstat displays information about active and inactive memory, where active memory pages are those which have been recently used; they may be clean (disk contents are up to date) or dirty (need to be flushed to disk eventually). With the -d option to get a table of disk statistics. Use -p partition to view statistics for a particular partition only. OOM \u00b6 One way to deal with memory pressure would be to permit memory allocations to succeed as long as free memory is available and then fail when all memory is exhausted. Alternatively, use swap space on disk as \"secondary memory\" to push some of the resident memory out when under memory pressure. Linux also permits the system to overcommit memory (only for user processes, not kernel processes), so that it can grant memory requests that exceed the size of RAM plus swap. Every time a child process is forked, it receives a copy of the entire memory space of the parent. Linux uses the COW (copy on write) technique, unless one of the child processes modifies its memory space, no actual copy needs be made. However, the kernel has to assume that the copy might need to be done. You can modify overcommission by setting the value of /proc/sys/vm/overcommit_memory: 0: (default) Permit overcommission, but refuse obvious overcommits, and give root users somewhat more memory allocation than normal users. 1: All memory requests are allowed to overcommit. 2: Turn off overcommission. Memory requests will fail when the total memory commit reaches the size of the swap space plus a configurable percentage (50 by default) of RAM. This factor is modified changing /proc/sys/vm/overcommit_ratio. If available memory is exhausted, Linux invokes the OOM-killer (Out Of Memory) to decide which process(es) should be exterminated to open up some memory. A value called the badness is computed (which can be read from /proc/[pid]/oom_score ) for each process on the system and the order of the killing. oom_score_adj can be directly adjusted to override the badness score of a process. IO Monitoring \u00b6 Disk performance problems can be strongly coupled to other factors, such as insufficient memory or inadequate network hardware and tuning. Both real-time monitoring and tracing are necessary tools for locating and mitigating disk bottlenecks. A system can be considered as I/O-bound when the CPU is found sitting idle waiting for I/O to complete, or the network is waiting to clear buffers. What appears to be insufficient memory can result from too slow I/O; if memory buffers that are being used for reading and writing fill up, it may appear that memory is the problem, when the real problem is that buffers are not filling up or emptying out fast enough. Network transfers may also be waiting for I/O to complete and cause network throughput to suffer. iostat \u00b6 iostat is the basic workhorse utility for monitoring I/O device activity on the system. i.e. iostat [OPTIONS] [devices] [interval] [count] IO statistics given (broken out by disk partition and logical partitions if LVM is used): tps (I/O transactions per second; logical requests can be merged into one actual request) blocks read and written per unit time, where the blocks are generally sectors of 512 bytes total blocks read and written With -k shows results in KB instead of blocks; with -m shows in MB. With -N or -d shows the device name. With -x shows some extended statistics. iotop \u00b6 iotop displays a table of current I/O usage and updates periodically. The be and rt entries in the PRIO field stand for best effort and real time. ionice \u00b6 ionice utility lets you set both the I/O scheduling class and priority for a given process. i.e. ionice [-c class] [-n priority] [-p pid ] [COMMAND [ARGS] ] If a pid is given with the -p argument results apply to the requested process, otherwise it is the process that will be started by COMMAND with possible arguments. The -c parameter specifies the I/O scheduling class: 0 - default 1 - real time, get first access to the disk, can starve other processes; the priority defines how big a time slice each process gets 2 - best effort, programs serviced in round-robin fashion, according to priority settings (default) 3 - idle - no access to disk I/O unless no other program has asked for it for a defined period The Best Effort and Real Time classes take the -n argument which gives the priority, which can range from 0 to 7, with 0 being the highest priority. Benchmarking \u00b6 bonnie++ is a widely available benchmarking program that tests and measures the performance of drives and filesystems. Results can be read from the terminal window or directed to a file, and also to a csv format. Companion programs, bon_csv2html and bon_csv2txt , can be used convert to html and plain text output formats. fs_mark benchmark gives a low level bashing to file systems, using heavily asynchronous I/O across multiple directories and drives. I/O Scheduling \u00b6 The I/O scheduler provides the interface between the generic block layer and low-level physical device drivers. The I/O scheduling layer prioritize and order the requests from VM and VFS before they are given to the block devices. Any I/O scheduling algorithm has to satisfy certain requirements: Hardware access times should be minimized; i.e., requests should be ordered according to physical location on the disk. This leads to an elevator scheme where requests are inserted in the pending queue in physical order. Requests should be merged to the extent possible to get as big a contiguous region as possible, which also minimizes disk access time. Requests should be satisfied with as low a latency as is feasible; indeed, in some cases, determinism (in the sense of deadlines) may be important. Write operations can usually wait to migrate from caches to disk without stalling processes. Read operations, however, almost always require a process to wait for completion before proceeding further. Favoring reads over writes leads to better parallelism and system responsiveness. Processes should share the I/O bandwidth in a fair, or at least consciously prioritized fashion; even if it means some overall performance slowdown of the I/O layer, process throughput should not suffer inordinately. At least one of the I/O scheduling algorithms must be compiled into the kernel. The scheduler for each device can be selected or viewed at run time. i.e. /sys/block/sda/queue/scheduler and scheduler-specific tunables can be found in /sys/block/sda/queue/iosched . Filesystems and VFS \u00b6 A UNIX-like filesystem uses a tree hierarchy. Multiple filesystems can be merged together into a single tree structure. Linux uses a virtual filesystem layer (VFS) to communicate with the filesystem software. Local filesystems generally reside within a disk partition which can be a physical partition on a disk, or a logical partition controlled by a Logical Volume Manager (LVM). Filesystems can also be of a network nature and their true physical embodiment completely hidden to the local system across the network. inodes \u00b6 An inode is a data structure on disk that describes and stores file attributes such as name, location, file attributes (permissions, ownership, size, etc.), access & modify times and others. Filenames are not stored in the inode; they are stored in the directory that contains the files. Hard/soft links \u00b6 A directory file is a particular type of file that is used to associate file names and inodes. Two ways to associate (or link) a file name with an inode: Hard links point to an inode.\u200b All hard linked files have to be on the same filesystem. Changing the content of a hard linked file in one place may change it in other places. Soft (or symbolic) links point to a file name which has an associated inode. Soft linked files may be on different filesystems. If the target does not yet exist or is not yet mounting, it can be dangling. A nice article explaning the differences between hard and soft links: https://linuxgazette.net/105/pitcher.html When two or more directory entries to point to the same inode (hard links), a file can be known by multiple names, each of which has its own place in the directory structure. When a process refers to a pathname, the kernel searches directories to find the corresponding inode number. After the name has been converted to an inode number, the inode is loaded into memory and is used by subsequent requests. VFS \u00b6 When an application needs to access a file, it interacts with the VFS abstraction layer, which then translates all the I/O system calls (reading, writing, etc.) into specific code relevant to the particular actual filesystem. Neither the specific actual filesystem or physical media and hardware on which it resides need be considered by applications. Network filesystems (such as NFS) can also be handled transparently. This permits Linux to work with more filesystem varieties than any other operating system. Commonly used filesystems include ext4, xfs, btrfs, squashfs, nfs and vfat . A list of currently supported filesystems is at /proc/filesystems . The ones with nodev are special filesystems which do not reside on storage. Journaling Filesystems \u00b6 Journaling filesystems recover from system crashes or ungraceful shutdowns with little or no corruption, and do so very rapidly. In a journaling filesystem, operations are grouped into transactions . A transaction must be completed without error, atomically; otherwise, the filesystem is not changed. A log file is maintained of transactions. When an error occurs, usually only the last transaction needs to be examined. Disk Partitioning \u00b6 Reasons for doing disk partitions: Separation of user and application data from operating system files Sharing between operating systems and/or machines Security enhancement by imposing different quotas and permissions for different system parts Size concerns; keeping variable and volatile storage isolated from stable Performance enhancement of putting most frequently used data on faster storage media Swap space can be isolated from data and also used for hibernation storage. A common partition layout contains a /boot partition, a partition for the root filesystem / , a swap partition, and a partition for the /home directory tree. It is more difficult to resize a partition after installing the OS. common disk types \u00b6 SATA (Serial Advanced Technology Attachment) - SATA disks were designed to replace the old IDE (Integrated Drive Electronics) drives. They offer a smaller cable size (7 pins), native hot swapping, and faster and more efficient data transfer. They are seen as SCSI devices. SCSI (Small Computer Systems Interface) - SCSI disks range from narrow (8 bit bus) to wide (16 bit bus), with a transfer rate from about 5 MB per second (narrow, standard SCSI) to about 160 MB per second (Ultra-Wide SCSI-3). SCSI has numerous versions such as Fast, Wide, and Ultra, Ultrawide. SAS (Serial Attached SCSI) - SAS uses a newer point-to-point protocol, has a better performance than SATA disks and is better suited for servers. Learn more SAS vs SATA USB (Universal Serial Bus) - These include flash drives and floppies. And are seen as SCSI devices. SSD (Solid State Drives) - Modern SSD drives have come down in price, have no moving parts, use less power than drives with rotational media, and have faster transfer speeds. Internal SSDs are even installed with the same form factor and in the same enclosures as conventional drives. SSDs still cost a bit more, but price is decreasing. It is common to have both SSDs and rotational drives in the same machines, with frequently accessed and performance critical data transfers taking place on the SSDs. Disk drives \u00b6 Rotational disks are composed of one or more platters and each platter is read by one or more heads . Heads read a circular track off a platter as the disk spins. Circular tracks are divided into data blocks called sectors. A cylinder is a group which consists of the same track on all platters. The physical structural image has become less and less relevant as internal electronics on the drive actually obscure much of it. Use sudo fdisk -l /dev/sdc to list the partition table without entering interactive mode. fdisk is a menu-driven partition table editor and included in any Linux installation. It is the most standard and one of the most flexible of the partition table editors. As with any other partition table editor, make sure that you either write down the current partition table settings or make a copy of the current settings before making changes. No actual changes are made until you write the partition table to the disk by entering w. It is therefore important to verify your partition table is correct (with p) before writing to disk with w. If something is wrong, you can jump out safely with q. After the edit is made, either reboot or use sudo partprobe -s to make the change taking in effect. parted is the GNU tool that create, remove, resize, and move partitions (including certain filesystems). cat /proc/partitions to examine what partitions the operating system is currently aware of. Partition Organization \u00b6 Disks are divided into partitions. A partition is a physically contiguous region on the disk. Two partition schemes: MBR (Master Boot Record) GPT (GUID Partition Table) MBR dates back to the early days of MSDOS. When using MBR, a disk may have up to four primary partitions . One (and only one) of the primary partitions can be designated as an extended partition , which can be subdivided further into logical partitions with 15 total partitions possible. GPT is on all modern systems and is based on UEFI (Unified Extensible Firmware Interface). By default, it may have up to 128 primary partitions. When using the GPT scheme, there is no need for extended partitions. Partitions can be up to 233 TB in size (with MBR, the limit is just 2TB ). MBR Parition Table \u00b6 The disk partition table is contained within the disk's Master Boot Record (MBR), and is the 64 bytes following the 446 byte boot record. One partition on a disk may be marked active. When the system boots, that partition is where the MBR looks for items to load. The structure of the MBR is defined by an operating system-independent convention. The first 446 bytes are reserved for the program code. They typically hold part of a boot loader program . There are 2 more bytes at the end of the MBR known as the magic number, signature word, or end of sector marker , which always have the value 0x55AA . Each entry in the partition table is 16 bytes long and contains information: Active bit Beginning address in cylinder/head/sectors (CHS) format Partition type code, indicating: xfs, LVM, ntfs, ext4, swap, etc. Ending address in CHS Start sector, counting linearly from 0 Number of sectors in partition. Linux only uses the last two fields for addressing, using the linear block addressing (LBA) method. For MBR systems, dd can be used for converting and copying files. However, be careful using dd : a simple typing error or misused option could destroy your entire disk. The following command will backup the MBR (along with the partition table): $ dd if=/dev/sda of=mbrbackup bs=512 count=1 The MBR can be restored using the following command: $ sudo dd if=mbrbackup of=/dev/sda bs=512 count=1 GPT Partition table \u00b6 Modern hardware comes with GPT support; MBR support will gradually fade away. The Protective MBR is for backwards compatibility, so UEFI systems can be booted the old way. There are two copies of the GPT header, at the beginning and at the end of the disk, describing metadata: List of usable blocks on disk Number of partitions Size of partition entries. Each partition entry has a minimum size of 128 bytes . The blkid utility shows information about partitions. i.e. sudo blkid /dev/sda8 blkid is a utility to locate block devices and report on their attributes, such as type of contents, tokens, LABEL or UUID lsblk presents block device information in a tree format Devices Nodes \u00b6 The Linux kernel interacts at a low level with disks through device nodes normally found in the /dev directory. Device nodes for SCSI and SATA disks follow a simple xxy[z] naming convention, where xx is the device type (usually sd), y is the letter for the drive number (a, b, c, etc.), and z is the partition number. sd means SCSI or SATA disk. Back in the days where IDE disks could be found, they would have been /dev/hda3, /dev/hdb Filesystem Features \u00b6 File Attributes \u00b6 Extended Attributes associate metadata not interpreted directly by the filesystem with files: user, trusted, security, and system The system namespace is used for Access Control Lists (ACLs), and the security namespace is used by SELinux. Flag values are stored in the file inode and may be modified and set only by the root user. They are viewed with lsattr filename and set with chattr [+|-|=mode] filename . Flags that can be set: i: immutable - A file with the immutable attribute cannot be modified (not even by root). It cannot be deleted or renamed. No hard link can be created to it, and no data can be written to the file. Only the superuser can set or clear this attribute. a: append-only - A file with the append-only attribute set can only be opened in append mode for writing. Only the superuser can set or clear this attribute. d: no-dump - A file with the no-dump attribute set is ignored when the dump program is run. This is useful for swap and cache files that you don't want to waste time backing up. A: no atime update - A file with the no-atime-update attribute set will not modify its atime (access time) record when the file is accessed but not modified. This can increase the performance on some systems because it reduces the amount of disk I/O; say some files that are being accessed very frequently. mkfs \u00b6 mkfs is a utility for formatting (making) a filesystem on a partition. See the fs programs with ls -lhF /sbin/mkfs* . General usage: mkfs [-t fstype] [options] [device-file] Each filesystem type has its own particular formatting options and its own mkfs program, and can be learned from its man page. fsck \u00b6 fsck a utility designed to check for errors and may fix if found any. Like mkfs , it also have many variations for different filesystem types, view with ls -lhF /sbin/fsck* . General usage: fsck [-t fstype] [options] [device-file] Usually, you do not need to specify the filesystem type, as fsck can figure it out by examining the superblocks at the start of the partition. You can control whether any errors found should be fixed one by one manually with the -r option, or automatically, as best possible, by using the -a option. fsck is run automatically after a set number of mounts or a set interval since the last time it was run or after an abnormal shutdown. It should only be run on unmounted filesystems. You can force a check of all mounted filesystems at boot by doing: sudo touch /forcefsck then reboot. /etc/fstab \u00b6 During system initialization, the command mount -a is executed in order to mount all filesystems listed in /etc/fstab . /etc/fstab is used to define mountable file systems and devices on startup. This may include both local and remote network-mounted filesystems, such as NFS and samba filesystems. Each record in the /etc/fstab file contains white space separated files of information about a filesystem to be mounted: Device file name (such as /dev/sda1 ), label, or UUID Mount point for the filesystem (where in the tree structure is it to be inserted) Filesystem type A comma-separated list of options dump frequency used by the dump -w command, or a zero which is ignored by dump fsck pass number or a zero - meaning do not fsck this partition Linux systems have long had the ability to mount a filesystem only when it is needed, which is done through autofs . While autofs is very flexible and well understood, systemd -based systems (including all enterprise Linux distributions) come with automount facilities built into the systemd framework. check filesystem usage \u00b6 df (disk free) is used to look at filesystem usage. The option -h shows human-readable format, -T shows filesystem type, and -i shoes inode information. du (disk usage) is used to look at both disk capacity and usage. The option -a lists all files in addition to directories. find . -maxdepth 1 -type d -exec du -shx {} \\; | sort -hr shows directory size total and sort in decending order. swap \u00b6 Linux employs a virtual memory system which overcommits functions in two ways such that: programs do not actually use all the memory they are given permission to use when under memory pressure, less active memory regions may be swapped out to disk, to be recalled only when needed again Swapping is usually done to one or more dedicated partitions or files; Linux permits multiple swap areas. Each area has a priority, and lower priority areas are not used until higher priority areas are filled. In most situations, the recommended swap size is the total RAM on the system. Current swap in use can be found under /proc/swaps . Its usage can be viewed with free -m Commands involved with swap are mkswap : format a swap paritition or file swapon : activate a swap partition or file swapoff : deactivate a swap partition or file At any given time, most memory is in use for caching file contents to prevent actually going to the disk any more than necessary and are never swapped out since it is pointless; instead, dirty pages are flushed out to disk. Linux memory used by the kernel itself is never swapped out. Quotas \u00b6 Disk quotas allow administrators to control the maximum space particular users (or groups) are allowed. Commands involved with quota: quotacheck : generates and updates quota accounting files use option -u for user files and -g for group files pass in the filesystem to update, otherwise use -a to apply/update all filesystems in /etc/fstab generally only run after quota is initially turned on, or fsck reports errors in a filesystem quotaon : enables quota accounting quotaoff : disables quota accounting edquota : edit user or group quotas use option -u for edit user quota and -g for group use option -p to copy quota value from a user/group to another use option -t to set grace periods quota reports usage and limits Quotas may be enabled or disabled on a per-filesystem basis. Quota operations require the existence of the files aquota.user and aquota.group in the root directory of the filesystem using quotas. Linux supports the use of quotas based on user and group IDs. Quotas for users and groups may be set for disk blocks and/or inodes. In addition, soft and hard limits may be set, as well as grace periods: Soft limits may be exceeded for a grace period. Hard limits may never be exceeded. Steps to set up quota: first make sure you have mounted the filesystem with the user and/or group quota mount options usrquota grpquota in /etc/fstab . Then run quotacheck on the filesystem to set it up. Then enable quota and use edquota to set limits. For example: # add in /etc/fstab (assume /home is on a dedicated partition) /dev/sda5 /home ext4 defaults,usrquota 1 2 # test with the following commands: $ sudo mount -o remount /home $ sudo quotacheck -vu /home $ sudo quotaon -vu /home $ sudo edquota someusername filesystem types \u00b6 ext4 \u00b6 The ext4 filesystem can support volumes up to 1 EB and file sizes up to 16 TB . Until very recently, ext4 was the most frequent default choice of Linux distributions, due to its excellent combination of performance, integrity, and stability. An extent is a group of contiguous blocks. Use of extents can improve large file performance and reduce fragmentation . Extents replace the older block mapping mechanism from ext3. ext4 is backwards compatible with ext3 and ext2. It can pre-allocate disk space for a file. The allocated space is usually guaranteed and contiguous . It also uses a performance technique called allocate-on-flush (delays block allocation until it writes data to disk). ext4 breaks the 32,000 subdirectory limit of ext3. ext4 uses checksums for the journal which improves reliability. This can also safely avoid a disk I/O wait during journalling. ext4 provides timestamps measured in nanoseconds. The superblock is stored in block 0 of the disk, and contains global information about the ext4 filesystem: Mount count and maximum mount count (every time the disk is successfully mounted, mount count is incremented; the filesystem is checked every maximum-mount-counts or every 180 days whichever comes first) Block size (block size can be set through the mkfs command)\u200b Blocks per group Free block count Free inode count Operating System ID All fields in ext4 are written to disk in little-endian order, except the journal. An ext4 filesystem is split into a set of block groups . The block allocator tries to keep each file\u2019s blocks within the same block group to reduce seek times. The default block size is 4 KB, which would create a block group of 128 MB. For block group 0 , the first 1024 bytes are unused (to allow for boot sectors, etc), and the superblock will start at the first block after block group 0, then followed by the group descriptors and a number of GDT (Group Descriptor Table) blocks. These are followed by the data block bitmap, the inode bitmap, the inode table, and the data blocks. In blocks view, they are like Super Block | Group Descriptors | Data Block Bitmap | Inode Bitmap | Inode Table (n blocks) | Data Blocks (n blocks) The first and second blocks are the same in every block group , and comprise the Superblock and the Group Descriptors. Under normal circumstances, only those in the first block group are used by the kernel; the duplicate copies are only referenced when the filesystem is being checked. If everything is OK, the kernel merely copies them over from the first block group. If there is a problem with the master copies, it goes to the next and so on until a healthy one is found and the filesystem structure is rebuilt. This redundancy makes it very difficult to thoroughly fry an ext2/3/4 filesystem. As an optimization, today not all block groups have a copy of the superblock and group descriptors. To view which block holds backups use dumpe2fs . Use the dumpe2fs program to get information about a particular partition, such as limits, capabilities, flags, and other attributes. tune2fs can be used to change filesystem parameters. Use -l to list the contents of the superblock (same as global information from dumpe2fs ). xfs \u00b6 The xfs filesystem was engineered to deal with large data sets for SGI systems, as well as handle parallel I/O tasks very effectively. xfs can handle up to 16 EB (exabytes) for the total filesystem and up to 8 EB for an individual file and implements methods for high performance: Allowing DMA (Direct Memory Access) I/O Guaranteeing an I/O rate Adjusting stripe size to match underlying RAID or LVM storage devices. xfs also journals quota information for fast recovery on uncleanly unmounted filesystem. xfs maintenance can be done online, for defragmenting, resizing (enlarge), and dumping/restoring Backup and restore can be done with xfsdump and xfsrestore utilites. xfs also supports per-directory quotas, use xfs_quota command. Check out more xfs-related utilities with man -k xfs btrfs \u00b6 btrfs stands for B-tree filesystem, which is intended to address the lack of pooling, snapshots, checksums, and integral multi-device spanning in other Linux filesystems such as ext4. One of the main features is the ability to take frequent snapshots of entire filesystems, or sub-volumes of entire filesystems in virtually no time. Because btrfs makes extensive use of COW techniques (Copy on Write), such a snapshot does not involve any more initial space for data blocks or any I/O activity except for some metadata updating. One can easily revert to the state described by earlier snapshots and even induce the kernel to reboot off an earlier root filesystem snapshot. btrfs maintains its own internal framework for adding or removing new partitions and/or physical media to existing filesystems, much as LVM (Logical Volume Management) does. disk encryption \u00b6 Filesystems may be encrypted to protect them from both prying eyes and attempts to corrupt the data they contain. Encryption can be chosen at installation or incorporated later. It is straightforward to create and format encrypted partitions at a later time, but you cannot encrypt an already existing partition in place without a data copying operation. Modern Linux distributions provide block device level encryption mainly through the use of LUKS (Linux Unified Key Setup). LUKS is built on top of cryptsetup to encrypt filesystems. The general form of a command is: cryptsetup [OPTION...] <action> <action-specific> Examine /proc/crypto to see the encryption methods your system supports, and choose one to encrypt a filesystem. Example: # encrypt a partition using LUKS # (a passphrase will be prompted to enter) sudo cryptsetup luksFormat --cipher aes /dev/sdc12 # make the volume available (create unencrypted passthrough device) sudo cryptsetup --verbose luksOpen /dev/sdc12 SECRET # format the partition sudo mkfs.ext4 /dev/mapper/SECRET # mount it sudo mount /dev/mapper/SECRET /mnt # unmount it sudo umount /mnt # remove the volume association sudo cryptsetup --verbose luksClose SECRET To mount an encrypted filesystem at boot time, first ensure it has an entry in /etc/fstab , then additionally add an entry to /etc/crypttab (which will prompt for passphrase on boot time). Read more with man crypttab . LVM \u00b6 LVM (Logical Volume Management) breaks up one virtual partition into multiple chunks, each of which can be on different partitions and/or disks. Logical volumes are created by putting all the devices into a large pool of disk space (the volume group ), and then allocating space from the pool to create a logical volume . Additional devices can be added to the logical volume at any time. Logical volumes have features similar to RAID devices. They can actually be built on top of a RAID device. This would give the logical volume the redundancy of a RAID device with the scalability of LVM. LVM makes it easy to change the size of the logical partitions and filesystems, to add more storage space, rearrange things, etc. Use of logical volumes is a mechanism for creating filesystems which can span more than one physical disk. LVM does create a definite additional performance cost that comes from the overhead of the LVM layer. However, even on non-RAID systems, if you use striping (splitting of data to more than one disk), you can achieve some parallelization improvements. Volume and Volume Groups \u00b6 Partitions are converted to physical volumes and multiple physical volumes are grouped into volume groups; there can be more than one volume group on the system. Space in the volume group is divided into extents; these are 4 MB in size by default. Logical volumes are allocated from volume groups: Can be defined by the size or number of extents Filesystems are built on logical volumes Can be named anything. The hierarchy in layers: Physical Drives -> Parititions -> Physical Volumes -> Volume Groups -> Logical Volumes -> File Systems Commands to manipulate physical volumes: pvcreate : Converts a partition to a physical volume. pvdisplay : Shows the physical volumes being used. pvmove : Moves the data from one physical volume within the volume group to others; this might be required if a disk or partition is being removed for some reason. It would then be followed by: pvremove : Remove a partition from a physical volume. Commands to manipulate volume groups: vgcreate : Creates volume groups. vgextend : Adds to volume groups. vgreduce : Shrinks volume groups. use man lvm to view more utilities related. There are a number of utilities that manipulate logical volumes, and a short list can be viewed with ls -lF /sbin/lv* Commands to manipulate logical volumes: lvcreate allocates logical volumes from within volume groups. The size can be specified either in bytes or number of extents (remember, they are 4 MB by default). Names can be anything desired. lvdisplay reports on available logical volumes. lvresize expands or shrinks a logical volume. Some variations: lvextend , lvreduce , resize2fs Manipulate LV \u00b6 Steps involved in setting up and using a new logical volume, as an example: # Create partitions on disk drives. sudo fdisk /dev/sdb # Create physical volumes from the partitions. sudo pvcreate /dev/sdb1 sudo pvcreate /dev/sdc1 # Create the volume group named 'vg'. sudo vgcreate -s 16M vg /dev/sdb1 sudo vgextend vg /dev/sdc1 # Allocate logical volumes from the volume group. sudo lvcreate -L 50G -n mylvm vg # Format the logical volumes. sudo mkfs -t ext4 /dev/vg/mylvm # Mount the logical volumes (also update /etc/fstab as needed). sudo mkdir /mylvm sudo mount /dev/vg/mylvm /mylvm # Persistent mount on reboot, add to /etc/fstab /dev/vg/mylvm /mylvm ext4 defaults 1 2 Resizing a logical volume is quick and easy compare to doing so with a physical paritition that already contains a filesystem. Extents for a logical volume can be added or subtracted from the logical volume, and they can come from anywhere in the volume group; they need not be from physically contiguous sections of the disk. When expanding a logical volume with a filesystem, you must first expand the volume , and then expand the filesystem .\u200b When shrinking a logical volume with a filesystem, you must first shrink the filesystem , and then shrink the volume . This is done using lvresize . As an example of shrinking: sudo lvresize -r -L 20 GB /dev/VG/mylvm . The -r option causes resizing of the filesystem at the same time as the volume size is changed. The filesystem cannot be mounted when being shrunk. sudo lvresize -r -L +100M /dev/vg/mylvm to grow a logical volume, while the plus sign (+) indicates adding space. Note that you need NOT unmount the filesystem to grow it. LVM Snapshots \u00b6 LVM snapshots create an exact copy of an existing logical volume. They are useful for backups, application testing, and deploying VMs (Virtual Machines). The original state of the snapshot is kept as the block map. Snapshots only use space for storing deltas: When the original logical volume changes, original data blocks are copied to the snapshot. If data is added to snapshot, it is stored only there. Use -s option with lvcreate to create snapshots, i.e. lvcreate -l 128 -s -n mysnap /dev/vg/mylvm RAID \u00b6 RAID (Redundant Array of Independent Disks) spreads I/O over multiple disks. This can really increase performance in modern disk controller interfaces, such as SCSI, which can perform the work in parallel efficiently. RAID can be implemented either in software or in hardware. One disadvantage of using hardware RAID is that if the disk controller fails, it must be replaced by a compatible controller, which may not be easy to obtain. When using software RAID, the same disks can be attached to and work with any disk controller. Three essential features of RAID: mirroring: writing the same data to more than one disk striping: splitting of data to more than one disk parity: extra data is stored to allow problem detection and repair, yielding fault tolerance. RAID devices are typically created by combining partitions from several disks together to create filesystems which are larger than any one drive. Striping provides better performance by spreading the information over multiple devices so simultaneous writes are possible. Mirroring writes the same information to multiple drives, giving better redundancy. mdadm is used to create and manage RAID devices, with the array name, /dev/mdX . RAID Levels \u00b6 As a general rule, adding more disks improves performance. RAID 0 - only striping available RAID 1 - only mirroring RAID 5 - uses a rotating parity stripe, at least three disks required. Can endure one disk failure without loss of data RAID 6 - striped disks with dual parity, at least four disks required, and can handle two disk failures RAID 10 - mirrored and striped data set, at least four disks required Software RAID \u00b6 The essential steps in configuring a software RAID device are: Create partitions on each disk (type fd in fdisk) Create RAID device with mdadm Format RAID device Add device to /etc/fstab Mount RAID device Capture RAID details to ensure persistence Examine /proc/mdstat to see the RAID status as an example: $ sudo fdisk /dev/sdb $ sudo fdisk /dev/sdc $ sudo mdadm --create /dev/md0 --level = 1 --raid-disks = 2 /dev/sdbX /dev/sdcX $ sudo mkfs.ext4 /dev/md0 $ sudo bash -c \"mdadm --detail --scan >> /etc/mdadm.conf\" $ sudo mkdir /myraid $ sudo mount /dev/md0 /myraid $ sudo cat >> /etc/fstab <<EOF /dev/md0 /myraid ext4 defaults 0 2 EOF Use mdadm -S /dev/md0 to stop the RAID device. Use mdadm --detail /dev/mdX to show the current status of a RAID device. You can also use mdmonitor, which requires configuring /etc/mdadm.conf To help ensure any reduction in that redundancy is fixed as quick as possible, a hot spare can be used. To create a hot spare when creating the RAID aray, use -x <number_of_spares> , i.e. sudo mdadm --create /dev/md0 -l 5 -n3 -x 1 /dev/sda8 /dev/sda9 /dev/sda10 /dev/sdb2 To restore the tested drive, or a new drive in a legitimate failure situation, first remove the \"faulty\" member, then add the \"new\" member, use --add --remove , i.e. mdadm --remove /dev/md0 /dev/sdb2; mdadm --add /dev/md0 /dev/sde2 Kernel Services \u00b6 Narrowly defined, Linux is only the kernel of the operating system, which includes many other components, such as libraries and applications that interact with the kernel. The kernel is the essential central component that connects the hardware to the software and manages system resources, such as memory and CPU time allocation among competing applications and services. It handles all connected devices using device drivers, and makes the devices available for operating system use. A system running only a kernel has rather limited functionality. It will be found only in dedicated and focused embedded devices. The main responsibilities of the kernel include: System initialization and boot up Process scheduling Memory management Controlling access to hardware I/O (Input/Output) between applications and storage devices Implementation of local and network filesystems Security control, both locally (such as filesystem permissions) and over the network Networking control Kernel Command Line \u00b6 Various parameters are passed to the system at boot on the kernel command line, typically placed on the linux line in the GRUB config file, such as /boot/grub , or in a place like /boot/efi/EFI/centos/grub.cfg (however, RHEL/CentOS 8 saves in /boot/grub2/grubenv ), which may look like linux boot/vmlinuz-4.19.0 root=UUID=7ef4e747-afae-90b4-9be8be8d0258 ro quiet crashkernel=384M-:128M linuxefi /boot/vmlinuz-5.2.9 root=UUID=77461ee7-c34a-4c5f-b0bc-29f4feecc743 ro crashkernel=auto rhgb quiet linux16 /boot/vmlinuz-3.19.1.0 root=UUID=178d0092-4154-4688-af24-cda272265e08 ro vconsole.keymap=us crashkernel=auto vconsole.font=latarcyrheb-sun16 rhgb quiet LANG=en_US.UTF-8 Everything after the vmlinuz file specified is an option. Any options not understood by the kernel will be passed to init (pid = 1). The complete list of kernel boot parameters can be found at its documentation or using man bootparam . Use cat /proc/cmdline to see what command line a system was booted with. sysctl \u00b6 sysctl interface can be used to read and tune kernel parameters at run time. View kernel parameters with sysctl -a ; each value corresponds to a particular pseudofile residing under /proc/sys , with directory slashes being replaced by dots. To apply a change at runtime: $ sudo sh -c 'echo 1 > /proc/sys/net/ipv4/ip_forward' # or execute as root user $ sudo sysctl net.ipv4.ip_forward = 1 If settings are placed in /etc/sysctl.conf (see man sysctl.conf for details), settings can be fixed at boot time (and reloaded at run time with sysctl -p ) Vendors put their settings in files in the /usr/lib/sysctl.d/ directory. These can be added to or supplanted by files placed in /etc/sysctl.d Kernel Modules \u00b6 The Linux kernel makes extensive use of modules , which contain important software that can be dynamically loaded and unloaded as needed after the system starts. This flexibility also aids in development of new features as system reboots are almost never needed to test during development and debugging. modprobe \u00b6 There are a number of utility programs that are used with kernel modules: lsmod - List loaded modules. insmod - Directly load modules. rmmod - Directly remove modules. modprobe - Load or unload modules, using a pre-built module database with dependency and location information, which does dependency modules check and loads/unloads automatically. i.e. modprobe e1000e , modprobe -r e1000e depmod - Rebuild the module dependency database. modinfo - Display information about a module. modprobe requires a module dependency database be updated. Use depmod to generate or update the file /lib/modules/$(uname -r)/modules.dep All files in the /etc/modprobe.d subdirectory tree which end with the .conf extension are scanned when modules are loaded and unloaded using modprobe. These config files control some parameters when loading with modprobe . You can also disable specific modules to avoid them being loaded. The format of files in /etc/modprobe.d is simple: one command per line, with blank lines and lines starting with # ignored; a backslash at the end of a line causes it to continue on the next line. Some rules for modprobe to keep in mind: It is impossible to unload a module being used by one or more other modules It is impossible to unload a module that is being used by one or more processes When a module is loaded with modprobe, the system will automatically load any other modules that need to be loaded first When a module is unloaded with modprobe -r, the system will automatically unload any other modules being used by the module (if not used by other modules) Much information about modules can also be seen in the /sys/module/<module_name> pseudo-filesystem directory tree, such as parameters. Devices and udev \u00b6 udev stands for User Device management . It dynamically discovers built-in hardware as well as peripheral devices during boot time or ad-hoc plugged at run time. udev handles loading and unloading device drivers with proper configurations as need, including: Device naming Device file and symlink creating Setting file attributes Taking other needed actions (such as execute a program) udev runs as a daemon (either udevd or systemd-udevd) and monitors a netlink socket. When new devices are initialized or removed, the uevent kernel facility sends a message through the socket, which udev receives and takes appropriate action to create or remove device nodes of the right names and properties according to the rules. The three components of udev are: The libudev library which allows access to information about the devices The udevd or systemd-udevd daemon that manages the /dev directory The udevadm utility for control and diagnostics The main configuration file is /etc/udev/udev.conf . It contains information such as where to place device nodes, default permissions and ownership, etc udev rules \u00b6 When devices are added or removed from the system, udev receives a message from the kernel. It then parses the rules files under /etc/udev/rules.d , /run/udev/rules.d , or /usr/lib/udev/rules.d for actions. A rules file may look like 60-persistent-storage.rules . There are two separate parts defined on a single line: one or more match pairs denoted by == . These try to match a device\u2019s attributes and/or characteristics to some value. one or more assignment key-value pairs that assign a value to a name, such as a file name, assignment, even file permissions, etc. The format for a udev rule is simple: <match><op>value [, ...] <assignment><op>value [, ... ] . If no matching rule is found, it uses the default device node name and other attributes. Example of a rules file: $ cat /etc/udev/rules.d/60-vboxdrv.rules KERNEL==\"vboxdrv\", NAME=\"vboxdrv\", OWNER=\"root\", GROUP=\"vboxusers\", MODE=\"0660\" KERNEL==\"vboxdrvu\", NAME=\"vboxdrvu\", OWNER=\"root\", GROUP=\"root\", MODE=\"0666\" KERNEL==\"vboxnetctl\", NAME=\"vboxnetctl\", OWNER=\"root\", GROUP=\"vboxusers\", MODE=\"0660\" SUBSYSTEM==\"usb_device\", ACTION==\"add\", RUN+=\"/usr/lib/virtualbox/VBoxCreateUSBNode.sh $major $minor $attr{bDeviceClass}\" SUBSYSTEM==\"usb\", ACTION==\"add\", ENV{DEVTYPE}==\"usb_device\", RUN+=\"/usr/lib/virtualbox/VBoxCreateUSBNode.sh $major $minor $attr{bDeviceClass}\" System administrators can control how udev operates and craft special udev rules. Device Nodes \u00b6 Device nodes are used by programs to communicate with devices through nodes using normal I/O methods. Character and block devices have device nodes; network devices do not. A device driver may use multiple device nodes. Device nodes are located in the /dev directory. Device nodes can be created with: sudo mknod [-m mode] /dev/name <type> </major> <minor> The major and minor numbers identify the driver associated with the device, with the driver uniquely reserving a group of numbers. In most cases, device nodes of the same type (block or character) with the same major number use the same driver. The major and minor numbers appear in the same place that file size would when looking at a normal file with ls . The minor number is used only by the device driver to differentiate between the different devices it may control, or how they are used. These may either be different instances of the same kind of device, (such as the first and second sound card, or hard disk partition) or different modes of operation of a given device (such as different density floppy drive media). Device numbers have meaning in user-space as well. Two system calls, mknod() and stat() , return information about major and minor numbers. Virtualization \u00b6 Virtual Machines are a virtualized instance of an entire operating system, and may fulfill the role of a server or a desktop/workstation. The outside world sees the VM as if it were an actual physical machine, present somewhere on the network. Applications running in the VM are generally unaware of their non-physical environment.\u200b Other kinds of virtualization: Network - The details of the actual physical network, such as the type of hardware, routers, etc., are abstracted and need not be known by software running on it and configuring it. Storage - Multiple network storage devices are configured to look like one big storage unit, such as a disk. Examples: Network Attached Storage or NAS. Application - An application is isolated into a standalone format, such as a container Virtualization is developed and evolving for several reasons: Enables better hardware utilization Operating systems often progress more quickly than hardware It is microcode-driven CPUs enhanced to support virtualization led to a boost in performance, easier configuration, and more flexibility in VM installation and migration From early mainframes to mini-computers, virtualization has been used for expanding limits, debugging and administration enhancements. A host is the underlying physical operating system managing one or more virtual machines. A guest is the VM which is an instance of a complete operating system, running one or more applications. The guest should not care what host it is running on and can be migrated from one host to another. Low-level performance tuning on areas such as CPU utilization, networking throughput, memory utilization, is often best done on the host, while application tuning will be done mostly on the guest. Emulation \u00b6 The first implementations of virtualization on the PC architecture were through the use of emulators . An Emulator runs completely in software. It is useful for running virtual machines on different architectures, such as running a pretend ARM guest machine on an X86 host. Performance is relatively slow. Hypervisors \u00b6 The host system acts as the hypervisor that initiates, terminates, and manages guests. It also called Virtual Machine Monitor (VMM). Two basic methods of virtualization: hardware virtualization - The guest system runs without being aware it is running as a virtualized guest, and does not require modifications to be run in this fashion. \u200bIt is also known as Full Virtualization. para-virtualization - The guest system is aware it is running in a virtualized environment, and has been modified specifically to work with it. You can check directly if your CPU supports hardware virtualization extensions by looking at /proc/cpuinfo ; if you have an IVT-capable chip, you will see vmx in the flags field; and, if you have an AMD-V capable chip, you will see svm in the same field. You may also have to ensure the virtualization capability is turned on in your CMOS. The hypervisor can be: External to the host operating system kernel: VMware Internal to the host operating system kernel: KVM Going past Emulation, the merging of the hypervisor program into a specially-designed lightweight kernel was the next step in the Virtualization deployment. i.e. the KVM project added hypervisor capabilities into the Linux kernel. Specific CPU chip functions and facilities were required and deployed for this type of virtualization.\u200b libvirt \u00b6 The libvirt project is a toolkit to interact with virtualization technologies. It provides management for virtual machines, virtual networks, and storage, and is available on all enterprise Linux distributions. KVM \u00b6 KVM uses the Linux kernel for computing resources, including memory management, scheduling, synchronization, and more. When running a virtual machine, KVM engages in a co-processing relationship with the Linux kernel. Managing KVM can be done with command line tools include: virt-* and qemu-* . Graphical interfaces include virt-manager, kimchi, OpenStack, oVirt , etc. Containers \u00b6 Further integration between the hypervisor and the Linux kernel allowed the creation of operating system-level virtual machines, or containers. Containers share many facilities in the Linux kernel, and make use of namespaces and cgroups . Containers are very lightweight and reduce the overhead associated with having whole virtual machines. OS container is a flavor that runs an image of an operating system with the ability to run init processes and spawn multiple applications. i.e. LXC (Linux Containers) Application virtualization runs one application for each container. Many single application containers are typically initialized on a single machine which creates a greater flexibility and reduces overhead normally associated with virtualization. Virtual machines run a complete operating system, and can run many services and applications. Virtual machines use more resources than a container. \u200bContainers usually run one thing. Containers are more portable, and can be run inside a VM. Scaling workloads is different for containers and virtual machines. Orchestration systems such as Kubernetes or Mesos can decide on the proper quantity of containers needed, do load balancing, replicate images and remove them, etc., as needed. Docker \u00b6 Docker is an application-level virtualization using many individual images to build up the necessary services to support the target application. These images are packaged into containers - they are components in containers. Images may contain: Application code\u200b Runtime libraries\u200b System tools\u200b \u200bOr just about anything required for an application Images may reside on a Docker Hub or a registry server. An application can be packaged up with all of its dependent code and services and deployed as a single unit with the minimum of overhead. This deployment can be easily repeated as often as desired. Podman \u00b6 RHEL8/CentOS8 have replaced pure docker with podman. Podman uses a child/parent forking model for container creation and management, while Docker uses a server/client model with a daemon running in background for management. Emulation layer enables backwards compatibility with docker commands. Promised benefits include better security and less overhead. Account Management \u00b6 Linux systems provide a multi-user environment which permits people and processes to have separate simultaneous working environments: Providing each user with their own individualized private space Creating particular user accounts for specific dedicated purposes Distinguishing privileges among users User Accounts \u00b6 Normal user accounts are for people who will work on the system. Some user accounts (like the daemon account) exist for the purpose of allowing processes to run as a user other than root. Each user on the system has a corresponding line in the /etc/passwd file that describes their basic account attributes. A line in /etc/passwd consists of: user name, user password (use /etc/shadow if value is x ), UID, GID, comment, home directory, login shell Add user with sudo useradd <username> , which will create an account using default algorithms for assigning user and group id , home directory, and shell choice; the defaults can easily be overruled by using options to useradd . i.e. sudo useradd dexter will: The next available UID greater than UID_MIN (specified in /etc/login.defs ) by default is assigned as dexter's UID. The convention most Linux distributions have used is that any account with a user ID less than 1000 is considered special and belongs to the system; normal user accounts start at 1000 (UID_MIN defined in /etc/login.defs ) A group called dexter with a GID=UID is also created and assigned as dexter's primary group. aka Primary Group ID , and sometimes called User Private Groups (UPG) A home directory /home/dexter is created and owned by dexter. dexter's login shell will be /bin/bash . The contents of /etc/skel is copied to /home/dexter . By default, /etc/skel includes startup files for bash and for the X Window system. An entry of either !! or ! is placed in the password field of the /etc/shadow file for dexter's entry, thus requiring the administrator to assign a password for the account to be usable. Use of /etc/shadow enables password aging on a per user basis. At the same time, it also allows for maintaining greater security of hashed passwords since it has permission 400 while /etc/passwd has permission 644. /etc/shadow contains one record (one line) for each user, such as daemon:*:16141:0:99999:7::: . The colon-separated fields are: username: unique user name password: the hashed (sha512) value of the password lastchange: date that password was last changed mindays: minimum days before password can be changed maxdays: maximum days after which password must be changed warn: days before password expires that the user is warned grace: days after password expires that account is disabled expire: date that account is/will be disabled reserved: reserved field Note the dates are stored as the number of days since Jan. 1, 1970 (the epoch date). The username in each record must match exactly that found in /etc/passwd , and also must appear in the identical order . The password hash is the string \"$6$\" followed by an eight-character salt value , which is then followed by a $ and an 88-character (sha512) password hash. Additionally, userdel is used to remove user accounts, and usermod is used to change characteristics of a user account. You can lock a user account to prevent login by usermod -L <username> (and unlock with -U option). Linux ships with some system accounts that are locked (such as bin, daemon, or sys), which means they can run programs, but can never login to the system and have no valid password associated with them. These accounts has /sbin/nologin as their login shell. Attempt to login will show message from /etc/nologin.txt . You can also lock a user account by setting an expiration date on an account, with chage : chage [-m mindays] [-M maxdays] [-d lastday] [-I inactive] [-E expiredate] [-W warndays] user . i.e. sudo chage -E 2014-09-11 morgan . Only the root user can use chage . Normal user can run chage -l to check when their password or account will expire. Passwords can be changed with passwd ; a normal user can change only their own password, while root can change any user password. Sudoers \u00b6 The /etc/sudoers defines the sudo access. Usually you don't need to temper with this file and just add new user to the sudo group to grant sudo access. The file has default access mode of 400. To do quick edit, use sudo visudo . The syntax of lines in the /etc/sudoers file is users hosts=(user:group) commands , where group can be omitted. You should use drop-ins under /etc/sudoers.d/ to add additional sudoer access, and leave the main sudoers file unchanged. Account Restriction \u00b6 One can set restricted shell as login shell /bin/rbash , which is equivalent to /bin/bash -r . It: Prevents the user from using cd to change directories. Prevents setting the SHELL, ENV or PATH environment variables. Prohibits specifying path or command names containing / . Restricts redirection of output and/or input. However, it is fairly easy to defeat the restricted shell. Read more at \"Escaping Restricted Shell rbash\" and \"Linux Restricted Shell Bypass\" You can also set up restricted user account which: Uses the restricted shell Limits available system programs and user applications Limits system resources Limits access times Limits access locations When the restricted shell is invoked, it executes $HOME/.bash profile without restriction. This is why the user must not have either write or execute permissions on the home directory. Make sure that when you set up such an account that you do NOT inadvertently add system directories to the PATH environment variable, because this allows the restricted user the ability to execute other system programs, such as an unrestricted shell. By default, root logins through the network are generally prohibited for security reasons. It is generally recommended that all root access be through su , or sudo (causing an audit trail of all root access through sudo ). PAM (Pluggable Authentication Modules) can also be used to restrict which users are allowed to su to root. It might also be worth it to configure auditd to log all commands executed as root. ssh \u00b6 SSH (Secure SHell) exists for the needs to login through the network into a remote system, or to transfer files to and from a remote machine. User-specific ssh configuration files are created under every user's home directory in the hidden .ssh directory. Within the directory: id_rsa : The user's private encryption key id_rsa.pub : The user's public encryption key authorized_keys : A list of public keys that are permitted to login known_hosts : A list of hosts from which logins have been allowed in the past config : A configuration file for specifying various options First, a user has to generate their private and public encryption keys with ssh-keygen . The private keys must never be shared. The public key, however, should be given to any machine with which you want to permit password-less access . It should also be added to your authorized_keys file, together with all the public keys from other users who have accounts on your machine and you want to permit password-less access to their accounts. Group Accounts \u00b6 Linux systems form collections of users called groups which share some common purpose, and share certain files and directories and maintain some common privileges. Groups are defined in /etc/group , which has the same role for groups as /etc/passwd has for users. Each line of the file looks like: groupname:password:GID:user1,user2,... . A Linux user has one primary group; this is listed in /etc/passwd and will also be listed in /etc/group . A user may belong to between 0 and 15 secondary groups. Group passwords may be set, but only if /etc/gshadow exists. GID is the group identifier. Values between 0 and 99 are for system groups. Values between 100 and GID_MIN (as defined in /etc/login.defs and usually the same as UID_MIN) are considered special. Values over GID_MIN are for UPG (User Private Groups) Group Management \u00b6 Group accounts may be managed and maintained with: groupadd : Add a new group. groupmod : Modify a group and add new users. groupdel : Remove a group. usermod : Manage a user's group memberships by giving a complete list of groups, or add new group memberships Note you will have to log out and log back in again for the new group membership to be effective. Group membership can be identified by running either of the following commands: groups [user1 user2 ...] or id -Gn [user1 user2 ...] User Private Groups \u00b6 Each user will have his or her own group, and additional members may be added to someone's private group in /etc/group . By default, users whose accounts are created with useradd have: primary GID = UID and the group name is also identical to the user name. As specified in /etc/profile , the umask is set to 002 for all users created with UPG. Under this scheme, user files are thus created with permissions 664 (rw-rw-r--) and directories with 775 (rwxrwxr-x). File permissions and Ownership \u00b6 When viewing file permission with ls -l a_file , i.e. -rw-rw-r-- 1 coop aproject 1601 Mar 9 15:04 a_file . There are nine more which indicate the access rights granted to potential file users, with each three characters grouped as a triplet. owner: the user who owns the file (also called user) group: the group of users who have access other: the rest of the world (also called world) Each of the triplets can have each of the following sets: r: read access is allowed w: write access is allowed x: execute access is allowed In addition, other specialized permissions exist for each category, such as the setuid/setgid permissions. Any request to access a file requires comparison of the credentials and identity of the requesting user to those of the owner of the file. Manage permissions and ownership \u00b6 Changing file permissions is done with chmod . Permissions can be represented either as a bitmap, usually written in octal, or in a symbolic form. Octal bitmaps usually look like 0755 , while symbolic representations look like u+rwx, g+rwx, o+rx . The octal number representation is the sum for each digit of: 4 if the read permission is desired 2 if the write permission is desired 1 if execute permission is desired Changing the group is done with chgrp . You can only change group ownership to groups that you are a member of. One can change file ownership and group ownership at the same time with chown . The option -R applies the change recursively umask \u00b6 The default permissions given when creating a file are read/write for owner, group and world (0666) , and for a directory it is read/write/execute for everyone (0777) . However, the actual permissions have changed to 664 for the file and 775 for the directory as they have been modified by the current umask=002 whose purpose is to show which permissions should be denied. You can change the umask at any time with the umask command; which is the most conventional value set by system administrators for users. This value is combined with the file creation permissions to get the actual result; i.e., 0666 & ~002 = 0664; i.e., rw-rw-r-- Filesystem ACLs \u00b6 POSIX ACLs (Access Control Lists) extend the simpler user, group, and world system, by granting privileges to specific users or groups of users when accessing certain objects or classes of objects. Files and directories can be shared without using 777 permissions. All major filesystems used in modern Linux distributions incorporate the ACL extensions, and one can use the option -acl when mounting. A default set of ACLs is created at system install. Use getfacl/setfacl to get/set ACLs. New files inherit the default ACL (if set) from the directory they reside in. Also mv and cp -p preserve ACLs. Pluggable Authentication Modules (PAM) \u00b6 Historically, authentication of users was performed individually by individual applications; i.e., su, login, and ssh would each authenticate and establish user accounts independently of each other. Most modern Linux applications have been written or rewritten to exploit PAM so that authentication can be done in one uniform way, using libpam . PAM incorporates the following concepts: PAM-aware applications Configuration files in /etc/pam.d/ PAM modules in the libpam* libraries, which can be found in different locations depending on the Linux distribution PAM Rules \u00b6 Each file in /etc/pam.d/ corresponds to a service and each (non-commented) line in the file specifies a rule. The rule is formatted as a list of space-separated tokens, the first two of which are case insensitive: type control module-path module-arguments The type controls the step of the authentication process: auth: Instructs the application to prompt the user for identification (username, password, etc). May set credentials and grant privileges. account: Checks on aspects of the user's account, such as password aging, access control, etc. password: Responsible for updating the user authentication token , usually a password. session: Used to provide functions before and after the session is established (such as setting up environment, logging, etc.) The control flag controls how the success or failure of a module affects the overall authentication process: required: Must return success for the service to be granted. If part of a stack, all other modules are still executed. Application is not told which module or modules failed. requisite: Same as required, except a failure in any module terminates the stack and a return status is sent to the application. optional: Module is not required. If it is the only module, then its return status to application may cause failure. sufficient: If this module succeeds, then no subsequent modules in the stack are executed. If it fails, then it doesn't necessarily cause the stack to fail, unless it is the only one in the stack. LDAP Authentication \u00b6 The Lightweight Directory Access Protocol (LDAP) is an industry standard protocol for using and administering distributed directory services over the network , and is meant to be both open and vendor-neutral. With LDAP, each system (or client) connects to a centralized LDAP server for user authentication. Using Transport Layer Security (TLS) makes it a secure option and is recommended. When you configure a system for LDAP authentication, five files are changed: /etc/openldap/ldap.conf /etc/pam_ldap.conf /etc/nslcd.conf /etc/sssd/sssd.conf /etc/nsswitch.conf You can edit these files manually or use one of the utility programs available ( system-config-authentication or authconfig-tui ). Network \u00b6 Network Addresses \u00b6 The IP address is the number that identifies your system on the network. IP addresses are used to uniquely identify nodes across the internet. They are registered through ISPs (Internet Service Providers). IPv4 is a 32-bit address, composed of 4 octets (an octet is just 8 bits, or a byte); IPv6 is a 128-bit address, composed of 8 16-bit octet pairs. In either case, a set of reserved addresses is also included. IPv4 Address Types \u00b6 Unicast - An address associated with a specific host. i.e. 140.211.169.4 Network - An address whose host portion is set to all binary zeroes. i.e. 192.168.1.0 Broadcast - An address to which each member of a particular network will listen. i.e. 172.16.255.255 Multicast - An address to which appropriately configured nodes will listen. Only nodes specifically configured to pay attention to a specific multicast address will interpret packets for that multicast group Reserved IPv4 Addresses \u00b6 Certain addresses and address ranges are reserved for special purposes: 127.x.x.x - Reserved for the loopback (local system) interface 0.0.0.0 - Used by systems that do NOT yet know their own address. Protocols like DHCP and BOOTP use this address when attempting to communicate with a server. 255.255.255.255 - Generic broadcast private address, reserved for internal use. These addresses are never assigned or registered to anyone. They are generally not routable. Others 10.0.0.0 - 10.255.255.255 172.16.0.0 - 172.31.255.255 192.168.0.0 - 192.168.255.255 etc. IPv4 Address Classes \u00b6 Historically, IP addresses are based on defined classes. Classes A, B, and C are used to distinguish a network portion of the address from a host portion of the address, for routing purposes. Network Class Highest order octet range Notes A 1-127 networks, 16,772,214 hosts per network, 127.x.x.x reserved for loopback B 128-191 16,384 networks, 65,534 hosts per network C 192-223 2,097,152 networks, 254 hosts per network D 224-239 Multicast addresses E 240-254 Reserved address range IPv6 Address Types \u00b6 Unicast - a packet is delivered to one interface link-local - Auto-configured for every interface to have one. Non-routable. global - Dynamically or manually assigned. Routable. or reserved for documentation Multicast - a packet is delivered to multiple interfaces Anycast - a packet is delivered to the nearest of multiple interfaces in terms of routing distance IPv4-mapped - an IPv4 address mapped to IPv6, i.e. ::FFFF:a.b.c.d/96 IPv6 has some special types of addresses such as loopback, which is assigned to the lo interface, as ::1/128 . Netmasks \u00b6 Netmask is used to determine how much of the address is used for the network portion and how much for the host portion as we have seen. It is also used to determine network and broadcast addresses. Network Class Decimal Hex Binary A 255.0.0.0 ff:00:00:00 11111111 00000000 00000000 00000000 B 255.255.0.0 ff:ff:00:00 11111111 11111111 00000000 00000000 C 255.255.255.0 ff:ff:ff:00 11111111 11111111 11111111 00000000 The network address is obtained by logical AND ing ( & ) the IP address with the netmask . We are interested in the network addresses because they define a local network which consists of a collection of nodes connected via the same media and sharing the same network address. All nodes on the same network can directly see each other . For example: 172.16.2.17 ip address & 255.255.0.0 netmask ----------------------------- 172.16.0.0 network address hostname \u00b6 The hostname is simply a label to distinguish a networked device from other nodes. The hostname is generally specified at installation time, and can be modified at any time later. The hostname for a machine can be checked with command hostname . To change hostname only once before next reboot, just execute sudo hostname <new_hostname> ; to change it persistently, do sudo hostnamectl set-hostname <new_hostname> . Hostname configuration is stored under /etc/ . On Red Hat-based systems this was /etc/sysconfig/network , on Debian-based systems this was /etc/hostname and on SUSE-based systems it was /etc/HOSTNAME . For DNS purposes, hostnames are appended with a period (dot) and a domain name , so that a machine with a hostname of antje could have a fully qualified domain name (FQDN) of antje.linuxfoundation.org. Network Devices and Configs \u00b6 network devices \u00b6 Unlike block and character devices, network devices are not associated with special device files (aka device nodes). Network devices are known by their names, which usually consist of a type identifier followed by a number: eth0, eth1, eno1, eno2, etc. , for ethernet devices. wlan0, wlan1, wlan2, wlp3s0, wlp3s2, etc. , for wireless devices. br0, br1, br2, etc. , for bridge interfaces. vmnet0, vmnet1, vmnet2, etc. , for virtual devices for communicating with virtual clients Historically, multiple virtual devices could be associated with single physical devices; these were named with colons and numbers; so, eth0:0 would be the first alias on the eth0 device. This was done to support multiple IP addresses on one network card, but deprecated today. It is also not compatible with IPv6. ip \u00b6 ip is the command line utility used to configure, control and query interface parameters and control devices, routing, etc. It is more efficient and versatile than ifconfig because it uses netlink sockets, rather than ioctl() system calls. ip basic syntax is ip [ OPTIONS ] OBJECT { COMMAND | help } . It can also be used with ip [ -force ] -batch filename to process commands from a file. The OBJECT argument describes what kind of action is going to be performed; the COMMANDS depends on the OBJECT selected: OBJECT Function address IPv4 or IPv6 protocol device address link Network Devices maddress Multicast Address monitor Watch for netlink messages route Routing table entry rule Rule in the routing policy database tunnel Tunnel over IP Some examples of using ip : $ ip link show - Show information for all network interfaces $ ip -s link show eth0 - Show information for the eth0 network interface, including statistics $ sudo ip addr add 192.168.1.7 dev eth0 - Set the IP address for eth0 $ sudo ip link set eth0 down - Bring interface eth0 down $ sudo ip link set eth0 mtu 1480 - Set MTU to 1480 bytes for interface eth0 $ sudo ip route add 172.16.1.0/24 via 192.168.1.5 - Set route to network ifconfig \u00b6 ifconfig is a system administration utility long found in UNIX-like operating systems used to configure, control, and query network interface parameters from either the command line or from system configuration scripts. Some examples of using ifconfig : $ ifconfig - Display information about all interfaces $ ifconfig eth0 - Display information about only eth0 $ sudo ifconfig eth0 192.168.1.50 - Set the IP address to 192.168.1.50 on interface eth0 $ sudo ifconfig eth0 netmask 255.255.255.0 - Set the netmask to 24-bit $ sudo ifconfig eth0 up - Bring interface eth0 up $ sudo ifconfig eth0 down - Bring interface eth0 down $ sudo ifconfig eth0 mtu 1480 - Set the MTU (Maximum Transfer Unit) to 1480 bytes for interface eth0 PNIDN \u00b6 The Predictable Network Interface Device Names (PNIDN) is strongly correlated with the use of udev and integration with systemd. There are now 5 types of names that devices can be given: Incorporating Firmware or BIOS provided index numbers for on-board devices Example: eno1 Incorporating Firmware or BIOS provided PCI Express hotplug slot index numbers Example: ens1 Incorporating physical and/or geographical location of the hardware connection Example: enp2s0 These names are correlated with the physical locations of the hardware on the PCI system: $ lspci | grep Ethernet Incorporating the MAC address Example: enx7837d1ea46da Using the old classic method Example: eth0 You can choose to turn off the new scheme and go back to the classic names. NIC Config Files \u00b6 Each distribution has its own set of files and/or directories, and they may be slightly different, depending on your distribution version. Red Hat /etc/sysconfig/network /etc/sysconfig/network-scripts/ifcfg-ethX /etc/sysconfig/network-scripts/ifcfg-ethX:Y /etc/sysconfig/network-scripts/route-ethX Debian /etc/network/interfaces SUSE /etc/sysconfig/network Network Manager \u00b6 As a system was booted, it consulted the network configuration files in the /etc directory subtree in order to establish the interface properties such as static or dynamic (DCHP) address configuration, whether the device should be started at boot, etc. If there were multiple network devices, policies had to be established as to what order they would be brought up, which networks they would connect to, what they would be called, etc. Modern systems often have dynamic configurations: Networks may change as a device is moved from place to place. Wireless devices may have a large choice of networks to hook into. Devices may change as hardware such as wireless devices, are plugged in or turned on and off. Use of a GUI tool, nmtui or nmcli (for scripting) are the common ways to manage networks. Examples of nmcli can be found at Networking/CLI Fedora wiki webpage or with man nmcli-examples Routing \u00b6 Routing is the process of selecting paths in a network along which to send network traffic. The routing table is a list of routes to other networks managed by the system. It defines paths to all networks and hosts, sending remote traffic to routers. To see the current routing table, you can use route -n or ip route . The default route is the way packets are sent when there is no other match in the routing table for reaching the specified network, which can be obtained dynamically using DHCP or manually configured (static). You can set the default gateway at runtime with: sudo route add default gw 192.168.1.10 enp2s0 To make persistent change, do: add GATEWAY=x.x.x.x to /etc/sysconfig/network for Red Hat systems or /etc/sysconfig/network-scripts/ifcfg-ethX for device-specific change add gateway=x.x.x.x to /etc/network/interfaces for Debian systems Static Routes \u00b6 Static routes are used to control packet flow when there is more than one router or route. They are defined for each interface and can be either persistent or non-persistent. When the system can access more than one router, or perhaps there are multiple interfaces, it is useful to selectively control which packets go to which router. To make static route at run time, do ip route add 10.5.0.0/16 via 192.168.1.100 To make persistent change, do: add 10.5.0.0/16 via 172.17.9.1 to /etc/sysconfig/network-scripts/route-ethX for Red Hat system add following to /etc/network/interfaces for Debian systems: iface eth1 inet dhcp post-up route add -host 10.1.2.51 eth1 post-up route add -host 10.1.2.52 eth1 for SUSE systems add following to files such as /etc/sysconfig/network/ifroute-eth0 : # Destination Gateway Netmask Interface [Type] [Options] # where each field is separated by tabs 192.168.1.150 192.168.1.1 255.255.255.255 eth0 10.1.1.150 192.168.233.1.1 eth0 10.1.1.0/24 192.168.1.1 - eth0 Name Resolution \u00b6 Name resolution is the act of translating hostnames to the IP addresses of their hosts. There are two facilities for doing this translation: Static name resolution (using /etc/hosts ). Dynamic name resolution (using DNS servers). Commands used to resolve the IP address of a hostname: [dig | host | nslookup] linuxfoundation.org /etc/hosts \u00b6 /etc/hosts holds a local database of hostnames and IP addresses. It contains a set of records (each taking one line) which map IP addresses with corresponding hostnames and aliases. Such static name resolution is primarily used for local, small, isolated networks. It is generally checked before DNS is attempted to resolve an address; however, this priority can be controlled by /etc/nsswitch.conf (not often used today). The other host-related files in /etc are /etc/hosts.deny and /etc/hosts.allow . The allow file is searched first and the deny file is only searched if the query is not found there. /etc/host.conf contains general configuration information; it is rarely used. DNS \u00b6 If name resolution cannot be done locally using /etc/hosts , then the system will query a DNS (Domain Name Server) server. DNS is dynamic and consists of a network of servers which a client uses to look up names. The service is distributed; any one DNS server has only information about its zone of authority; however, all of them together can cooperate to resolve any name. The machine's usage of DNS is configured in /etc/resolv.conf , which historically has looked like: search example.com aps.org nameserver 192.168.1.1 nameserver 8.8.8.8 which: Can specify particular domains to search Defines a strict order of nameservers to query May be manually configured or updated from a service such as DHCP (Dynamic Host Configuration Protocol) Most modern systems will have an /etc/hosts.resolv file generated automatically which was generated by NetworkManager invoking DHCP on the primary network interface. Network Diagnostics \u00b6 Some utilities that helps diagnosis network: ping - Sends 64-byte test packets to designated network hosts and tries to report back on the time required to reach it, any lost packets, and some other parameters. You can see whether the network is working and the host is reachable. traceroute - Displays a network path to a destination. It shows the routers packets flow through to get to a host, as well as the time it takes for each hop. mtr - It combines the functionality of ping and traceroute , and creates a continuously updated display. dig - It is useful for testing DNS functionality. Note that one can also use host or nslookup , older programs that also try to return DNS information about a host. Firewall \u00b6 A firewall is a network security system that monitors and controls all network traffic. It applies rules on both incoming and outgoing network connections and packets and builds flexible barriers depending on the level of trust and network topography (or topology) of a given connection. Firewalls can be hardware or software based. They are found both in network routers, as well as in individual computers, or network nodes. Many firewalls also have routing capabilities. Information is transmitted\u200b across networks in the form of packets, and each one of these packets has: Header, Payload, Footer. The header and footer contain information about destination and source addresses, what kind of packet it is, and which protocol it obeys, various flags, which packet number this is in a stream, and \u200ball sorts of other metadata about transmissions. The actual data is in the payload. Almost all firewalls are based on Packet Filtering . \u200bPacket filtering intercepts packets at one or more stages in the network transmission, including application, transport, network, and datalink. A firewall establishes a set of rules by which each packet may be: Accepted or rejected based on content, address, etc.\u200b Mangled in some way Redirected to another address Inspected for security reasons, etc. The next generation of firewalls were based on stateful filters, which also examine the connection state of the packet, to see if it is a new connection, \u200bpart of an already existing one, or part of none. Denial of service attacks can bombard this kind of firewall to try and overwhelm it. The third generation of firewalls is called Application Layer Firewalls, and are aware of the kind of application and protocol the connection is using. They can block anything which should not be part of the normal flow.\u200b Firewall Configurations \u00b6 Configuring the firewall with low-level cli tools such as iptables, firewall-cmd, ufw , or GUI tools such as system-config-firewall, firewall-config, gufw, yast . firewalld \u00b6 firewalld is the Dynamic Firewall Manager. It utilizes network/firewall zones which have defined levels of trust for network interfaces or connections. It supports both IPv4 and IPv6 protocols. Additionally, it separates runtime and permanent (persistent) changes to configuration, and also includes interfaces for services or applications to add firewall rules. Configuration files are kept in /etc/firewalld (primary) and /usr/lib/firewalld As a service, firewalld replaces the older iptables. It is an error to run both services, firewalld and iptables, at the same time. The command line tool to manage firewalld is firewall-cmd . Note that if you have more than one network interface when using IPv4, you have to turn on ip forwarding, with: sudo sysctl net.ipv4.ip_forward = 1 echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward sudo sysctl -p zones \u00b6 firewalld works with zones, each of which has a defined level of trust and a certain known behavior for incoming and outgoing packets. Each interface belongs to a particular zone (normally, it is NetworkManager which informs firewalld which zone is applicable) which can be changed with fireweall-cmd . Types of zones and their effects: drop - All incoming packets are dropped with no reply. Only outgoing connections are permitted. block - All incoming network connections are rejected. The only permitted connections are those from within the system. public - Do not trust any computers on the network; only certain consciously selected incoming connections are permitted. external - Used when masquerading is being used, such as in routers. Trust levels are the same as in public. dmz (Demilitarized Zone) - Used when access to some (but not all) services are to be allowed to the public. Only particular incoming connections are allowed. work - Trust (but not completely) connected nodes to be not harmful. Only certain incoming connections are allowed. home - You mostly trust the other network nodes, but still select which incoming connections are allowed. internal - Similar to the work zone. trusted - All network connections are allowed. On system installation, most, if not all Linux distributions, will select the public zone as default for all interfaces. Any zone can be bound not just to a network interface, but also to particular network addresses. A packet is associated with a zone if: It comes from a source address already bound to the zone; or if not, It comes from an interface bound to the zone. Any packet not fitting the above criteria is assigned to the default zone (i.e, usually public). # example commands for adding rules for setting zones on the level of interface, IP addresses, service, port and protocal sudo firewall-cmd --permanent --zone = internal --change-interface = eno1 sudo firewall-cmd --permanent --zone = trusted --add-source = 192 .168.1.0/24 sudo firewall-cmd --permanent --zone = home --add-service = dhcp sudo firewall-cmd --zone = home --add-port = 21 /tcp System Startup and Shutdown \u00b6 The boot sequence basic steps are: The BIOS/UEFI locates and executes the boot program, or boot loader. POST (Power On Self Test) is run to check the memory and hardware and then search a specific location or device for a boot program boot program is found in MBR or using UEFI. It is usually GRUB. The boot loader loads the kernel. kernel need to be decompressed, then performs hardware checks, gains access to important peripheral hardware The kernel starts the init process (pid=1). init manages system initialization, using systemd or the older Upstart and SysVinit startup scripts. GRUB \u00b6 Virtually, all (non-embedded) modern Linux distributions use GRUB (GRand Unified Boot Loader). efibootmgr is not actually a boot loader, but is a boot manager, used in conjunction with GRUB on multi-boot EFI systems. Some important features of GRUB are: Alternative operating systems can be chosen at boot time. Alternative kernels and/or initial ramdisks can be chosen at boot time for a given operating system. Boot parameters can be easily changed at boot time without having to edit configuration files, etc., in advance. At boot, a basic configuration file is read, /boot/grub/grub.cfg , or /boot/grub2/grub.cfg , or /boot/efi/EFI/redhat/grub.cfg . This file is auto-generated by update-grub (or grub-mkconfig or grub2-mkconfig ) based on configuration files in the /etc/grub.d directory and on /etc/default/grub and should not be edited by hand. Upon system boot, after the initial POST and BIOS stages, GRUB will be entered and display a menu containing a list of bootable images either from one or more Linux distributions or operating systems, with submenus. After selecting an entry, you can type e for edit and then enter into an interactive shell to edit the particular boot option. If there are serious problems, like not being able to find a configuration file, GRUB reverts back to a pure shell mode and you may be able to rescue the system without resorting to rescue media. In both GRUB versions, the first hard drive is denoted as hd0, the second is hd1, etc. However, in Version 1, partitions start counting from 0, and in Version 2 from 1. For example: sda1 is (hd0,1) in GRUB 2, but (hd0,0) in GRUB 1. On systems configured with Boot Loader Specification Configuration (BLSCFG), one still uses the usual grub commands when installing or updating kernels, but detailed information and options for each kernel are found in /boot/loader/entries . This new scheme can be turned on/off with grub2-switch-to-blscfg or altering the variable GRUB_ENABLE_BLSCFGS=[true|false] in /etc/default/grub . Configuration Files in /etc \u00b6 For historical reasons, Linux distributions evolved their own rules about exactly where to place some information in /etc . For example, all Red Hat-derived systems make extensive use of /etc/sysconfig/ , while Debian-based systems have used /etc/default/ . Interestingly, RHEL and SUSE use both.\u200b There should be only text files found under /etc , no binary formats or data. /etc/sysconfig \u00b6 Files in this directory and its subdirectories are used by many system utilities services, often consulted when the system starts and stops services or queries their status. /etc/default \u00b6 The files are used to provide extra options when starting a service and typically contain code to set environment variables. shutdown \u00b6 shutdown is used to bring the system down in a secure fashion, notifying all users that the system is going down and then stopping it in a graceful and non-destructive way. After it is shut down, the system is either halted or rebooted. There are also the legacy commands reboot , halt , and poweroff This article shows a checklist of things to do pre and post reboot. /sbin/init \u00b6 /sbin/init (usually just called init) is the first user-level process (or task) run on the system and continues to run until the system is shutdown. Traditionally, it has been considered the parent of all user processes, although technically that is not true, as some processes are started directly by the kernel. init coordinates the later stages of the boot process, configures all aspects of the environment, and starts the processes needed for logging into the system. init also works closely with the kernel in cleaning up after processes when they terminate. Traditionally, nearly all distributions based the init process on UNIX's venerable SysVinit' software. However, this scheme was developed decades ago under rather different circumstances: The target was multi-user mainframe systems (and not personal computers, laptops, and other devices) The target was a single processor system startup was viewed as a serial process, divided into a series of sequential stages (run levels). Startup (and shutdown) time was seen rare and not an important matter; it was far less important than getting things right. Modern systems have required newer methods with enhanced capabilities. systemd \u00b6 The systemd system and session manager for Linux is now dominant in all major distributions. Features include the following: Boots faster than previous init systems Provides aggressive parallelization capabilities Uses socket and D-Bus activation for starting services Replaces shell scripts with programs Offers on-demand starting of daemons Keeps track of processes using cgroups Maintains mount and automount points Implements an elaborate transactional dependency-based service control logic Can work as a drop-in replacement for SysVinit and is compatible with SysVinit scripts. systemd is backward compatible with SysVinit and the concept of runlevels is supported via runlevel targets . The telinit program is emulated to work with runlevels. systemd prefers to use a set of standardized configuration files, it can also use distribution-dependent legacy configuration files as a fall-back. Runlevel Target 0 poweroff.target 1 rescue.target 2,3,4 multi-user.target 5 graphical.target 6 reboot.target The isolate command will immediately stop processes that are not enabled in the new unit, possibly including the graphical environment or terminal you are currently using # immediately change the target sudo systemctl isolate multi-user.target # change the default target, effective after each reboot sudo systemctl enable multi-user.target sudo systemctl set-default multi-user.target Use man systemd.special to learn more. systemctl \u00b6 systemctl is the main utility for managing services. Its basic syntax is: systemctl [options] command [name] . Some examples: systemctl - To show the status of everything that systemd controls systemctl list-units -t service --all - To show all available services systemctl list-units -t service - To show only active services sudo systemctl start/stop foo.service - To start (activate) or stop (deactivate) one or more units (a service or a socket) sudo systemctl enable/disable sshd.service - To enable/disable a service For most commands, you can omit the .service attached to the service name. Also worth read SysVinit to Systemd Cheetsheet . .service file \u00b6 To make a service start up when system boots, we need to configure it and put under /etc/systemd/system/multi-user.target.wants/ . A simple service file may look like the following. For more info check man systemd.service or here [Unit] Description=Simple notifying service [Service] Type=notify ExecStart=/usr/sbin/simple-notifying-service [Install] WantedBy=multi-user.target Backup and Recovery \u00b6 Obviously, files essential to your organization require backup. Configuration files may change frequently, and along with individual user's files, require backup as well. Logging files can be important and worth to backup if you have to investigate your system's history, which can be particularly important for detecting intrusions and other security violations. The simplest backup scheme is to do a full backup of everything once, and then perform incremental backups of everything that subsequently changes. While full backups can take a lot of time, restoring from incremental backups can be more difficult and time consuming. Thus, you can use a mix of both to optimize time and effort. An example of one useful strategy involving tapes (you can easily substitute other media in the description): Use tape 1 for a full backup on Friday. Use tapes 2-5 for incremental backups on Monday-Thursday. Use tape 6 for full backup on second Friday. Use tapes 2-5 for incremental backups on second Monday-Thursday. Do not overwrite tape 1 until completion of full backup on tape 6. After full backup to tape 6, move tape 1 to external location for disaster recovery. For next full backup (next Friday) get tape 1 and exchange for tape 6. A good rule of thumb is to have at least two weeks of backups available. Backup utilities \u00b6 cpio and tar - create and extract archives of files. gzip, bzip2, xz - create archive files to be written to disk, magnetic tape, or any other device which can hold files. Archives are very useful for transferring files from one filesystem or machine to another. dd - transfer raw data between media. It can copy entire partitions or entire disks. rsync - synchronize directory subtrees or entire filesystems across a network, or between different filesystem locations on a local machine. dump and restore - old but designed specifically for backups. They read from the filesystem directly (which is more efficient). However, they must be restored only on the same filesystem type that they came from. There are newer alternatives. mt - useful for querying and positioning tapes before performing backups and restores. dd \u00b6 dd is a common UNIX-based program whose primary purpose is the low-level copying and conversion of raw data. It is used to copy a specified number of bytes or blocks, performing on-the-fly byte order conversions, as well as being able to convert data from one form to another. It can also be used to copy regions of raw device files, for example backing up the boot sector of a hard disk, or to read fixed amounts of data from special files like /dev/zero or /dev/random. The basic syntax is: dd if=input-file of=output-file options rsync \u00b6 rsync (remote synchronize) is used to transfer files across a network (or between different locations on the same machine). The basic syntax is: rsync [options] sourcefile destinationfile The source and destination can take the form of target:path , where target can be in the form of [user@]host . The user@ part is optional and used if the remote user is different from the local user. You have to be very careful with rsync about exact location specifications (especially if you use the --delete option), so it is highly recommended to use the --dry-run option first, and then repeat if the projected action looks correct. rsync is very clever; it checks local files against remote files in small chunks, and it is very efficient in that when copying one directory to a similar directory, only the differences are copied over the network. This synchronizes the second directory with the first directory. You may often use the -r option, which causes rsync to recursively walk down the directory tree copying all files and directories below the one listed as the sourcefile. Thus, a very useful way to back up a project directory might be similar to: rsync -r project-X archive-machine:archives/project-X A simple (and very effective and very fast) backup strategy is to simply duplicate directories or partitions across a network with rsync commands and to do so frequently. cpio \u00b6 cpio (copy in and out) is a general file archiver utility that has been around since the earliest days of UNIX and was originally designed for tape backups. It is light weight than tar . The -o or --create option tells cpio to copy files out to an archive, which reads a list of file names (one per line) from standard input and writes the archive to standard output. The -i or --extract option tells cpio to copy files from an archive, reading the archive from standard input. The -t or --list option tells cpio to list the archive contents. Backup programs \u00b6 Amanda - (Advanced Maryland Automatic Network Disk Archiver) uses native utilities (including tar and dump) but is far more robust and controllable. Amanda is generally available on Enterprise Linux systems through the usual repositories. Bacula - designed for automatic backup on heterogenous networks. It can be rather complicated to use and is recommended (by its authors) only to experienced administrators. Bacula is generally available on Enterprise Linux systems through the usual repositories. Clonezilla - a very robust disk cloning program, which can make images of disks and deploy them, either to restore a backup, or to be used for ghosting, to provide an image that can be used to install many machines. Linux Security Modules \u00b6 Linux Security Modules (LSM) emphasis the idea of implementing mandatory access controls over the variety of requests made to the kernel in a way that: Minimizes changes to the kernel Minimizes overhead on the kernel Permits flexibility and choice between different implementations, each of which is presented as a self-contained LSM (Linux Security Module) The basic idea is to hook system calls and insert code whenever an application requests a transition to kernel (system) mode. This code makes sure permissions are valid, malicious intent is protected against, by invoking security-related functional steps before and/or after a system call is fulfilled by the kernel. The current LSM implementations: SELinux, AppArmor, Smack, Tomoyo. SELinux \u00b6 SELinux was originally developed by the United States NSA (National Security Administration) and has been brought into a large usage base. Operationally, SELinux is a set of security rules that are used to determine which processes can access which files, directories, ports, and other items on the system. It works with three conceptual quantities: Contexts - Context are labels to files, processes and ports. Examples of contexts are SELinux user, role, type, level. Rules - Rules describe access control in terms of contexts, processes, files, ports, users, etc. Policies - Policies are a set of rules that describe what system-wide access control decisions should be made by SELinux. SELinux modes are selected (and explained) in /etc/sysconfig/selinux (CentOS and openSUSE) or /etc/selinux/config (Ubuntu). The sestatus utility can display the current mode and policy. The list of SELinux modes: Enforcing - All SELinux code is operative and access is denied according to policy. All violations are audited and logged. Permissive - Enables SELinux code, but only audits and warns about operations that would be denied in enforcing mode. Disabled - Completely disables SELinux kernel and application code, leaving the system without any of its protections. Read this comprehensive guide to install and enable SELinux . getenforce, setenforce \u00b6 getenforce can be used to get the current SELinux mode. setenforce can be used to switch between enforcing and permissive modes on the fly while the system is in operation, but it does not allow you to enable/disable SELinux completely. Disable SELinux is done through either: Edit the SELinux configuration file and set SELINUX=disabled Add selinux=0 to the kernel parameter list when rebooting Note that disabling SELinux on systems in which SELinux will be re-enabled is not recommended. It is preferable to use the permissive mode instead of disabling SELinux, so as to avoid relabeling the entire filesystem, which can be time-consuming. Policies \u00b6 SELinux configuration file also sets the SELinux policy. Multiple policies are allowed, but only one can be active at a time. Changing the policy may require a reboot of the system and a time-consuming re-labeling of filesystem contents. Each policy has files which must be installed under /etc/selinux/[SELINUXTYPE] . Some common SELinux policies: targeted - The default policy in which SELinux is more restricted to targeted processes. User processes and init processes are not targeted, while network service processes are targeted. SELinux enforces memory restrictions for all processes, which reduces the vulnerability to buffer overflow attacks. minimum - A modification of the targeted policy where only selected processes are protected. MLS - The Multi-Level Security policy is much more restrictive; all processes are placed in fine-grained security domains with particular policies. SELinux policy behavior can be configured at runtime without rewriting the policy. This is accomplished by configuring SELinux Booleans, which are policy parameters that can be enabled and disabled: getsebool - to see booleans setsebool - to set booleans default the change is not persistent. Make it persistent with option -P semanage boolean -l - to see persistent boolean settings Contexts \u00b6 There are four SELinux contexts: User, Role, Type, Level. Use -Z with utilities such as ls , ps , cp , mv , and mkdir . To see the context associated witha file/process. Type is the most commonly utilized context, and its label should end with _t , as in kernel_t . You can use chcon to change a file's context, as in chcon -t etc_t somefile . Note that newly created files inherit the context from their parent directory, but when moving files, it is the context of the source directory which may be preserved. In that event, use restorecon which resets file contexts, based on parent directory settings. i.e. restorecon -Rv /home/jimih on a directory will correct the context recursively for files under. To configure the default context for a newly created directory, use semanage fcontext (from policycoreutils-python package). After the change, a call to restorecon is still required. Monitoring Access \u00b6 SELinux comes with a set of tools that collect issues at run time, log these issues and propose solutions to prevent same issues from happening again, via setroubleshoot-server package. AppArmor \u00b6 AppArmor is an LSM alternative to SELinux, used by SUSE, Ubuntu and other distributions. It: Provides Mandatory Access Control (MAC) Allows administrators to associate a security profile to a program which restricts its capabilities Is considered easier (by some but not all) to use than SELinux Is considered filesystem-neutral (no security labels required) In addition to manually specifying profiles, AppArmor includes a learning mode, in which violations of the profile are logged, but not prevented. This log can then be turned into a profile, based on the program's typical behavior. To view its status, do sudo apparmor_status or sudo aa_status . AppArmor Modes: Enforce Mode - Applications are prevented from acting in ways which are restricted. Attempted violations are reported to the system logging files. This is the default mode. A profile can be set to this mode with aa-enforce. Complain Mode - Policies are not enforced, but attempted policy violations are reported. This is also called the learning mode. A profile can be set to this mode with aa-complain. Profiles restrict how executable programs that have pathnames on the system can be used. Linux distributions come with\u200b pre-packaged profiles or installed with an AppArmor package ( apparmor-profiles ), included in /etc/apparmor.d . Some common AppArmor utilities: Program Use apparmor_status Show status of all profiles and processes with profiles apparmor_notify Show a summary for AppArmor log messages complain Set a specified profile to complain mode enforce Set a specified profile to enforce mode disable Unload a specified profile from the current kernel and prevent from being loaded on system startup logprof Scan log files, and, if AppArmor events that are not covered by existing profiles have been recorded, suggest how to take into account, and, if approved, modify and reload easyprof Help set up a basic AppArmor profile for a program Local System Security \u00b6 Security can be defined in terms of the system's ability to regularly do what it is supposed to do, integrity and correctness of the system, and ensuring that the system is only available to those authorized to use it. The biggest problem with security is to find that appropriate mix of security and productivity; if security restrictions are tight, opaque, and difficult, especially with ineffective measures, users will circumvent procedures. It is important to create and publicize to your organization a clear security policy that is descriptive, easy to understand, and constantly updated. Policies should be generic and specify enforcement actions and response to breach. Essential aspects to cover: Confidentiality Data Integrity Availability Consistency Control Audit You should make sure that the data is correct and the system behaves as it is expected to do. There should be processes in effect to determine who is given access to your system. Risk Analysis \u00b6 Risk analysis is based on the following three questions: What do I want to protect (identify assets)? What am I protecting it against (identify threats)? How much time, personnel, and money is needed to provide adequate protection? Two basic philosophies found in use in most computing environments: Anything not expressly permitted is denied.\u200b Anything not expressly forbidden is permitted. The first choice is tighter: a user is allowed to do only what is clearly and explicitly specified as permissible without privilege. This is the most commonly used philosophy. The second choice builds a more liberal environment where users are allowed to do anything except what is expressly forbidden. It implies a high degree of assumed trust and is less often deployed for obvious reasons. Patch system updates \u00b6 Most attacks exploit known security holes and are deployed in the time period between revelation of a problem and patches being applied. It is critical to pay attention to your Linux distributor's updates and upgrades and apply them as soon as possible. Hardware Accessibility \u00b6 Any time hardware is physically accessible security can be compromised by: Key logging: Recording the real time activity of a computer user including the keys they press. The captured data can either be stored locally or transmitted to remote machines. Network sniffing: Capturing and viewing the network packet level data on your network. Booting with a live or rescue disk. Remounting and modifying disk content. Physical access to a system makes it possible for attackers to easily leverage several attack vectors, in a way that makes all operating system level recommendations irrelevant. Thus, security policy should start with requirements on how to properly secure physical access to servers and workstations. Necessary protective steps include: Locking down workstations and servers Protecting your network links against access by people you do not trust Protecting your keyboards where passwords are entered to ensure the keyboards cannot be tampered with Configuring password protection of the BIOS in such a way that the system cannot be booted with a live or rescue CD/DVD or USB key Setting a BIOS password protects against unauthorized persons changing the boot options to gain access to your system. You can secure the boot process further with a secure bootloader password to prevent someone from bypassing the user authentication step. Secure Mount Filesystem \u00b6 When a filesystem is mounted, either at the command line with a mount command, or automatically by inclusion in /etc/fstab, various options can be specified to enhance security: nodev - Do not interpret character or block special devices on the filesystem. nosuid - The set-user-identifier or set-group-identifier bits are not to take effect.\u200b noexec - Restrict direct execution of any binaries on the mounted filesystem. ro - Mount the filesystem in read-only mode as in: mount -o ro,noexec,nodev /dev/sda2 /edsel By setting the setuid (set user ID) flag on an executable file, you modify this normal behavior by giving the program the access rights of the owner rather than the user of the program. Similar rule apply for setgid bit for giving runtime group access rights. By default, when a file is created in a directory, it is owned by the user and group of the user that created it. Using the setgid setting on the directory changes this so that files created in the directory are group owned by the group owner of the directory. This allows you to create a shared directory in which a group of users can share files. Set the setuid bit with chmod u+s somefile , and setgid bit with chmod g+s somefile Troubleshooting \u00b6 Troubleshooting involves taking a number of steps which need to be repeated iteratively until solutions are found. A basic recipe might be: Characterize the problem Reproduce the problem Always try the easy things first Eliminate possible causes one at a time Change only one thing at a time; if that doesn't fix the problem, change it back Check the system logs (/var/log/messages, /var/log/secure, etc.) for further information System Rescue \u00b6 Sooner or later a system is likely to undergo a significant failure. System Rescue media in the form of optical disks or portable USB drives can be used to fix the situation. Booting into either emergency or single user mode can enable using the full suite of Linux tools to repair the system back to normal function. The rescue image can be mounted and use chroot to change into that environment. You may install software packages from inside the chroot-ed environment. You may also be able to install them from outside the chroot-ed environment. i.e. sudo rpm -ivh --force --root=/mnt/sysimage /mnt/source/Packages/vsftpd-2*.rpm . Emergency boot media are useful when your system won't boot due to some issue such as missing, misconfigured, or corrupted files or a misconfigured service. In emergency mode you are booted into the most minimal environment possible. The root filesystem is mounted read-only, no init scripts are run and almost nothing is set up. To enter emergency mode, you need to select an entry from the GRUB boot menu and then hit e for edit. Then add the word emergency to the kernel command line before telling the system to boot. If your system boots, but does not allow you to log in when it has completed booting, try single user mode: init is started Services are not started Network is not activated All possible filesystems are mounted root access is granted without a password A system maintenance command line shell is launched In this mode, your system boots to runlevel 1 (in SysVinit language). To enter single user mode, you need to select an entry from the GRUB boot menu and then hit e for edit. Then add the word single to the kernel command line before telling the system to boot.","title":"Linux System Administration"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#linux-filesystem-tree-layout","text":"Linux's One Big Filesystem - diagrammed as an inverted tree with root directory / at top of tree. The large logical filesystem may have many distinct filesystems mounted at various points appear as subdirectories. Data Distinctions Sharable - data can be shared between hosts Non-sharable - data that must be specific to a particular host Static - data such as binaries, libraries, documentation, that does not change without system admin's actions Variable - data that change without a system admin's help The Filesystem Hierarchy Standard (FHS) document specifies the main directories that need to be present on a Linux host.","title":"Linux Filesystem Tree Layout"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#bin","text":"Contains executable programs and scripts needed by both system administrators and unprivileged users, which are required when no other filesystems have yet been mounted (single user or recovery mode). Required programs which must exist in /bin/ include: cat, chgrp, chmod, chown, cp, date, dd, df, dmesg, echo, false, hostname, kill, ln, login, ls, mkdir, mknod, more, mount, mv, ps, pwd, rm, rmdir, sed, sh, stty, su, sync, true, umount and uname","title":"/bin"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#boot","text":"Essential files for booting the system. Stores data used before the kernel begins executing user-mode programs vmlinz - the compressed Linux kernel initramfs or initrd - initial RAM filesystem, mounted before the real root fs is available config - configure the kernel compilation System.map - Kernel symbol table, used for debugging","title":"/boot"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#dev","text":"Contains special device files (also known as device nodes) which represent devices built into or connected to the system. Such device files represent character (byte-stream) and block I/O devices. Network devices do not have device nodes in Linux, and are instead referenced by name, such as eth1 or wlan0. All modern Linux distributions use the udev system, which creates nodes in /dev only as needed when devices are found. On ancient systems (or embedded devices, it can be created by MAKEDEV or mknod at install or at any other time, as needed.","title":"/dev"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#etc","text":"Contains machine-local configuration files and some startup scripts. Files and directories which may be found in this directory include: csh.login, exports, fstab, ftpusers, gateways, gettydefs, group, host.conf, hosts.allow, hosts.deny, hosts,equiv, hosts.lpd, inetd.conf, inittab, issue, ld.so.conf, motd, mtab, mtools.conf, networks, passwd, printcap, profile, protocols, resolv.conf, rpc, securetty, services, shells, syslog.conf /etc/skel - Contains skeleton files used to populate newly created home directories. /etc/systemd - Contains or points to configuration scripts for starting, stopping system services when using systemd. /etc/init.d - Contains startup and shut down scripts when using System V initialization. /etc/default/ - Contains configurations for many default actions, like cron, useradd","title":"/etc"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#opt","text":"This directory is designed for software packages that wish to keep all or most of their files in one isolated place, rather than scatter them all over the system in directories shared by other software. For example, if dolphy_app were the name of a package which resided under /opt, then all of its files should reside in directories under /opt/dolphy_app, including /opt/dolphy_app/bin for binaries and /opt/dolphy_app/man for any man pages. This can make both installing and uninstalling software relatively easy, as everything is in one convenient isolated location in a predictable and structured manner. The directories /opt/bin, /opt/doc, /opt/include, /opt/info, /opt/lib, and /opt/man are reserved for local system administrator use. Packages may provide files which are linked or copied to these reserved directories, but the packages must also be able to function without the programs being in these special directories. Most systems do not populate these directories.","title":"/opt"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#proc","text":"Mount point for a pseudo-filesystem, where all information resides only in memory. The kernel exposes some important data structures through /proc entries Important pseudo-files, including /proc/interrupts, /proc/meminfo, /proc/mounts, and /proc/partitions , provide an up-to-the-moment glimpse of the system's hardware. Others, like /proc/filesystems and the /proc/sys/ directory, provide system configuration information and interfaces. The process directories contain information about each running process on the system.","title":"/proc"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#sys","text":"Mount point for the sysfs pseudo-filesystem where all information resides only in memory. It contains information about devices and drivers, kernel modules, system configuration structures, etc. sysfs is used both to gather information about the system, and modify its behavior while running. Almost all pseudo-files in /sys contain only one line, or value.","title":"/sys"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#sbin","text":"Contains binaries essential for booting, restoring, recovering, and/or repairing in addition to those binaries in the /bin directory. The following programs should be included in this directory (if their subsystems are installed): fdisk, fsck, getty, halt, ifconfig, init, mkfs, mkswap, reboot, route, swapon, swapoff, update","title":"/sbin"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#usr","text":"Can be thought of as a secondary hierarchy used for files which are not needed for system booting. This directory is typically read-only data. It contains binaries which are not needed in single user mode.","title":"/usr"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#var","text":"Contains variable (or volatile) data files that change frequently during system operation. These include: Log files Spool directories and files Administrative data files Transient and temporary files, such as cache contents. For security reasons, it is often considered a good idea to mount /var as a separate filesystem. Furthermore, if the directory gets filled up, it should not lock up the system.","title":"/var"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#run","text":"The purpose of /run is to store transient files: those that contain runtime information, which may need to be written early in system startup, and which do not need to be preserved when rebooting. Generally, /run is implemented as an empty mount point, with a tmpfs ram disk (like /dev/shm ) mounted there at runtime. Thus, this is a pseudo-filesystem existing only in memory. Some existing locations, such as /var/run and /var/lock , will be now just symbolic links to directories under /run.","title":"/run"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#processes","text":"","title":"Processes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#definition","text":"A program is a set of instructions, along with any internal or external data used while carrying the instructions out. People often distinguish between programs, which are compiled into a binary executable form; and scripts, which need to be run by an interpreter such as bash, Python or Perl. A process is an instance of a program in execution. Every process has a pid (Process ID), a ppid (Parent Process ID), and a pgid (Process Group ID). Every process has program code, data, variables, file descriptors, and an environment. For historical reasons, the largest PID has been limited to a 16-bit number, or 32768 . It is possible to alter this value by changing /proc/sys/kernel/pid_max . Eventually when process id reaches the max, it will start again at PID = 300 . A program may be composed of multiple simultaneous threads (multithreading), each of which is considered as its own process.","title":"Definition"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#process-attributes","text":"At any given moment, the process may take a snapshot of itself by trapping the state of its CPU registers, where it is executing in the program, what is in the process' memory, and other information. This is the context (state) of the process, which is critical to the kernel's ability to do context switching . Every process has permissions based on which user has called it to execute. It may also have permissions based on who owns its program file. Programs which are marked with an \"s\" execute bit have a different \"effective\" user id than their \"real\" user id. These programs are referred to as setuid programs . They run with the user-id of the user who owns the program, where a non-setuid program runs with the permissions of the user who starts it. Note that setuid programs owned by root can be a security problem . Also Note that setuid bit only works on binary executables , not on scripts, as a security measure. Every process has resources such as allocated memory, file handles, etc. When a process is started, it is isolated in its own user space to protect it from other processes. Processes do not have direct access to hardware. Hardware is managed by the kernel, so a process must use system calls to indirectly access hardware. System calls are the fundamental interface between an application and the kernel.","title":"Process Attributes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#control-process-with-ulimit","text":"ulimit is a built-in bash command that displays or resets a number of resource limits associated with processes running under a shell. The changes only affect the current shell . To make changes that are effective for all logged-in users, you need to modify /etc/security/limits.conf A system administrator may need to change some of these values in either direction: To restrict capabilities so an individual user and/or process cannot exhaust system resources, such as memory, cpu time or the maximum number of processes on the system. To expand capabilities so a process does not run into resource limits; for example, a server handling many clients may find that the default of 1024 open files makes its work impossible to perform.","title":"Control process with ulimit"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#creating-processes","text":"An average Linux system is always creating new processes. This is often called forking ; the original parent process keeps running , while the new child process starts. Often, rather than just a fork, one follows it with an exec , where the parent process terminates, and the child process inherits the process ID of the parent . Take sshd daemon as an example: it is started when the init process executes the sshd init script, then the daemon process listens for ssh requests from remote users.When a request is received, sshd creates a new copy of itself to service the request. Each remote user gets their own copy of the sshd daemon running to service their remote login. The sshd process will start the login program to validate the remote user. If the authentication succeeds, the login process will fork off a shell (say bash) to interpret the user commands, and so on. systemd-based systems run a special process named kthreadd with pid=2 whose job is to adopt orphaned children , who will then show ppid=2. What happens when a user executes a command in a command shell interpreter, such as bash? A new process is created (forked from the user's login shell). A wait system call puts the parent shell process to sleep. The command is loaded onto the child process's space via the exec system call. In other words, the code for the command replaces the bash program in the child process's memory space. The command completes executing, and the child process dies via the exit system call. The parent shell is re-awakened by the death of the child process and proceeds to issue a new shell prompt. The parent shell then waits for the next command request from the user, at which time the cycle will be repeated. If a command is issued for background processing (by adding an ampersand & at the end of the command line), the parent shell skips the wait request and is free to issue a new shell prompt immediately, allowing the background process to execute in parallel . Otherwise, for foreground requests, the shell waits until the child process has completed or is stopped via a signal. Use jobs -l to view background processes. Use kill -15 %<job_number> to send a SIG_TERM to that process. Use fg %<job_number to bring a background process to the foreground. Use bg %<job_number> after CTRL+Z to resume a stopped process. Some shell commands (such as echo and kill) are built into the shell itself, and do not involve loading of program files. For these commands, neither a fork nor an exec is issued for the execution.","title":"Creating Processes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#process-states","text":"Processes can be in one of several possible states. The scheduler manages all of the processes. The process state is reported by the process listing. Running - process is executing on a CPU or sitting in the run queue waiting for a new time slice Sleeping / Waiting - process is waiting on a request (usually I/O) Stopped - process has been suspended, commonly when the program's memory, CPU registers, or other attributes are being examined, after which can be resumed. Zombie / Defunct- process terminates and no other process or parent process requests for its exit state. if the parent of a process dies, it is adopted by init (PID=1) or kthreadd (PID=2)","title":"Process States"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#execution-modes","text":"A process may be executing in either user mode or system mode (kernel mode). What instructions can be executed depends on the mode and is enforced at the hardware, not software, level. The mode is not a state of the system; it is a state of the processor, as in a multi-core or multi-CPU system each unit can be in its own individual state.","title":"Execution Modes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#user-mode","text":"Each process executing in user mode has its own memory space , parts of which may be shared with other processes; except for the shared memory segments, a user process is not able to read or write into or from the memory space of any other process. Even a process run by the root user or as a setuid program runs in user mode, except when jumping into a system call, and has only limited ability to access hardware.","title":"User Mode"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#system-kernel-mode","text":"CPU has full access to all hardware on the system , including peripherals, memory, disks, etc. If an application needs access to these resources, it must issue a system call, which causes a context switch from user mode to kernel mode. This procedure must be followed when reading and writing from files, creating a new process, etc. Application code never runs in kernel mode, only the system call itself which is kernel code. When the system call is complete, a return value is produced and the process returns to user mode with the inverse context switch.","title":"System (Kernel) Mode"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#daemons","text":"A daemon process is a background process whose sole purpose is to provide some specific service to users of the system. They can be quite efficient because they only operate when needed. Many daemons are started at boot time. Daemon names often (but not always) end with d, e.g. httpd and systemd-udevd. Daemons may respond to external events (systemd-udevd) or elapsed time (crond). Daemons generally have no controlling terminal and no standard input/output - devices. Daemons sometimes provide better security control. Some examples include xinetd, httpd, lpd, and vsftpd.","title":"Daemons"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#nice-value-set-priorities","text":"Process priority can be controlled through the nice and renice commands. renice is used to raise or lower the nice value of an already running process. Since the early days of UNIX, the idea has been that a nice process lowers its priority to yield to others. Thus, the higher the niceness is, the lower the priority. The niceness value can range from -20 (the highest priority) to +19 (the lowest priority). Note that increasing the niceness of a process does not mean it won't run; it may even get all the CPU time if there is nothing else with which to compete. By default, only a superuser can decrease the niceness. After a non-privileged user has increased the nice value, only a superuser can lower it back. It is possible to give normal users the ability to decrease their niceness within a predetermined range, by editing /etc/security/limits.conf .","title":"nice value set priorities"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#static-and-shared-libraries","text":"static library - The code for the library functions is inserted in the program at compile time, and does not change thereafter, even if the library is updated. shared library - The code for the library functions is loaded into the program at run time, and if the library is changed later, the running program runs with the new library modifications. Using shared libraries is more efficient because they can be used by many applications at once; memory usage, executable sizes, and application load time are reduced. Shared Libraries are also called Dynamic Link Libraries (DLLs). Under Linux, shared libraries are (and must be) carefully versioned to avoid DLL Hell. ldd can be used to ascertain what shared libraries an executable requires. It shows the .so name of the library and what file it actually points to. ldconfig is generally run at boot time (but can be run anytime), and uses /etc/ld.so.conf , which lists the directories that will be searched for shared libraries. ldconfig must be run as root, and shared libraries should only be stored in system directories when they are stable and useful. The linker also first search any directories specified in the environment variable LD_LIBRARY_PATH , a colon separated list of directories.","title":"Static and Shared Libraries"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#signals","text":"Signals are one of the oldest methods of Inter-Process Communication (IPC) and are used to notify processes about asynchronous events (or exceptions). Signals can only be sent between processes owned by the same user or from a process owned by the superuser to any process. When a process receives a signal, what it does will depend on the way the program is written, or just respond according to system defaults. SIGKILL(#9) and SIGSTOP(#19) cannot be handled, and will always terminate the program. Use kill -l to view list of signals. man 7 signal will give further documentation. Generally, signals are used to handle two things: Exceptions detected by hardware (such as an illegal memory reference) Exceptions generated by the environment (such as the premature death of a process from the user's terminal) Since a process cannot send a signal directly to another process, it must ask the kernel to send the signal. We use kill to send signals (by either number or name) to a process. killall kills all processes with a given name. pkill is similar to kill but uses name instead of pid. Note that POSIX says one should use signal names, not numbers, which are allowed to be completely implementation dependent. $ kill 1234 $ kill -9 1234 $ kill -SIGTERM 1234 $ killall bash $ killall -9 bash $ killall -SIGKILL bash $ pkill -HUP rsyslogd Signal Value Default Action POSIX? Meaning SIGHUP 1 Terminate Yes Hangup detected on controlling terminal or death of controlling process SIGINT 2 Terminate Yes Interrupt from keyboard SIGQUIT 3 Core dump Yes Quit from keyboard SIGILL 4 Core dump Yes Illegal instruction SIGTRAP 5 Core dump No Trace/breakpoint trap for debugging SIGABTR SIGIOT 6 Core dump Yes Abnormal termination SIGBUS 7 Core dump Yes Bus error SIGFPE 8 Core dump Yes Floating point exception SIGKILL 9 Terminate Yes Kill signal (cannot be caught or ignored) SIGUSR1 10 Terminate Yes User-defined signal 1 SIGSEGV 11 Core dump Yes Invalid memory reference SIGUSR2 12 Terminate Yes User-defined signal 2 SIGPIPE 13 Terminate Yes Broken pipe: write to pipe with no readers SIGALRM 14 Terminate Yes Timer signal from alarm SIGTERM 15 Terminate Yes Process termination SIGSTKFLT 16 Terminate No Stack fault on math co-processor SIGCHLD 17 Ignore Yes Child stopped or terminated SIGCONT 18 Continue Yes Continue if stopped SIGSTOP 19 Stop Yes Stop process (can not be caught or ignored) SIGTSTP 20 Stop Yes Stop types at tty SIGTTIN 21 Stop Yes Background process requires tty input SIGTTOU 22 Stop Yes Background process requires tty output SIGURG 23 Ignore No Urgent condition on socket (4.2 BSD) SIGXCPU 24 Core dump Yes CPU time limit exceeded (4.2 BSD) SIGXFSZ 25 Core dump Yes File size limit exceeded (4.2 BSD) SIGVTALRM 26 Terminate No Virtual alarm clock (4.2 BSD) SIGPROF 27 Terminate No Profile alarm clock (4.2 BSD) SIGWINCH 28 Ignore No Window resize signal (4.3 BSD, Sun) SIGIO SIGPOLL 29 Terminate No I/O now possible (4.2 BSD) (System V) SIGPWR 30 Terminate No Power Failure (System V) SIGSYS SIGUNUSED 31 Terminate No Bad System Called. Unused signal","title":"Signals"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#package-management-systems","text":"Use of Packages and Package Management System helps keep track of files and metadata in an automated, predictable and reliable way, so that system administrators can automate the process of installing, upgrading, configuring and removing software packages and scale to thousands of systems without requiring manual work on each individual system. Automation: No need for manual installs and upgrades. Scalability: Install packages on one system, or 10,000 systems. Repeatability and predictability. Security and auditing. A given package may contain executable files, data files, documentation, installation scripts and configuration files. Also included are metadata attributes such as version numbers, checksums, vendor information, dependencies, descriptions, etc. Upon installation, all that information is stored locally into an internal database, which can be conveniently queried for version status and update information.","title":"Package Management Systems"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#package-types","text":"Binary Packages Binary packages contain files ready for deployment, including executable files and libraries. These are architecture dependent. Source Packages Source packages are used to generate binary packages; you should always be able to rebuild a binary package from the source package. One source package can be used for multiple architectures. Architecture-independent Architecture-independent packages contain files and scripts that run under script interpreters, as well as documentation and configuration files. Meta-packages Meta-packages are groups of associated packages that collect everything needed to install a relatively large subsystem, such as a desktop environment, or an office suite, etc.","title":"Package Types"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#packaing-tool-levels","text":"Low Level Utilities This simply installs or removes a single package, or a list of packages, each one of which is individually and specifically named. Dependencies are not fully handled, only warned about or produce an error: If another package needs to be installed, first installation will fail. If the package is needed by another package, removal will fail. The rpm and dpkg utilities play this role for the packaging systems that use them. High Level Utilities If another package or group of packages needs to be installed before software can be installed, such needs will be satisfied. If removing a package interferes with another installed package, the administrator will be given the choice of either aborting, or removing all affected software. The dnf and zypper utilities (and the older yum ) take care of the dependency resolution for rpm systems, and apt-get and apt-cache and other utilities take care of it for dpkg systems.","title":"Packaing Tool Levels"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#package-sources","text":"Every distribution has one or more package repositories where system utilities go to obtain software and to update with new versions. It is the job of the distribution to make sure all packages in the repositories play well with each other. There are always other external repositories which can be added to the standard distribution-supported list. i.e. EPEL (Extra Packages for Enterprise Linux) fit well with RHEL since their source is Fedora and the maintainers are close to Red Hat. Building your own package allows you to control exactly what goes in the software and exactly how it is installed. Creating needed symbolic links Creating directories as needed Setting permissions Anything that can be scripted.","title":"Package Sources"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#source-control-management-system","text":"","title":"Source control Management System"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#git","text":"Git has two important data structures: an object database and a directory cache . The object database contains objects of three varieties: Blobs: Chunks of binary data containing file contents Trees: Sets of blobs including file names and attributes, giving the directory structure Commits: Changesets describing tree snapshots. The directory cache captures the state of the directory tree.","title":"Git"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#rpm","text":"RPM stood for Redhat Package Manager. All files related to a specific task or a subsystem are packaged into a single file, which also contains information about how and where to install and uninstall the files. RPM allows builders to keep the changes necessary for building on Linux separate from the original source. This capability facilitates incorporating new versions of the code, because build-related changes are all in one place. It also facilitates building versions of Linux for different architectures. rpm does not retrieve packages over the network unless given a specific URL to draw from. It installs from the local machine using absolute or relative paths. The standard naming convention for a binary RPM package is: <name>-<version>-<release>.<distro>.<architecture>.rpm sed-4.5-2.e18.x86_64.rpm /var/lib/rpm is the default system directory which holds RPM database files in the form of Berkeley DB hash files. The database files should not be manually modified; updates should be done only through the use of the rpm program. One can use --dbpath to specify another location for database, and --rebuilddb to rebuild the database indices from the installed package headers. Helper programs and scripts used by RPM reside in /usr/lib/rpm . You can create an rpmrc file to specify default settings for rpm. By default, rpm looks for it in: /usr/lib/rpm/rpmrc /etc/rpmrc ~/.rpmrc specified by --rcfile","title":"RPM"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#queries","text":"Run rpm inquiries with the -q option, which can be combined with numerous other query options: -f: allows you to determine which package a file came from -l: lists the contents of a specific package -a: all the packages installed on the system -i: information about the package -p: run the query against a package file instead of the package database --requires: return a list of prerequisites for a package --whatprovides: show what installed package provides a particular requisite package i.e. to list files within a package, do rpm -qilp <package_rpm>","title":"Queries"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#verifying-packages","text":"Run rpm verify inquiries with the -V option to verify whether the files from a particular package are consistent with the system\u2019s RPM database. In the output (you only see output if there is a problem) each of the characters denotes the result of a comparison of attribute(s) of the file to the value of those attribute(s) recorded in the database. A single \u201d.\u201d (period) means the test passed, while a single \u201d?\u201d (question mark) indicates the test could not be performed (e.g. file permissions prevent reading). Otherwise, the character denotes the failure of the corresponding --verify test. S: filesize differs M: mode differs (permissions and file type) 5: MD5 sum differs D: device major/minor number mismatch L: readLink path mismatch U: user ownership differs G: group ownership differs T: mTime differs","title":"Verifying Packages"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#installing-package","text":"The command to install a package sudo rpm -ivh <package_name>... , or sudo rpm -Uvh <package_name>... for upgrading if package is already installed. sudo rpm -U --oldpackage <package_name>... for downgrading. Tasks performed during installation: Performs dependency checks Performs conflict checks Executes commands required before installation Deals intelligently with configuration files if doing an update, config files from original installation will be kept with .rpmsave extension Unpacks files from packages and installs them with correct attributes Executes commands required after installation Updates the system RPM database The -e <package_name> option causes rpm to uninstall (erase) a package. It would fail with an error message if the package you are attempting to uninstall is required by other packages on the system. A successful uninstall produces no output. --test option can be used for a dry-run. -vv is for more verbose information. Command sudo rpm -Fvh <package_name>... will attempt to freshen the packages, only when the older versions of the packages were installed, then upgrade will happen. Good for applying lots of patches at once. When upgrading Linux kernel, it is recommended to use -i instead of -U which will remove the older kernel and it is irreversible.","title":"Installing package"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#dpkg","text":"DPKG (Debian Package) is the packaging system used to install, remove, and manage software packages under Debian Linux and other distributions derived from it. Package files have a .deb suffix and the DPKG database resides in the /var/lib/dpkg directory. The standard naming convention for a binary package is: <name>_<version>-<revision_number>_<architecture>.deb logrotate_3.14.0-4ubuntu3_amd64.deb For historical reasons, the 64-bit x86 platform is called amd64 rather than x86_64 In the Debian packaging system, a source package consists of at least three files: An upstream tarball, ending with .tar.gz. This is the unmodified source as it comes from the package maintainers. A description file, ending with .dsc, containing the package name and other metadata, such as architecture and dependencies. A second tarball that contains any patches to the upstream source, and additional files created for the package, and ends with a name .debian.tar.gz or .diff.gz, depending on distribution.","title":"DPKG"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#queries_1","text":"-l: List all packages installed -L: List files installed in the wget package -s: Show information about an installed package -I: Show information about a package file -c: List files in a package file -S: Show what package owns /etc/init/networking.conf -V: Verify the installed package's integrity","title":"Queries"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#installing-package_1","text":"The command to install or upgrade a package sudo dpkg -i package.deb . To remove all of an installed package except for its configurtion files, use sudo dpkg -r package . To complete remove a package include the configuration files, use sudo dpkg -P package .","title":"Installing package"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#dnf-and-yum","text":"The higher-level package management systems (such as dnf, yum, apt and zypper ) work with databases of available software and incorporate the tools needed to find, install, update, and uninstall software in a highly intelligent fashion. Use both local and remote repositories as a source to install and update binary, as well as source software packages. Automate the install, upgrade, and removal of software packages. Resolve dependencies automatically. Save time in search and download packages comparing to doing so manually. Configuration files are located in /etc/yum.repos.d directory and have a .repo extension. You can toggle use of a particular repo on or off by changing the value of enabled, or using the --disablerepo somerepo and --enablerepo somerepo options. [repo-name] name=Description of the repository baseurl=ht\u200ctp://somesystem.com/path/to/repo enabled=1 gpgcheck=1 dnf replaced yum during the RHEL/CentOS 7 to 8 transition. dnf is backwards compatible - almost all common yum commands still work.","title":"DNF and YUM"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#queries_2","text":"dnf info package-name: Displays information about a package dnf list [installed | updates | available ]: Lists packages installed, available, or updates dnf search [keyword]: Find package by name or information in its metadata dnf grouplist: Shows information about package groups installed, available and updates dnf groupinfo packagegroup: Shows information about a package group dnf provides /path/to/file: Shows the owner of the package for file","title":"Queries"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#installing-package_2","text":"sudo dnf install package: Installs a package from a repository; also resolves and installs dependencies sudo dnf localinstall package-file: Installs a package from a local rpm file sudo dnf groupinstall 'group-name': Installs a specific software group from a repository; also resolves and installs dependencies for each package in the group sudo dnf remove package: Removes a package from the system sudo dnf update [package]: Updates a package from a repository (if no package listed, updates all packages) During installation (or update), if a package has a configuration file which is updated, it will rename the old configuration file with a .rpmsave extension. If the old configuration file will still work with the new software, it will name the new configuration file with a .rpmnew extension. some other useful actions to perform: - sudo dnf list \"dnf-plugin*\": Lists additional dnf plugins - sudo dnf repolist: Shows a list of enabled repositories - sudo dnf shell: Provides an interactive shell in which to run multiple dnf commands (the second form executes the commands in file.txt) - sudo dnf install --downloadonly package: Downloads the packages for you (it stores them in the /var/cache/dnf directory) - sudo dnf history: Views the history of dnf commands on the system, and with the correction options, even undoes or redoes previous commands - sudo dnf clean [packages|metadata|expire-cache|rpmdb|plugins|all]: Cleans up locally stored files and metadata under /var/cache/dnf. This saves space and does house cleaning of obsolete data Similar to dnf , zypper is the command line tool for installing and managing .rpm packages in SUSE Linux and openSUSE.","title":"Installing package"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#apt","text":"For use on Debian-based systems, the APT (Advanced Packaging Tool) set of programs provides a higher level of intelligent services for using the underlying dpkg program, and plays the same role as dnf on Red Hat-based systems. The main utilities are apt-get and apt-cache . It can automatically resolve dependencies when installing, updating and removing packages. It accesses external software repositories, synchronizing with them and retrieving and installing software as needed. The APT system works with Debian packages whose files have a .deb extension. Read more about it from Debian packages webpage and the Ubuntu packages webpage","title":"APT"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#queries_3","text":"Queries are done using the apt-cache or apt-file utilities. You may have to install apt-file first, and update its database, as in: $ sudo apt-get install apt-file $ sudo apt-file update apt-cache search apache2: Searches the repository for a package named apache2 apt-cache show apache2: Displays basic information about the apache2 package apt-cache showpkg apache2: Displays detailed information about the apache2 package apt-cache depends apache2: Lists all dependent packages for apache2 apt-file search apache2.conf: Searches the repository for a file named apache2.conf apt-file list apache2: Lists all files in the apache2 package","title":"Queries"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#installing-package_3","text":"Used to install new packages or update a package which is already installed: sudo apt-get install [package]: Used to install new packages or update a package which is already installed sudo apt-get remove [package]: Used to remove a package from the system (this does not remove the configuration files) sudo apt-get --purge remove [package]: Used to remove a package and its configuration files from a system sudo apt-get update: Used to synchronize the package index files with their sources. The indexes of available packages are fetched from the location(s) specified in /etc/apt/sources.list sudo apt-get upgrade: Apply all available updates to packages already installed sudo apt-get autoremove: Gets rid of any packages not needed anymore, such as older Linux kernel versions sudo apt-get clean: Cleans out cache files and any archived package files that have been installed","title":"Installing package"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#linux-system-monitoring","text":"Linux distributions come with many standard performance and profiling tools which make use of mounted pseudo-filesystems, especially /proc and secondarily /sys . Some frequently used utilities: Process and Load Monitoring Utility Purpose Package top Process activity, dynamically updated procps uptime How long the system is running and the average load procps ps Detailed information about processes procps pstree A tree of processes and their connections psmisc (or pstree) mpstat Multiple processor usage sysstat iostat CPU utilization and I/O statistics sysstat sar Display and collect information about system activity sysstat numastat Information about NUMA (Non-Uniform Memory Architecture) numactl strace Information about all system calls a process makes strace Memory Monitoring Utility Purpose Package free Brief summary of memory usage procps vmstat Detailed virtual memory statistics and block I/O, dynamically updated procps pmap Process memory map procps I/O Monitoring Utility Purpose Package iostat CPU utilization and I/O statistics sysstat sar Display and collect information about system activity sysstat vmstat Detailed virtual memory statistics and block I/O, dynamically updated procps Network Monitoring Utility Purpose Package netstat detailed networking statistics netstat iptraf Gather information on network interfaces iptraf tcpdump Detailed analysis of network packets and traffic tcpdump wireshark Detailed network traffic analysis wireshark","title":"Linux System Monitoring"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#sar","text":"sar stands for the Systems Activity Reporter. It is an all-purpose tool for gathering system activity and performance data and creating reports that are readable by humans. sar backend is sadc (system activity data collector), which is usually cron-based /etc/cron.d/sysstat and accumulates the statistics (logs in /var/log/sa ). Option Meaning -A Almost all information -b I/O and transfer rate statistics (similar to iostat) -B Paging statistics including page faults -d Block device activity (similar to iostat -x) -n Network statistics -P Per CPU statistics (as in sar -P ALL 3) -q Queue lengths (run queue, processes and threads) -r Memory utilization statistics -S Swap utilization statistics -u CPU utilization (default) -v Statistics about inodes and files and file handles -w Context switching statistics -W Swapping statistics, pages in and out per second -f Extract information from specified file, created by the -o option -o Save readings in the file specified, to be read in later with the -f option","title":"sar"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#log-files","text":"System log files are under /var/log , controlled by the syslogd (usually rsyslogd on modern systems) daemon. Important system messages are located at /var/log/messages for RHEL and /var/log/syslog for Debian. boot.log holds system boot messages, and secure holds security-related messages. You can view new messages continuously as new lines appear with sudo tail -f /var/log/messages or view kernel-related messages with dmesg -w . logrotate program is run periodically and keeps four previous copies (by default) of the log files (optionally compressed) and is controlled by /etc/logrotate.conf .","title":"Log files"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#stress-and-stress-ng","text":"stress is a C language program written by Amos Waterland and is designed to place a configurable amount of stress by generating various kinds of workloads on the system. Install stress-ng (enhanced version of stress): $ git clone git://kernel.ubuntu.com/cking/stress-ng.git $ cd stress-ng $ make $ sudo make install # fork 8 CPU-intensive processes via sqrt() calculation # fork 4 IO-intensive processes via sync() # fork 6 mem-intensive processes via malloc() and each allocate 256MB by default (override with --vm-bytes 128M) # run stress test for 20 seconds $ stress-ng -c 8 -i 4 -m 6 -t 20s","title":"stress and stress-ng"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#process-monitoring","text":"","title":"Process Monitoring"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ps","text":"ps displays characteristics and statistics associated with processes, all of which are garnered from the /proc directory associated with the process. ps has existed in all UNIX-like operating system variants and the options rule is a little different: UNIX options, which must be preceded by -, and which may be grouped. BSD options, which must not be preceded by -, and which may be grouped. GNU long options, each of which must be preceded by --. Some common choices of options are: $ ps aux # show all processes $ ps -elf # show process and parent process id, nice value $ ps -eL # show shorter summary about pid and commands $ ps -C \"bash\" # show process using that command $ ps -o pid,user,uid,priority,cputime,pmem,size,command # costomize the outputs","title":"ps"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#pstree","text":"pstree gives a visual description of the process ancestry and multi-threaded applications. i.e. pstree -aAp 2408 checks process tree of process with PID 2408, which shows the child processes spawned by that process. Another way to see that is doing ls -l /proc/2408/task Use -p to show process IDs, use -H [pid] to highlight [pid] and its ancestors.","title":"pstree"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#top","text":"top is used to display processes with highest CPU usage. Processes are initially sorted by CPU usage. If not run in secure mode (top s) user can signal processes: Press the k key Give a PID when prompted Give a signal number when prompted","title":"top"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#memory-montioring","text":"One can use free -m to see a brief summary of memory usage: total, used, free, shared, buff/cached, available. The pseudofile /proc/meminfo contains a wealth of information about how memory is being used. The /proc/sys/vm directory contains many tunable knobs to control the Virtual Memory system, which can be changed either by directly writing to the entry, or using the sysctl utility. The primary (inter-related) tasks are: Controlling flushing parameters; i.e., how many pages are allowed to be dirty and how often they are flushed out to disk. Controlling swap behavior; i.e., how much pages that reflect file contents are allowed to remain in memory, as opposed to those that need to be swapped out as they have no other backing store. Controlling how much memory overcommission is allowed, since many programs never need the full amount of memory they request, particularly because of copy on write (COW) techniques. The usual best practice is to adjust one thing at a time and look for effects.","title":"Memory Montioring"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#vmstat","text":"vmstat is a multi-purpose tool that displays information about memory, paging, I/O, processor activity and processes. i.e. vmstat [options] [delay] [count] If delay is given in seconds, the report is repeated at that interval count times; if count is not given, vmstat will keep reporting statistics forever until killed by a signal. If the option -S m is given, memory statistics will be in MB instead of KB. With the -a option, vmstat displays information about active and inactive memory, where active memory pages are those which have been recently used; they may be clean (disk contents are up to date) or dirty (need to be flushed to disk eventually). With the -d option to get a table of disk statistics. Use -p partition to view statistics for a particular partition only.","title":"vmstat"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#oom","text":"One way to deal with memory pressure would be to permit memory allocations to succeed as long as free memory is available and then fail when all memory is exhausted. Alternatively, use swap space on disk as \"secondary memory\" to push some of the resident memory out when under memory pressure. Linux also permits the system to overcommit memory (only for user processes, not kernel processes), so that it can grant memory requests that exceed the size of RAM plus swap. Every time a child process is forked, it receives a copy of the entire memory space of the parent. Linux uses the COW (copy on write) technique, unless one of the child processes modifies its memory space, no actual copy needs be made. However, the kernel has to assume that the copy might need to be done. You can modify overcommission by setting the value of /proc/sys/vm/overcommit_memory: 0: (default) Permit overcommission, but refuse obvious overcommits, and give root users somewhat more memory allocation than normal users. 1: All memory requests are allowed to overcommit. 2: Turn off overcommission. Memory requests will fail when the total memory commit reaches the size of the swap space plus a configurable percentage (50 by default) of RAM. This factor is modified changing /proc/sys/vm/overcommit_ratio. If available memory is exhausted, Linux invokes the OOM-killer (Out Of Memory) to decide which process(es) should be exterminated to open up some memory. A value called the badness is computed (which can be read from /proc/[pid]/oom_score ) for each process on the system and the order of the killing. oom_score_adj can be directly adjusted to override the badness score of a process.","title":"OOM"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#io-monitoring","text":"Disk performance problems can be strongly coupled to other factors, such as insufficient memory or inadequate network hardware and tuning. Both real-time monitoring and tracing are necessary tools for locating and mitigating disk bottlenecks. A system can be considered as I/O-bound when the CPU is found sitting idle waiting for I/O to complete, or the network is waiting to clear buffers. What appears to be insufficient memory can result from too slow I/O; if memory buffers that are being used for reading and writing fill up, it may appear that memory is the problem, when the real problem is that buffers are not filling up or emptying out fast enough. Network transfers may also be waiting for I/O to complete and cause network throughput to suffer.","title":"IO Monitoring"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#iostat","text":"iostat is the basic workhorse utility for monitoring I/O device activity on the system. i.e. iostat [OPTIONS] [devices] [interval] [count] IO statistics given (broken out by disk partition and logical partitions if LVM is used): tps (I/O transactions per second; logical requests can be merged into one actual request) blocks read and written per unit time, where the blocks are generally sectors of 512 bytes total blocks read and written With -k shows results in KB instead of blocks; with -m shows in MB. With -N or -d shows the device name. With -x shows some extended statistics.","title":"iostat"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#iotop","text":"iotop displays a table of current I/O usage and updates periodically. The be and rt entries in the PRIO field stand for best effort and real time.","title":"iotop"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ionice","text":"ionice utility lets you set both the I/O scheduling class and priority for a given process. i.e. ionice [-c class] [-n priority] [-p pid ] [COMMAND [ARGS] ] If a pid is given with the -p argument results apply to the requested process, otherwise it is the process that will be started by COMMAND with possible arguments. The -c parameter specifies the I/O scheduling class: 0 - default 1 - real time, get first access to the disk, can starve other processes; the priority defines how big a time slice each process gets 2 - best effort, programs serviced in round-robin fashion, according to priority settings (default) 3 - idle - no access to disk I/O unless no other program has asked for it for a defined period The Best Effort and Real Time classes take the -n argument which gives the priority, which can range from 0 to 7, with 0 being the highest priority.","title":"ionice"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#benchmarking","text":"bonnie++ is a widely available benchmarking program that tests and measures the performance of drives and filesystems. Results can be read from the terminal window or directed to a file, and also to a csv format. Companion programs, bon_csv2html and bon_csv2txt , can be used convert to html and plain text output formats. fs_mark benchmark gives a low level bashing to file systems, using heavily asynchronous I/O across multiple directories and drives.","title":"Benchmarking"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#io-scheduling","text":"The I/O scheduler provides the interface between the generic block layer and low-level physical device drivers. The I/O scheduling layer prioritize and order the requests from VM and VFS before they are given to the block devices. Any I/O scheduling algorithm has to satisfy certain requirements: Hardware access times should be minimized; i.e., requests should be ordered according to physical location on the disk. This leads to an elevator scheme where requests are inserted in the pending queue in physical order. Requests should be merged to the extent possible to get as big a contiguous region as possible, which also minimizes disk access time. Requests should be satisfied with as low a latency as is feasible; indeed, in some cases, determinism (in the sense of deadlines) may be important. Write operations can usually wait to migrate from caches to disk without stalling processes. Read operations, however, almost always require a process to wait for completion before proceeding further. Favoring reads over writes leads to better parallelism and system responsiveness. Processes should share the I/O bandwidth in a fair, or at least consciously prioritized fashion; even if it means some overall performance slowdown of the I/O layer, process throughput should not suffer inordinately. At least one of the I/O scheduling algorithms must be compiled into the kernel. The scheduler for each device can be selected or viewed at run time. i.e. /sys/block/sda/queue/scheduler and scheduler-specific tunables can be found in /sys/block/sda/queue/iosched .","title":"I/O Scheduling"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#filesystems-and-vfs","text":"A UNIX-like filesystem uses a tree hierarchy. Multiple filesystems can be merged together into a single tree structure. Linux uses a virtual filesystem layer (VFS) to communicate with the filesystem software. Local filesystems generally reside within a disk partition which can be a physical partition on a disk, or a logical partition controlled by a Logical Volume Manager (LVM). Filesystems can also be of a network nature and their true physical embodiment completely hidden to the local system across the network.","title":"Filesystems and VFS"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#inodes","text":"An inode is a data structure on disk that describes and stores file attributes such as name, location, file attributes (permissions, ownership, size, etc.), access & modify times and others. Filenames are not stored in the inode; they are stored in the directory that contains the files.","title":"inodes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#hardsoft-links","text":"A directory file is a particular type of file that is used to associate file names and inodes. Two ways to associate (or link) a file name with an inode: Hard links point to an inode.\u200b All hard linked files have to be on the same filesystem. Changing the content of a hard linked file in one place may change it in other places. Soft (or symbolic) links point to a file name which has an associated inode. Soft linked files may be on different filesystems. If the target does not yet exist or is not yet mounting, it can be dangling. A nice article explaning the differences between hard and soft links: https://linuxgazette.net/105/pitcher.html When two or more directory entries to point to the same inode (hard links), a file can be known by multiple names, each of which has its own place in the directory structure. When a process refers to a pathname, the kernel searches directories to find the corresponding inode number. After the name has been converted to an inode number, the inode is loaded into memory and is used by subsequent requests.","title":"Hard/soft links"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#vfs","text":"When an application needs to access a file, it interacts with the VFS abstraction layer, which then translates all the I/O system calls (reading, writing, etc.) into specific code relevant to the particular actual filesystem. Neither the specific actual filesystem or physical media and hardware on which it resides need be considered by applications. Network filesystems (such as NFS) can also be handled transparently. This permits Linux to work with more filesystem varieties than any other operating system. Commonly used filesystems include ext4, xfs, btrfs, squashfs, nfs and vfat . A list of currently supported filesystems is at /proc/filesystems . The ones with nodev are special filesystems which do not reside on storage.","title":"VFS"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#journaling-filesystems","text":"Journaling filesystems recover from system crashes or ungraceful shutdowns with little or no corruption, and do so very rapidly. In a journaling filesystem, operations are grouped into transactions . A transaction must be completed without error, atomically; otherwise, the filesystem is not changed. A log file is maintained of transactions. When an error occurs, usually only the last transaction needs to be examined.","title":"Journaling Filesystems"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#disk-partitioning","text":"Reasons for doing disk partitions: Separation of user and application data from operating system files Sharing between operating systems and/or machines Security enhancement by imposing different quotas and permissions for different system parts Size concerns; keeping variable and volatile storage isolated from stable Performance enhancement of putting most frequently used data on faster storage media Swap space can be isolated from data and also used for hibernation storage. A common partition layout contains a /boot partition, a partition for the root filesystem / , a swap partition, and a partition for the /home directory tree. It is more difficult to resize a partition after installing the OS.","title":"Disk Partitioning"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#common-disk-types","text":"SATA (Serial Advanced Technology Attachment) - SATA disks were designed to replace the old IDE (Integrated Drive Electronics) drives. They offer a smaller cable size (7 pins), native hot swapping, and faster and more efficient data transfer. They are seen as SCSI devices. SCSI (Small Computer Systems Interface) - SCSI disks range from narrow (8 bit bus) to wide (16 bit bus), with a transfer rate from about 5 MB per second (narrow, standard SCSI) to about 160 MB per second (Ultra-Wide SCSI-3). SCSI has numerous versions such as Fast, Wide, and Ultra, Ultrawide. SAS (Serial Attached SCSI) - SAS uses a newer point-to-point protocol, has a better performance than SATA disks and is better suited for servers. Learn more SAS vs SATA USB (Universal Serial Bus) - These include flash drives and floppies. And are seen as SCSI devices. SSD (Solid State Drives) - Modern SSD drives have come down in price, have no moving parts, use less power than drives with rotational media, and have faster transfer speeds. Internal SSDs are even installed with the same form factor and in the same enclosures as conventional drives. SSDs still cost a bit more, but price is decreasing. It is common to have both SSDs and rotational drives in the same machines, with frequently accessed and performance critical data transfers taking place on the SSDs.","title":"common disk types"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#disk-drives","text":"Rotational disks are composed of one or more platters and each platter is read by one or more heads . Heads read a circular track off a platter as the disk spins. Circular tracks are divided into data blocks called sectors. A cylinder is a group which consists of the same track on all platters. The physical structural image has become less and less relevant as internal electronics on the drive actually obscure much of it. Use sudo fdisk -l /dev/sdc to list the partition table without entering interactive mode. fdisk is a menu-driven partition table editor and included in any Linux installation. It is the most standard and one of the most flexible of the partition table editors. As with any other partition table editor, make sure that you either write down the current partition table settings or make a copy of the current settings before making changes. No actual changes are made until you write the partition table to the disk by entering w. It is therefore important to verify your partition table is correct (with p) before writing to disk with w. If something is wrong, you can jump out safely with q. After the edit is made, either reboot or use sudo partprobe -s to make the change taking in effect. parted is the GNU tool that create, remove, resize, and move partitions (including certain filesystems). cat /proc/partitions to examine what partitions the operating system is currently aware of.","title":"Disk drives"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#partition-organization","text":"Disks are divided into partitions. A partition is a physically contiguous region on the disk. Two partition schemes: MBR (Master Boot Record) GPT (GUID Partition Table) MBR dates back to the early days of MSDOS. When using MBR, a disk may have up to four primary partitions . One (and only one) of the primary partitions can be designated as an extended partition , which can be subdivided further into logical partitions with 15 total partitions possible. GPT is on all modern systems and is based on UEFI (Unified Extensible Firmware Interface). By default, it may have up to 128 primary partitions. When using the GPT scheme, there is no need for extended partitions. Partitions can be up to 233 TB in size (with MBR, the limit is just 2TB ).","title":"Partition Organization"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#mbr-parition-table","text":"The disk partition table is contained within the disk's Master Boot Record (MBR), and is the 64 bytes following the 446 byte boot record. One partition on a disk may be marked active. When the system boots, that partition is where the MBR looks for items to load. The structure of the MBR is defined by an operating system-independent convention. The first 446 bytes are reserved for the program code. They typically hold part of a boot loader program . There are 2 more bytes at the end of the MBR known as the magic number, signature word, or end of sector marker , which always have the value 0x55AA . Each entry in the partition table is 16 bytes long and contains information: Active bit Beginning address in cylinder/head/sectors (CHS) format Partition type code, indicating: xfs, LVM, ntfs, ext4, swap, etc. Ending address in CHS Start sector, counting linearly from 0 Number of sectors in partition. Linux only uses the last two fields for addressing, using the linear block addressing (LBA) method. For MBR systems, dd can be used for converting and copying files. However, be careful using dd : a simple typing error or misused option could destroy your entire disk. The following command will backup the MBR (along with the partition table): $ dd if=/dev/sda of=mbrbackup bs=512 count=1 The MBR can be restored using the following command: $ sudo dd if=mbrbackup of=/dev/sda bs=512 count=1","title":"MBR Parition Table"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#gpt-partition-table","text":"Modern hardware comes with GPT support; MBR support will gradually fade away. The Protective MBR is for backwards compatibility, so UEFI systems can be booted the old way. There are two copies of the GPT header, at the beginning and at the end of the disk, describing metadata: List of usable blocks on disk Number of partitions Size of partition entries. Each partition entry has a minimum size of 128 bytes . The blkid utility shows information about partitions. i.e. sudo blkid /dev/sda8 blkid is a utility to locate block devices and report on their attributes, such as type of contents, tokens, LABEL or UUID lsblk presents block device information in a tree format","title":"GPT Partition table"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#devices-nodes","text":"The Linux kernel interacts at a low level with disks through device nodes normally found in the /dev directory. Device nodes for SCSI and SATA disks follow a simple xxy[z] naming convention, where xx is the device type (usually sd), y is the letter for the drive number (a, b, c, etc.), and z is the partition number. sd means SCSI or SATA disk. Back in the days where IDE disks could be found, they would have been /dev/hda3, /dev/hdb","title":"Devices Nodes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#filesystem-features","text":"","title":"Filesystem Features"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#file-attributes","text":"Extended Attributes associate metadata not interpreted directly by the filesystem with files: user, trusted, security, and system The system namespace is used for Access Control Lists (ACLs), and the security namespace is used by SELinux. Flag values are stored in the file inode and may be modified and set only by the root user. They are viewed with lsattr filename and set with chattr [+|-|=mode] filename . Flags that can be set: i: immutable - A file with the immutable attribute cannot be modified (not even by root). It cannot be deleted or renamed. No hard link can be created to it, and no data can be written to the file. Only the superuser can set or clear this attribute. a: append-only - A file with the append-only attribute set can only be opened in append mode for writing. Only the superuser can set or clear this attribute. d: no-dump - A file with the no-dump attribute set is ignored when the dump program is run. This is useful for swap and cache files that you don't want to waste time backing up. A: no atime update - A file with the no-atime-update attribute set will not modify its atime (access time) record when the file is accessed but not modified. This can increase the performance on some systems because it reduces the amount of disk I/O; say some files that are being accessed very frequently.","title":"File Attributes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#mkfs","text":"mkfs is a utility for formatting (making) a filesystem on a partition. See the fs programs with ls -lhF /sbin/mkfs* . General usage: mkfs [-t fstype] [options] [device-file] Each filesystem type has its own particular formatting options and its own mkfs program, and can be learned from its man page.","title":"mkfs"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#fsck","text":"fsck a utility designed to check for errors and may fix if found any. Like mkfs , it also have many variations for different filesystem types, view with ls -lhF /sbin/fsck* . General usage: fsck [-t fstype] [options] [device-file] Usually, you do not need to specify the filesystem type, as fsck can figure it out by examining the superblocks at the start of the partition. You can control whether any errors found should be fixed one by one manually with the -r option, or automatically, as best possible, by using the -a option. fsck is run automatically after a set number of mounts or a set interval since the last time it was run or after an abnormal shutdown. It should only be run on unmounted filesystems. You can force a check of all mounted filesystems at boot by doing: sudo touch /forcefsck then reboot.","title":"fsck"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#etcfstab","text":"During system initialization, the command mount -a is executed in order to mount all filesystems listed in /etc/fstab . /etc/fstab is used to define mountable file systems and devices on startup. This may include both local and remote network-mounted filesystems, such as NFS and samba filesystems. Each record in the /etc/fstab file contains white space separated files of information about a filesystem to be mounted: Device file name (such as /dev/sda1 ), label, or UUID Mount point for the filesystem (where in the tree structure is it to be inserted) Filesystem type A comma-separated list of options dump frequency used by the dump -w command, or a zero which is ignored by dump fsck pass number or a zero - meaning do not fsck this partition Linux systems have long had the ability to mount a filesystem only when it is needed, which is done through autofs . While autofs is very flexible and well understood, systemd -based systems (including all enterprise Linux distributions) come with automount facilities built into the systemd framework.","title":"/etc/fstab"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#check-filesystem-usage","text":"df (disk free) is used to look at filesystem usage. The option -h shows human-readable format, -T shows filesystem type, and -i shoes inode information. du (disk usage) is used to look at both disk capacity and usage. The option -a lists all files in addition to directories. find . -maxdepth 1 -type d -exec du -shx {} \\; | sort -hr shows directory size total and sort in decending order.","title":"check filesystem usage"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#swap","text":"Linux employs a virtual memory system which overcommits functions in two ways such that: programs do not actually use all the memory they are given permission to use when under memory pressure, less active memory regions may be swapped out to disk, to be recalled only when needed again Swapping is usually done to one or more dedicated partitions or files; Linux permits multiple swap areas. Each area has a priority, and lower priority areas are not used until higher priority areas are filled. In most situations, the recommended swap size is the total RAM on the system. Current swap in use can be found under /proc/swaps . Its usage can be viewed with free -m Commands involved with swap are mkswap : format a swap paritition or file swapon : activate a swap partition or file swapoff : deactivate a swap partition or file At any given time, most memory is in use for caching file contents to prevent actually going to the disk any more than necessary and are never swapped out since it is pointless; instead, dirty pages are flushed out to disk. Linux memory used by the kernel itself is never swapped out.","title":"swap"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#quotas","text":"Disk quotas allow administrators to control the maximum space particular users (or groups) are allowed. Commands involved with quota: quotacheck : generates and updates quota accounting files use option -u for user files and -g for group files pass in the filesystem to update, otherwise use -a to apply/update all filesystems in /etc/fstab generally only run after quota is initially turned on, or fsck reports errors in a filesystem quotaon : enables quota accounting quotaoff : disables quota accounting edquota : edit user or group quotas use option -u for edit user quota and -g for group use option -p to copy quota value from a user/group to another use option -t to set grace periods quota reports usage and limits Quotas may be enabled or disabled on a per-filesystem basis. Quota operations require the existence of the files aquota.user and aquota.group in the root directory of the filesystem using quotas. Linux supports the use of quotas based on user and group IDs. Quotas for users and groups may be set for disk blocks and/or inodes. In addition, soft and hard limits may be set, as well as grace periods: Soft limits may be exceeded for a grace period. Hard limits may never be exceeded. Steps to set up quota: first make sure you have mounted the filesystem with the user and/or group quota mount options usrquota grpquota in /etc/fstab . Then run quotacheck on the filesystem to set it up. Then enable quota and use edquota to set limits. For example: # add in /etc/fstab (assume /home is on a dedicated partition) /dev/sda5 /home ext4 defaults,usrquota 1 2 # test with the following commands: $ sudo mount -o remount /home $ sudo quotacheck -vu /home $ sudo quotaon -vu /home $ sudo edquota someusername","title":"Quotas"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#filesystem-types","text":"","title":"filesystem types"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ext4","text":"The ext4 filesystem can support volumes up to 1 EB and file sizes up to 16 TB . Until very recently, ext4 was the most frequent default choice of Linux distributions, due to its excellent combination of performance, integrity, and stability. An extent is a group of contiguous blocks. Use of extents can improve large file performance and reduce fragmentation . Extents replace the older block mapping mechanism from ext3. ext4 is backwards compatible with ext3 and ext2. It can pre-allocate disk space for a file. The allocated space is usually guaranteed and contiguous . It also uses a performance technique called allocate-on-flush (delays block allocation until it writes data to disk). ext4 breaks the 32,000 subdirectory limit of ext3. ext4 uses checksums for the journal which improves reliability. This can also safely avoid a disk I/O wait during journalling. ext4 provides timestamps measured in nanoseconds. The superblock is stored in block 0 of the disk, and contains global information about the ext4 filesystem: Mount count and maximum mount count (every time the disk is successfully mounted, mount count is incremented; the filesystem is checked every maximum-mount-counts or every 180 days whichever comes first) Block size (block size can be set through the mkfs command)\u200b Blocks per group Free block count Free inode count Operating System ID All fields in ext4 are written to disk in little-endian order, except the journal. An ext4 filesystem is split into a set of block groups . The block allocator tries to keep each file\u2019s blocks within the same block group to reduce seek times. The default block size is 4 KB, which would create a block group of 128 MB. For block group 0 , the first 1024 bytes are unused (to allow for boot sectors, etc), and the superblock will start at the first block after block group 0, then followed by the group descriptors and a number of GDT (Group Descriptor Table) blocks. These are followed by the data block bitmap, the inode bitmap, the inode table, and the data blocks. In blocks view, they are like Super Block | Group Descriptors | Data Block Bitmap | Inode Bitmap | Inode Table (n blocks) | Data Blocks (n blocks) The first and second blocks are the same in every block group , and comprise the Superblock and the Group Descriptors. Under normal circumstances, only those in the first block group are used by the kernel; the duplicate copies are only referenced when the filesystem is being checked. If everything is OK, the kernel merely copies them over from the first block group. If there is a problem with the master copies, it goes to the next and so on until a healthy one is found and the filesystem structure is rebuilt. This redundancy makes it very difficult to thoroughly fry an ext2/3/4 filesystem. As an optimization, today not all block groups have a copy of the superblock and group descriptors. To view which block holds backups use dumpe2fs . Use the dumpe2fs program to get information about a particular partition, such as limits, capabilities, flags, and other attributes. tune2fs can be used to change filesystem parameters. Use -l to list the contents of the superblock (same as global information from dumpe2fs ).","title":"ext4"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#xfs","text":"The xfs filesystem was engineered to deal with large data sets for SGI systems, as well as handle parallel I/O tasks very effectively. xfs can handle up to 16 EB (exabytes) for the total filesystem and up to 8 EB for an individual file and implements methods for high performance: Allowing DMA (Direct Memory Access) I/O Guaranteeing an I/O rate Adjusting stripe size to match underlying RAID or LVM storage devices. xfs also journals quota information for fast recovery on uncleanly unmounted filesystem. xfs maintenance can be done online, for defragmenting, resizing (enlarge), and dumping/restoring Backup and restore can be done with xfsdump and xfsrestore utilites. xfs also supports per-directory quotas, use xfs_quota command. Check out more xfs-related utilities with man -k xfs","title":"xfs"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#btrfs","text":"btrfs stands for B-tree filesystem, which is intended to address the lack of pooling, snapshots, checksums, and integral multi-device spanning in other Linux filesystems such as ext4. One of the main features is the ability to take frequent snapshots of entire filesystems, or sub-volumes of entire filesystems in virtually no time. Because btrfs makes extensive use of COW techniques (Copy on Write), such a snapshot does not involve any more initial space for data blocks or any I/O activity except for some metadata updating. One can easily revert to the state described by earlier snapshots and even induce the kernel to reboot off an earlier root filesystem snapshot. btrfs maintains its own internal framework for adding or removing new partitions and/or physical media to existing filesystems, much as LVM (Logical Volume Management) does.","title":"btrfs"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#disk-encryption","text":"Filesystems may be encrypted to protect them from both prying eyes and attempts to corrupt the data they contain. Encryption can be chosen at installation or incorporated later. It is straightforward to create and format encrypted partitions at a later time, but you cannot encrypt an already existing partition in place without a data copying operation. Modern Linux distributions provide block device level encryption mainly through the use of LUKS (Linux Unified Key Setup). LUKS is built on top of cryptsetup to encrypt filesystems. The general form of a command is: cryptsetup [OPTION...] <action> <action-specific> Examine /proc/crypto to see the encryption methods your system supports, and choose one to encrypt a filesystem. Example: # encrypt a partition using LUKS # (a passphrase will be prompted to enter) sudo cryptsetup luksFormat --cipher aes /dev/sdc12 # make the volume available (create unencrypted passthrough device) sudo cryptsetup --verbose luksOpen /dev/sdc12 SECRET # format the partition sudo mkfs.ext4 /dev/mapper/SECRET # mount it sudo mount /dev/mapper/SECRET /mnt # unmount it sudo umount /mnt # remove the volume association sudo cryptsetup --verbose luksClose SECRET To mount an encrypted filesystem at boot time, first ensure it has an entry in /etc/fstab , then additionally add an entry to /etc/crypttab (which will prompt for passphrase on boot time). Read more with man crypttab .","title":"disk encryption"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#lvm","text":"LVM (Logical Volume Management) breaks up one virtual partition into multiple chunks, each of which can be on different partitions and/or disks. Logical volumes are created by putting all the devices into a large pool of disk space (the volume group ), and then allocating space from the pool to create a logical volume . Additional devices can be added to the logical volume at any time. Logical volumes have features similar to RAID devices. They can actually be built on top of a RAID device. This would give the logical volume the redundancy of a RAID device with the scalability of LVM. LVM makes it easy to change the size of the logical partitions and filesystems, to add more storage space, rearrange things, etc. Use of logical volumes is a mechanism for creating filesystems which can span more than one physical disk. LVM does create a definite additional performance cost that comes from the overhead of the LVM layer. However, even on non-RAID systems, if you use striping (splitting of data to more than one disk), you can achieve some parallelization improvements.","title":"LVM"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#volume-and-volume-groups","text":"Partitions are converted to physical volumes and multiple physical volumes are grouped into volume groups; there can be more than one volume group on the system. Space in the volume group is divided into extents; these are 4 MB in size by default. Logical volumes are allocated from volume groups: Can be defined by the size or number of extents Filesystems are built on logical volumes Can be named anything. The hierarchy in layers: Physical Drives -> Parititions -> Physical Volumes -> Volume Groups -> Logical Volumes -> File Systems Commands to manipulate physical volumes: pvcreate : Converts a partition to a physical volume. pvdisplay : Shows the physical volumes being used. pvmove : Moves the data from one physical volume within the volume group to others; this might be required if a disk or partition is being removed for some reason. It would then be followed by: pvremove : Remove a partition from a physical volume. Commands to manipulate volume groups: vgcreate : Creates volume groups. vgextend : Adds to volume groups. vgreduce : Shrinks volume groups. use man lvm to view more utilities related. There are a number of utilities that manipulate logical volumes, and a short list can be viewed with ls -lF /sbin/lv* Commands to manipulate logical volumes: lvcreate allocates logical volumes from within volume groups. The size can be specified either in bytes or number of extents (remember, they are 4 MB by default). Names can be anything desired. lvdisplay reports on available logical volumes. lvresize expands or shrinks a logical volume. Some variations: lvextend , lvreduce , resize2fs","title":"Volume and Volume Groups"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#manipulate-lv","text":"Steps involved in setting up and using a new logical volume, as an example: # Create partitions on disk drives. sudo fdisk /dev/sdb # Create physical volumes from the partitions. sudo pvcreate /dev/sdb1 sudo pvcreate /dev/sdc1 # Create the volume group named 'vg'. sudo vgcreate -s 16M vg /dev/sdb1 sudo vgextend vg /dev/sdc1 # Allocate logical volumes from the volume group. sudo lvcreate -L 50G -n mylvm vg # Format the logical volumes. sudo mkfs -t ext4 /dev/vg/mylvm # Mount the logical volumes (also update /etc/fstab as needed). sudo mkdir /mylvm sudo mount /dev/vg/mylvm /mylvm # Persistent mount on reboot, add to /etc/fstab /dev/vg/mylvm /mylvm ext4 defaults 1 2 Resizing a logical volume is quick and easy compare to doing so with a physical paritition that already contains a filesystem. Extents for a logical volume can be added or subtracted from the logical volume, and they can come from anywhere in the volume group; they need not be from physically contiguous sections of the disk. When expanding a logical volume with a filesystem, you must first expand the volume , and then expand the filesystem .\u200b When shrinking a logical volume with a filesystem, you must first shrink the filesystem , and then shrink the volume . This is done using lvresize . As an example of shrinking: sudo lvresize -r -L 20 GB /dev/VG/mylvm . The -r option causes resizing of the filesystem at the same time as the volume size is changed. The filesystem cannot be mounted when being shrunk. sudo lvresize -r -L +100M /dev/vg/mylvm to grow a logical volume, while the plus sign (+) indicates adding space. Note that you need NOT unmount the filesystem to grow it.","title":"Manipulate LV"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#lvm-snapshots","text":"LVM snapshots create an exact copy of an existing logical volume. They are useful for backups, application testing, and deploying VMs (Virtual Machines). The original state of the snapshot is kept as the block map. Snapshots only use space for storing deltas: When the original logical volume changes, original data blocks are copied to the snapshot. If data is added to snapshot, it is stored only there. Use -s option with lvcreate to create snapshots, i.e. lvcreate -l 128 -s -n mysnap /dev/vg/mylvm","title":"LVM Snapshots"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#raid","text":"RAID (Redundant Array of Independent Disks) spreads I/O over multiple disks. This can really increase performance in modern disk controller interfaces, such as SCSI, which can perform the work in parallel efficiently. RAID can be implemented either in software or in hardware. One disadvantage of using hardware RAID is that if the disk controller fails, it must be replaced by a compatible controller, which may not be easy to obtain. When using software RAID, the same disks can be attached to and work with any disk controller. Three essential features of RAID: mirroring: writing the same data to more than one disk striping: splitting of data to more than one disk parity: extra data is stored to allow problem detection and repair, yielding fault tolerance. RAID devices are typically created by combining partitions from several disks together to create filesystems which are larger than any one drive. Striping provides better performance by spreading the information over multiple devices so simultaneous writes are possible. Mirroring writes the same information to multiple drives, giving better redundancy. mdadm is used to create and manage RAID devices, with the array name, /dev/mdX .","title":"RAID"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#raid-levels","text":"As a general rule, adding more disks improves performance. RAID 0 - only striping available RAID 1 - only mirroring RAID 5 - uses a rotating parity stripe, at least three disks required. Can endure one disk failure without loss of data RAID 6 - striped disks with dual parity, at least four disks required, and can handle two disk failures RAID 10 - mirrored and striped data set, at least four disks required","title":"RAID Levels"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#software-raid","text":"The essential steps in configuring a software RAID device are: Create partitions on each disk (type fd in fdisk) Create RAID device with mdadm Format RAID device Add device to /etc/fstab Mount RAID device Capture RAID details to ensure persistence Examine /proc/mdstat to see the RAID status as an example: $ sudo fdisk /dev/sdb $ sudo fdisk /dev/sdc $ sudo mdadm --create /dev/md0 --level = 1 --raid-disks = 2 /dev/sdbX /dev/sdcX $ sudo mkfs.ext4 /dev/md0 $ sudo bash -c \"mdadm --detail --scan >> /etc/mdadm.conf\" $ sudo mkdir /myraid $ sudo mount /dev/md0 /myraid $ sudo cat >> /etc/fstab <<EOF /dev/md0 /myraid ext4 defaults 0 2 EOF Use mdadm -S /dev/md0 to stop the RAID device. Use mdadm --detail /dev/mdX to show the current status of a RAID device. You can also use mdmonitor, which requires configuring /etc/mdadm.conf To help ensure any reduction in that redundancy is fixed as quick as possible, a hot spare can be used. To create a hot spare when creating the RAID aray, use -x <number_of_spares> , i.e. sudo mdadm --create /dev/md0 -l 5 -n3 -x 1 /dev/sda8 /dev/sda9 /dev/sda10 /dev/sdb2 To restore the tested drive, or a new drive in a legitimate failure situation, first remove the \"faulty\" member, then add the \"new\" member, use --add --remove , i.e. mdadm --remove /dev/md0 /dev/sdb2; mdadm --add /dev/md0 /dev/sde2","title":"Software RAID"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#kernel-services","text":"Narrowly defined, Linux is only the kernel of the operating system, which includes many other components, such as libraries and applications that interact with the kernel. The kernel is the essential central component that connects the hardware to the software and manages system resources, such as memory and CPU time allocation among competing applications and services. It handles all connected devices using device drivers, and makes the devices available for operating system use. A system running only a kernel has rather limited functionality. It will be found only in dedicated and focused embedded devices. The main responsibilities of the kernel include: System initialization and boot up Process scheduling Memory management Controlling access to hardware I/O (Input/Output) between applications and storage devices Implementation of local and network filesystems Security control, both locally (such as filesystem permissions) and over the network Networking control","title":"Kernel Services"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#kernel-command-line","text":"Various parameters are passed to the system at boot on the kernel command line, typically placed on the linux line in the GRUB config file, such as /boot/grub , or in a place like /boot/efi/EFI/centos/grub.cfg (however, RHEL/CentOS 8 saves in /boot/grub2/grubenv ), which may look like linux boot/vmlinuz-4.19.0 root=UUID=7ef4e747-afae-90b4-9be8be8d0258 ro quiet crashkernel=384M-:128M linuxefi /boot/vmlinuz-5.2.9 root=UUID=77461ee7-c34a-4c5f-b0bc-29f4feecc743 ro crashkernel=auto rhgb quiet linux16 /boot/vmlinuz-3.19.1.0 root=UUID=178d0092-4154-4688-af24-cda272265e08 ro vconsole.keymap=us crashkernel=auto vconsole.font=latarcyrheb-sun16 rhgb quiet LANG=en_US.UTF-8 Everything after the vmlinuz file specified is an option. Any options not understood by the kernel will be passed to init (pid = 1). The complete list of kernel boot parameters can be found at its documentation or using man bootparam . Use cat /proc/cmdline to see what command line a system was booted with.","title":"Kernel Command Line"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#sysctl","text":"sysctl interface can be used to read and tune kernel parameters at run time. View kernel parameters with sysctl -a ; each value corresponds to a particular pseudofile residing under /proc/sys , with directory slashes being replaced by dots. To apply a change at runtime: $ sudo sh -c 'echo 1 > /proc/sys/net/ipv4/ip_forward' # or execute as root user $ sudo sysctl net.ipv4.ip_forward = 1 If settings are placed in /etc/sysctl.conf (see man sysctl.conf for details), settings can be fixed at boot time (and reloaded at run time with sysctl -p ) Vendors put their settings in files in the /usr/lib/sysctl.d/ directory. These can be added to or supplanted by files placed in /etc/sysctl.d","title":"sysctl"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#kernel-modules","text":"The Linux kernel makes extensive use of modules , which contain important software that can be dynamically loaded and unloaded as needed after the system starts. This flexibility also aids in development of new features as system reboots are almost never needed to test during development and debugging.","title":"Kernel Modules"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#modprobe","text":"There are a number of utility programs that are used with kernel modules: lsmod - List loaded modules. insmod - Directly load modules. rmmod - Directly remove modules. modprobe - Load or unload modules, using a pre-built module database with dependency and location information, which does dependency modules check and loads/unloads automatically. i.e. modprobe e1000e , modprobe -r e1000e depmod - Rebuild the module dependency database. modinfo - Display information about a module. modprobe requires a module dependency database be updated. Use depmod to generate or update the file /lib/modules/$(uname -r)/modules.dep All files in the /etc/modprobe.d subdirectory tree which end with the .conf extension are scanned when modules are loaded and unloaded using modprobe. These config files control some parameters when loading with modprobe . You can also disable specific modules to avoid them being loaded. The format of files in /etc/modprobe.d is simple: one command per line, with blank lines and lines starting with # ignored; a backslash at the end of a line causes it to continue on the next line. Some rules for modprobe to keep in mind: It is impossible to unload a module being used by one or more other modules It is impossible to unload a module that is being used by one or more processes When a module is loaded with modprobe, the system will automatically load any other modules that need to be loaded first When a module is unloaded with modprobe -r, the system will automatically unload any other modules being used by the module (if not used by other modules) Much information about modules can also be seen in the /sys/module/<module_name> pseudo-filesystem directory tree, such as parameters.","title":"modprobe"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#devices-and-udev","text":"udev stands for User Device management . It dynamically discovers built-in hardware as well as peripheral devices during boot time or ad-hoc plugged at run time. udev handles loading and unloading device drivers with proper configurations as need, including: Device naming Device file and symlink creating Setting file attributes Taking other needed actions (such as execute a program) udev runs as a daemon (either udevd or systemd-udevd) and monitors a netlink socket. When new devices are initialized or removed, the uevent kernel facility sends a message through the socket, which udev receives and takes appropriate action to create or remove device nodes of the right names and properties according to the rules. The three components of udev are: The libudev library which allows access to information about the devices The udevd or systemd-udevd daemon that manages the /dev directory The udevadm utility for control and diagnostics The main configuration file is /etc/udev/udev.conf . It contains information such as where to place device nodes, default permissions and ownership, etc","title":"Devices and udev"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#udev-rules","text":"When devices are added or removed from the system, udev receives a message from the kernel. It then parses the rules files under /etc/udev/rules.d , /run/udev/rules.d , or /usr/lib/udev/rules.d for actions. A rules file may look like 60-persistent-storage.rules . There are two separate parts defined on a single line: one or more match pairs denoted by == . These try to match a device\u2019s attributes and/or characteristics to some value. one or more assignment key-value pairs that assign a value to a name, such as a file name, assignment, even file permissions, etc. The format for a udev rule is simple: <match><op>value [, ...] <assignment><op>value [, ... ] . If no matching rule is found, it uses the default device node name and other attributes. Example of a rules file: $ cat /etc/udev/rules.d/60-vboxdrv.rules KERNEL==\"vboxdrv\", NAME=\"vboxdrv\", OWNER=\"root\", GROUP=\"vboxusers\", MODE=\"0660\" KERNEL==\"vboxdrvu\", NAME=\"vboxdrvu\", OWNER=\"root\", GROUP=\"root\", MODE=\"0666\" KERNEL==\"vboxnetctl\", NAME=\"vboxnetctl\", OWNER=\"root\", GROUP=\"vboxusers\", MODE=\"0660\" SUBSYSTEM==\"usb_device\", ACTION==\"add\", RUN+=\"/usr/lib/virtualbox/VBoxCreateUSBNode.sh $major $minor $attr{bDeviceClass}\" SUBSYSTEM==\"usb\", ACTION==\"add\", ENV{DEVTYPE}==\"usb_device\", RUN+=\"/usr/lib/virtualbox/VBoxCreateUSBNode.sh $major $minor $attr{bDeviceClass}\" System administrators can control how udev operates and craft special udev rules.","title":"udev rules"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#device-nodes","text":"Device nodes are used by programs to communicate with devices through nodes using normal I/O methods. Character and block devices have device nodes; network devices do not. A device driver may use multiple device nodes. Device nodes are located in the /dev directory. Device nodes can be created with: sudo mknod [-m mode] /dev/name <type> </major> <minor> The major and minor numbers identify the driver associated with the device, with the driver uniquely reserving a group of numbers. In most cases, device nodes of the same type (block or character) with the same major number use the same driver. The major and minor numbers appear in the same place that file size would when looking at a normal file with ls . The minor number is used only by the device driver to differentiate between the different devices it may control, or how they are used. These may either be different instances of the same kind of device, (such as the first and second sound card, or hard disk partition) or different modes of operation of a given device (such as different density floppy drive media). Device numbers have meaning in user-space as well. Two system calls, mknod() and stat() , return information about major and minor numbers.","title":"Device Nodes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#virtualization","text":"Virtual Machines are a virtualized instance of an entire operating system, and may fulfill the role of a server or a desktop/workstation. The outside world sees the VM as if it were an actual physical machine, present somewhere on the network. Applications running in the VM are generally unaware of their non-physical environment.\u200b Other kinds of virtualization: Network - The details of the actual physical network, such as the type of hardware, routers, etc., are abstracted and need not be known by software running on it and configuring it. Storage - Multiple network storage devices are configured to look like one big storage unit, such as a disk. Examples: Network Attached Storage or NAS. Application - An application is isolated into a standalone format, such as a container Virtualization is developed and evolving for several reasons: Enables better hardware utilization Operating systems often progress more quickly than hardware It is microcode-driven CPUs enhanced to support virtualization led to a boost in performance, easier configuration, and more flexibility in VM installation and migration From early mainframes to mini-computers, virtualization has been used for expanding limits, debugging and administration enhancements. A host is the underlying physical operating system managing one or more virtual machines. A guest is the VM which is an instance of a complete operating system, running one or more applications. The guest should not care what host it is running on and can be migrated from one host to another. Low-level performance tuning on areas such as CPU utilization, networking throughput, memory utilization, is often best done on the host, while application tuning will be done mostly on the guest.","title":"Virtualization"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#emulation","text":"The first implementations of virtualization on the PC architecture were through the use of emulators . An Emulator runs completely in software. It is useful for running virtual machines on different architectures, such as running a pretend ARM guest machine on an X86 host. Performance is relatively slow.","title":"Emulation"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#hypervisors","text":"The host system acts as the hypervisor that initiates, terminates, and manages guests. It also called Virtual Machine Monitor (VMM). Two basic methods of virtualization: hardware virtualization - The guest system runs without being aware it is running as a virtualized guest, and does not require modifications to be run in this fashion. \u200bIt is also known as Full Virtualization. para-virtualization - The guest system is aware it is running in a virtualized environment, and has been modified specifically to work with it. You can check directly if your CPU supports hardware virtualization extensions by looking at /proc/cpuinfo ; if you have an IVT-capable chip, you will see vmx in the flags field; and, if you have an AMD-V capable chip, you will see svm in the same field. You may also have to ensure the virtualization capability is turned on in your CMOS. The hypervisor can be: External to the host operating system kernel: VMware Internal to the host operating system kernel: KVM Going past Emulation, the merging of the hypervisor program into a specially-designed lightweight kernel was the next step in the Virtualization deployment. i.e. the KVM project added hypervisor capabilities into the Linux kernel. Specific CPU chip functions and facilities were required and deployed for this type of virtualization.\u200b","title":"Hypervisors"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#libvirt","text":"The libvirt project is a toolkit to interact with virtualization technologies. It provides management for virtual machines, virtual networks, and storage, and is available on all enterprise Linux distributions.","title":"libvirt"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#kvm","text":"KVM uses the Linux kernel for computing resources, including memory management, scheduling, synchronization, and more. When running a virtual machine, KVM engages in a co-processing relationship with the Linux kernel. Managing KVM can be done with command line tools include: virt-* and qemu-* . Graphical interfaces include virt-manager, kimchi, OpenStack, oVirt , etc.","title":"KVM"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#containers","text":"Further integration between the hypervisor and the Linux kernel allowed the creation of operating system-level virtual machines, or containers. Containers share many facilities in the Linux kernel, and make use of namespaces and cgroups . Containers are very lightweight and reduce the overhead associated with having whole virtual machines. OS container is a flavor that runs an image of an operating system with the ability to run init processes and spawn multiple applications. i.e. LXC (Linux Containers) Application virtualization runs one application for each container. Many single application containers are typically initialized on a single machine which creates a greater flexibility and reduces overhead normally associated with virtualization. Virtual machines run a complete operating system, and can run many services and applications. Virtual machines use more resources than a container. \u200bContainers usually run one thing. Containers are more portable, and can be run inside a VM. Scaling workloads is different for containers and virtual machines. Orchestration systems such as Kubernetes or Mesos can decide on the proper quantity of containers needed, do load balancing, replicate images and remove them, etc., as needed.","title":"Containers"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#docker","text":"Docker is an application-level virtualization using many individual images to build up the necessary services to support the target application. These images are packaged into containers - they are components in containers. Images may contain: Application code\u200b Runtime libraries\u200b System tools\u200b \u200bOr just about anything required for an application Images may reside on a Docker Hub or a registry server. An application can be packaged up with all of its dependent code and services and deployed as a single unit with the minimum of overhead. This deployment can be easily repeated as often as desired.","title":"Docker"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#podman","text":"RHEL8/CentOS8 have replaced pure docker with podman. Podman uses a child/parent forking model for container creation and management, while Docker uses a server/client model with a daemon running in background for management. Emulation layer enables backwards compatibility with docker commands. Promised benefits include better security and less overhead.","title":"Podman"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#account-management","text":"Linux systems provide a multi-user environment which permits people and processes to have separate simultaneous working environments: Providing each user with their own individualized private space Creating particular user accounts for specific dedicated purposes Distinguishing privileges among users","title":"Account Management"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#user-accounts","text":"Normal user accounts are for people who will work on the system. Some user accounts (like the daemon account) exist for the purpose of allowing processes to run as a user other than root. Each user on the system has a corresponding line in the /etc/passwd file that describes their basic account attributes. A line in /etc/passwd consists of: user name, user password (use /etc/shadow if value is x ), UID, GID, comment, home directory, login shell Add user with sudo useradd <username> , which will create an account using default algorithms for assigning user and group id , home directory, and shell choice; the defaults can easily be overruled by using options to useradd . i.e. sudo useradd dexter will: The next available UID greater than UID_MIN (specified in /etc/login.defs ) by default is assigned as dexter's UID. The convention most Linux distributions have used is that any account with a user ID less than 1000 is considered special and belongs to the system; normal user accounts start at 1000 (UID_MIN defined in /etc/login.defs ) A group called dexter with a GID=UID is also created and assigned as dexter's primary group. aka Primary Group ID , and sometimes called User Private Groups (UPG) A home directory /home/dexter is created and owned by dexter. dexter's login shell will be /bin/bash . The contents of /etc/skel is copied to /home/dexter . By default, /etc/skel includes startup files for bash and for the X Window system. An entry of either !! or ! is placed in the password field of the /etc/shadow file for dexter's entry, thus requiring the administrator to assign a password for the account to be usable. Use of /etc/shadow enables password aging on a per user basis. At the same time, it also allows for maintaining greater security of hashed passwords since it has permission 400 while /etc/passwd has permission 644. /etc/shadow contains one record (one line) for each user, such as daemon:*:16141:0:99999:7::: . The colon-separated fields are: username: unique user name password: the hashed (sha512) value of the password lastchange: date that password was last changed mindays: minimum days before password can be changed maxdays: maximum days after which password must be changed warn: days before password expires that the user is warned grace: days after password expires that account is disabled expire: date that account is/will be disabled reserved: reserved field Note the dates are stored as the number of days since Jan. 1, 1970 (the epoch date). The username in each record must match exactly that found in /etc/passwd , and also must appear in the identical order . The password hash is the string \"$6$\" followed by an eight-character salt value , which is then followed by a $ and an 88-character (sha512) password hash. Additionally, userdel is used to remove user accounts, and usermod is used to change characteristics of a user account. You can lock a user account to prevent login by usermod -L <username> (and unlock with -U option). Linux ships with some system accounts that are locked (such as bin, daemon, or sys), which means they can run programs, but can never login to the system and have no valid password associated with them. These accounts has /sbin/nologin as their login shell. Attempt to login will show message from /etc/nologin.txt . You can also lock a user account by setting an expiration date on an account, with chage : chage [-m mindays] [-M maxdays] [-d lastday] [-I inactive] [-E expiredate] [-W warndays] user . i.e. sudo chage -E 2014-09-11 morgan . Only the root user can use chage . Normal user can run chage -l to check when their password or account will expire. Passwords can be changed with passwd ; a normal user can change only their own password, while root can change any user password.","title":"User Accounts"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#sudoers","text":"The /etc/sudoers defines the sudo access. Usually you don't need to temper with this file and just add new user to the sudo group to grant sudo access. The file has default access mode of 400. To do quick edit, use sudo visudo . The syntax of lines in the /etc/sudoers file is users hosts=(user:group) commands , where group can be omitted. You should use drop-ins under /etc/sudoers.d/ to add additional sudoer access, and leave the main sudoers file unchanged.","title":"Sudoers"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#account-restriction","text":"One can set restricted shell as login shell /bin/rbash , which is equivalent to /bin/bash -r . It: Prevents the user from using cd to change directories. Prevents setting the SHELL, ENV or PATH environment variables. Prohibits specifying path or command names containing / . Restricts redirection of output and/or input. However, it is fairly easy to defeat the restricted shell. Read more at \"Escaping Restricted Shell rbash\" and \"Linux Restricted Shell Bypass\" You can also set up restricted user account which: Uses the restricted shell Limits available system programs and user applications Limits system resources Limits access times Limits access locations When the restricted shell is invoked, it executes $HOME/.bash profile without restriction. This is why the user must not have either write or execute permissions on the home directory. Make sure that when you set up such an account that you do NOT inadvertently add system directories to the PATH environment variable, because this allows the restricted user the ability to execute other system programs, such as an unrestricted shell. By default, root logins through the network are generally prohibited for security reasons. It is generally recommended that all root access be through su , or sudo (causing an audit trail of all root access through sudo ). PAM (Pluggable Authentication Modules) can also be used to restrict which users are allowed to su to root. It might also be worth it to configure auditd to log all commands executed as root.","title":"Account Restriction"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ssh","text":"SSH (Secure SHell) exists for the needs to login through the network into a remote system, or to transfer files to and from a remote machine. User-specific ssh configuration files are created under every user's home directory in the hidden .ssh directory. Within the directory: id_rsa : The user's private encryption key id_rsa.pub : The user's public encryption key authorized_keys : A list of public keys that are permitted to login known_hosts : A list of hosts from which logins have been allowed in the past config : A configuration file for specifying various options First, a user has to generate their private and public encryption keys with ssh-keygen . The private keys must never be shared. The public key, however, should be given to any machine with which you want to permit password-less access . It should also be added to your authorized_keys file, together with all the public keys from other users who have accounts on your machine and you want to permit password-less access to their accounts.","title":"ssh"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#group-accounts","text":"Linux systems form collections of users called groups which share some common purpose, and share certain files and directories and maintain some common privileges. Groups are defined in /etc/group , which has the same role for groups as /etc/passwd has for users. Each line of the file looks like: groupname:password:GID:user1,user2,... . A Linux user has one primary group; this is listed in /etc/passwd and will also be listed in /etc/group . A user may belong to between 0 and 15 secondary groups. Group passwords may be set, but only if /etc/gshadow exists. GID is the group identifier. Values between 0 and 99 are for system groups. Values between 100 and GID_MIN (as defined in /etc/login.defs and usually the same as UID_MIN) are considered special. Values over GID_MIN are for UPG (User Private Groups)","title":"Group Accounts"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#group-management","text":"Group accounts may be managed and maintained with: groupadd : Add a new group. groupmod : Modify a group and add new users. groupdel : Remove a group. usermod : Manage a user's group memberships by giving a complete list of groups, or add new group memberships Note you will have to log out and log back in again for the new group membership to be effective. Group membership can be identified by running either of the following commands: groups [user1 user2 ...] or id -Gn [user1 user2 ...]","title":"Group Management"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#user-private-groups","text":"Each user will have his or her own group, and additional members may be added to someone's private group in /etc/group . By default, users whose accounts are created with useradd have: primary GID = UID and the group name is also identical to the user name. As specified in /etc/profile , the umask is set to 002 for all users created with UPG. Under this scheme, user files are thus created with permissions 664 (rw-rw-r--) and directories with 775 (rwxrwxr-x).","title":"User Private Groups"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#file-permissions-and-ownership","text":"When viewing file permission with ls -l a_file , i.e. -rw-rw-r-- 1 coop aproject 1601 Mar 9 15:04 a_file . There are nine more which indicate the access rights granted to potential file users, with each three characters grouped as a triplet. owner: the user who owns the file (also called user) group: the group of users who have access other: the rest of the world (also called world) Each of the triplets can have each of the following sets: r: read access is allowed w: write access is allowed x: execute access is allowed In addition, other specialized permissions exist for each category, such as the setuid/setgid permissions. Any request to access a file requires comparison of the credentials and identity of the requesting user to those of the owner of the file.","title":"File permissions and Ownership"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#manage-permissions-and-ownership","text":"Changing file permissions is done with chmod . Permissions can be represented either as a bitmap, usually written in octal, or in a symbolic form. Octal bitmaps usually look like 0755 , while symbolic representations look like u+rwx, g+rwx, o+rx . The octal number representation is the sum for each digit of: 4 if the read permission is desired 2 if the write permission is desired 1 if execute permission is desired Changing the group is done with chgrp . You can only change group ownership to groups that you are a member of. One can change file ownership and group ownership at the same time with chown . The option -R applies the change recursively","title":"Manage permissions and ownership"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#umask","text":"The default permissions given when creating a file are read/write for owner, group and world (0666) , and for a directory it is read/write/execute for everyone (0777) . However, the actual permissions have changed to 664 for the file and 775 for the directory as they have been modified by the current umask=002 whose purpose is to show which permissions should be denied. You can change the umask at any time with the umask command; which is the most conventional value set by system administrators for users. This value is combined with the file creation permissions to get the actual result; i.e., 0666 & ~002 = 0664; i.e., rw-rw-r--","title":"umask"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#filesystem-acls","text":"POSIX ACLs (Access Control Lists) extend the simpler user, group, and world system, by granting privileges to specific users or groups of users when accessing certain objects or classes of objects. Files and directories can be shared without using 777 permissions. All major filesystems used in modern Linux distributions incorporate the ACL extensions, and one can use the option -acl when mounting. A default set of ACLs is created at system install. Use getfacl/setfacl to get/set ACLs. New files inherit the default ACL (if set) from the directory they reside in. Also mv and cp -p preserve ACLs.","title":"Filesystem ACLs"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#pluggable-authentication-modules-pam","text":"Historically, authentication of users was performed individually by individual applications; i.e., su, login, and ssh would each authenticate and establish user accounts independently of each other. Most modern Linux applications have been written or rewritten to exploit PAM so that authentication can be done in one uniform way, using libpam . PAM incorporates the following concepts: PAM-aware applications Configuration files in /etc/pam.d/ PAM modules in the libpam* libraries, which can be found in different locations depending on the Linux distribution","title":"Pluggable Authentication Modules (PAM)"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#pam-rules","text":"Each file in /etc/pam.d/ corresponds to a service and each (non-commented) line in the file specifies a rule. The rule is formatted as a list of space-separated tokens, the first two of which are case insensitive: type control module-path module-arguments The type controls the step of the authentication process: auth: Instructs the application to prompt the user for identification (username, password, etc). May set credentials and grant privileges. account: Checks on aspects of the user's account, such as password aging, access control, etc. password: Responsible for updating the user authentication token , usually a password. session: Used to provide functions before and after the session is established (such as setting up environment, logging, etc.) The control flag controls how the success or failure of a module affects the overall authentication process: required: Must return success for the service to be granted. If part of a stack, all other modules are still executed. Application is not told which module or modules failed. requisite: Same as required, except a failure in any module terminates the stack and a return status is sent to the application. optional: Module is not required. If it is the only module, then its return status to application may cause failure. sufficient: If this module succeeds, then no subsequent modules in the stack are executed. If it fails, then it doesn't necessarily cause the stack to fail, unless it is the only one in the stack.","title":"PAM Rules"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ldap-authentication","text":"The Lightweight Directory Access Protocol (LDAP) is an industry standard protocol for using and administering distributed directory services over the network , and is meant to be both open and vendor-neutral. With LDAP, each system (or client) connects to a centralized LDAP server for user authentication. Using Transport Layer Security (TLS) makes it a secure option and is recommended. When you configure a system for LDAP authentication, five files are changed: /etc/openldap/ldap.conf /etc/pam_ldap.conf /etc/nslcd.conf /etc/sssd/sssd.conf /etc/nsswitch.conf You can edit these files manually or use one of the utility programs available ( system-config-authentication or authconfig-tui ).","title":"LDAP Authentication"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#network","text":"","title":"Network"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#network-addresses","text":"The IP address is the number that identifies your system on the network. IP addresses are used to uniquely identify nodes across the internet. They are registered through ISPs (Internet Service Providers). IPv4 is a 32-bit address, composed of 4 octets (an octet is just 8 bits, or a byte); IPv6 is a 128-bit address, composed of 8 16-bit octet pairs. In either case, a set of reserved addresses is also included.","title":"Network Addresses"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ipv4-address-types","text":"Unicast - An address associated with a specific host. i.e. 140.211.169.4 Network - An address whose host portion is set to all binary zeroes. i.e. 192.168.1.0 Broadcast - An address to which each member of a particular network will listen. i.e. 172.16.255.255 Multicast - An address to which appropriately configured nodes will listen. Only nodes specifically configured to pay attention to a specific multicast address will interpret packets for that multicast group","title":"IPv4 Address Types"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#reserved-ipv4-addresses","text":"Certain addresses and address ranges are reserved for special purposes: 127.x.x.x - Reserved for the loopback (local system) interface 0.0.0.0 - Used by systems that do NOT yet know their own address. Protocols like DHCP and BOOTP use this address when attempting to communicate with a server. 255.255.255.255 - Generic broadcast private address, reserved for internal use. These addresses are never assigned or registered to anyone. They are generally not routable. Others 10.0.0.0 - 10.255.255.255 172.16.0.0 - 172.31.255.255 192.168.0.0 - 192.168.255.255 etc.","title":"Reserved IPv4 Addresses"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ipv4-address-classes","text":"Historically, IP addresses are based on defined classes. Classes A, B, and C are used to distinguish a network portion of the address from a host portion of the address, for routing purposes. Network Class Highest order octet range Notes A 1-127 networks, 16,772,214 hosts per network, 127.x.x.x reserved for loopback B 128-191 16,384 networks, 65,534 hosts per network C 192-223 2,097,152 networks, 254 hosts per network D 224-239 Multicast addresses E 240-254 Reserved address range","title":"IPv4 Address Classes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ipv6-address-types","text":"Unicast - a packet is delivered to one interface link-local - Auto-configured for every interface to have one. Non-routable. global - Dynamically or manually assigned. Routable. or reserved for documentation Multicast - a packet is delivered to multiple interfaces Anycast - a packet is delivered to the nearest of multiple interfaces in terms of routing distance IPv4-mapped - an IPv4 address mapped to IPv6, i.e. ::FFFF:a.b.c.d/96 IPv6 has some special types of addresses such as loopback, which is assigned to the lo interface, as ::1/128 .","title":"IPv6 Address Types"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#netmasks","text":"Netmask is used to determine how much of the address is used for the network portion and how much for the host portion as we have seen. It is also used to determine network and broadcast addresses. Network Class Decimal Hex Binary A 255.0.0.0 ff:00:00:00 11111111 00000000 00000000 00000000 B 255.255.0.0 ff:ff:00:00 11111111 11111111 00000000 00000000 C 255.255.255.0 ff:ff:ff:00 11111111 11111111 11111111 00000000 The network address is obtained by logical AND ing ( & ) the IP address with the netmask . We are interested in the network addresses because they define a local network which consists of a collection of nodes connected via the same media and sharing the same network address. All nodes on the same network can directly see each other . For example: 172.16.2.17 ip address & 255.255.0.0 netmask ----------------------------- 172.16.0.0 network address","title":"Netmasks"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#hostname","text":"The hostname is simply a label to distinguish a networked device from other nodes. The hostname is generally specified at installation time, and can be modified at any time later. The hostname for a machine can be checked with command hostname . To change hostname only once before next reboot, just execute sudo hostname <new_hostname> ; to change it persistently, do sudo hostnamectl set-hostname <new_hostname> . Hostname configuration is stored under /etc/ . On Red Hat-based systems this was /etc/sysconfig/network , on Debian-based systems this was /etc/hostname and on SUSE-based systems it was /etc/HOSTNAME . For DNS purposes, hostnames are appended with a period (dot) and a domain name , so that a machine with a hostname of antje could have a fully qualified domain name (FQDN) of antje.linuxfoundation.org.","title":"hostname"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#network-devices-and-configs","text":"","title":"Network Devices and Configs"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#network-devices","text":"Unlike block and character devices, network devices are not associated with special device files (aka device nodes). Network devices are known by their names, which usually consist of a type identifier followed by a number: eth0, eth1, eno1, eno2, etc. , for ethernet devices. wlan0, wlan1, wlan2, wlp3s0, wlp3s2, etc. , for wireless devices. br0, br1, br2, etc. , for bridge interfaces. vmnet0, vmnet1, vmnet2, etc. , for virtual devices for communicating with virtual clients Historically, multiple virtual devices could be associated with single physical devices; these were named with colons and numbers; so, eth0:0 would be the first alias on the eth0 device. This was done to support multiple IP addresses on one network card, but deprecated today. It is also not compatible with IPv6.","title":"network devices"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ip","text":"ip is the command line utility used to configure, control and query interface parameters and control devices, routing, etc. It is more efficient and versatile than ifconfig because it uses netlink sockets, rather than ioctl() system calls. ip basic syntax is ip [ OPTIONS ] OBJECT { COMMAND | help } . It can also be used with ip [ -force ] -batch filename to process commands from a file. The OBJECT argument describes what kind of action is going to be performed; the COMMANDS depends on the OBJECT selected: OBJECT Function address IPv4 or IPv6 protocol device address link Network Devices maddress Multicast Address monitor Watch for netlink messages route Routing table entry rule Rule in the routing policy database tunnel Tunnel over IP Some examples of using ip : $ ip link show - Show information for all network interfaces $ ip -s link show eth0 - Show information for the eth0 network interface, including statistics $ sudo ip addr add 192.168.1.7 dev eth0 - Set the IP address for eth0 $ sudo ip link set eth0 down - Bring interface eth0 down $ sudo ip link set eth0 mtu 1480 - Set MTU to 1480 bytes for interface eth0 $ sudo ip route add 172.16.1.0/24 via 192.168.1.5 - Set route to network","title":"ip"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#ifconfig","text":"ifconfig is a system administration utility long found in UNIX-like operating systems used to configure, control, and query network interface parameters from either the command line or from system configuration scripts. Some examples of using ifconfig : $ ifconfig - Display information about all interfaces $ ifconfig eth0 - Display information about only eth0 $ sudo ifconfig eth0 192.168.1.50 - Set the IP address to 192.168.1.50 on interface eth0 $ sudo ifconfig eth0 netmask 255.255.255.0 - Set the netmask to 24-bit $ sudo ifconfig eth0 up - Bring interface eth0 up $ sudo ifconfig eth0 down - Bring interface eth0 down $ sudo ifconfig eth0 mtu 1480 - Set the MTU (Maximum Transfer Unit) to 1480 bytes for interface eth0","title":"ifconfig"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#pnidn","text":"The Predictable Network Interface Device Names (PNIDN) is strongly correlated with the use of udev and integration with systemd. There are now 5 types of names that devices can be given: Incorporating Firmware or BIOS provided index numbers for on-board devices Example: eno1 Incorporating Firmware or BIOS provided PCI Express hotplug slot index numbers Example: ens1 Incorporating physical and/or geographical location of the hardware connection Example: enp2s0 These names are correlated with the physical locations of the hardware on the PCI system: $ lspci | grep Ethernet Incorporating the MAC address Example: enx7837d1ea46da Using the old classic method Example: eth0 You can choose to turn off the new scheme and go back to the classic names.","title":"PNIDN"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#nic-config-files","text":"Each distribution has its own set of files and/or directories, and they may be slightly different, depending on your distribution version. Red Hat /etc/sysconfig/network /etc/sysconfig/network-scripts/ifcfg-ethX /etc/sysconfig/network-scripts/ifcfg-ethX:Y /etc/sysconfig/network-scripts/route-ethX Debian /etc/network/interfaces SUSE /etc/sysconfig/network","title":"NIC Config Files"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#network-manager","text":"As a system was booted, it consulted the network configuration files in the /etc directory subtree in order to establish the interface properties such as static or dynamic (DCHP) address configuration, whether the device should be started at boot, etc. If there were multiple network devices, policies had to be established as to what order they would be brought up, which networks they would connect to, what they would be called, etc. Modern systems often have dynamic configurations: Networks may change as a device is moved from place to place. Wireless devices may have a large choice of networks to hook into. Devices may change as hardware such as wireless devices, are plugged in or turned on and off. Use of a GUI tool, nmtui or nmcli (for scripting) are the common ways to manage networks. Examples of nmcli can be found at Networking/CLI Fedora wiki webpage or with man nmcli-examples","title":"Network Manager"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#routing","text":"Routing is the process of selecting paths in a network along which to send network traffic. The routing table is a list of routes to other networks managed by the system. It defines paths to all networks and hosts, sending remote traffic to routers. To see the current routing table, you can use route -n or ip route . The default route is the way packets are sent when there is no other match in the routing table for reaching the specified network, which can be obtained dynamically using DHCP or manually configured (static). You can set the default gateway at runtime with: sudo route add default gw 192.168.1.10 enp2s0 To make persistent change, do: add GATEWAY=x.x.x.x to /etc/sysconfig/network for Red Hat systems or /etc/sysconfig/network-scripts/ifcfg-ethX for device-specific change add gateway=x.x.x.x to /etc/network/interfaces for Debian systems","title":"Routing"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#static-routes","text":"Static routes are used to control packet flow when there is more than one router or route. They are defined for each interface and can be either persistent or non-persistent. When the system can access more than one router, or perhaps there are multiple interfaces, it is useful to selectively control which packets go to which router. To make static route at run time, do ip route add 10.5.0.0/16 via 192.168.1.100 To make persistent change, do: add 10.5.0.0/16 via 172.17.9.1 to /etc/sysconfig/network-scripts/route-ethX for Red Hat system add following to /etc/network/interfaces for Debian systems: iface eth1 inet dhcp post-up route add -host 10.1.2.51 eth1 post-up route add -host 10.1.2.52 eth1 for SUSE systems add following to files such as /etc/sysconfig/network/ifroute-eth0 : # Destination Gateway Netmask Interface [Type] [Options] # where each field is separated by tabs 192.168.1.150 192.168.1.1 255.255.255.255 eth0 10.1.1.150 192.168.233.1.1 eth0 10.1.1.0/24 192.168.1.1 - eth0","title":"Static Routes"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#name-resolution","text":"Name resolution is the act of translating hostnames to the IP addresses of their hosts. There are two facilities for doing this translation: Static name resolution (using /etc/hosts ). Dynamic name resolution (using DNS servers). Commands used to resolve the IP address of a hostname: [dig | host | nslookup] linuxfoundation.org","title":"Name Resolution"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#etchosts","text":"/etc/hosts holds a local database of hostnames and IP addresses. It contains a set of records (each taking one line) which map IP addresses with corresponding hostnames and aliases. Such static name resolution is primarily used for local, small, isolated networks. It is generally checked before DNS is attempted to resolve an address; however, this priority can be controlled by /etc/nsswitch.conf (not often used today). The other host-related files in /etc are /etc/hosts.deny and /etc/hosts.allow . The allow file is searched first and the deny file is only searched if the query is not found there. /etc/host.conf contains general configuration information; it is rarely used.","title":"/etc/hosts"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#dns","text":"If name resolution cannot be done locally using /etc/hosts , then the system will query a DNS (Domain Name Server) server. DNS is dynamic and consists of a network of servers which a client uses to look up names. The service is distributed; any one DNS server has only information about its zone of authority; however, all of them together can cooperate to resolve any name. The machine's usage of DNS is configured in /etc/resolv.conf , which historically has looked like: search example.com aps.org nameserver 192.168.1.1 nameserver 8.8.8.8 which: Can specify particular domains to search Defines a strict order of nameservers to query May be manually configured or updated from a service such as DHCP (Dynamic Host Configuration Protocol) Most modern systems will have an /etc/hosts.resolv file generated automatically which was generated by NetworkManager invoking DHCP on the primary network interface.","title":"DNS"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#network-diagnostics","text":"Some utilities that helps diagnosis network: ping - Sends 64-byte test packets to designated network hosts and tries to report back on the time required to reach it, any lost packets, and some other parameters. You can see whether the network is working and the host is reachable. traceroute - Displays a network path to a destination. It shows the routers packets flow through to get to a host, as well as the time it takes for each hop. mtr - It combines the functionality of ping and traceroute , and creates a continuously updated display. dig - It is useful for testing DNS functionality. Note that one can also use host or nslookup , older programs that also try to return DNS information about a host.","title":"Network Diagnostics"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#firewall","text":"A firewall is a network security system that monitors and controls all network traffic. It applies rules on both incoming and outgoing network connections and packets and builds flexible barriers depending on the level of trust and network topography (or topology) of a given connection. Firewalls can be hardware or software based. They are found both in network routers, as well as in individual computers, or network nodes. Many firewalls also have routing capabilities. Information is transmitted\u200b across networks in the form of packets, and each one of these packets has: Header, Payload, Footer. The header and footer contain information about destination and source addresses, what kind of packet it is, and which protocol it obeys, various flags, which packet number this is in a stream, and \u200ball sorts of other metadata about transmissions. The actual data is in the payload. Almost all firewalls are based on Packet Filtering . \u200bPacket filtering intercepts packets at one or more stages in the network transmission, including application, transport, network, and datalink. A firewall establishes a set of rules by which each packet may be: Accepted or rejected based on content, address, etc.\u200b Mangled in some way Redirected to another address Inspected for security reasons, etc. The next generation of firewalls were based on stateful filters, which also examine the connection state of the packet, to see if it is a new connection, \u200bpart of an already existing one, or part of none. Denial of service attacks can bombard this kind of firewall to try and overwhelm it. The third generation of firewalls is called Application Layer Firewalls, and are aware of the kind of application and protocol the connection is using. They can block anything which should not be part of the normal flow.\u200b","title":"Firewall"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#firewall-configurations","text":"Configuring the firewall with low-level cli tools such as iptables, firewall-cmd, ufw , or GUI tools such as system-config-firewall, firewall-config, gufw, yast .","title":"Firewall Configurations"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#firewalld","text":"firewalld is the Dynamic Firewall Manager. It utilizes network/firewall zones which have defined levels of trust for network interfaces or connections. It supports both IPv4 and IPv6 protocols. Additionally, it separates runtime and permanent (persistent) changes to configuration, and also includes interfaces for services or applications to add firewall rules. Configuration files are kept in /etc/firewalld (primary) and /usr/lib/firewalld As a service, firewalld replaces the older iptables. It is an error to run both services, firewalld and iptables, at the same time. The command line tool to manage firewalld is firewall-cmd . Note that if you have more than one network interface when using IPv4, you have to turn on ip forwarding, with: sudo sysctl net.ipv4.ip_forward = 1 echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward sudo sysctl -p","title":"firewalld"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#zones","text":"firewalld works with zones, each of which has a defined level of trust and a certain known behavior for incoming and outgoing packets. Each interface belongs to a particular zone (normally, it is NetworkManager which informs firewalld which zone is applicable) which can be changed with fireweall-cmd . Types of zones and their effects: drop - All incoming packets are dropped with no reply. Only outgoing connections are permitted. block - All incoming network connections are rejected. The only permitted connections are those from within the system. public - Do not trust any computers on the network; only certain consciously selected incoming connections are permitted. external - Used when masquerading is being used, such as in routers. Trust levels are the same as in public. dmz (Demilitarized Zone) - Used when access to some (but not all) services are to be allowed to the public. Only particular incoming connections are allowed. work - Trust (but not completely) connected nodes to be not harmful. Only certain incoming connections are allowed. home - You mostly trust the other network nodes, but still select which incoming connections are allowed. internal - Similar to the work zone. trusted - All network connections are allowed. On system installation, most, if not all Linux distributions, will select the public zone as default for all interfaces. Any zone can be bound not just to a network interface, but also to particular network addresses. A packet is associated with a zone if: It comes from a source address already bound to the zone; or if not, It comes from an interface bound to the zone. Any packet not fitting the above criteria is assigned to the default zone (i.e, usually public). # example commands for adding rules for setting zones on the level of interface, IP addresses, service, port and protocal sudo firewall-cmd --permanent --zone = internal --change-interface = eno1 sudo firewall-cmd --permanent --zone = trusted --add-source = 192 .168.1.0/24 sudo firewall-cmd --permanent --zone = home --add-service = dhcp sudo firewall-cmd --zone = home --add-port = 21 /tcp","title":"zones"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#system-startup-and-shutdown","text":"The boot sequence basic steps are: The BIOS/UEFI locates and executes the boot program, or boot loader. POST (Power On Self Test) is run to check the memory and hardware and then search a specific location or device for a boot program boot program is found in MBR or using UEFI. It is usually GRUB. The boot loader loads the kernel. kernel need to be decompressed, then performs hardware checks, gains access to important peripheral hardware The kernel starts the init process (pid=1). init manages system initialization, using systemd or the older Upstart and SysVinit startup scripts.","title":"System Startup and Shutdown"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#grub","text":"Virtually, all (non-embedded) modern Linux distributions use GRUB (GRand Unified Boot Loader). efibootmgr is not actually a boot loader, but is a boot manager, used in conjunction with GRUB on multi-boot EFI systems. Some important features of GRUB are: Alternative operating systems can be chosen at boot time. Alternative kernels and/or initial ramdisks can be chosen at boot time for a given operating system. Boot parameters can be easily changed at boot time without having to edit configuration files, etc., in advance. At boot, a basic configuration file is read, /boot/grub/grub.cfg , or /boot/grub2/grub.cfg , or /boot/efi/EFI/redhat/grub.cfg . This file is auto-generated by update-grub (or grub-mkconfig or grub2-mkconfig ) based on configuration files in the /etc/grub.d directory and on /etc/default/grub and should not be edited by hand. Upon system boot, after the initial POST and BIOS stages, GRUB will be entered and display a menu containing a list of bootable images either from one or more Linux distributions or operating systems, with submenus. After selecting an entry, you can type e for edit and then enter into an interactive shell to edit the particular boot option. If there are serious problems, like not being able to find a configuration file, GRUB reverts back to a pure shell mode and you may be able to rescue the system without resorting to rescue media. In both GRUB versions, the first hard drive is denoted as hd0, the second is hd1, etc. However, in Version 1, partitions start counting from 0, and in Version 2 from 1. For example: sda1 is (hd0,1) in GRUB 2, but (hd0,0) in GRUB 1. On systems configured with Boot Loader Specification Configuration (BLSCFG), one still uses the usual grub commands when installing or updating kernels, but detailed information and options for each kernel are found in /boot/loader/entries . This new scheme can be turned on/off with grub2-switch-to-blscfg or altering the variable GRUB_ENABLE_BLSCFGS=[true|false] in /etc/default/grub .","title":"GRUB"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#configuration-files-in-etc","text":"For historical reasons, Linux distributions evolved their own rules about exactly where to place some information in /etc . For example, all Red Hat-derived systems make extensive use of /etc/sysconfig/ , while Debian-based systems have used /etc/default/ . Interestingly, RHEL and SUSE use both.\u200b There should be only text files found under /etc , no binary formats or data.","title":"Configuration Files in /etc"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#etcsysconfig","text":"Files in this directory and its subdirectories are used by many system utilities services, often consulted when the system starts and stops services or queries their status.","title":"/etc/sysconfig"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#etcdefault","text":"The files are used to provide extra options when starting a service and typically contain code to set environment variables.","title":"/etc/default"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#shutdown","text":"shutdown is used to bring the system down in a secure fashion, notifying all users that the system is going down and then stopping it in a graceful and non-destructive way. After it is shut down, the system is either halted or rebooted. There are also the legacy commands reboot , halt , and poweroff This article shows a checklist of things to do pre and post reboot.","title":"shutdown"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#sbininit","text":"/sbin/init (usually just called init) is the first user-level process (or task) run on the system and continues to run until the system is shutdown. Traditionally, it has been considered the parent of all user processes, although technically that is not true, as some processes are started directly by the kernel. init coordinates the later stages of the boot process, configures all aspects of the environment, and starts the processes needed for logging into the system. init also works closely with the kernel in cleaning up after processes when they terminate. Traditionally, nearly all distributions based the init process on UNIX's venerable SysVinit' software. However, this scheme was developed decades ago under rather different circumstances: The target was multi-user mainframe systems (and not personal computers, laptops, and other devices) The target was a single processor system startup was viewed as a serial process, divided into a series of sequential stages (run levels). Startup (and shutdown) time was seen rare and not an important matter; it was far less important than getting things right. Modern systems have required newer methods with enhanced capabilities.","title":"/sbin/init"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#systemd","text":"The systemd system and session manager for Linux is now dominant in all major distributions. Features include the following: Boots faster than previous init systems Provides aggressive parallelization capabilities Uses socket and D-Bus activation for starting services Replaces shell scripts with programs Offers on-demand starting of daemons Keeps track of processes using cgroups Maintains mount and automount points Implements an elaborate transactional dependency-based service control logic Can work as a drop-in replacement for SysVinit and is compatible with SysVinit scripts. systemd is backward compatible with SysVinit and the concept of runlevels is supported via runlevel targets . The telinit program is emulated to work with runlevels. systemd prefers to use a set of standardized configuration files, it can also use distribution-dependent legacy configuration files as a fall-back. Runlevel Target 0 poweroff.target 1 rescue.target 2,3,4 multi-user.target 5 graphical.target 6 reboot.target The isolate command will immediately stop processes that are not enabled in the new unit, possibly including the graphical environment or terminal you are currently using # immediately change the target sudo systemctl isolate multi-user.target # change the default target, effective after each reboot sudo systemctl enable multi-user.target sudo systemctl set-default multi-user.target Use man systemd.special to learn more.","title":"systemd"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#systemctl","text":"systemctl is the main utility for managing services. Its basic syntax is: systemctl [options] command [name] . Some examples: systemctl - To show the status of everything that systemd controls systemctl list-units -t service --all - To show all available services systemctl list-units -t service - To show only active services sudo systemctl start/stop foo.service - To start (activate) or stop (deactivate) one or more units (a service or a socket) sudo systemctl enable/disable sshd.service - To enable/disable a service For most commands, you can omit the .service attached to the service name. Also worth read SysVinit to Systemd Cheetsheet .","title":"systemctl"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#service-file","text":"To make a service start up when system boots, we need to configure it and put under /etc/systemd/system/multi-user.target.wants/ . A simple service file may look like the following. For more info check man systemd.service or here [Unit] Description=Simple notifying service [Service] Type=notify ExecStart=/usr/sbin/simple-notifying-service [Install] WantedBy=multi-user.target","title":".service file"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#backup-and-recovery","text":"Obviously, files essential to your organization require backup. Configuration files may change frequently, and along with individual user's files, require backup as well. Logging files can be important and worth to backup if you have to investigate your system's history, which can be particularly important for detecting intrusions and other security violations. The simplest backup scheme is to do a full backup of everything once, and then perform incremental backups of everything that subsequently changes. While full backups can take a lot of time, restoring from incremental backups can be more difficult and time consuming. Thus, you can use a mix of both to optimize time and effort. An example of one useful strategy involving tapes (you can easily substitute other media in the description): Use tape 1 for a full backup on Friday. Use tapes 2-5 for incremental backups on Monday-Thursday. Use tape 6 for full backup on second Friday. Use tapes 2-5 for incremental backups on second Monday-Thursday. Do not overwrite tape 1 until completion of full backup on tape 6. After full backup to tape 6, move tape 1 to external location for disaster recovery. For next full backup (next Friday) get tape 1 and exchange for tape 6. A good rule of thumb is to have at least two weeks of backups available.","title":"Backup and Recovery"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#backup-utilities","text":"cpio and tar - create and extract archives of files. gzip, bzip2, xz - create archive files to be written to disk, magnetic tape, or any other device which can hold files. Archives are very useful for transferring files from one filesystem or machine to another. dd - transfer raw data between media. It can copy entire partitions or entire disks. rsync - synchronize directory subtrees or entire filesystems across a network, or between different filesystem locations on a local machine. dump and restore - old but designed specifically for backups. They read from the filesystem directly (which is more efficient). However, they must be restored only on the same filesystem type that they came from. There are newer alternatives. mt - useful for querying and positioning tapes before performing backups and restores.","title":"Backup utilities"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#dd","text":"dd is a common UNIX-based program whose primary purpose is the low-level copying and conversion of raw data. It is used to copy a specified number of bytes or blocks, performing on-the-fly byte order conversions, as well as being able to convert data from one form to another. It can also be used to copy regions of raw device files, for example backing up the boot sector of a hard disk, or to read fixed amounts of data from special files like /dev/zero or /dev/random. The basic syntax is: dd if=input-file of=output-file options","title":"dd"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#rsync","text":"rsync (remote synchronize) is used to transfer files across a network (or between different locations on the same machine). The basic syntax is: rsync [options] sourcefile destinationfile The source and destination can take the form of target:path , where target can be in the form of [user@]host . The user@ part is optional and used if the remote user is different from the local user. You have to be very careful with rsync about exact location specifications (especially if you use the --delete option), so it is highly recommended to use the --dry-run option first, and then repeat if the projected action looks correct. rsync is very clever; it checks local files against remote files in small chunks, and it is very efficient in that when copying one directory to a similar directory, only the differences are copied over the network. This synchronizes the second directory with the first directory. You may often use the -r option, which causes rsync to recursively walk down the directory tree copying all files and directories below the one listed as the sourcefile. Thus, a very useful way to back up a project directory might be similar to: rsync -r project-X archive-machine:archives/project-X A simple (and very effective and very fast) backup strategy is to simply duplicate directories or partitions across a network with rsync commands and to do so frequently.","title":"rsync"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#cpio","text":"cpio (copy in and out) is a general file archiver utility that has been around since the earliest days of UNIX and was originally designed for tape backups. It is light weight than tar . The -o or --create option tells cpio to copy files out to an archive, which reads a list of file names (one per line) from standard input and writes the archive to standard output. The -i or --extract option tells cpio to copy files from an archive, reading the archive from standard input. The -t or --list option tells cpio to list the archive contents.","title":"cpio"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#backup-programs","text":"Amanda - (Advanced Maryland Automatic Network Disk Archiver) uses native utilities (including tar and dump) but is far more robust and controllable. Amanda is generally available on Enterprise Linux systems through the usual repositories. Bacula - designed for automatic backup on heterogenous networks. It can be rather complicated to use and is recommended (by its authors) only to experienced administrators. Bacula is generally available on Enterprise Linux systems through the usual repositories. Clonezilla - a very robust disk cloning program, which can make images of disks and deploy them, either to restore a backup, or to be used for ghosting, to provide an image that can be used to install many machines.","title":"Backup programs"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#linux-security-modules","text":"Linux Security Modules (LSM) emphasis the idea of implementing mandatory access controls over the variety of requests made to the kernel in a way that: Minimizes changes to the kernel Minimizes overhead on the kernel Permits flexibility and choice between different implementations, each of which is presented as a self-contained LSM (Linux Security Module) The basic idea is to hook system calls and insert code whenever an application requests a transition to kernel (system) mode. This code makes sure permissions are valid, malicious intent is protected against, by invoking security-related functional steps before and/or after a system call is fulfilled by the kernel. The current LSM implementations: SELinux, AppArmor, Smack, Tomoyo.","title":"Linux Security Modules"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#selinux","text":"SELinux was originally developed by the United States NSA (National Security Administration) and has been brought into a large usage base. Operationally, SELinux is a set of security rules that are used to determine which processes can access which files, directories, ports, and other items on the system. It works with three conceptual quantities: Contexts - Context are labels to files, processes and ports. Examples of contexts are SELinux user, role, type, level. Rules - Rules describe access control in terms of contexts, processes, files, ports, users, etc. Policies - Policies are a set of rules that describe what system-wide access control decisions should be made by SELinux. SELinux modes are selected (and explained) in /etc/sysconfig/selinux (CentOS and openSUSE) or /etc/selinux/config (Ubuntu). The sestatus utility can display the current mode and policy. The list of SELinux modes: Enforcing - All SELinux code is operative and access is denied according to policy. All violations are audited and logged. Permissive - Enables SELinux code, but only audits and warns about operations that would be denied in enforcing mode. Disabled - Completely disables SELinux kernel and application code, leaving the system without any of its protections. Read this comprehensive guide to install and enable SELinux .","title":"SELinux"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#getenforce-setenforce","text":"getenforce can be used to get the current SELinux mode. setenforce can be used to switch between enforcing and permissive modes on the fly while the system is in operation, but it does not allow you to enable/disable SELinux completely. Disable SELinux is done through either: Edit the SELinux configuration file and set SELINUX=disabled Add selinux=0 to the kernel parameter list when rebooting Note that disabling SELinux on systems in which SELinux will be re-enabled is not recommended. It is preferable to use the permissive mode instead of disabling SELinux, so as to avoid relabeling the entire filesystem, which can be time-consuming.","title":"getenforce, setenforce"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#policies","text":"SELinux configuration file also sets the SELinux policy. Multiple policies are allowed, but only one can be active at a time. Changing the policy may require a reboot of the system and a time-consuming re-labeling of filesystem contents. Each policy has files which must be installed under /etc/selinux/[SELINUXTYPE] . Some common SELinux policies: targeted - The default policy in which SELinux is more restricted to targeted processes. User processes and init processes are not targeted, while network service processes are targeted. SELinux enforces memory restrictions for all processes, which reduces the vulnerability to buffer overflow attacks. minimum - A modification of the targeted policy where only selected processes are protected. MLS - The Multi-Level Security policy is much more restrictive; all processes are placed in fine-grained security domains with particular policies. SELinux policy behavior can be configured at runtime without rewriting the policy. This is accomplished by configuring SELinux Booleans, which are policy parameters that can be enabled and disabled: getsebool - to see booleans setsebool - to set booleans default the change is not persistent. Make it persistent with option -P semanage boolean -l - to see persistent boolean settings","title":"Policies"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#contexts","text":"There are four SELinux contexts: User, Role, Type, Level. Use -Z with utilities such as ls , ps , cp , mv , and mkdir . To see the context associated witha file/process. Type is the most commonly utilized context, and its label should end with _t , as in kernel_t . You can use chcon to change a file's context, as in chcon -t etc_t somefile . Note that newly created files inherit the context from their parent directory, but when moving files, it is the context of the source directory which may be preserved. In that event, use restorecon which resets file contexts, based on parent directory settings. i.e. restorecon -Rv /home/jimih on a directory will correct the context recursively for files under. To configure the default context for a newly created directory, use semanage fcontext (from policycoreutils-python package). After the change, a call to restorecon is still required.","title":"Contexts"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#monitoring-access","text":"SELinux comes with a set of tools that collect issues at run time, log these issues and propose solutions to prevent same issues from happening again, via setroubleshoot-server package.","title":"Monitoring Access"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#apparmor","text":"AppArmor is an LSM alternative to SELinux, used by SUSE, Ubuntu and other distributions. It: Provides Mandatory Access Control (MAC) Allows administrators to associate a security profile to a program which restricts its capabilities Is considered easier (by some but not all) to use than SELinux Is considered filesystem-neutral (no security labels required) In addition to manually specifying profiles, AppArmor includes a learning mode, in which violations of the profile are logged, but not prevented. This log can then be turned into a profile, based on the program's typical behavior. To view its status, do sudo apparmor_status or sudo aa_status . AppArmor Modes: Enforce Mode - Applications are prevented from acting in ways which are restricted. Attempted violations are reported to the system logging files. This is the default mode. A profile can be set to this mode with aa-enforce. Complain Mode - Policies are not enforced, but attempted policy violations are reported. This is also called the learning mode. A profile can be set to this mode with aa-complain. Profiles restrict how executable programs that have pathnames on the system can be used. Linux distributions come with\u200b pre-packaged profiles or installed with an AppArmor package ( apparmor-profiles ), included in /etc/apparmor.d . Some common AppArmor utilities: Program Use apparmor_status Show status of all profiles and processes with profiles apparmor_notify Show a summary for AppArmor log messages complain Set a specified profile to complain mode enforce Set a specified profile to enforce mode disable Unload a specified profile from the current kernel and prevent from being loaded on system startup logprof Scan log files, and, if AppArmor events that are not covered by existing profiles have been recorded, suggest how to take into account, and, if approved, modify and reload easyprof Help set up a basic AppArmor profile for a program","title":"AppArmor"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#local-system-security","text":"Security can be defined in terms of the system's ability to regularly do what it is supposed to do, integrity and correctness of the system, and ensuring that the system is only available to those authorized to use it. The biggest problem with security is to find that appropriate mix of security and productivity; if security restrictions are tight, opaque, and difficult, especially with ineffective measures, users will circumvent procedures. It is important to create and publicize to your organization a clear security policy that is descriptive, easy to understand, and constantly updated. Policies should be generic and specify enforcement actions and response to breach. Essential aspects to cover: Confidentiality Data Integrity Availability Consistency Control Audit You should make sure that the data is correct and the system behaves as it is expected to do. There should be processes in effect to determine who is given access to your system.","title":"Local System Security"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#risk-analysis","text":"Risk analysis is based on the following three questions: What do I want to protect (identify assets)? What am I protecting it against (identify threats)? How much time, personnel, and money is needed to provide adequate protection? Two basic philosophies found in use in most computing environments: Anything not expressly permitted is denied.\u200b Anything not expressly forbidden is permitted. The first choice is tighter: a user is allowed to do only what is clearly and explicitly specified as permissible without privilege. This is the most commonly used philosophy. The second choice builds a more liberal environment where users are allowed to do anything except what is expressly forbidden. It implies a high degree of assumed trust and is less often deployed for obvious reasons.","title":"Risk Analysis"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#patch-system-updates","text":"Most attacks exploit known security holes and are deployed in the time period between revelation of a problem and patches being applied. It is critical to pay attention to your Linux distributor's updates and upgrades and apply them as soon as possible.","title":"Patch system updates"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#hardware-accessibility","text":"Any time hardware is physically accessible security can be compromised by: Key logging: Recording the real time activity of a computer user including the keys they press. The captured data can either be stored locally or transmitted to remote machines. Network sniffing: Capturing and viewing the network packet level data on your network. Booting with a live or rescue disk. Remounting and modifying disk content. Physical access to a system makes it possible for attackers to easily leverage several attack vectors, in a way that makes all operating system level recommendations irrelevant. Thus, security policy should start with requirements on how to properly secure physical access to servers and workstations. Necessary protective steps include: Locking down workstations and servers Protecting your network links against access by people you do not trust Protecting your keyboards where passwords are entered to ensure the keyboards cannot be tampered with Configuring password protection of the BIOS in such a way that the system cannot be booted with a live or rescue CD/DVD or USB key Setting a BIOS password protects against unauthorized persons changing the boot options to gain access to your system. You can secure the boot process further with a secure bootloader password to prevent someone from bypassing the user authentication step.","title":"Hardware Accessibility"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#secure-mount-filesystem","text":"When a filesystem is mounted, either at the command line with a mount command, or automatically by inclusion in /etc/fstab, various options can be specified to enhance security: nodev - Do not interpret character or block special devices on the filesystem. nosuid - The set-user-identifier or set-group-identifier bits are not to take effect.\u200b noexec - Restrict direct execution of any binaries on the mounted filesystem. ro - Mount the filesystem in read-only mode as in: mount -o ro,noexec,nodev /dev/sda2 /edsel By setting the setuid (set user ID) flag on an executable file, you modify this normal behavior by giving the program the access rights of the owner rather than the user of the program. Similar rule apply for setgid bit for giving runtime group access rights. By default, when a file is created in a directory, it is owned by the user and group of the user that created it. Using the setgid setting on the directory changes this so that files created in the directory are group owned by the group owner of the directory. This allows you to create a shared directory in which a group of users can share files. Set the setuid bit with chmod u+s somefile , and setgid bit with chmod g+s somefile","title":"Secure Mount Filesystem"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#troubleshooting","text":"Troubleshooting involves taking a number of steps which need to be repeated iteratively until solutions are found. A basic recipe might be: Characterize the problem Reproduce the problem Always try the easy things first Eliminate possible causes one at a time Change only one thing at a time; if that doesn't fix the problem, change it back Check the system logs (/var/log/messages, /var/log/secure, etc.) for further information","title":"Troubleshooting"},{"location":"Linux/Concepts/Linux_Foundation_System_Admin/#system-rescue","text":"Sooner or later a system is likely to undergo a significant failure. System Rescue media in the form of optical disks or portable USB drives can be used to fix the situation. Booting into either emergency or single user mode can enable using the full suite of Linux tools to repair the system back to normal function. The rescue image can be mounted and use chroot to change into that environment. You may install software packages from inside the chroot-ed environment. You may also be able to install them from outside the chroot-ed environment. i.e. sudo rpm -ivh --force --root=/mnt/sysimage /mnt/source/Packages/vsftpd-2*.rpm . Emergency boot media are useful when your system won't boot due to some issue such as missing, misconfigured, or corrupted files or a misconfigured service. In emergency mode you are booted into the most minimal environment possible. The root filesystem is mounted read-only, no init scripts are run and almost nothing is set up. To enter emergency mode, you need to select an entry from the GRUB boot menu and then hit e for edit. Then add the word emergency to the kernel command line before telling the system to boot. If your system boots, but does not allow you to log in when it has completed booting, try single user mode: init is started Services are not started Network is not activated All possible filesystems are mounted root access is granted without a password A system maintenance command line shell is launched In this mode, your system boots to runlevel 1 (in SysVinit language). To enter single user mode, you need to select an entry from the GRUB boot menu and then hit e for edit. Then add the word single to the kernel command line before telling the system to boot.","title":"System Rescue"},{"location":"Linux/Concepts/Networking-Concepts/","text":"Network and Internet Knowledge must knows for Linux \u00b6 what makes up a network \u00b6 some concepts node: devices having an Internet IP address, such as PC, Linux, ADSL modem, printer, etc. server: devices that serves contents workstation, client: devices that consumes contents from servers Network Interface Card, NIC: provide network access, usually each node has 1+ NIC. network access point: provides IP address topology: the way nodes are connected with each other route or gateway: having 2+ access points, can connect to 2+ different networks. Local Area Network, LAN close transmission distance, like within a building or an university campus. Can use expensive connection materials. Fast. Wide Area Network, WAN far transmission distance, like between cities, and use cheap connection materials. Slower and less reliable. Network protocols \u00b6 OSI seven layers Open System Interconnection (OSI). From the top layer to the bottom layer: Application Layer: defines how data reach specific application (Presentation Layer): transform data to satisfy network standard (Session Layer): process to process Transport Layer: defines TCP, UDP packet format Network Layer: defines IP protocols, establish/maintain/terminate connections, path selection in routing Data-link Layer: transform/interpret data packet to/from 0s and 1s Physical Layer: physical wires and media; transmit data as 0s and 1s via fibers While TCP/IP uses five of the seven: Application -> Transport -> Network -> Link -> Physical MAC address Each machine has a MAC (Media Access Control) address, that is unique and given when the NIC is manufactured. MAC packets size can vary from 64 bytes to 1500 bytes. IP Packet Header Versions: this IP packet version, like IPv4 IHL (Internet Header Length): tells the length of this IP header Type of Service: [PPPDTRUU] PPP: shows priority of this IP packet D: if 0 means normal latency, 1 means low latency T: if 0 means normal throughput, 1 means high throughput R: if 0 means normal reliability, 1 means high reliability UU: unused by setting this part, Gigabit Ethernet can make this packet high-speed and low-latency Total Length: shows the total length of this packet, including header and data portions. Max 65535 bytes Identification: since MAC frame max size is 1500 bytes, larger packet need to be broken down into smaller pieces, and each will have this Id for reassemble Flags: [0DM] D: if 0 means can be fragmented, 1 means cannot be fragmented M: if 0 means this IP is the last fragment, 1 means it is not the last fragment Fragment Offset: means this IP fragment's position in the original IP packet. Like an ordering number Time To Live (TTL): [0-255], shows how many routers to pass before this packet being discarded Protocol Number: this number represents a specific protocol. 6 is TCP; 17 is UDP; 4 is IP; 1 is ICMP (Internet Control Message Protocol) ... Header Checksum: to check the integrity of this packet's content Source Address: source IP address Destination Address: destination IP address Options: extra parameters, such as security, router, timestamp Padding: use padding to fill bits that are unused by Options IP address composition and classification Example: 192.168.0.0 ~ 192.168.0.255 Here 192.168.0 is the Net_ID, 0 ~ 255 is the Host_ID Within the same network, the Net_ID doesn't change within the same LAN Within the same network, machines can use CSMA/CD broadcast information, and also send message from Host_ID as 0 is reserved for Network IP; and as 255 is reserved for Broadcast IP. IP has five classifications: Class A: Net_ID starts with 0 0xxxxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx |--net--|-----------host----------| 0.xx.xx.xx ~ 127.xx.xx.xx Class B: Net_ID starts with 10 10xxxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx |--net-----------|----------host--| 128.xx.xx.xx ~ 191.xx.xx.xx Class C: Net_ID starts with 110 110xxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx |--net--------------------|-host--| 192.xx.xx.xx ~ 223.xx.xx.xx Class D: Net_ID starts with 1110. For the purpose of multicast 1110xxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx 224.xx.xx.xx ~ 239.xx.xx.xx Class E: Net_ID starts with 1111. Reserved and unused. 1111xxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx 240.xx.xx.xx ~ 255.xx.xx.xx Netmask (Subnet mask) Each Class A's Host_ID range is 0.0.1 ~ 255.255.255, which is definitely too large and broadcasting could be devastating. We need to divide it further to make subnets: to take some bits from Host_ID and make it part of Net_ID. Netmask is the trick to divide one large network segment into smaller ones. Example: 192.168.0.0 ~ 192.168.0.255, a Class C Netmask instruction: First IP: 11000000.10101000.00000000.00000000 Last IP: 11000000.10101000.00000000.11111111 |---------Net_ID----------|-host--| Netmask : 11111111.11111111.11111111.00000000 # binary rep. : 255.255.255.0 # decimal rep. Network: 192.168.0.0 Broadcast: 192.168.0.255 # In this way, the representation of Class ABC are: Class A: 11111111.00000000.00000000.00000000 # 255.0.0.0 Class B: 11111111.11111111.00000000.00000000 # 255.255.0.0 Class C: 11111111.11111111.11111111.00000000 # 255.255.255.0 After dividing the network segment into two subnets: Netmask: 11111111.11111111.11111111.10000000 # 255.255.255.128 First subnet : Network: 11000000.10101000.00000000.0 0000000 Broadcast:11000000.10101000.00000000.0 1111111 |---------Net_ID------------|-host-| Second subnet: Network: 11000000.10101000.00000000.1 0000000 Broadcast:11000000.10101000.00000000.1 1111111 |---------Net_ID------------|-host-| IP types There are two types of IP for IPv4: public IP: planned by INTERNIC, required to get on Internet private IP: cannot be used to get on Internet, used for LAN computers. private IP reserved some IP at A B C classifications of IP: Class A: 10.0.0.0 ~ 10.255.255.255 Class B: 172.16.0.0 ~ 172.31.255.255 CLass C: 192.168.0.0 ~ 192.168.255.255 these IPs are used for communication within a private LAN packets from private IP cannot be sent onto Internet CIDR (Classless Interdomain Routing) Network/Netmask 192.168.0.0/255.255.255.0 192.168.0.0/24 # because Net_ID has 24 bits # since 192.168.0.0 is Class C, when it comes to the need we need to break the rule and make 192.168.0.0 ~ 192.168.255.255 available, we are doing CIDR! loopback IP This IP segment is 127.0.0.0/8 on Class A. The frequently used \"localhost\" is 127.0.0.1, and there is no need for NIC. How IP is assigned static IP: check for usable IP and directly set your IP to a fixed value. dial-up IP: check in with your ISP with acocunt and password and get an IP assigned to you. DHCP: one machine designated for assigning IP to other machines IP and MAC: Link Layer ARP and RARP ARP (Address Resolution Protocol), to transmit message via Ethernet RARP(Reverse ARP), to reverse the ARP packets' contents ICMP (Internet Control Message Protocol), wrapped by IP (Book P77) TCP Packet Header The ports are ranging from 1 ~ 65535 (16 bits) Some ports are privileged and reserved for special purposes: Port service 20 FTP-data 21 FTP-command 22 SSH 23 Telnet 25 SMTP 53 DNS 80 WWW 110 POP3 443 https","title":"Linux Network Concepts"},{"location":"Linux/Concepts/Networking-Concepts/#network-and-internet-knowledge-must-knows-for-linux","text":"","title":"Network and Internet Knowledge must knows for Linux"},{"location":"Linux/Concepts/Networking-Concepts/#what-makes-up-a-network","text":"some concepts node: devices having an Internet IP address, such as PC, Linux, ADSL modem, printer, etc. server: devices that serves contents workstation, client: devices that consumes contents from servers Network Interface Card, NIC: provide network access, usually each node has 1+ NIC. network access point: provides IP address topology: the way nodes are connected with each other route or gateway: having 2+ access points, can connect to 2+ different networks. Local Area Network, LAN close transmission distance, like within a building or an university campus. Can use expensive connection materials. Fast. Wide Area Network, WAN far transmission distance, like between cities, and use cheap connection materials. Slower and less reliable.","title":"what makes up a network"},{"location":"Linux/Concepts/Networking-Concepts/#network-protocols","text":"OSI seven layers Open System Interconnection (OSI). From the top layer to the bottom layer: Application Layer: defines how data reach specific application (Presentation Layer): transform data to satisfy network standard (Session Layer): process to process Transport Layer: defines TCP, UDP packet format Network Layer: defines IP protocols, establish/maintain/terminate connections, path selection in routing Data-link Layer: transform/interpret data packet to/from 0s and 1s Physical Layer: physical wires and media; transmit data as 0s and 1s via fibers While TCP/IP uses five of the seven: Application -> Transport -> Network -> Link -> Physical MAC address Each machine has a MAC (Media Access Control) address, that is unique and given when the NIC is manufactured. MAC packets size can vary from 64 bytes to 1500 bytes. IP Packet Header Versions: this IP packet version, like IPv4 IHL (Internet Header Length): tells the length of this IP header Type of Service: [PPPDTRUU] PPP: shows priority of this IP packet D: if 0 means normal latency, 1 means low latency T: if 0 means normal throughput, 1 means high throughput R: if 0 means normal reliability, 1 means high reliability UU: unused by setting this part, Gigabit Ethernet can make this packet high-speed and low-latency Total Length: shows the total length of this packet, including header and data portions. Max 65535 bytes Identification: since MAC frame max size is 1500 bytes, larger packet need to be broken down into smaller pieces, and each will have this Id for reassemble Flags: [0DM] D: if 0 means can be fragmented, 1 means cannot be fragmented M: if 0 means this IP is the last fragment, 1 means it is not the last fragment Fragment Offset: means this IP fragment's position in the original IP packet. Like an ordering number Time To Live (TTL): [0-255], shows how many routers to pass before this packet being discarded Protocol Number: this number represents a specific protocol. 6 is TCP; 17 is UDP; 4 is IP; 1 is ICMP (Internet Control Message Protocol) ... Header Checksum: to check the integrity of this packet's content Source Address: source IP address Destination Address: destination IP address Options: extra parameters, such as security, router, timestamp Padding: use padding to fill bits that are unused by Options IP address composition and classification Example: 192.168.0.0 ~ 192.168.0.255 Here 192.168.0 is the Net_ID, 0 ~ 255 is the Host_ID Within the same network, the Net_ID doesn't change within the same LAN Within the same network, machines can use CSMA/CD broadcast information, and also send message from Host_ID as 0 is reserved for Network IP; and as 255 is reserved for Broadcast IP. IP has five classifications: Class A: Net_ID starts with 0 0xxxxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx |--net--|-----------host----------| 0.xx.xx.xx ~ 127.xx.xx.xx Class B: Net_ID starts with 10 10xxxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx |--net-----------|----------host--| 128.xx.xx.xx ~ 191.xx.xx.xx Class C: Net_ID starts with 110 110xxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx |--net--------------------|-host--| 192.xx.xx.xx ~ 223.xx.xx.xx Class D: Net_ID starts with 1110. For the purpose of multicast 1110xxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx 224.xx.xx.xx ~ 239.xx.xx.xx Class E: Net_ID starts with 1111. Reserved and unused. 1111xxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx 240.xx.xx.xx ~ 255.xx.xx.xx Netmask (Subnet mask) Each Class A's Host_ID range is 0.0.1 ~ 255.255.255, which is definitely too large and broadcasting could be devastating. We need to divide it further to make subnets: to take some bits from Host_ID and make it part of Net_ID. Netmask is the trick to divide one large network segment into smaller ones. Example: 192.168.0.0 ~ 192.168.0.255, a Class C Netmask instruction: First IP: 11000000.10101000.00000000.00000000 Last IP: 11000000.10101000.00000000.11111111 |---------Net_ID----------|-host--| Netmask : 11111111.11111111.11111111.00000000 # binary rep. : 255.255.255.0 # decimal rep. Network: 192.168.0.0 Broadcast: 192.168.0.255 # In this way, the representation of Class ABC are: Class A: 11111111.00000000.00000000.00000000 # 255.0.0.0 Class B: 11111111.11111111.00000000.00000000 # 255.255.0.0 Class C: 11111111.11111111.11111111.00000000 # 255.255.255.0 After dividing the network segment into two subnets: Netmask: 11111111.11111111.11111111.10000000 # 255.255.255.128 First subnet : Network: 11000000.10101000.00000000.0 0000000 Broadcast:11000000.10101000.00000000.0 1111111 |---------Net_ID------------|-host-| Second subnet: Network: 11000000.10101000.00000000.1 0000000 Broadcast:11000000.10101000.00000000.1 1111111 |---------Net_ID------------|-host-| IP types There are two types of IP for IPv4: public IP: planned by INTERNIC, required to get on Internet private IP: cannot be used to get on Internet, used for LAN computers. private IP reserved some IP at A B C classifications of IP: Class A: 10.0.0.0 ~ 10.255.255.255 Class B: 172.16.0.0 ~ 172.31.255.255 CLass C: 192.168.0.0 ~ 192.168.255.255 these IPs are used for communication within a private LAN packets from private IP cannot be sent onto Internet CIDR (Classless Interdomain Routing) Network/Netmask 192.168.0.0/255.255.255.0 192.168.0.0/24 # because Net_ID has 24 bits # since 192.168.0.0 is Class C, when it comes to the need we need to break the rule and make 192.168.0.0 ~ 192.168.255.255 available, we are doing CIDR! loopback IP This IP segment is 127.0.0.0/8 on Class A. The frequently used \"localhost\" is 127.0.0.1, and there is no need for NIC. How IP is assigned static IP: check for usable IP and directly set your IP to a fixed value. dial-up IP: check in with your ISP with acocunt and password and get an IP assigned to you. DHCP: one machine designated for assigning IP to other machines IP and MAC: Link Layer ARP and RARP ARP (Address Resolution Protocol), to transmit message via Ethernet RARP(Reverse ARP), to reverse the ARP packets' contents ICMP (Internet Control Message Protocol), wrapped by IP (Book P77) TCP Packet Header The ports are ranging from 1 ~ 65535 (16 bits) Some ports are privileged and reserved for special purposes: Port service 20 FTP-data 21 FTP-command 22 SSH 23 Telnet 25 SMTP 53 DNS 80 WWW 110 POP3 443 https","title":"Network protocols"},{"location":"Linux/Concepts/Server-Basics/","text":"Linux Server Setup and Maintain \u00b6 netstat IP 127.0.0.1 - available on the local machine only. 0.0.0.0 - open to the Internet Frequently used ports 80: www 22: ssh 21: ftp 25: mail 111: RPC (remote process control) 631: CPUS (printer services) Install CentOS requirements on P15 Linux Error Debugging \u00b6 Skipped, Book2 P194 Linux Network Security \u00b6 Linux already comes with two layers of Firewall IP Filtering or Net Filtering It analyzes TCP/IP header to filter unwanted packets TCP Wrappers /etc/hosts.allow and /etc/hosts.deny settings decide whether to use the packet. Linux Software Distribution and Update \u00b6 RPM one of the most popular Linux Software package manager among CentOS, Fedora, SuSE, Red Hat, Mandriva, etc. Packages are precompiled binaries. Tarball Use source code to compile and install. Every update need recompile. dpkg Provided with Debian Linux. Packages are precompiled binaries. To achieve Auto-updates, tarball is not a choice to have. yum CentOS and Fedora used over FTP or WWW to auto update software apt from Debian. Portable. you Yast Online Update, by SuSE urpmi Mandriva Linux yum install, search, list, info, grouplist, groupinstall ... Book2 P226 yum update will update the OS netstat -tunp show service, PID, IP and port nmap -sTU localhost show all TCP/UDP ports nmap -sP localhost show all hosts in current LAN Find and stop a service which rpcbind rpm -qf /sbin/rpcbind rpm -qc rpcbind | grep init /etc/init.d/rpcbind stop Book2 P242 SELinux Routing and Router \u00b6 Router is for deciding packet path and direction. Use route to view routing and set preferences. Use traceroute to analyze each packet flow. Dynamic routing can be achieved with the help of software. IP Aliasing can make virtual connector to make the NIC having multiple IP. good for testing easy way to having computers within the same network segment can use ifconfig to achieve this: ifconfig [device] [IP] netmask [netmask IP] [up|down] modify /etc/sysctl.conf to turn a Linux machine into a router: set net.ipv4.ip_forward=1 The way Linux sets routing table differs by: static routing: set routing table from Kernel. dynamic routing: using software like Quagga or Zebra to monitor network changes and update Linux Kernel routing tables Turn Linux into a Router Book2 P277 Firewall and NAT Server \u00b6 Firewall is used to filter Internet packets according to some rules. NAT (Network Address Translation) Server is like an IP sharer with added functionality of IP translation. Linux NAT server can change packet IP header to target IP, to let private IP packet change into having a public IP and connect to Internet. Netfilter analyze the Internet packets enters the machine, analyze their headers and decide to proceed or block. Netfilter provides iptables for filtering command. refuse some packets to enter some machines refuse some packets from specific IPs refuse some packets with specific flags refuse some packets by its MAC address TCP Wrappers analyze programs' access to Networks according to a set of rules. Proxy Server The proxy server carries out the request of its clients. Client could be offline from Internet, only need to connect to the proxy server. This single Proxy Server can be combined with a firewall to keep all machines behind the wall safe from attacks. Can also monitor the traffic in/out. iptables iptables [-t tables] [-L] [-nv] -t: follows table, like nat or filter -L: list table rules -n: not look up IP and hostname, faster -v: show more info like number of packet, connection.. What's being shown: target: ACCEPT, REJECT, or DROP prot: protocol: tcp, udp, icmp, or all opt: other optional info source: this rule is valid on source IP destination: this rule is valid on destination IP iptables-save can list the complete unformatted information more on rules setting on Book2 P318 It is best to use scripts to setup firewall. NAT Server NAT can change the source/destination IP and ports info from the packets. i.e. : (SNAT) when a machine in the LAN sent a request out, it first reaches the router and the NAT replaces its source IP with a public IP (this mapping will be stored in NAT table) and the packet is forwarded onto the Internet. (DNAT) When a packet of response is back, it reaches the router and the NAT replaces its public IP with the machine IP from the mapping in NAT table. Then this packet continue to reach its target machine in the LAN. Settings using iptables see Book2 P341 iptables -t nat -A POSTROUTING -o eth0 -j SNAT --to-source 192.168.1.210-192.168.1.220 iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j DNAT --to-destination 192.168.100.10:80 Domain and Static IP \u00b6 each router uses DHCP to get a public IP from ISP periodatically. If your server is built in this case, for the outside to know its IP, need to have the new IP udpated with DNSs periodically as well. Servers in LAN \u00b6 Remote Server Remote Server allows sharing Unix Like machines computing power Allows you to change server/machine settings anytime anywhere.","title":"Basic Linux Server Concepts & Commands"},{"location":"Linux/Concepts/Server-Basics/#linux-server-setup-and-maintain","text":"netstat IP 127.0.0.1 - available on the local machine only. 0.0.0.0 - open to the Internet Frequently used ports 80: www 22: ssh 21: ftp 25: mail 111: RPC (remote process control) 631: CPUS (printer services) Install CentOS requirements on P15","title":"Linux Server Setup and Maintain"},{"location":"Linux/Concepts/Server-Basics/#linux-error-debugging","text":"Skipped, Book2 P194","title":"Linux Error Debugging"},{"location":"Linux/Concepts/Server-Basics/#linux-network-security","text":"Linux already comes with two layers of Firewall IP Filtering or Net Filtering It analyzes TCP/IP header to filter unwanted packets TCP Wrappers /etc/hosts.allow and /etc/hosts.deny settings decide whether to use the packet.","title":"Linux Network Security"},{"location":"Linux/Concepts/Server-Basics/#linux-software-distribution-and-update","text":"RPM one of the most popular Linux Software package manager among CentOS, Fedora, SuSE, Red Hat, Mandriva, etc. Packages are precompiled binaries. Tarball Use source code to compile and install. Every update need recompile. dpkg Provided with Debian Linux. Packages are precompiled binaries. To achieve Auto-updates, tarball is not a choice to have. yum CentOS and Fedora used over FTP or WWW to auto update software apt from Debian. Portable. you Yast Online Update, by SuSE urpmi Mandriva Linux yum install, search, list, info, grouplist, groupinstall ... Book2 P226 yum update will update the OS netstat -tunp show service, PID, IP and port nmap -sTU localhost show all TCP/UDP ports nmap -sP localhost show all hosts in current LAN Find and stop a service which rpcbind rpm -qf /sbin/rpcbind rpm -qc rpcbind | grep init /etc/init.d/rpcbind stop Book2 P242 SELinux","title":"Linux Software Distribution and Update"},{"location":"Linux/Concepts/Server-Basics/#routing-and-router","text":"Router is for deciding packet path and direction. Use route to view routing and set preferences. Use traceroute to analyze each packet flow. Dynamic routing can be achieved with the help of software. IP Aliasing can make virtual connector to make the NIC having multiple IP. good for testing easy way to having computers within the same network segment can use ifconfig to achieve this: ifconfig [device] [IP] netmask [netmask IP] [up|down] modify /etc/sysctl.conf to turn a Linux machine into a router: set net.ipv4.ip_forward=1 The way Linux sets routing table differs by: static routing: set routing table from Kernel. dynamic routing: using software like Quagga or Zebra to monitor network changes and update Linux Kernel routing tables Turn Linux into a Router Book2 P277","title":"Routing and Router"},{"location":"Linux/Concepts/Server-Basics/#firewall-and-nat-server","text":"Firewall is used to filter Internet packets according to some rules. NAT (Network Address Translation) Server is like an IP sharer with added functionality of IP translation. Linux NAT server can change packet IP header to target IP, to let private IP packet change into having a public IP and connect to Internet. Netfilter analyze the Internet packets enters the machine, analyze their headers and decide to proceed or block. Netfilter provides iptables for filtering command. refuse some packets to enter some machines refuse some packets from specific IPs refuse some packets with specific flags refuse some packets by its MAC address TCP Wrappers analyze programs' access to Networks according to a set of rules. Proxy Server The proxy server carries out the request of its clients. Client could be offline from Internet, only need to connect to the proxy server. This single Proxy Server can be combined with a firewall to keep all machines behind the wall safe from attacks. Can also monitor the traffic in/out. iptables iptables [-t tables] [-L] [-nv] -t: follows table, like nat or filter -L: list table rules -n: not look up IP and hostname, faster -v: show more info like number of packet, connection.. What's being shown: target: ACCEPT, REJECT, or DROP prot: protocol: tcp, udp, icmp, or all opt: other optional info source: this rule is valid on source IP destination: this rule is valid on destination IP iptables-save can list the complete unformatted information more on rules setting on Book2 P318 It is best to use scripts to setup firewall. NAT Server NAT can change the source/destination IP and ports info from the packets. i.e. : (SNAT) when a machine in the LAN sent a request out, it first reaches the router and the NAT replaces its source IP with a public IP (this mapping will be stored in NAT table) and the packet is forwarded onto the Internet. (DNAT) When a packet of response is back, it reaches the router and the NAT replaces its public IP with the machine IP from the mapping in NAT table. Then this packet continue to reach its target machine in the LAN. Settings using iptables see Book2 P341 iptables -t nat -A POSTROUTING -o eth0 -j SNAT --to-source 192.168.1.210-192.168.1.220 iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j DNAT --to-destination 192.168.100.10:80","title":"Firewall and NAT Server"},{"location":"Linux/Concepts/Server-Basics/#domain-and-static-ip","text":"each router uses DHCP to get a public IP from ISP periodatically. If your server is built in this case, for the outside to know its IP, need to have the new IP udpated with DNSs periodically as well.","title":"Domain and Static IP"},{"location":"Linux/Concepts/Server-Basics/#servers-in-lan","text":"Remote Server Remote Server allows sharing Unix Like machines computing power Allows you to change server/machine settings anytime anywhere.","title":"Servers in LAN"},{"location":"Linux/Concepts/Shell_Scripting/","text":"Shell Scripting \u00b6 Advantages of shell scripting: automate tasks and reduce risk of errors combine long and repetitive sequences of commands into a single command share procedures among several users provide controlled interface to users create new commands using combination of utilities and logic quick prototyping, no compilation The first line of a shell script, which starts with #! , contains the full path of the command interpreter . Commonly used script interpreters: /usr/bin/perl /bin/sh, include its varieties: /bin/bash /bin/csh /bin/tcsh /bin/ksh /bin/zsh /usr/bin/python A shell script can be started with bash , or ./script_name or full_path_to_script when the user executing it has the priviledge to execute it. Basic syntax and special characters: Character Description # Used to add a comment, except when used as #, or as #! when starting a script \\ Used at the end of a line to indicate continuation on to the next line ; Used to interpret what follows as a new command to be executed next $ Indicates what follows is an environment variable > Redirect output >> Append output < Redirect input | Used to pipe the result into the next command All shell scripts generate a return value (stored in $? shell variable) upon finishing execution, which can be explicitly set with the exit statement. Return values permit a process to monitor the exit state of another process to take actions for different scenarios. Use the concatenation operator \\ (the backslash character) to split long commands over multiple lines , but still executed as one command. The ; (semicolon) character is used to separate commands and execute them sequentially regardless the return value of each command. If subsequent commands run only when previous command succeeds (return value 0), use the && (and) operator to chain the commands. The || (or) runs a chain of commands until one succeeds (return value 0). Chaining commands is noNOTt the same as piping them; with piped commands, succeeding commands begin operating on data streams produced by earlier ones before they complete, while in chaining each step exits before the next one starts. Shell scripts always have access to applications such as rm, ls, df, vi, gzip (aka shell built-ins full list can be viewed by help command), which are programs compiled from lower level programming languages such as C . Other compiled applications are binary executable files residing in /usr/bin . Within a script, the parameter or an argument is represented with a $ and a number or special character . $0 is the script name; $1 $2 ... is the first, second, ... argument; $* has all arguments; $# has number of arguments. You may need to substitute the result of a command as a portion of another command, using $() or within backticks (`) so that the specified command will be executed in a newly launched shell environment, and the standard output of the shell will be inserted where the command substitution is done. Get a list of environment variables with the env, set, printenv commands. By default, the variables created within a script are available only to the subsequent steps of that script. Any child processes (sub-shells) do NOT have automatic access to the values of these variables; to make them available, export the variables. Typing export with no arguments will give a list of all currently exported environment variables. A function is a code block that implements a set of operations. Functions are useful for executing procedures multiple times, perhaps with varying input variables (the first argument can be referred to as $1, the second as $2, and so on). function_name () { command... } The if statement takes actions depend on the evaluation of specified conditions. In bash scripts, the condition block uses double brackets instead of single brackets (supported by sh), which allows more enhanced conditional syntax. if condition then statements elif condition2 statements else statements fi # compact form if TEST-COMMANDS ; then CONSEQUENT-COMMANDS ; fi Some condition operators on files in this table. Full list of conditions use man 1 test . Boolean expressions operators are no surprise && || ! Condition Meaning -e file Checks if the file exists. -d file Checks if the file is a directory. -f file Checks if the file is a regular file (i.e. not a symbolic link, device node, directory, etc.) -s file Checks if the file is of non-zero size. -g file Checks if the file has sgid set. -u file Checks if the file has suid set. -r file Checks if the file is readable. -w file Checks if the file is writable. -x file Checks if the file is executable. When comparing numbers , use -gt -lt -ge -le -eq -ne ; comparing strings ue == != or > < for their sorting order. For arithmetic expressions, use $(()) , let shell built-in, or expr utility. i.e. X=$(expr $ARG + 8) is same as X=$((ARG + 8)) and same as let X=( $ARG + 8 ) . A string variable contains a sequence of text characters. It can include letters, numbers, symbols and punctuation marks. Access string length with ${#VAR_NAME} . To extract the first n characters of a string we can specify: ${string:0:n} ; To extract all characters in a string after a dot (.), use the following expression: ${string#*.} , so anything after #* is the pattern we are looking for. The case statement is used in scenarios where the actual value of a variable can lead to different execution paths, usually used for handling command-line options. It is readable and easy to compare many values at once. case expression in pattern1 ) execute commands ;; pattern2 ) execute commands ;; pattern3 ) execute commands ;; pattern4 | pattern5 | pattern6 ) execute commands ;; * ) execute some default commands or nothing ;; esac There are three types of loop constructs , for, while, until . # for must operate on each item in a list of items for variable-name in list # list can be a variable, a command, or hard-coded values separated by space do execute one iteration for each item in the list until the list is finished done # while loop goes on as long as condition is evalutated as true while condition do Commands for execution ---- done # until loop goes on as long as condition is evalutated as false until condition do Commands for execution ---- done A command (without brackets) can be used as condition so that its return value is evaluated: 0 is true and other values are false. Debugging bash scripts either by running it with bash -x ./script or bracketing parts of the script code with set -x and set +x . The debug mode traces and prefixes each command with the + character, and prints each command's output. Temporary files (and directories) are meant to store data for a short time. Usually, one should arrange it so that these files disappear when the program using them terminates. To do so use the mktemp utility. i.e. TEMP=$(mktemp /tmp/tempfile.XXXXXXXX) and for temp dir TEMPDIR=$(mktemp -d /tmp/tempdir.XXXXXXXX) . XXXXXXXX is replaced by mktemp with random characters to ensure the name of the temporary file cannot be easily predicted and is only known within your program. It is often useful to generate random numbers and other random data for certain tasks. Such random numbers can be easily generated (from the entropy pool) by using the $RANDOM environment variable . /dev/random is used where very high quality randomness is required, such as one-time pad or key generation, but it is relatively slow to provide values. /dev/urandom reuses the internal pool to produce more pseudo-random bits .","title":"Shell Scripting"},{"location":"Linux/Concepts/Shell_Scripting/#shell-scripting","text":"Advantages of shell scripting: automate tasks and reduce risk of errors combine long and repetitive sequences of commands into a single command share procedures among several users provide controlled interface to users create new commands using combination of utilities and logic quick prototyping, no compilation The first line of a shell script, which starts with #! , contains the full path of the command interpreter . Commonly used script interpreters: /usr/bin/perl /bin/sh, include its varieties: /bin/bash /bin/csh /bin/tcsh /bin/ksh /bin/zsh /usr/bin/python A shell script can be started with bash , or ./script_name or full_path_to_script when the user executing it has the priviledge to execute it. Basic syntax and special characters: Character Description # Used to add a comment, except when used as #, or as #! when starting a script \\ Used at the end of a line to indicate continuation on to the next line ; Used to interpret what follows as a new command to be executed next $ Indicates what follows is an environment variable > Redirect output >> Append output < Redirect input | Used to pipe the result into the next command All shell scripts generate a return value (stored in $? shell variable) upon finishing execution, which can be explicitly set with the exit statement. Return values permit a process to monitor the exit state of another process to take actions for different scenarios. Use the concatenation operator \\ (the backslash character) to split long commands over multiple lines , but still executed as one command. The ; (semicolon) character is used to separate commands and execute them sequentially regardless the return value of each command. If subsequent commands run only when previous command succeeds (return value 0), use the && (and) operator to chain the commands. The || (or) runs a chain of commands until one succeeds (return value 0). Chaining commands is noNOTt the same as piping them; with piped commands, succeeding commands begin operating on data streams produced by earlier ones before they complete, while in chaining each step exits before the next one starts. Shell scripts always have access to applications such as rm, ls, df, vi, gzip (aka shell built-ins full list can be viewed by help command), which are programs compiled from lower level programming languages such as C . Other compiled applications are binary executable files residing in /usr/bin . Within a script, the parameter or an argument is represented with a $ and a number or special character . $0 is the script name; $1 $2 ... is the first, second, ... argument; $* has all arguments; $# has number of arguments. You may need to substitute the result of a command as a portion of another command, using $() or within backticks (`) so that the specified command will be executed in a newly launched shell environment, and the standard output of the shell will be inserted where the command substitution is done. Get a list of environment variables with the env, set, printenv commands. By default, the variables created within a script are available only to the subsequent steps of that script. Any child processes (sub-shells) do NOT have automatic access to the values of these variables; to make them available, export the variables. Typing export with no arguments will give a list of all currently exported environment variables. A function is a code block that implements a set of operations. Functions are useful for executing procedures multiple times, perhaps with varying input variables (the first argument can be referred to as $1, the second as $2, and so on). function_name () { command... } The if statement takes actions depend on the evaluation of specified conditions. In bash scripts, the condition block uses double brackets instead of single brackets (supported by sh), which allows more enhanced conditional syntax. if condition then statements elif condition2 statements else statements fi # compact form if TEST-COMMANDS ; then CONSEQUENT-COMMANDS ; fi Some condition operators on files in this table. Full list of conditions use man 1 test . Boolean expressions operators are no surprise && || ! Condition Meaning -e file Checks if the file exists. -d file Checks if the file is a directory. -f file Checks if the file is a regular file (i.e. not a symbolic link, device node, directory, etc.) -s file Checks if the file is of non-zero size. -g file Checks if the file has sgid set. -u file Checks if the file has suid set. -r file Checks if the file is readable. -w file Checks if the file is writable. -x file Checks if the file is executable. When comparing numbers , use -gt -lt -ge -le -eq -ne ; comparing strings ue == != or > < for their sorting order. For arithmetic expressions, use $(()) , let shell built-in, or expr utility. i.e. X=$(expr $ARG + 8) is same as X=$((ARG + 8)) and same as let X=( $ARG + 8 ) . A string variable contains a sequence of text characters. It can include letters, numbers, symbols and punctuation marks. Access string length with ${#VAR_NAME} . To extract the first n characters of a string we can specify: ${string:0:n} ; To extract all characters in a string after a dot (.), use the following expression: ${string#*.} , so anything after #* is the pattern we are looking for. The case statement is used in scenarios where the actual value of a variable can lead to different execution paths, usually used for handling command-line options. It is readable and easy to compare many values at once. case expression in pattern1 ) execute commands ;; pattern2 ) execute commands ;; pattern3 ) execute commands ;; pattern4 | pattern5 | pattern6 ) execute commands ;; * ) execute some default commands or nothing ;; esac There are three types of loop constructs , for, while, until . # for must operate on each item in a list of items for variable-name in list # list can be a variable, a command, or hard-coded values separated by space do execute one iteration for each item in the list until the list is finished done # while loop goes on as long as condition is evalutated as true while condition do Commands for execution ---- done # until loop goes on as long as condition is evalutated as false until condition do Commands for execution ---- done A command (without brackets) can be used as condition so that its return value is evaluated: 0 is true and other values are false. Debugging bash scripts either by running it with bash -x ./script or bracketing parts of the script code with set -x and set +x . The debug mode traces and prefixes each command with the + character, and prints each command's output. Temporary files (and directories) are meant to store data for a short time. Usually, one should arrange it so that these files disappear when the program using them terminates. To do so use the mktemp utility. i.e. TEMP=$(mktemp /tmp/tempfile.XXXXXXXX) and for temp dir TEMPDIR=$(mktemp -d /tmp/tempdir.XXXXXXXX) . XXXXXXXX is replaced by mktemp with random characters to ensure the name of the temporary file cannot be easily predicted and is only known within your program. It is often useful to generate random numbers and other random data for certain tasks. Such random numbers can be easily generated (from the entropy pool) by using the $RANDOM environment variable . /dev/random is used where very high quality randomness is required, such as one-time pad or key generation, but it is relatively slow to provide values. /dev/urandom reuses the internal pool to produce more pseudo-random bits .","title":"Shell Scripting"},{"location":"Linux/References/Account-Administration/","text":"This notes covers users accounts on Linux. login and non-login shell \u00b6 login shell - need to go through login process each time started non-login shell - no need login, i.e. after you login through X window, your bash assignment required no password again, neither when you initiated bash terminal there. The two shells read different configuration files login shell: /etc/profile : overall system settings, don't change vars like PATH, MAIL, USER, HOSTNAME, HISTSIZE call other config files, like /etc/inputrc , /etc/profile.d/*.sh , /etc/sysconfig/i18n ~/.bash_profile or ~/.bash_login or ~/.profile : personal settings non-login shell: ~/.bashrc key_press - - results ctrl-c interrupt ctrl-d enter EOF ctrl-m like enter ctrl-s pulse screen output ctrl-q continue screen output ctrl-u delete a line of command ctrl-z pulse current process tty use [alt-ctrl-(F1-F6)] to access tty1-6 welcome message at login is in /etc/issue welcome message at login for telnet is in /etc/issue.net message after login is in /etc/motd User Identification \u00b6 UID and GID Linux use UID to identify users. These information are stored in /etc/passwd. Restrictions: id range feature 0 system admin, or root. Other account UID could be changed to 0 to get root access but not recommended [1, 499] system account, reserved to system services, not useable by users to login [500, 2^23-1] for normal users Similar concept for GID, in /etc/group Use id to check some user's UID GID and groups info. id user_name Linux Login process locate account in /etc/passwd , read UID and GID, as well as home directory and shell settings. check password from /etc/shadow by looking up UID password match, login success, control access granted A line in /etc/passwd looks like root:x:0:0:root:/root:/bin/bash parts are: Account name Password. Not here anymore, in /etc/shadow instead UID GID User info home directory shell A line in /etc/shadow looks like root:$1$/30QpE5e$y9N/D0bh6rAACBEz.hqo00:14126:0:99999:7::: parts are: Account name Password (encrypted) Recent changed date (in number of days from 1970/1/1) Password unchangeable for N days Password required changes in N days Password change remainder N days before due Password expiration tolerance in N days Password expiration date (like 3) reserved passwd can change the password of current user. When forgot root password reboot and enter single-user maintenance mode, use passwd to change it. boot from CD, mount /root and change /etc/shadow , delete the password part (next login root requires no password), then login to root use passwd to change it. Groups \u00b6 A line in /etc/group looks like root:x:0:root parts are: Group name Group password (not here anymore, in /etc/gshadow) GID All accounts that joined this group Effective group and Initial group initial group : the group access given upon successful login. 4th column in /etc/passwd effective group : the group that is under effect now. It can be the group current user belongs to, user can own the group access of the file groups shows the groups this user belongs to the first group output is the effective group use newgrp to switch to other group as effective group (starts a new shell. Need to exit if wish to go back to previous shell) root can add a user into a group using usermod . It can also be done by the group admin using gpasswd A line in /etc/gshadow looks like root:::root parts are: Group name Password Group admin account All accounts joined this group This file's purpose is to add group admins for each group, for which group admin can help root add user to a group. User management \u00b6 useradd create a user useradd [-u UID] [-g init_group] [-G secondary_group] [-mM] [-c notes] [-d home_dir_path] [-s shell_path] user_name use passwd user_name to change password and add encryption the default values come from /etc/default/useradd, /etc/login.defs, and /etc/skel/* passwd , chage can both edit user passwords usermod can change existing account's info, like home directory, password expiration date, freeze account, etc. usermod [-cdegGlsuLU] username userdel deletes user and all data associated with /etc/passwd, /etc/shadow, /etc/group, /etc/gshadow, etc. userdel [-r] user_name -r means delete home dir as well finger lookup user related information. finger [-s] user_name use finger by it self will list current users logged in use chfn to add more info about a user: chfn [-foph] user_name Forgot to add a home directory after creating a user? Follow this: ll -d ~guest # make sure it is not there cp -a /etc/skel /home/guest chown -R guest:guest /home/guest chmod 700 /home/guest Group management \u00b6 groupadd add a new group groupadd [-g gid] [-r] group_name -r setup system group groupmod modify a group groupmod [-g gid] [-n new_group_name] group_name groupdel delete a group groupdel group_name gpasswd to add a group admin or change a group's password gpasswd [-A user1, ...] [-M user, ...] [-rR] group_name -A (root) -a (group admin) give users admin privilege -d remove users from group admin -M add users to this group -r remove password -R invalidate group password Linux ACL Privilege \u00b6 ACL stands for Access Control List, applies to individual privileges on top of the traditional owner, group, others access rights. AcCL supports access for single user for single file rwx setting. More on Book P505. sudo \u00b6 sudo is a handy command to become root for only executing a command with root's privilege. A user must be added to /etc/sudoers to be able to use sudo . The command to do so is visudo . More on P512 on Book Special shell \u00b6 /sbin/nologin use this shell to limit the user account to not able to log onto the system shell. i.e. when the user should have only the mail server access. PAM (Pluggable Authentication Modules) \u00b6 Many program uses PAM for password functions Take an example as a call to passwd 1. user exec /usr/bin/passwd , and enter password 2. passwd calls PAM module for verification 3. PAM will check /etc/pam.d and look for passwd configuration 4. based on /etc/pam.d/passwd , use corresponding PAM module to verify 5. return result to passwd 6. passwd decide next action /etc/pam.d/passwd What is inside the file: auth include system-auth account include system-auth password include system-auth ( Type ) ( Flag ) ( Parameter ) Type: auth: authentication, check user identification account: authorization, check whether user has specific access session: what env setting available for this session password: change password Control-flag: required: required check, on success/failure still proceed requisite: must be checked success to proceed, otherwise return failure sufficient: final checking step optional: mostly just showing information, not for checking include: call next word for verification password include system-auth means call system-auth instead See on Book P521 PAM can set a disk quota for each user by changing /etc/security/limits.conf user_name soft fsize 90000 # gives warning when reach this much, in KB user_name hard fsize 100000 # cannot exceed this @group_name soft fsize 100000 # limit for groups use ulimit can check usage limits, ulimit -a login error logs /var/log/secure and /var/log/messages contains logs for users logging into this machine look up usage of a user w and who shows users currently logged in lastlog shows last login time for each user message or broadcast write <user_name> will directly display a message to that user mesg n to disable receiving, mesg y to enable wall \"broadcast: message\" to let a message show for all users mail use mail to receive/send mails in Linux mail username@host -s \"subject\" Hello nice to meet you. Bye! . # this dot must be there to finish the draft you can write email in a file first then mail user -s \"subject\" < filename","title":"Manage Accounts & Groups"},{"location":"Linux/References/Account-Administration/#login-and-non-login-shell","text":"login shell - need to go through login process each time started non-login shell - no need login, i.e. after you login through X window, your bash assignment required no password again, neither when you initiated bash terminal there. The two shells read different configuration files login shell: /etc/profile : overall system settings, don't change vars like PATH, MAIL, USER, HOSTNAME, HISTSIZE call other config files, like /etc/inputrc , /etc/profile.d/*.sh , /etc/sysconfig/i18n ~/.bash_profile or ~/.bash_login or ~/.profile : personal settings non-login shell: ~/.bashrc key_press - - results ctrl-c interrupt ctrl-d enter EOF ctrl-m like enter ctrl-s pulse screen output ctrl-q continue screen output ctrl-u delete a line of command ctrl-z pulse current process tty use [alt-ctrl-(F1-F6)] to access tty1-6 welcome message at login is in /etc/issue welcome message at login for telnet is in /etc/issue.net message after login is in /etc/motd","title":"login and non-login shell"},{"location":"Linux/References/Account-Administration/#user-identification","text":"UID and GID Linux use UID to identify users. These information are stored in /etc/passwd. Restrictions: id range feature 0 system admin, or root. Other account UID could be changed to 0 to get root access but not recommended [1, 499] system account, reserved to system services, not useable by users to login [500, 2^23-1] for normal users Similar concept for GID, in /etc/group Use id to check some user's UID GID and groups info. id user_name Linux Login process locate account in /etc/passwd , read UID and GID, as well as home directory and shell settings. check password from /etc/shadow by looking up UID password match, login success, control access granted A line in /etc/passwd looks like root:x:0:0:root:/root:/bin/bash parts are: Account name Password. Not here anymore, in /etc/shadow instead UID GID User info home directory shell A line in /etc/shadow looks like root:$1$/30QpE5e$y9N/D0bh6rAACBEz.hqo00:14126:0:99999:7::: parts are: Account name Password (encrypted) Recent changed date (in number of days from 1970/1/1) Password unchangeable for N days Password required changes in N days Password change remainder N days before due Password expiration tolerance in N days Password expiration date (like 3) reserved passwd can change the password of current user. When forgot root password reboot and enter single-user maintenance mode, use passwd to change it. boot from CD, mount /root and change /etc/shadow , delete the password part (next login root requires no password), then login to root use passwd to change it.","title":"User Identification"},{"location":"Linux/References/Account-Administration/#groups","text":"A line in /etc/group looks like root:x:0:root parts are: Group name Group password (not here anymore, in /etc/gshadow) GID All accounts that joined this group Effective group and Initial group initial group : the group access given upon successful login. 4th column in /etc/passwd effective group : the group that is under effect now. It can be the group current user belongs to, user can own the group access of the file groups shows the groups this user belongs to the first group output is the effective group use newgrp to switch to other group as effective group (starts a new shell. Need to exit if wish to go back to previous shell) root can add a user into a group using usermod . It can also be done by the group admin using gpasswd A line in /etc/gshadow looks like root:::root parts are: Group name Password Group admin account All accounts joined this group This file's purpose is to add group admins for each group, for which group admin can help root add user to a group.","title":"Groups"},{"location":"Linux/References/Account-Administration/#user-management","text":"useradd create a user useradd [-u UID] [-g init_group] [-G secondary_group] [-mM] [-c notes] [-d home_dir_path] [-s shell_path] user_name use passwd user_name to change password and add encryption the default values come from /etc/default/useradd, /etc/login.defs, and /etc/skel/* passwd , chage can both edit user passwords usermod can change existing account's info, like home directory, password expiration date, freeze account, etc. usermod [-cdegGlsuLU] username userdel deletes user and all data associated with /etc/passwd, /etc/shadow, /etc/group, /etc/gshadow, etc. userdel [-r] user_name -r means delete home dir as well finger lookup user related information. finger [-s] user_name use finger by it self will list current users logged in use chfn to add more info about a user: chfn [-foph] user_name Forgot to add a home directory after creating a user? Follow this: ll -d ~guest # make sure it is not there cp -a /etc/skel /home/guest chown -R guest:guest /home/guest chmod 700 /home/guest","title":"User management"},{"location":"Linux/References/Account-Administration/#group-management","text":"groupadd add a new group groupadd [-g gid] [-r] group_name -r setup system group groupmod modify a group groupmod [-g gid] [-n new_group_name] group_name groupdel delete a group groupdel group_name gpasswd to add a group admin or change a group's password gpasswd [-A user1, ...] [-M user, ...] [-rR] group_name -A (root) -a (group admin) give users admin privilege -d remove users from group admin -M add users to this group -r remove password -R invalidate group password","title":"Group management"},{"location":"Linux/References/Account-Administration/#linux-acl-privilege","text":"ACL stands for Access Control List, applies to individual privileges on top of the traditional owner, group, others access rights. AcCL supports access for single user for single file rwx setting. More on Book P505.","title":"Linux ACL Privilege"},{"location":"Linux/References/Account-Administration/#sudo","text":"sudo is a handy command to become root for only executing a command with root's privilege. A user must be added to /etc/sudoers to be able to use sudo . The command to do so is visudo . More on P512 on Book","title":"sudo"},{"location":"Linux/References/Account-Administration/#special-shell","text":"/sbin/nologin use this shell to limit the user account to not able to log onto the system shell. i.e. when the user should have only the mail server access.","title":"Special shell"},{"location":"Linux/References/Account-Administration/#pam-pluggable-authentication-modules","text":"Many program uses PAM for password functions Take an example as a call to passwd 1. user exec /usr/bin/passwd , and enter password 2. passwd calls PAM module for verification 3. PAM will check /etc/pam.d and look for passwd configuration 4. based on /etc/pam.d/passwd , use corresponding PAM module to verify 5. return result to passwd 6. passwd decide next action /etc/pam.d/passwd What is inside the file: auth include system-auth account include system-auth password include system-auth ( Type ) ( Flag ) ( Parameter ) Type: auth: authentication, check user identification account: authorization, check whether user has specific access session: what env setting available for this session password: change password Control-flag: required: required check, on success/failure still proceed requisite: must be checked success to proceed, otherwise return failure sufficient: final checking step optional: mostly just showing information, not for checking include: call next word for verification password include system-auth means call system-auth instead See on Book P521 PAM can set a disk quota for each user by changing /etc/security/limits.conf user_name soft fsize 90000 # gives warning when reach this much, in KB user_name hard fsize 100000 # cannot exceed this @group_name soft fsize 100000 # limit for groups use ulimit can check usage limits, ulimit -a login error logs /var/log/secure and /var/log/messages contains logs for users logging into this machine look up usage of a user w and who shows users currently logged in lastlog shows last login time for each user message or broadcast write <user_name> will directly display a message to that user mesg n to disable receiving, mesg y to enable wall \"broadcast: message\" to let a message show for all users mail use mail to receive/send mails in Linux mail username@host -s \"subject\" Hello nice to meet you. Bye! . # this dot must be there to finish the draft you can write email in a file first then mail user -s \"subject\" < filename","title":"PAM (Pluggable Authentication Modules)"},{"location":"Linux/References/Freq-Tasks/","text":"Not a place to explain things, but for some quick reference to do things in Linux Add_User_To_Sudoers \u00b6 # first add user to group sudo usermod -aG sudo <user> # makesure this line is in /etc/sudoers %sudo ALL =( ALL ) ALL Benchmarking_Flash_Drive \u00b6 # General command to use: # 1. create a large file for benchmarking from device 0 dd if = /dev/zero of = ./largefile bs = 1M count = 1024 # 2. Write to a drive dd if = path/to/input_file of = /path/to/output_file bs = block_size count = number_of_blocks # Reference: # https://www.binarytides.com/linux-test-drive-speed/ # alternatively, use Ubuntu's disk software for benchmark Boot_into_another_OS_of_Different_GRUB_Version \u00b6 # https://howtoubuntu.org/how-to-repair-restore-reinstall-grub-2-with-a-ubuntu-live-cd lsblk # find out which disk to boot into sudo mount -t ext4 /dev/sdXY /mnt sudo mount --bind /dev /mnt/dev && sudo mount --bind /dev/pts /mnt/dev/pts && sudo mount --bind /proc /mnt/proc && sudo mount --bind /sys /mnt/sys sudo chroot /mnt grub-install /dev/sdX grub-install --recheck /dev/sdX update-grub exit && sudo umount /mnt/sys /mnt/proc /mnt/dev/pts /mnt/dev && sudo umount /mnt Burn_Image_To_Flash_Drive \u00b6 lsblk -p # -> see which devices are connected umount /dev/sdX [ 1 -9 ] # -> unmount the usb device drives # -> replace 'X' with the device letter [a-z] dd bs = 4M if = path/to/linux_image.img of = /dev/sdX conv = fsync status = progress # -> the command to copy the image into the drive # -> replace 'X' with the device letter [a-z] sync # flush out write cache so the drive is okay to unmount Change_hostname_on_Linux_machine \u00b6 sudo hostnamectl --transient set-hostname $hostname sudo hostnamectl --static set-hostname $hostname sudo hostnamectl --pretty set-hostname $hostname sudo sed -i s/<old-hostname>/ $hostname /g /etc/hosts Change_RaspberryPi_OS_Keyboard \u00b6 sudo nano /etc/default/keyboard # update 'XKBLAYOUT' to 'us' Check_CPU_Temperature/Volt \u00b6 # On RaspberryPi: /opt/vc/bin/vcgencmd measure_temp /opt/vc/bin/vcgencmd measure_volts core # view a list of available commands: /opt/vc/bin/vcgencmd commands # Install a tool called lm-sensors sudo apt install lm-sensors sudo -i sensors-detect sensors # to check various sensor temperatures # Note this method does not work if the system does not have any sensors attached to it. i.e. like a RaspberryPi Check_USB_Devices \u00b6 lsusb # view a list of plugged in devices via USB ports dmesg # view the log related to USB devices Configuring_Locale_Settings \u00b6 export LANGUAGE = en_US.UTF-8 export LANG = en_US.UTF-8 export LC_ALL = en_US.UTF-8 locale-gen en_US.UTF-8 dpkg-reconfigure locales Create_SSH_Key_Pair \u00b6 # creates an SSH key pair using RSA encryption and a bit length of 4096 ssh-keygen -m PEM -t rsa -b 4096 cat ~/.ssh/id_rsa.pub # cp to clipboard cat ~/.ssh/id_rsa.pub | xclip # on Linux cat ~/.ssh/id_rsa.pub | pbcopy # on Mac Disable/Enable_X_Windows_On_Startup \u00b6 This method works for Ubuntu to disable X window (or Gnome) <br/> ### Older systems vim /etc/default/grub # Find the line: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash\" # this enables UI GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash text\" # this disables UI sudo update-grub startx # can still get back into user-interface in the tty # Newer systems 15.01 and newer sudo systemctl set-default multi-user.target # this disables UI sudo systemctl set-default graphical.target # this enables UI # other methods sudo systemctl stop gdm sudo systemctl stop lightdm sudo telinit 3 # use 5 to bring it back ## On CentOS where systemctl is not supported # edit /etc/inittab id:3:initdefault: Disable_wireless_on_startup \u00b6 # both of following shows ip address ifconfig ip addr show # take down/up a network interface ip link set eth1 down ip link set eth1 up # alternatively sudo ifdown wlan0 sudo ifup wlan0 # alternatively sudo ifconfig wlan0 down sudo ifconfig wlan0 up Do_Mount_FileSystem \u00b6 mkdir -p /path/to/mountpoint lsblk # know the device name ls -l /dev/disk/by-label # show partition name # alternatively sudo lsblk -o name,mountpoint,label,size,uuid mount /dev/sdX /path/to/mountpoint # enable mount on start up https://wiki.archlinux.org/index.php/Fstab vim /etc/fstab # format: # device mountpoint fstype options dump fsck /dev/sdb1 /path/to/mountpoint ntfs defaults 0 1 # View a list of mounted filesystems less /proc/mounts # Install support for different filesystems sudo apt-get install exfat-fuse exfat-utils Edit_Service_Command \u00b6 most services has configs under: - /etc/default/NAME - /etc/init.d/NAME - /etc/init/NAME.conf Enable_Service_On_Startup \u00b6 First create a script to run for scheduling the service on startup Then put it into /etc/init.d Note that the start of the script should have something like this right below the shbang # The following is called LSBInitScripts ### BEGIN INIT INFO # Provides: haltusbpower # Required-Start: $all # Required-Stop: # Default-Start: 2 3 4 5 (comment: run levels) # Default-Stop: 0 1 6 # Short-Description: Halts USB power... ### END INIT INFO # cleans up when the script receives SIGINT or SIGTERM function cleanup () { local pids = $( jobs -pr ) [ -n \" $pids \" ] && kill $pids exit 0 } function goodbye () { echo \"\" echo \"Goodbye...\" echo \"\" } trap \"cleanup\" SIGINT SIGTERM trap \"goodbye\" EXIT Next step update-rc.d <script_name> defaults # create a file in cat <<EOF >> /lib/systemd/system/myservice.service [Unit] Description=Example systemd service. [Service] Type=simple PIDFile=/var/run/myservice/myservice.pid LimitNOFILE=16384 ExecStart=/bin/bash /usr/bin/test_service.sh ExecReload=/bin/kill -HUP $MAINPID [Install] WantedBy=multi-user.target EOF sudo chmod 644 /lib/systemd/system/myservice.service ln -s /lib/systemd/system/myservice.service /etc/systemd/system/myservice.service ln -s /lib/systemd/system/myservice.service /etc/systemd/system/multi-user.target.wants/myservice.service systemctl enable myservice systemctl is-enabled myservice systemctl is-active myservice systemctl disable myservice Most services manage their configs in /etc/<servicename>/<servicename>_config More here: https://linoxide.com/linux-how-to/enable-disable-services-ubuntu-systemd-upstart/ Run-levels: http://www.linfo.org/runlevel_def.html LSBInitScripts: https://wiki.debian.org/LSBInitScripts Enable_ssh_on_new_Linux_machine \u00b6 # install ssh sudo apt-get install openssh-server Enable_ssh_port_forwarding \u00b6 vim ~/.ssh/config Host <host> HostName <host_name> StrictHostKeyChecking no LocalForward <port> <host_name>:<new_port> LocalForward <port> <host_name>:<new_port> ... # then this becomes possible: mysql -h 127 .0.0.1 -P 10001 -u <user> -p # and will be forwarded to the configured host_name Enable_sshfs \u00b6 mkdir /media/remote/fs/mountpoint sshfs -o idmap = user username@hostname:/path/to/dir /media/remote/fs/mountpoint # to unmount: fusermount -u /media/remote/fs/mountpoint Enable_static_ip_on_Linux_machine \u00b6 # for Debian route add default gw { ROUTER-IP-ADDRESS } { INTERFACE-NAME } sudo ifconfig eth0 192 .168.1.30 netmask 255 .255.255.0 # add set this in /etc/network/interfaces file # auto eth0 static ip on start up iface eth0 inet static address 192 .168.1.30 network 192 .168.1.0 netmask 255 .255.255.0 broadcast 192 .168.1.255 gateway 192 .168.1.1 dns-nameservers 192 .168.1.1 # for CentOS # use a CLI GUI nmtui edit eth0 # OR Do it yourself # update /etc/sysconfig/network-scripts/ifcfg-eth0 # and make sure these fields are updated as follows DEVICE = eth0 BOOTPROTO = none ONBOOT = yes PREFIX = 24 IPADDR = 192 .168.100.5 # desired static IP # EOF # IF DOES NOT EXIST, here is a template TYPE = Ethernet BOOTPROTO = none IPADDR = 192 .168.100.5 # Desired static server IP # PREFIX = 24 # Subnet # GATEWAY = 192 .168.1.1 # Set default gateway IP # DNS1 = 192 .168.1.1 # Set dns servers # DNS2 = 8 .8.8.8 DNS3 = 8 .8.4.4 DEFROUTE = yes IPV4_FAILURE_FATAL = no IPV6INIT = no # Disable ipv6 # NAME = eth0 UUID = 41171a6f-bce1-44de-8a6e-cf5e782f8bd6 # created using 'uuidgen eth0' command # DEVICE = eth0 ONBOOT = yes # EOF systemctl restart network # then do this # alternatively, if above method didn't work ip = <desired_static_ip> dns = <router_dns_address> ns = <name_server_address | 8 .8.8.8> # here router dns is usually the router admin portal address with 0 as the last of the four numbers, i.e. 192.168.1.0 sudo cat <<EOT >> /etc/dhcpcd.conf interface eth0 static ip_address=$ip/24 static routers=$dns static domain_name_servers=$dns EOT sudo reboot Enable_Disable_Swap_Memory \u00b6 sudo dphys-swapfile swapoff && \\ sudo dphys-swapfile uninstall && \\ sudo update-rc.d dphys-swapfile remove # verify empty means disabled sudo swapon --summary Entering_Rescue_Mode \u00b6 # reboot the machine and at the grub boot menu, choose the correct Distro and press 'e' # find the line 'linux' and add this at the end of the line systemd.unit = rescue.target # press Ctrl-X to write (emacs command) Entering_Grub_Rescue \u00b6 # after updating the partition that touches the root filesystem, very likely # you will stuck in Grub rescue mode # do follow steps ls ls ( hd0,msdosx ) # or something like this from the above command # until one shows something not 'Device Not Found' set prefix =( hd0,msdos6 ) /boot/grub insmod normal normal # after boot into the os successfully sudo update-grub # which is the same as running 'grub-mkconfig -o /boot/grub/grub.cfg' sudo grub-install /dev/sda # If the drive is hd0 the equivalent is sda, if it's hd1 then use sdb # might need to replace the disk uuid with the newer one # install grub 2 # http://ftp.gnu.org/gnu/grub/ Find_USB_Cam \u00b6 sudo apt-get install v4l-utils v4l2-ctl --list-devices # view more info about this device sudo v4l2-ctl --device = /dev/video0 --all v4l2-ctl --list-formats-ext # show available resolution settings ffmpeg -f v4l2 -list_formats all -i /dev/video0 # show supported resolutions ffprobe <file> # check video resolution Find_File_Structure_Difference \u00b6 diff -qr dir-1/ dir-2 # alternatively, use a tool called meld sudo apt-get install meld Find_Device_Info \u00b6 dmesg | egrep -i --color 'cdrom|dvd|cd/rw|writer' Find_Search_Files \u00b6 locate file # uses database search to find file names that match a given pattern find /home/pi -mmin -3 -ls # give files changed in /home/pi in last 3 minutes find /home/pi -name \"*.bak\" -exec rm {} ';' # find and remove files named like xxx.bak in /home/pi find /home/pi -name \"*.bak\" -ok rm {} ';' # ask for permission before executing the rm command for each file find / -ctime -3 # find files whose inode metadata (ownership, permissions) changed within last 3 days find / -atime +3 # find files accessed earlier than 3 days before find / -mtime 3 # find files modified/written exactly 3 days before find / -size +10M # find files greater than 10 MB in size Find_Packages_To_Install \u00b6 # debian linux sudo apt-get update && sudo apt-get upgrade sudo apt-cache search <pkg> sudo apt-get install <pkg> # downgrade a package sudo apt-cache showpkg <pkg> # list pkg versions sudo aptitude install <pkg> = <version> # pin a package version sudo apt-mark hold <pkg> # to install .deb file sudo dpkg -i /path/to/deb/file # alternatively sudo apt install /path/to/deb/file Find_My_IP_Address \u00b6 # give private IP address ifconfig # look for ethX or enX, physical connections ip addr hostname -I # give public IP address via an echo server curl https://checkip.amazonaws.com curl https://icanhazip.com Find_Other_Machines_On_Local_Network \u00b6 sudo nmap -sA 192 .168.1.0/24 Fix_Slow_Mouse_Over_Bluetooth \u00b6 # a common problem on Raspberry pi # add this to the end of the single line in /boot/cmdline.txt (or update it if exists) # the number can be 0-8, the lower the more frequently the mouse is polled usbhid.mousepoll = 0 Get_Date_Time_Formatted \u00b6 date +%Y%m%d-%H # give format 20190718-02 date +%M # give format 15 Get_My_Ip_Address_On_the_Internet \u00b6 # Here are a few ways to get it: curl ifconfig.me curl -4/-6 icanhazip.com curl ipinfo.io/ip curl api.ipify.org curl checkip.dyndns.org dig +short myip.opendns.com @resolver1.opendns.com host myip.opendns.com resolver1.opendns.com curl ident.me curl bot.whatismyipaddress.com curl ipecho.net/plain # Following gives the private Ip Address on the LAN: ifconfig -a ip addr ( ip a ) hostname -I | awk '{print $1}' ip route get 1 .2.3.4 | awk '{print $7}' ( Fedora ) Wifi-Settings\u2192 click the setting icon next to the Wifi name that you are connected to \u2192 Ipv4 and Ipv6 both can be seen nmcli -p device show Install_Multi_Distro_Linux_On_A_Machine \u00b6 # a good article explains all this, not going to put the steps here # see https://www.garron.me/en/linux/dual-boot-two-linux-distributions-distros.html # see reduce root fs size https://www.thegeekdiary.com/how-to-shrink-root-filesystem-on-centos-rhel-6/ Make_Time_Elapse_Video_With_Webcam \u00b6 streamer -o 0000 .jpeg -s 640x480 -j 100 -t 2000 -r 1 ffmpeg -framerate 1 /5 -i img%04d.jpeg -c:v libx264 -r 30 -pix_fmt yuv420p out.mp4 ffmpeg -framerate 4 -i %04d.jpeg -c:v libx264 -pix_fmt yuv420p video.mp4 ffmpeg -f concat -safe 0 -i out%01d.mp4 -c copy combined.mp4 -t says how many frames(images) to record, -r says frames per second Network_Settings \u00b6 Usually stored in /etc/NetworkManager/system-connections/NETGEAR-VPN.nmconnection for Ubuntu Might need root access to view Network logs can be found in /var/run/syslog service network restart Open_A_Port_To_LAN \u00b6 # use ufw on ubuntu sudo ufw allow 3306 # to delete/remove the rule sudo ufw delete allow 3306 # use iptables on other Linux flavors sudo iptables -A INPUT -p tcp --dport 3306 -j ACCEPT sudo iptables -A INPUT -p tcp --sport 3306 -j ACCEPT # to delete/remove the rule sudo iptables -D INPUT -p tcp --dport 3306 -j ACCEPT sudo iptables -D INPUT -p tcp --sport 3306 -j ACCEPT RaspberryPi \u00b6 # RaspberryPi Configs (terminal and ssh friendly) ## default password is also changed this way instead of the passwd command sudo raspi-config Remote_Print_From_Terminal \u00b6 lp file.pdf lpstat -p -d # will list available printers use options: -P 3 -5 #for pages, separated by commas -o sides = two-sided-long-edge #for print on both sides -n 5 #for number of copies lp filename -P 1 -5 -d ps114 -o sides = two-sided-long-edge -n 1 lpr - \\# 1 -P ps114 -o sides = two-sided-long-edge filename cancel printer-name cancel -u zhongkai Resize_disk_partitions \u00b6 fdisk -l # shows current partitions fdisk <dev> # edit a drive's partitions # use 'p' to see current partitions # if the root is the last partition then follows the free space, then it is very easy # use 'd' to delete the root partition, take note its start sector # use 'n' to create the new partition, put in the start sector and leave the end sector as default # use 'p' to make sure it looks right # use 'w' to write out and reboot resize2fs <dev-subpartition-name> # might want to double check the soft links under /dev/disks/by-uuid # if you messed with the dev labels by re-partitioning # after performing re-partition it is likely to get stuck in grub recovery mode # try the advice on the #Entering_Grub_Rescue section on this page # or try the recovery steps from this post # https://askubuntu.com/questions/119597/grub-rescue-error-unknown-filesystem Setup_Cronjob \u00b6 crontab -e # Add an entry with the schedule string followed by the command minute ( 0 -59 ) hour ( 0 -23 ) day ( 1 -31 ) month ( 1 -12 ) weekday ( 0 -6 ) command # Like this: 29 0 * * * /usr/bin/example Setup_Reverse_Proxy \u00b6 Use nginx to set it up sudo apt-get install nginx # verify going to this machine's ip address and can see the nginx server page Files containing routing rules go in the /etc/nginx/sites-available/ folder. To activate those rules you need to create a symlink of those files to the /etc/nginx/sites-enabled/ folder. server { listen 80 ; listen [ :: ] :80 ; server_name example.com www.example.com ; # domains that will need to be handled location / { proxy_pass http://10.0.0.3:3000 ; # the local IP address and port of the machine that your chosen domains should to redirect to proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection 'upgrade' ; proxy_set_header Host $host ; proxy_cache_bypass $http_upgrade ; } } You can add multiple config files, a config file can contain multiple server{} server blocks, a server block can contain multiple location{} blocks. Use certbot to generate certs sudo apt-get install python-certbot-nginx -t stretch-backports sudo certbot certonly --webroot -w /var/www/example.com/ -d example.com -d www.example.com And the cert files are put in /etc/letsencrypt/live/example.com/ Now update the default config with server { listen 443 ssl http2 ; listen [ :: ] :443 ssl http2 ; server_name example.com www.example.com ; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem ; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem ; ssl_stapling on ; ssl_stapling_verify on ; add_header Strict-Transport-Security \"max-age=31536000\" ; access_log /var/log/nginx/sub.log combined ; location /.well-known { alias /var/www/example.com/.well-known ; } location / { # reverse proxy commands } } View_Port_Binding \u00b6 netstat -nlp # add -t if want TCP only lsof -i TCP ss -ap View_Service_Logs \u00b6 systemctl --state = failed systemctl status <service-name> service <service-name> status journalctl -u <service-name> -b","title":"Freq Linux Tasks"},{"location":"Linux/References/Freq-Tasks/#add_user_to_sudoers","text":"# first add user to group sudo usermod -aG sudo <user> # makesure this line is in /etc/sudoers %sudo ALL =( ALL ) ALL","title":"Add_User_To_Sudoers"},{"location":"Linux/References/Freq-Tasks/#benchmarking_flash_drive","text":"# General command to use: # 1. create a large file for benchmarking from device 0 dd if = /dev/zero of = ./largefile bs = 1M count = 1024 # 2. Write to a drive dd if = path/to/input_file of = /path/to/output_file bs = block_size count = number_of_blocks # Reference: # https://www.binarytides.com/linux-test-drive-speed/ # alternatively, use Ubuntu's disk software for benchmark","title":"Benchmarking_Flash_Drive"},{"location":"Linux/References/Freq-Tasks/#boot_into_another_os_of_different_grub_version","text":"# https://howtoubuntu.org/how-to-repair-restore-reinstall-grub-2-with-a-ubuntu-live-cd lsblk # find out which disk to boot into sudo mount -t ext4 /dev/sdXY /mnt sudo mount --bind /dev /mnt/dev && sudo mount --bind /dev/pts /mnt/dev/pts && sudo mount --bind /proc /mnt/proc && sudo mount --bind /sys /mnt/sys sudo chroot /mnt grub-install /dev/sdX grub-install --recheck /dev/sdX update-grub exit && sudo umount /mnt/sys /mnt/proc /mnt/dev/pts /mnt/dev && sudo umount /mnt","title":"Boot_into_another_OS_of_Different_GRUB_Version"},{"location":"Linux/References/Freq-Tasks/#burn_image_to_flash_drive","text":"lsblk -p # -> see which devices are connected umount /dev/sdX [ 1 -9 ] # -> unmount the usb device drives # -> replace 'X' with the device letter [a-z] dd bs = 4M if = path/to/linux_image.img of = /dev/sdX conv = fsync status = progress # -> the command to copy the image into the drive # -> replace 'X' with the device letter [a-z] sync # flush out write cache so the drive is okay to unmount","title":"Burn_Image_To_Flash_Drive"},{"location":"Linux/References/Freq-Tasks/#change_hostname_on_linux_machine","text":"sudo hostnamectl --transient set-hostname $hostname sudo hostnamectl --static set-hostname $hostname sudo hostnamectl --pretty set-hostname $hostname sudo sed -i s/<old-hostname>/ $hostname /g /etc/hosts","title":"Change_hostname_on_Linux_machine"},{"location":"Linux/References/Freq-Tasks/#change_raspberrypi_os_keyboard","text":"sudo nano /etc/default/keyboard # update 'XKBLAYOUT' to 'us'","title":"Change_RaspberryPi_OS_Keyboard"},{"location":"Linux/References/Freq-Tasks/#check_cpu_temperaturevolt","text":"# On RaspberryPi: /opt/vc/bin/vcgencmd measure_temp /opt/vc/bin/vcgencmd measure_volts core # view a list of available commands: /opt/vc/bin/vcgencmd commands # Install a tool called lm-sensors sudo apt install lm-sensors sudo -i sensors-detect sensors # to check various sensor temperatures # Note this method does not work if the system does not have any sensors attached to it. i.e. like a RaspberryPi","title":"Check_CPU_Temperature/Volt"},{"location":"Linux/References/Freq-Tasks/#check_usb_devices","text":"lsusb # view a list of plugged in devices via USB ports dmesg # view the log related to USB devices","title":"Check_USB_Devices"},{"location":"Linux/References/Freq-Tasks/#configuring_locale_settings","text":"export LANGUAGE = en_US.UTF-8 export LANG = en_US.UTF-8 export LC_ALL = en_US.UTF-8 locale-gen en_US.UTF-8 dpkg-reconfigure locales","title":"Configuring_Locale_Settings"},{"location":"Linux/References/Freq-Tasks/#create_ssh_key_pair","text":"# creates an SSH key pair using RSA encryption and a bit length of 4096 ssh-keygen -m PEM -t rsa -b 4096 cat ~/.ssh/id_rsa.pub # cp to clipboard cat ~/.ssh/id_rsa.pub | xclip # on Linux cat ~/.ssh/id_rsa.pub | pbcopy # on Mac","title":"Create_SSH_Key_Pair"},{"location":"Linux/References/Freq-Tasks/#disableenable_x_windows_on_startup","text":"This method works for Ubuntu to disable X window (or Gnome) <br/> ### Older systems vim /etc/default/grub # Find the line: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash\" # this enables UI GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash text\" # this disables UI sudo update-grub startx # can still get back into user-interface in the tty # Newer systems 15.01 and newer sudo systemctl set-default multi-user.target # this disables UI sudo systemctl set-default graphical.target # this enables UI # other methods sudo systemctl stop gdm sudo systemctl stop lightdm sudo telinit 3 # use 5 to bring it back ## On CentOS where systemctl is not supported # edit /etc/inittab id:3:initdefault:","title":"Disable/Enable_X_Windows_On_Startup"},{"location":"Linux/References/Freq-Tasks/#disable_wireless_on_startup","text":"# both of following shows ip address ifconfig ip addr show # take down/up a network interface ip link set eth1 down ip link set eth1 up # alternatively sudo ifdown wlan0 sudo ifup wlan0 # alternatively sudo ifconfig wlan0 down sudo ifconfig wlan0 up","title":"Disable_wireless_on_startup"},{"location":"Linux/References/Freq-Tasks/#do_mount_filesystem","text":"mkdir -p /path/to/mountpoint lsblk # know the device name ls -l /dev/disk/by-label # show partition name # alternatively sudo lsblk -o name,mountpoint,label,size,uuid mount /dev/sdX /path/to/mountpoint # enable mount on start up https://wiki.archlinux.org/index.php/Fstab vim /etc/fstab # format: # device mountpoint fstype options dump fsck /dev/sdb1 /path/to/mountpoint ntfs defaults 0 1 # View a list of mounted filesystems less /proc/mounts # Install support for different filesystems sudo apt-get install exfat-fuse exfat-utils","title":"Do_Mount_FileSystem"},{"location":"Linux/References/Freq-Tasks/#edit_service_command","text":"most services has configs under: - /etc/default/NAME - /etc/init.d/NAME - /etc/init/NAME.conf","title":"Edit_Service_Command"},{"location":"Linux/References/Freq-Tasks/#enable_service_on_startup","text":"First create a script to run for scheduling the service on startup Then put it into /etc/init.d Note that the start of the script should have something like this right below the shbang # The following is called LSBInitScripts ### BEGIN INIT INFO # Provides: haltusbpower # Required-Start: $all # Required-Stop: # Default-Start: 2 3 4 5 (comment: run levels) # Default-Stop: 0 1 6 # Short-Description: Halts USB power... ### END INIT INFO # cleans up when the script receives SIGINT or SIGTERM function cleanup () { local pids = $( jobs -pr ) [ -n \" $pids \" ] && kill $pids exit 0 } function goodbye () { echo \"\" echo \"Goodbye...\" echo \"\" } trap \"cleanup\" SIGINT SIGTERM trap \"goodbye\" EXIT Next step update-rc.d <script_name> defaults # create a file in cat <<EOF >> /lib/systemd/system/myservice.service [Unit] Description=Example systemd service. [Service] Type=simple PIDFile=/var/run/myservice/myservice.pid LimitNOFILE=16384 ExecStart=/bin/bash /usr/bin/test_service.sh ExecReload=/bin/kill -HUP $MAINPID [Install] WantedBy=multi-user.target EOF sudo chmod 644 /lib/systemd/system/myservice.service ln -s /lib/systemd/system/myservice.service /etc/systemd/system/myservice.service ln -s /lib/systemd/system/myservice.service /etc/systemd/system/multi-user.target.wants/myservice.service systemctl enable myservice systemctl is-enabled myservice systemctl is-active myservice systemctl disable myservice Most services manage their configs in /etc/<servicename>/<servicename>_config More here: https://linoxide.com/linux-how-to/enable-disable-services-ubuntu-systemd-upstart/ Run-levels: http://www.linfo.org/runlevel_def.html LSBInitScripts: https://wiki.debian.org/LSBInitScripts","title":"Enable_Service_On_Startup"},{"location":"Linux/References/Freq-Tasks/#enable_ssh_on_new_linux_machine","text":"# install ssh sudo apt-get install openssh-server","title":"Enable_ssh_on_new_Linux_machine"},{"location":"Linux/References/Freq-Tasks/#enable_ssh_port_forwarding","text":"vim ~/.ssh/config Host <host> HostName <host_name> StrictHostKeyChecking no LocalForward <port> <host_name>:<new_port> LocalForward <port> <host_name>:<new_port> ... # then this becomes possible: mysql -h 127 .0.0.1 -P 10001 -u <user> -p # and will be forwarded to the configured host_name","title":"Enable_ssh_port_forwarding"},{"location":"Linux/References/Freq-Tasks/#enable_sshfs","text":"mkdir /media/remote/fs/mountpoint sshfs -o idmap = user username@hostname:/path/to/dir /media/remote/fs/mountpoint # to unmount: fusermount -u /media/remote/fs/mountpoint","title":"Enable_sshfs"},{"location":"Linux/References/Freq-Tasks/#enable_static_ip_on_linux_machine","text":"# for Debian route add default gw { ROUTER-IP-ADDRESS } { INTERFACE-NAME } sudo ifconfig eth0 192 .168.1.30 netmask 255 .255.255.0 # add set this in /etc/network/interfaces file # auto eth0 static ip on start up iface eth0 inet static address 192 .168.1.30 network 192 .168.1.0 netmask 255 .255.255.0 broadcast 192 .168.1.255 gateway 192 .168.1.1 dns-nameservers 192 .168.1.1 # for CentOS # use a CLI GUI nmtui edit eth0 # OR Do it yourself # update /etc/sysconfig/network-scripts/ifcfg-eth0 # and make sure these fields are updated as follows DEVICE = eth0 BOOTPROTO = none ONBOOT = yes PREFIX = 24 IPADDR = 192 .168.100.5 # desired static IP # EOF # IF DOES NOT EXIST, here is a template TYPE = Ethernet BOOTPROTO = none IPADDR = 192 .168.100.5 # Desired static server IP # PREFIX = 24 # Subnet # GATEWAY = 192 .168.1.1 # Set default gateway IP # DNS1 = 192 .168.1.1 # Set dns servers # DNS2 = 8 .8.8.8 DNS3 = 8 .8.4.4 DEFROUTE = yes IPV4_FAILURE_FATAL = no IPV6INIT = no # Disable ipv6 # NAME = eth0 UUID = 41171a6f-bce1-44de-8a6e-cf5e782f8bd6 # created using 'uuidgen eth0' command # DEVICE = eth0 ONBOOT = yes # EOF systemctl restart network # then do this # alternatively, if above method didn't work ip = <desired_static_ip> dns = <router_dns_address> ns = <name_server_address | 8 .8.8.8> # here router dns is usually the router admin portal address with 0 as the last of the four numbers, i.e. 192.168.1.0 sudo cat <<EOT >> /etc/dhcpcd.conf interface eth0 static ip_address=$ip/24 static routers=$dns static domain_name_servers=$dns EOT sudo reboot","title":"Enable_static_ip_on_Linux_machine"},{"location":"Linux/References/Freq-Tasks/#enable_disable_swap_memory","text":"sudo dphys-swapfile swapoff && \\ sudo dphys-swapfile uninstall && \\ sudo update-rc.d dphys-swapfile remove # verify empty means disabled sudo swapon --summary","title":"Enable_Disable_Swap_Memory"},{"location":"Linux/References/Freq-Tasks/#entering_rescue_mode","text":"# reboot the machine and at the grub boot menu, choose the correct Distro and press 'e' # find the line 'linux' and add this at the end of the line systemd.unit = rescue.target # press Ctrl-X to write (emacs command)","title":"Entering_Rescue_Mode"},{"location":"Linux/References/Freq-Tasks/#entering_grub_rescue","text":"# after updating the partition that touches the root filesystem, very likely # you will stuck in Grub rescue mode # do follow steps ls ls ( hd0,msdosx ) # or something like this from the above command # until one shows something not 'Device Not Found' set prefix =( hd0,msdos6 ) /boot/grub insmod normal normal # after boot into the os successfully sudo update-grub # which is the same as running 'grub-mkconfig -o /boot/grub/grub.cfg' sudo grub-install /dev/sda # If the drive is hd0 the equivalent is sda, if it's hd1 then use sdb # might need to replace the disk uuid with the newer one # install grub 2 # http://ftp.gnu.org/gnu/grub/","title":"Entering_Grub_Rescue"},{"location":"Linux/References/Freq-Tasks/#find_usb_cam","text":"sudo apt-get install v4l-utils v4l2-ctl --list-devices # view more info about this device sudo v4l2-ctl --device = /dev/video0 --all v4l2-ctl --list-formats-ext # show available resolution settings ffmpeg -f v4l2 -list_formats all -i /dev/video0 # show supported resolutions ffprobe <file> # check video resolution","title":"Find_USB_Cam"},{"location":"Linux/References/Freq-Tasks/#find_file_structure_difference","text":"diff -qr dir-1/ dir-2 # alternatively, use a tool called meld sudo apt-get install meld","title":"Find_File_Structure_Difference"},{"location":"Linux/References/Freq-Tasks/#find_device_info","text":"dmesg | egrep -i --color 'cdrom|dvd|cd/rw|writer'","title":"Find_Device_Info"},{"location":"Linux/References/Freq-Tasks/#find_search_files","text":"locate file # uses database search to find file names that match a given pattern find /home/pi -mmin -3 -ls # give files changed in /home/pi in last 3 minutes find /home/pi -name \"*.bak\" -exec rm {} ';' # find and remove files named like xxx.bak in /home/pi find /home/pi -name \"*.bak\" -ok rm {} ';' # ask for permission before executing the rm command for each file find / -ctime -3 # find files whose inode metadata (ownership, permissions) changed within last 3 days find / -atime +3 # find files accessed earlier than 3 days before find / -mtime 3 # find files modified/written exactly 3 days before find / -size +10M # find files greater than 10 MB in size","title":"Find_Search_Files"},{"location":"Linux/References/Freq-Tasks/#find_packages_to_install","text":"# debian linux sudo apt-get update && sudo apt-get upgrade sudo apt-cache search <pkg> sudo apt-get install <pkg> # downgrade a package sudo apt-cache showpkg <pkg> # list pkg versions sudo aptitude install <pkg> = <version> # pin a package version sudo apt-mark hold <pkg> # to install .deb file sudo dpkg -i /path/to/deb/file # alternatively sudo apt install /path/to/deb/file","title":"Find_Packages_To_Install"},{"location":"Linux/References/Freq-Tasks/#find_my_ip_address","text":"# give private IP address ifconfig # look for ethX or enX, physical connections ip addr hostname -I # give public IP address via an echo server curl https://checkip.amazonaws.com curl https://icanhazip.com","title":"Find_My_IP_Address"},{"location":"Linux/References/Freq-Tasks/#find_other_machines_on_local_network","text":"sudo nmap -sA 192 .168.1.0/24","title":"Find_Other_Machines_On_Local_Network"},{"location":"Linux/References/Freq-Tasks/#fix_slow_mouse_over_bluetooth","text":"# a common problem on Raspberry pi # add this to the end of the single line in /boot/cmdline.txt (or update it if exists) # the number can be 0-8, the lower the more frequently the mouse is polled usbhid.mousepoll = 0","title":"Fix_Slow_Mouse_Over_Bluetooth"},{"location":"Linux/References/Freq-Tasks/#get_date_time_formatted","text":"date +%Y%m%d-%H # give format 20190718-02 date +%M # give format 15","title":"Get_Date_Time_Formatted"},{"location":"Linux/References/Freq-Tasks/#get_my_ip_address_on_the_internet","text":"# Here are a few ways to get it: curl ifconfig.me curl -4/-6 icanhazip.com curl ipinfo.io/ip curl api.ipify.org curl checkip.dyndns.org dig +short myip.opendns.com @resolver1.opendns.com host myip.opendns.com resolver1.opendns.com curl ident.me curl bot.whatismyipaddress.com curl ipecho.net/plain # Following gives the private Ip Address on the LAN: ifconfig -a ip addr ( ip a ) hostname -I | awk '{print $1}' ip route get 1 .2.3.4 | awk '{print $7}' ( Fedora ) Wifi-Settings\u2192 click the setting icon next to the Wifi name that you are connected to \u2192 Ipv4 and Ipv6 both can be seen nmcli -p device show","title":"Get_My_Ip_Address_On_the_Internet"},{"location":"Linux/References/Freq-Tasks/#install_multi_distro_linux_on_a_machine","text":"# a good article explains all this, not going to put the steps here # see https://www.garron.me/en/linux/dual-boot-two-linux-distributions-distros.html # see reduce root fs size https://www.thegeekdiary.com/how-to-shrink-root-filesystem-on-centos-rhel-6/","title":"Install_Multi_Distro_Linux_On_A_Machine"},{"location":"Linux/References/Freq-Tasks/#make_time_elapse_video_with_webcam","text":"streamer -o 0000 .jpeg -s 640x480 -j 100 -t 2000 -r 1 ffmpeg -framerate 1 /5 -i img%04d.jpeg -c:v libx264 -r 30 -pix_fmt yuv420p out.mp4 ffmpeg -framerate 4 -i %04d.jpeg -c:v libx264 -pix_fmt yuv420p video.mp4 ffmpeg -f concat -safe 0 -i out%01d.mp4 -c copy combined.mp4 -t says how many frames(images) to record, -r says frames per second","title":"Make_Time_Elapse_Video_With_Webcam"},{"location":"Linux/References/Freq-Tasks/#network_settings","text":"Usually stored in /etc/NetworkManager/system-connections/NETGEAR-VPN.nmconnection for Ubuntu Might need root access to view Network logs can be found in /var/run/syslog service network restart","title":"Network_Settings"},{"location":"Linux/References/Freq-Tasks/#open_a_port_to_lan","text":"# use ufw on ubuntu sudo ufw allow 3306 # to delete/remove the rule sudo ufw delete allow 3306 # use iptables on other Linux flavors sudo iptables -A INPUT -p tcp --dport 3306 -j ACCEPT sudo iptables -A INPUT -p tcp --sport 3306 -j ACCEPT # to delete/remove the rule sudo iptables -D INPUT -p tcp --dport 3306 -j ACCEPT sudo iptables -D INPUT -p tcp --sport 3306 -j ACCEPT","title":"Open_A_Port_To_LAN"},{"location":"Linux/References/Freq-Tasks/#raspberrypi","text":"# RaspberryPi Configs (terminal and ssh friendly) ## default password is also changed this way instead of the passwd command sudo raspi-config","title":"RaspberryPi"},{"location":"Linux/References/Freq-Tasks/#remote_print_from_terminal","text":"lp file.pdf lpstat -p -d # will list available printers use options: -P 3 -5 #for pages, separated by commas -o sides = two-sided-long-edge #for print on both sides -n 5 #for number of copies lp filename -P 1 -5 -d ps114 -o sides = two-sided-long-edge -n 1 lpr - \\# 1 -P ps114 -o sides = two-sided-long-edge filename cancel printer-name cancel -u zhongkai","title":"Remote_Print_From_Terminal"},{"location":"Linux/References/Freq-Tasks/#resize_disk_partitions","text":"fdisk -l # shows current partitions fdisk <dev> # edit a drive's partitions # use 'p' to see current partitions # if the root is the last partition then follows the free space, then it is very easy # use 'd' to delete the root partition, take note its start sector # use 'n' to create the new partition, put in the start sector and leave the end sector as default # use 'p' to make sure it looks right # use 'w' to write out and reboot resize2fs <dev-subpartition-name> # might want to double check the soft links under /dev/disks/by-uuid # if you messed with the dev labels by re-partitioning # after performing re-partition it is likely to get stuck in grub recovery mode # try the advice on the #Entering_Grub_Rescue section on this page # or try the recovery steps from this post # https://askubuntu.com/questions/119597/grub-rescue-error-unknown-filesystem","title":"Resize_disk_partitions"},{"location":"Linux/References/Freq-Tasks/#setup_cronjob","text":"crontab -e # Add an entry with the schedule string followed by the command minute ( 0 -59 ) hour ( 0 -23 ) day ( 1 -31 ) month ( 1 -12 ) weekday ( 0 -6 ) command # Like this: 29 0 * * * /usr/bin/example","title":"Setup_Cronjob"},{"location":"Linux/References/Freq-Tasks/#setup_reverse_proxy","text":"Use nginx to set it up sudo apt-get install nginx # verify going to this machine's ip address and can see the nginx server page Files containing routing rules go in the /etc/nginx/sites-available/ folder. To activate those rules you need to create a symlink of those files to the /etc/nginx/sites-enabled/ folder. server { listen 80 ; listen [ :: ] :80 ; server_name example.com www.example.com ; # domains that will need to be handled location / { proxy_pass http://10.0.0.3:3000 ; # the local IP address and port of the machine that your chosen domains should to redirect to proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection 'upgrade' ; proxy_set_header Host $host ; proxy_cache_bypass $http_upgrade ; } } You can add multiple config files, a config file can contain multiple server{} server blocks, a server block can contain multiple location{} blocks. Use certbot to generate certs sudo apt-get install python-certbot-nginx -t stretch-backports sudo certbot certonly --webroot -w /var/www/example.com/ -d example.com -d www.example.com And the cert files are put in /etc/letsencrypt/live/example.com/ Now update the default config with server { listen 443 ssl http2 ; listen [ :: ] :443 ssl http2 ; server_name example.com www.example.com ; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem ; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem ; ssl_stapling on ; ssl_stapling_verify on ; add_header Strict-Transport-Security \"max-age=31536000\" ; access_log /var/log/nginx/sub.log combined ; location /.well-known { alias /var/www/example.com/.well-known ; } location / { # reverse proxy commands } }","title":"Setup_Reverse_Proxy"},{"location":"Linux/References/Freq-Tasks/#view_port_binding","text":"netstat -nlp # add -t if want TCP only lsof -i TCP ss -ap","title":"View_Port_Binding"},{"location":"Linux/References/Freq-Tasks/#view_service_logs","text":"systemctl --state = failed systemctl status <service-name> service <service-name> status journalctl -u <service-name> -b","title":"View_Service_Logs"},{"location":"Linux/References/Terminal-Tricks/","text":"Some things I found useful when using Linux terminal. tmux is a must when working with limited screen or terminals. A great tmux-guide . Execution Control \u00b6 Command - - Description Enter causes the current command line to be accepted Cursor location within the line does not matter If the line is not empty, it is added to the command history Esc Meta-prefix. If the Alt key is unavailable, the Esc key can be used in its place Ctrl-g Abort the current editing command Ctrl-_ Incrementally undo changes to the line Alt-r Revert all changes to the line Ctrl-l Clear the screen Ctrl-c interrupt current execution Ctrl-r back-ward search commands saved in history Editing Control Command - - Description Alt-f / Alt-rightkey move forward one word Alt-b / Alt-leftkey move backward one word Ctrl-a move to beginning of the line Ctrl-e move to end of the line Ctrl-u delete chars up to the beginning of the line Ctrl-k delete chars up to the ending of the line Text Transformation Command - - Description Alt-u Change the current word to uppercase. Alt-l Change the current word to lowercase. Alt-c Capitalize the current word. Alt-t Transpose words. Exchange the word at the point with the word preceding it. Alt-. recall the last argument from the previous command's argument list Beautify Terminal Outlook \u00b6 Modify the ~/.bashrc to set a preferred terminal style. The $PS1 bash env var represents the primary prompt string which is displayed when the shell is ready to read a command. It takes the form of PS1=\"[\\u@\\H \\W] \\$ \" Find the full documentation of supported special chars in man bash , at the section PROMPTING . Here are some of them that is frequently used: Char - - Description \\u username of current user \\h hostname up to the first dot in this machine's domain name \\H display the Full Qualified Domain Name (FQDN) \\W basename of current working directory \\$ if root display '#', else display '$' \\! display the history number of current command \\D{<date-format>} add formatted time of current timestamp You can change 3 aspects of the terminal outlook: Text Format - - Text color - - Text-background color 0: normal text 30: black 40: black 1: bold 31: red 41: red 4: underlined 32: green 42: green 33: yellow 43: yellow 34: blue 44: blue 35: purple 45: purple 36: cyan 46: cyan 37: white 47: white To modify the text with the styles, use a format of \\e[<bg-color>;<format>;<txt-color>m If you need to change multiple parts of the prompt to use different styles, then use multiple \\e[;;m followed by the parts. PS1=\"\\e[41;4;33m[\\u@\\h \\W] \\$ \" Rename file without typing the full path twice: mv /path/to/file{,.bak} the special syntax {,.bak} tells shell to repeat the full string with two substrings, one is empty and the other is .bak , which is the same as doing mv /path/to/file /path/to/file.bak Useful aliases \u00b6 # colorize/prettify some commands output alias diff = 'colordiff' # (need to install colordiff) alias egrep = 'egrep --color=auto' alias fgrep = 'fgrep --color=auto' alias grep = 'grep --color=auto' alias ls = 'ls --color=auto' alias ct = 'column -t' alias dfc = 'df -hPT | column -t' alias mountc = 'mount | column -t' # quick cd alias .. = 'cd ..' alias 2 .. = 'cd ../../' alias 3 .. = 'cd ../../../' alias 4 .. = 'cd ../../../../' alias ... = 'cd ../../../' alias .... = 'cd ../../../../' # quick clipboard (need to install xclip) alias setclip = \"xclip -selection c\" alias getclip = \"xclip -selection c -o\" # datetime alias d = 'date +%F' alias t = 'date +%T' alias dt = 'date +\"%F %T\"' alias now = 'date +\"[%F %T]\"' # k8s alias alias kv = 'kubectl version' alias getcontexts = 'kubectl config get-contexts' alias usecontext = 'kubectl config use-context' alias setnamespace = 'kubectl config set-context --current --namespace=' alias getpo = 'kubectl get pods' alias getdep = 'kubectl get deployments' alias getsvc = 'kubectl get services' alias getrs = 'kubectl get replicasets' alias execit = 'kubectl exec -it' alias despo = 'kubectl describe pod' alias desdep = 'kubectl describe deployment' alias logs = 'kubectl logs'","title":"Linux Terminal Tricks"},{"location":"Linux/References/Terminal-Tricks/#execution-control","text":"Command - - Description Enter causes the current command line to be accepted Cursor location within the line does not matter If the line is not empty, it is added to the command history Esc Meta-prefix. If the Alt key is unavailable, the Esc key can be used in its place Ctrl-g Abort the current editing command Ctrl-_ Incrementally undo changes to the line Alt-r Revert all changes to the line Ctrl-l Clear the screen Ctrl-c interrupt current execution Ctrl-r back-ward search commands saved in history Editing Control Command - - Description Alt-f / Alt-rightkey move forward one word Alt-b / Alt-leftkey move backward one word Ctrl-a move to beginning of the line Ctrl-e move to end of the line Ctrl-u delete chars up to the beginning of the line Ctrl-k delete chars up to the ending of the line Text Transformation Command - - Description Alt-u Change the current word to uppercase. Alt-l Change the current word to lowercase. Alt-c Capitalize the current word. Alt-t Transpose words. Exchange the word at the point with the word preceding it. Alt-. recall the last argument from the previous command's argument list","title":"Execution Control"},{"location":"Linux/References/Terminal-Tricks/#beautify-terminal-outlook","text":"Modify the ~/.bashrc to set a preferred terminal style. The $PS1 bash env var represents the primary prompt string which is displayed when the shell is ready to read a command. It takes the form of PS1=\"[\\u@\\H \\W] \\$ \" Find the full documentation of supported special chars in man bash , at the section PROMPTING . Here are some of them that is frequently used: Char - - Description \\u username of current user \\h hostname up to the first dot in this machine's domain name \\H display the Full Qualified Domain Name (FQDN) \\W basename of current working directory \\$ if root display '#', else display '$' \\! display the history number of current command \\D{<date-format>} add formatted time of current timestamp You can change 3 aspects of the terminal outlook: Text Format - - Text color - - Text-background color 0: normal text 30: black 40: black 1: bold 31: red 41: red 4: underlined 32: green 42: green 33: yellow 43: yellow 34: blue 44: blue 35: purple 45: purple 36: cyan 46: cyan 37: white 47: white To modify the text with the styles, use a format of \\e[<bg-color>;<format>;<txt-color>m If you need to change multiple parts of the prompt to use different styles, then use multiple \\e[;;m followed by the parts. PS1=\"\\e[41;4;33m[\\u@\\h \\W] \\$ \" Rename file without typing the full path twice: mv /path/to/file{,.bak} the special syntax {,.bak} tells shell to repeat the full string with two substrings, one is empty and the other is .bak , which is the same as doing mv /path/to/file /path/to/file.bak","title":"Beautify Terminal Outlook"},{"location":"Linux/References/Terminal-Tricks/#useful-aliases","text":"# colorize/prettify some commands output alias diff = 'colordiff' # (need to install colordiff) alias egrep = 'egrep --color=auto' alias fgrep = 'fgrep --color=auto' alias grep = 'grep --color=auto' alias ls = 'ls --color=auto' alias ct = 'column -t' alias dfc = 'df -hPT | column -t' alias mountc = 'mount | column -t' # quick cd alias .. = 'cd ..' alias 2 .. = 'cd ../../' alias 3 .. = 'cd ../../../' alias 4 .. = 'cd ../../../../' alias ... = 'cd ../../../' alias .... = 'cd ../../../../' # quick clipboard (need to install xclip) alias setclip = \"xclip -selection c\" alias getclip = \"xclip -selection c -o\" # datetime alias d = 'date +%F' alias t = 'date +%T' alias dt = 'date +\"%F %T\"' alias now = 'date +\"[%F %T]\"' # k8s alias alias kv = 'kubectl version' alias getcontexts = 'kubectl config get-contexts' alias usecontext = 'kubectl config use-context' alias setnamespace = 'kubectl config set-context --current --namespace=' alias getpo = 'kubectl get pods' alias getdep = 'kubectl get deployments' alias getsvc = 'kubectl get services' alias getrs = 'kubectl get replicasets' alias execit = 'kubectl exec -it' alias despo = 'kubectl describe pod' alias desdep = 'kubectl describe deployment' alias logs = 'kubectl logs'","title":"Useful aliases"},{"location":"Linux/References/Vim-Tricks/","text":"All Unix-like system have vi , but not necessarily other editors vim is an advanced version of vi, which supports syntax highlighting and helping software development. vimtutor launches a short but comprehensive tutorial for new learners. use vi/vim \u00b6 vi has three modes: standard mode , used when opening a file with vi, can use commands to move cursor, delete, copy, paste, and search and replace. insert mode , entered after pressing i, I, o, O, a, A, r, R during standard mode command line mode , entered after pressing :, /, ? , where you can search, read/write, or any other actions can be taken. Moving cursors \u00b6 Key Pressing - - Function h / left-arrow move left j / down-arrow move down k / up-arrow move up l / right-arrow move right w jump forwards to the start of a word e jump forwards to the end of a word b jump backwards to the start of a word numb N + hjkl move N times in that direction ctrl + f page down ctrl + b page up ctrl + d half page down ctrl + u half page up + move cursor to next non empty line - move cursor to previous non empty line numb N + [space] move cursor right N times 0 / [Home] move to the first char of this line ^ move to the first non-blank char of this line $ / [End] move to last char of this line H move to the first line first character of current view-port M move to the middle line first character of current view-port L move to the last line first character of current view-port G move to the last line of this file nG move to the nth line of this file gg move to the first line of this file zz center cursor on screen numb N + [enter] move cursor down for N lines Search and Replace \u00b6 Key Pressing - - Function /word move down and search for 'word' ?word move up and search for 'word' \\vword end with period. non-alphanumeric chars are interpreted as special regex symbols, no escaping needed n repeat last search action N repeat last action in reversed searching direction :n1,n2s/word1/word2/g find and replace! search for word1 between line n1 and n2 and replace them with word2 :1,$s/word1/word2/g find and replace from begin to the end of this file :1,$s/word1/word2/gc find and replace from begin to the end, and asking for confirmation for each match Delete, Copy, and Paste \u00b6 Key Pressing - - Function x, X x delete a char from right of cursor like [del], X delete one from left like [Backspace] nx delete n chars from right dd delete current line ndd delete n lines after cursor d1G delete all lines from cursor to the beginning of the file dG delete all lines from cursor to the end of the file d$ delete all chars from cursor to end of the line d0 delete all chars from cursor to start of the line D delete all chars for the rest of current line yy copy the line at the cursor nyy copy n lines after cursor y1G copy all lines from cursor to the beginning of the file yG copy all lines from cursor to the end of the file y0 copy all chars from cursor to start of the line y$ copy all chars from cursor to end of the line p, P p pastes on next line after cursor, P pastes on line before cursor J merge cursor line with next line u undo previous action ctrl-r redo previous action . repeat last action r <file> read in file and insert at current position switch modes \u00b6 Key Pressing - - Function i, I enter insert mode from current cursor a, A enter insert mode, a is from next position of cursor, A is from last position of current line o, O enter insert mode, o is insert a new line after cursor, O is insert a new line before cursor r, R enter replace mode, r is replacing cursor char once, R is to keep replacing cursor char until ESC pressed Esc leave current mode sh command opens an external command shell and resume after exiting the shell ! command % execute a command from within vi; % means the current file Save, quit, and line_numbers \u00b6 Key Pressing - - Function :w write into filesystem :w !sudo tee % save changes to a file that forgot using sudo vim :q quit the editor :q! quit without saving ZZ quit and ensure any edits saved :w [filename] save as another file :r [filename] read in contents from another file :n1,n2 w [filename] save lines between n1 and n2 as another file :! [command] temporarily leave vi and see shell command results :set nu / :set number display line numbers :set nonu cancel displaying line numbers vim features \u00b6 Visual Block \u00b6 Allows us to select things spanning lines Great feature to have, even some modern IDE doesn't have this! Key Pressing - - Function v select single chars where cursor passes V select lines where cursor passes ctrl-v select rectangular blocks where cursor passes o move to the other end of marked area O move to the other corner of block ab select the entire block within () where the cursor is aB select the entire block within {} where the cursor is ib select the contents of a block within () where the cursor is iB select the contents of a block within {} where the cursor is > add a tab indent to selected block < remove a tab indent to selected block ~ switch case for selected block y copy selected area d delete selected area Tip for multiple indenting, add a numb N before pressing > or < Tip under block mode while selecting multiple lines, repeat edits to all the lines by pressing I first, make the edits, then press Esc Registers \u00b6 Registers allows copying Key Pressing - - Function :reg show registers content :* contains contents on clipboard :% contains current filename \"xy yank into register 'x' (x could be a digit or a letter) \"xp paste contents of register 'x' \"xNp paste contents of register 'x' for N times Tip registers are stored in ~/.viminfo saved registers will be loaded again at next start of vim Tip Register-0 always contains the value of last yank command Marks \u00b6 Marks allows quickly jump to a saved position in a large file. Key Pressing - - Function :marks list of saved marks ma set current position for mark 'a' `a jump to position of mark 'a' y`a yank text to position of mark 'a' Multi-window editing \u00b6 This mode allow you to see files side-by-side, very useful! You can even have three windows at the same time! Key Pressing - - Function :sp open a split window for the same file on the bottom side of this window :vsp open a split window for the same file on the left side of this window :sp [filename] open a split window for a new file on the bottom side of this window :vsp [filename] open a split window for a new file on the left side of this window ctrl-w, then hjkl OR narrows move to the window in the direction ctrl-w, then w switch windows ctrl-w, then q quit current window split :q can quit one of the window as well, like above Multi-file editing \u00b6 Need to open more than one files on startup of vim command, like this vim hosts /etc/hosts Key Pressing - - Function :e edit a file in a new buffer (while attempt to close current buffer) :n edit next file :N edit previous file :bnext, :bn go to next buffer :bprev, :bp go to previous buffer :bd delete a buffer (close a file) :ls list all open buffers :files show files opened Multi-tab editing \u00b6 Probably not quite useful today, but still offers a way to organize your workspace within the same terminal using vim. Key Pressing - - Function :tabnew [filename] open a file in a new tab ctrl-w, then T move current split window to its own tab gt, :tabnext, :tabn move to next tab gT, :tabprev, :tabp move to previous tab Ngt move to tab N :tabmove N move current tab to Nth position (starting from 0) :tabclose, :tabc close current tab and all its windows :tabdo command run command on all tabs Vim env config setting \u00b6 your behavior is automatically recorded by vim, and saved at ~/.viminfo Key Pressing - - Function :set nu/nonu set or cancel line number :set hlsearch/nohlsearch high light search or not to :set autoindent/noautoindent autoindent or not to :set backup/nobackup auto file backup or not (backed up will be [filename]~) :set ruler/noruler ruler shows the portion of current viewing portion in the file :set showmode/noshowmode whether to show --INSERT-- mode etc. :set backspace=(0/1/2) 0 or 1 means can't backspace; 2 means can backspace anything :set all show all current config parameters :set show values different from default values :syntax on/off whether to turn on/off syntax highlighting :set bg=dark/light show color differently Above preferences doesn't save unless you have a ~/.vimrc file. Write a file like this: set hlsearch set backspace=2 set autoindent set ruler set showmode set nu set bg=dark syntax on When non-English Character become mess \u00b6 Traditional Chinese characters can be encoded into big5 or utf8. If the file encode and the display decoder doesn't match, random chars can be shown. Things may be related: Linux system default supported language data: /etc/sysconfig/i18n bash: LANG var file's original encode software that opened the file Usually Windows XP files use big5 by default. Set terminal LANG=zh_TW.big5 DOS and Linux linebreak \u00b6 use cat -A can see linebreak char of a file. DOS file lines end with ^M, we call CR (^M) and LF($); Linux only have LF ($) as linebreak char. We can convert it using dos2unix or unix2dos commands! unix2dos [-kn] filename [newfile_name] -k : keep file original mtime format (not update last modified time) -n : keep old file, make a new file for the converted","title":"Vim Tricks"},{"location":"Linux/References/Vim-Tricks/#use-vivim","text":"vi has three modes: standard mode , used when opening a file with vi, can use commands to move cursor, delete, copy, paste, and search and replace. insert mode , entered after pressing i, I, o, O, a, A, r, R during standard mode command line mode , entered after pressing :, /, ? , where you can search, read/write, or any other actions can be taken.","title":"use vi/vim"},{"location":"Linux/References/Vim-Tricks/#moving-cursors","text":"Key Pressing - - Function h / left-arrow move left j / down-arrow move down k / up-arrow move up l / right-arrow move right w jump forwards to the start of a word e jump forwards to the end of a word b jump backwards to the start of a word numb N + hjkl move N times in that direction ctrl + f page down ctrl + b page up ctrl + d half page down ctrl + u half page up + move cursor to next non empty line - move cursor to previous non empty line numb N + [space] move cursor right N times 0 / [Home] move to the first char of this line ^ move to the first non-blank char of this line $ / [End] move to last char of this line H move to the first line first character of current view-port M move to the middle line first character of current view-port L move to the last line first character of current view-port G move to the last line of this file nG move to the nth line of this file gg move to the first line of this file zz center cursor on screen numb N + [enter] move cursor down for N lines","title":"Moving cursors"},{"location":"Linux/References/Vim-Tricks/#search-and-replace","text":"Key Pressing - - Function /word move down and search for 'word' ?word move up and search for 'word' \\vword end with period. non-alphanumeric chars are interpreted as special regex symbols, no escaping needed n repeat last search action N repeat last action in reversed searching direction :n1,n2s/word1/word2/g find and replace! search for word1 between line n1 and n2 and replace them with word2 :1,$s/word1/word2/g find and replace from begin to the end of this file :1,$s/word1/word2/gc find and replace from begin to the end, and asking for confirmation for each match","title":"Search and Replace"},{"location":"Linux/References/Vim-Tricks/#delete-copy-and-paste","text":"Key Pressing - - Function x, X x delete a char from right of cursor like [del], X delete one from left like [Backspace] nx delete n chars from right dd delete current line ndd delete n lines after cursor d1G delete all lines from cursor to the beginning of the file dG delete all lines from cursor to the end of the file d$ delete all chars from cursor to end of the line d0 delete all chars from cursor to start of the line D delete all chars for the rest of current line yy copy the line at the cursor nyy copy n lines after cursor y1G copy all lines from cursor to the beginning of the file yG copy all lines from cursor to the end of the file y0 copy all chars from cursor to start of the line y$ copy all chars from cursor to end of the line p, P p pastes on next line after cursor, P pastes on line before cursor J merge cursor line with next line u undo previous action ctrl-r redo previous action . repeat last action r <file> read in file and insert at current position","title":"Delete, Copy, and Paste"},{"location":"Linux/References/Vim-Tricks/#switch-modes","text":"Key Pressing - - Function i, I enter insert mode from current cursor a, A enter insert mode, a is from next position of cursor, A is from last position of current line o, O enter insert mode, o is insert a new line after cursor, O is insert a new line before cursor r, R enter replace mode, r is replacing cursor char once, R is to keep replacing cursor char until ESC pressed Esc leave current mode sh command opens an external command shell and resume after exiting the shell ! command % execute a command from within vi; % means the current file","title":"switch modes"},{"location":"Linux/References/Vim-Tricks/#save-quit-and-line_numbers","text":"Key Pressing - - Function :w write into filesystem :w !sudo tee % save changes to a file that forgot using sudo vim :q quit the editor :q! quit without saving ZZ quit and ensure any edits saved :w [filename] save as another file :r [filename] read in contents from another file :n1,n2 w [filename] save lines between n1 and n2 as another file :! [command] temporarily leave vi and see shell command results :set nu / :set number display line numbers :set nonu cancel displaying line numbers","title":"Save, quit, and line_numbers"},{"location":"Linux/References/Vim-Tricks/#vim-features","text":"","title":"vim features"},{"location":"Linux/References/Vim-Tricks/#visual-block","text":"Allows us to select things spanning lines Great feature to have, even some modern IDE doesn't have this! Key Pressing - - Function v select single chars where cursor passes V select lines where cursor passes ctrl-v select rectangular blocks where cursor passes o move to the other end of marked area O move to the other corner of block ab select the entire block within () where the cursor is aB select the entire block within {} where the cursor is ib select the contents of a block within () where the cursor is iB select the contents of a block within {} where the cursor is > add a tab indent to selected block < remove a tab indent to selected block ~ switch case for selected block y copy selected area d delete selected area Tip for multiple indenting, add a numb N before pressing > or < Tip under block mode while selecting multiple lines, repeat edits to all the lines by pressing I first, make the edits, then press Esc","title":"Visual Block"},{"location":"Linux/References/Vim-Tricks/#registers","text":"Registers allows copying Key Pressing - - Function :reg show registers content :* contains contents on clipboard :% contains current filename \"xy yank into register 'x' (x could be a digit or a letter) \"xp paste contents of register 'x' \"xNp paste contents of register 'x' for N times Tip registers are stored in ~/.viminfo saved registers will be loaded again at next start of vim Tip Register-0 always contains the value of last yank command","title":"Registers"},{"location":"Linux/References/Vim-Tricks/#marks","text":"Marks allows quickly jump to a saved position in a large file. Key Pressing - - Function :marks list of saved marks ma set current position for mark 'a' `a jump to position of mark 'a' y`a yank text to position of mark 'a'","title":"Marks"},{"location":"Linux/References/Vim-Tricks/#multi-window-editing","text":"This mode allow you to see files side-by-side, very useful! You can even have three windows at the same time! Key Pressing - - Function :sp open a split window for the same file on the bottom side of this window :vsp open a split window for the same file on the left side of this window :sp [filename] open a split window for a new file on the bottom side of this window :vsp [filename] open a split window for a new file on the left side of this window ctrl-w, then hjkl OR narrows move to the window in the direction ctrl-w, then w switch windows ctrl-w, then q quit current window split :q can quit one of the window as well, like above","title":"Multi-window editing"},{"location":"Linux/References/Vim-Tricks/#multi-file-editing","text":"Need to open more than one files on startup of vim command, like this vim hosts /etc/hosts Key Pressing - - Function :e edit a file in a new buffer (while attempt to close current buffer) :n edit next file :N edit previous file :bnext, :bn go to next buffer :bprev, :bp go to previous buffer :bd delete a buffer (close a file) :ls list all open buffers :files show files opened","title":"Multi-file editing"},{"location":"Linux/References/Vim-Tricks/#multi-tab-editing","text":"Probably not quite useful today, but still offers a way to organize your workspace within the same terminal using vim. Key Pressing - - Function :tabnew [filename] open a file in a new tab ctrl-w, then T move current split window to its own tab gt, :tabnext, :tabn move to next tab gT, :tabprev, :tabp move to previous tab Ngt move to tab N :tabmove N move current tab to Nth position (starting from 0) :tabclose, :tabc close current tab and all its windows :tabdo command run command on all tabs","title":"Multi-tab editing"},{"location":"Linux/References/Vim-Tricks/#vim-env-config-setting","text":"your behavior is automatically recorded by vim, and saved at ~/.viminfo Key Pressing - - Function :set nu/nonu set or cancel line number :set hlsearch/nohlsearch high light search or not to :set autoindent/noautoindent autoindent or not to :set backup/nobackup auto file backup or not (backed up will be [filename]~) :set ruler/noruler ruler shows the portion of current viewing portion in the file :set showmode/noshowmode whether to show --INSERT-- mode etc. :set backspace=(0/1/2) 0 or 1 means can't backspace; 2 means can backspace anything :set all show all current config parameters :set show values different from default values :syntax on/off whether to turn on/off syntax highlighting :set bg=dark/light show color differently Above preferences doesn't save unless you have a ~/.vimrc file. Write a file like this: set hlsearch set backspace=2 set autoindent set ruler set showmode set nu set bg=dark syntax on","title":"Vim env config setting"},{"location":"Linux/References/Vim-Tricks/#when-non-english-character-become-mess","text":"Traditional Chinese characters can be encoded into big5 or utf8. If the file encode and the display decoder doesn't match, random chars can be shown. Things may be related: Linux system default supported language data: /etc/sysconfig/i18n bash: LANG var file's original encode software that opened the file Usually Windows XP files use big5 by default. Set terminal LANG=zh_TW.big5","title":"When non-English Character become mess"},{"location":"Linux/References/Vim-Tricks/#dos-and-linux-linebreak","text":"use cat -A can see linebreak char of a file. DOS file lines end with ^M, we call CR (^M) and LF($); Linux only have LF ($) as linebreak char. We can convert it using dos2unix or unix2dos commands! unix2dos [-kn] filename [newfile_name] -k : keep file original mtime format (not update last modified time) -n : keep old file, make a new file for the converted","title":"DOS and Linux linebreak"},{"location":"Linux/References/nano-Tricks/","text":"nano like vi , is a very common and popular text editor for Linux. File Processing \u00b6 Keys Functions Ctrl-S Save current file Ctrl-O Save as Ctrl-R Insert a file into current one Ctrl-X Close and exit Editing \u00b6 Keys Functions Ctrl-K Cut current line Alt-6 Copy current line Ctrl-U Paste Alt-U Undo Alt-E Redo Ctrl-Del Delete word Alt-Del Delete line Alt-A Toggle block mark mode Tab Indent marked Shift-Tab Unindent marked Search \u00b6 Keys Functions Ctrl-Q Search backward Ctrl-W Search forward Alt-Q Find next backward Alt-W Find next forward Alt-R Find and replace Moving cursor \u00b6 Keys Functions Ctrl-Left Move one word backward Ctrl-Right Move one word forward Ctrl-A Move to start of line Ctrl-E Move to end of line Ctrl-Y Move one page up Ctrl-V Move one page down Alt-G Go to line number Information \u00b6 Keys Functions Ctrl-C Current cursor position Alt-N Toggle line numbers Alt-P Toggle show whitespace Alt-X Toggle help lines Keys Functions ctrl-G get help ctrl-X leave nano ctrl-O save document ctrl-R read from another file ctrl-W search for text string ctrl-C learn the line number and column number at current cursor ctrl-_ input line number and move to it alt-Y syntax highlighting on/off alt-M support using mouse to move the cursor","title":"nano Reference"},{"location":"Linux/References/nano-Tricks/#file-processing","text":"Keys Functions Ctrl-S Save current file Ctrl-O Save as Ctrl-R Insert a file into current one Ctrl-X Close and exit","title":"File Processing"},{"location":"Linux/References/nano-Tricks/#editing","text":"Keys Functions Ctrl-K Cut current line Alt-6 Copy current line Ctrl-U Paste Alt-U Undo Alt-E Redo Ctrl-Del Delete word Alt-Del Delete line Alt-A Toggle block mark mode Tab Indent marked Shift-Tab Unindent marked","title":"Editing"},{"location":"Linux/References/nano-Tricks/#search","text":"Keys Functions Ctrl-Q Search backward Ctrl-W Search forward Alt-Q Find next backward Alt-W Find next forward Alt-R Find and replace","title":"Search"},{"location":"Linux/References/nano-Tricks/#moving-cursor","text":"Keys Functions Ctrl-Left Move one word backward Ctrl-Right Move one word forward Ctrl-A Move to start of line Ctrl-E Move to end of line Ctrl-Y Move one page up Ctrl-V Move one page down Alt-G Go to line number","title":"Moving cursor"},{"location":"Linux/References/nano-Tricks/#information","text":"Keys Functions Ctrl-C Current cursor position Alt-N Toggle line numbers Alt-P Toggle show whitespace Alt-X Toggle help lines Keys Functions ctrl-G get help ctrl-X leave nano ctrl-O save document ctrl-R read from another file ctrl-W search for text string ctrl-C learn the line number and column number at current cursor ctrl-_ input line number and move to it alt-Y syntax highlighting on/off alt-M support using mouse to move the cursor","title":"Information"},{"location":"Programming-Lang-Reference/Github-Markdown/","text":"Code # This is an \\<h1\\> tag ## This is an \\<h2\\> tag ###### This is an \\<h6\\> tag *This text will be italic* _This will also be italic_ **This text will be bold** __This will also be bold__ _You **can** combine them_ * Item 1 * Item 2 * Item 2a * Item 2b 1. Item 1 2. Item 2 3. Item 3 1. Item 3a 2. Item 3b ![Alt Text if image fail to load](/images/logo.png) http://github.com (automatic link conversion) [GitHub](http://github.com) (link with text) Block quotes - as Kanye West said: > We're living the future so > the present is our past. I think you should use an `<addr>` element here instead. ``` javascript function fancyAlert(arg) { if(arg) { $.facebox({div:'#foo'}) } } ``` - [x] @mentions, #refs, [links](), **formatting**, and <del>tags</del> supported - [x] list syntax required (any unordered or ordered list supported) - [x] this is a complete item - [ ] this is an incomplete item First Header | Second Header ------------ | ------------- Content from cell 1 | Content from cell 2 Content in the first column | Content in the second column ~~this~~ @octocat :+1: This PR looks great - it's ready to merge! :shipit: Following are the results redered from above formats Results This is an \\<h1> tag \u00b6 This is an \\<h2> tag \u00b6 This is an \\<h6> tag \u00b6 This text will be italic This will also be italic This text will be bold This will also be bold You can combine them Item 1 Item 2 Item 2a Item 2b Item 1 Item 2 Item 3 Item 3a Item 3b http://github.com (automatic link conversion) GitHub (link with text) Block quotes - as Kanye West said: We're living the future so the present is our past. I think you should use an <addr> element here instead. function fancyAlert ( arg ) { if ( arg ) { $ . facebox ({ div : '#foo' }) } } [x] @mentions, #refs, links , formatting , and tags supported [x] list syntax required (any unordered or ordered list supported) [x] this is a complete item [ ] this is an incomplete item First Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column ~~this~~ @octocat This PR looks great - it's ready to merge! :shipit:","title":"Github Markdown Guide"},{"location":"Programming-Lang-Reference/Github-Markdown/#this-is-an-h1-tag","text":"","title":"This is an \\&lt;h1> tag"},{"location":"Programming-Lang-Reference/Github-Markdown/#this-is-an-h2-tag","text":"","title":"This is an \\&lt;h2> tag"},{"location":"Programming-Lang-Reference/Github-Markdown/#this-is-an-h6-tag","text":"This text will be italic This will also be italic This text will be bold This will also be bold You can combine them Item 1 Item 2 Item 2a Item 2b Item 1 Item 2 Item 3 Item 3a Item 3b http://github.com (automatic link conversion) GitHub (link with text) Block quotes - as Kanye West said: We're living the future so the present is our past. I think you should use an <addr> element here instead. function fancyAlert ( arg ) { if ( arg ) { $ . facebox ({ div : '#foo' }) } } [x] @mentions, #refs, links , formatting , and tags supported [x] list syntax required (any unordered or ordered list supported) [x] this is a complete item [ ] this is an incomplete item First Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column ~~this~~ @octocat This PR looks great - it's ready to merge! :shipit:","title":"This is an \\&lt;h6> tag"},{"location":"Programming-Lang-Reference/Regex-Specs/","text":"The regular expression engine starts as soon as it can, grabs as much as it can, then tries to finish as soon as it can, while taking the first decision available to it. Anchors \u00b6 char - - usage ^ Start of string, or start of line in multi-line pattern \\A Start of string $ End of string, or end of line in multi-line pattern \\Z End of string \\b Word boundary \\B Not word boundary \\< Start of word \\> End of word Character Classes \u00b6 char - - usage \\c Control character \\s White space (space or tab) \\S Not white space \\d Digit, same as [0-9] \\D Not digit, same as [^0-9] \\w Alphanumeric (letters, numbers, underscore) \\W Not alphanumeric \\x Hexade cimal digit \\O Octal digit POSIX Classes \u00b6 char - - usage [:upper:] Upper case letters [:lower:] Lower case letters [:alpha:] All letters [:alnum:] Digits and letters [:digit:] Digits [:xdigit:] Hexade cimal digits [:punct:] Punctuation [:blank:] Space and tab [:space:] Blank characters [:cntrl:] Control characters [:graph:] Printed characters [:print:] Printed characters and spaces [:word:] Digits, letters and underscore Assertions \u00b6 char - - usage ?= Lookahead assertion ?! Negative lookahead ?<= Lookbehind assertion ?!= or ?<!-- Negative lookbehind ?--> Once-only Subexpression ?() Condition [if then] ?()| Condition [if then else] ?# Comment Quantifiers \u00b6 char - - usage * 0 or more + 1 or more ? 0 or 1 {3} Exactly 3 {3,} 3 or more {,5} at most 5 {3,5} 3, 4 or 5 Tip Add a ? to a quantifier to make it ungreedy. Escape Sequences \u00b6 char - - usage \\ Escape following character \\Q Begin literal sequence \\E End literal sequence Tip Within a literal sequence, no need to escape Metacharacters Metacharacters in regex need to be escaped in order to let regex recognize and match them. Common Metacharacters: ^ [ . $ { * ( \\ + ) | ? < > Special Characters \u00b6 char - - usage \\n New line \\r Carriage return \\t Tab \\v Vertical tab \\f Form feed \\xxx Octal character xxx \\xhh Hex character hh To match above special characters, need to escape the backslash like this \\\\n in a regex expression. Groups and Ranges \u00b6 char - - usage . Any character except new line (\\n) (a|b) a or b (...) Group (?:...) Passive (non-capturing) group [abc] Range (a or b or c) [^abc] Not (a or b or c) [a-q] Lower case letter from a to q [A-Q] Upper case letter from A to Q [0-7] Digit from 0 to 7 \\x Group/subpattern number \"x\" Tip Ranges are inclusive Pattern Modifiers \u00b6 char - - usage g Global match i * Case-insensitive m * Multiple lines s * Treat string as single line x * Allow comments and whitespace in pattern e * Evaluate replacement U * Ungreedy pattern Tip starred (*) are Perl-compatible Regular Expressions (PCRE) modifiers https://www.pcre.org/original/doc/html/index.html String Replacement \u00b6 char - - usage $n nth non-passive group $2 \"xyz\" in /^(abc(xyz))$/ $1 \"xyz\" in /^(?:abc)(xyz)$/ $` Before matched string $' After matched string $+ Last matched string $& Entire matched string Tip Some regex implem ent ations use '\\' instead of '$' (i.e. regex used by Splunk)","title":"Regex Cheatsheet"},{"location":"Programming-Lang-Reference/Regex-Specs/#anchors","text":"char - - usage ^ Start of string, or start of line in multi-line pattern \\A Start of string $ End of string, or end of line in multi-line pattern \\Z End of string \\b Word boundary \\B Not word boundary \\< Start of word \\> End of word","title":"Anchors"},{"location":"Programming-Lang-Reference/Regex-Specs/#character-classes","text":"char - - usage \\c Control character \\s White space (space or tab) \\S Not white space \\d Digit, same as [0-9] \\D Not digit, same as [^0-9] \\w Alphanumeric (letters, numbers, underscore) \\W Not alphanumeric \\x Hexade cimal digit \\O Octal digit","title":"Character Classes"},{"location":"Programming-Lang-Reference/Regex-Specs/#posix-classes","text":"char - - usage [:upper:] Upper case letters [:lower:] Lower case letters [:alpha:] All letters [:alnum:] Digits and letters [:digit:] Digits [:xdigit:] Hexade cimal digits [:punct:] Punctuation [:blank:] Space and tab [:space:] Blank characters [:cntrl:] Control characters [:graph:] Printed characters [:print:] Printed characters and spaces [:word:] Digits, letters and underscore","title":"POSIX Classes"},{"location":"Programming-Lang-Reference/Regex-Specs/#assertions","text":"char - - usage ?= Lookahead assertion ?! Negative lookahead ?<= Lookbehind assertion ?!= or ?<!-- Negative lookbehind ?--> Once-only Subexpression ?() Condition [if then] ?()| Condition [if then else] ?# Comment","title":"Assertions"},{"location":"Programming-Lang-Reference/Regex-Specs/#quantifiers","text":"char - - usage * 0 or more + 1 or more ? 0 or 1 {3} Exactly 3 {3,} 3 or more {,5} at most 5 {3,5} 3, 4 or 5 Tip Add a ? to a quantifier to make it ungreedy.","title":"Quantifiers"},{"location":"Programming-Lang-Reference/Regex-Specs/#escape-sequences","text":"char - - usage \\ Escape following character \\Q Begin literal sequence \\E End literal sequence Tip Within a literal sequence, no need to escape Metacharacters Metacharacters in regex need to be escaped in order to let regex recognize and match them. Common Metacharacters: ^ [ . $ { * ( \\ + ) | ? < >","title":"Escape Sequences"},{"location":"Programming-Lang-Reference/Regex-Specs/#special-characters","text":"char - - usage \\n New line \\r Carriage return \\t Tab \\v Vertical tab \\f Form feed \\xxx Octal character xxx \\xhh Hex character hh To match above special characters, need to escape the backslash like this \\\\n in a regex expression.","title":"Special Characters"},{"location":"Programming-Lang-Reference/Regex-Specs/#groups-and-ranges","text":"char - - usage . Any character except new line (\\n) (a|b) a or b (...) Group (?:...) Passive (non-capturing) group [abc] Range (a or b or c) [^abc] Not (a or b or c) [a-q] Lower case letter from a to q [A-Q] Upper case letter from A to Q [0-7] Digit from 0 to 7 \\x Group/subpattern number \"x\" Tip Ranges are inclusive","title":"Groups and Ranges"},{"location":"Programming-Lang-Reference/Regex-Specs/#pattern-modifiers","text":"char - - usage g Global match i * Case-insensitive m * Multiple lines s * Treat string as single line x * Allow comments and whitespace in pattern e * Evaluate replacement U * Ungreedy pattern Tip starred (*) are Perl-compatible Regular Expressions (PCRE) modifiers https://www.pcre.org/original/doc/html/index.html","title":"Pattern Modifiers"},{"location":"Programming-Lang-Reference/Regex-Specs/#string-replacement","text":"char - - usage $n nth non-passive group $2 \"xyz\" in /^(abc(xyz))$/ $1 \"xyz\" in /^(?:abc)(xyz)$/ $` Before matched string $' After matched string $+ Last matched string $& Entire matched string Tip Some regex implem ent ations use '\\' instead of '$' (i.e. regex used by Splunk)","title":"String Replacement"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/","text":"This set of notes is taken from learncpp.com . C++ language references: cplusplus.com cppreference.com Continue reading: https://www.learncpp.com/cpp-tutorial/stdvector-capacity-and-stack-behavior/ Overview \u00b6 C++ is one of the popular high level languages and requires compiling source code into machine code for computer to run. C++ excels in situations where high performance and precise control over memory and other resources is needed. Some common places where C++ shines: Video games Real-time systems, i.e. transportation, manufacturing Financial applications, i.e. stock exchange Graphical applications, simulations Productivity applications Embedded systems Audio and video processing AI and neural networks C++ is designed to allow the programmer a high degree of freedom to do what they want, which is both wonderful and dangerous. Basics \u00b6 Compiler \u00b6 Preprocessor \u00b6 Prior to compilation, the code file goes through a phase known as translation. This phase involves the interpretation of preprocessor , which can be throught of as a separate program that manipulates the text in each code file. Preprocessor directives are instructions that start with a # symbol and end with a newline (no semicolon). Directives tell the preprocessor to perform specific particular text manipulation tasks on normal code text (non-directive lines). Directives are only valid from the point of definition to the end of the file in which they are defined. #include is one type of directive, which tells preprocessor to replace it with the contents of the included file, which gets preprocessed as well recursively. Avoid using relative paths in #include directives. Instead, pass in -I option to specify alternate directory to look for source files. In this way, changing directory structure do not require updating the include paths. The #define directive can be used to create a macro , a rule that defines how input text is converted into replacement output text . Two basic types of macros: object-like macros, and function-like macros. Object-like macros are traditionally typed in all capital letters, using underscores to represent spaces. They used to be used as a cheaper alternative to constant variables and is considered a legacy feature. A speical use case for Object-like macro with no substitution text is to remove occurrance of certain text, or serve as a marker for #ifdef . #ifdef, #ifndef, and #endif allows the preprocessor to check whether an identifier has been previously #defined , and skip redefining it if that is the case. This is known as the header guard . #ifndef HEADERFILENAME_H #define HEADERFILENAME_H // declarations #endif #pragma once serves as an alternate form of header guards but only supported by modern compilers, not old compilers. In this way, when this header file is being included by multiple files that end up being merged together, they won't cause repeated declarations error. #if 0, #endif is commonly used to exclude a block of code from being compiled (as if they are commented out). Compilation \u00b6 During compilation, C++ compiler sequentially goes through each source code ( .h .c .cpp ) file in your program and does two things: check the syntax correctness translate code into machine language and create the object file ( .o .obj ) After the object files are created, the linker comes in: resolve dependencies merge object files into a single executable program pull in library files referenced, which are collections of precompiled and prepackaged code for reuse Compile code with g++ <source_files> -o <output_program> . The output program can be run directly with ./<output_program> . C++ has evolved and has many language standards. Pass in compiler flags during compilation to change the language standard. i.e. g++ <language_standard_flag> <source_files> -o <output_program> , where language_standard_flag can be -std=c++1/c++14/c++17/c++2a Documenting and Comments \u00b6 Single-line comment with // , multi-line comment with /* */ Indent with 4 spaces. Put the opening curly brace on the same line as the statement. Lines should keep a 80-char maximum and wrap to new line at appropriate place. Program entry \u00b6 Every C++ program must have a main function or it will fail to link. int main () { // main function body return 0 ; } The main function must have int return type, which will be the exit code. An exit code of 0 typically signals the program execution was successful. To make the program take command line arguments, write the main function this way: int main ( int argc , char * argv []) { // main function body return 0 ; } argc is the count of arguments passed to the program and is >= 1, since the first argument is always the absolute path to the program. argv is an array of C-style strings and holds the command line arguments. Data and Variables \u00b6 Declaration, Initialization \u00b6 Variable declaration can be one line each or multiple variables of the same type declared in one line: int a ; // declared but uninitialized int b , c ; // declare multiple vars in one line double d = 1.0 ; // inline copy constant initialize int e ( 5 ); // inline initialize int f { 10 }; // inline brace initialize (preferred) int g = { 100 }; // inline copy brace initialize int h {}; // inline value initialize to 0, aka zero initialization (default is 0 for int) Some rules of thumb: initialize your variables upon creation, using one of the inline methods mentioned above initialize one variable on each line prefer inline brace initialize since in some circumstances it reports error when wrong variable type is being assigned, i.e. double assigned to int prefer value initialization when the variable value will be replaced later before use Naming \u00b6 Reserved keywords must not be used as variable identifiers. Some naming rules of thumb for variables and function names: compose of letters, numbers, and underscore begin with a lower case letter use underscore to separate whole words, or use camelCase. Be consistent throught the program Data types \u00b6 Foundamental data types: Boolean: bool Character: char, wchar_t, char8_t (C++20), char16_t (C++11), char32_t (C++11) Integer: short, int, long, long long (C++11) Floating Point: float, double, long double Null Pointer: std::nullptr_t (C++11) Void: void Booleans are represented internally as integers. When printing you will get 1 and 0. To get true and false, set std::cout << std::boolalpha; , and similarly for stdin std::cin >> std::boolalpha; . Integers are signed by default. To declare unsigned integers, prefix variable type with unsigned keyword. i.e. unsigned int num; However, you should favor signed numbers over unsigned numbers for holding quantities (even quantities that should be non-negative) and mathematical operations. Avoid mixing signed and unsigned numbers. Use std::setprecision(<int>) to set the precision of floating numbers (default is 6). Although be careful changing this, since higher precision may introduce rounding errors. The _t suffix means type. The sizeof operator is a unary operator that takes either a type or a variable, and returns its size in bytes (of type std::size_t ). The integer types have unguaranteed size on different hardware. There is only a guaranteed minimal size. i.e. an int can be 2 bytes in old architecture but 4 bytes on modern architecture. Fixed-width integers are introduced by including the <cstdint> header to solve the issue of unguaranteed integer sizes on different hardware. They can be declared as: std :: int8_t std :: int16_t std :: int32_t std :: int64_t // unsigned std :: uint8_t std :: uint16_t std :: uint32_t std :: uint64_t Still, fixed-width integers are not guaranteed to be defined on all architectures. And for some modern architecture, using fixed-width integer might slow down the problem, as the CPU has to do extra work to process the smaller variables. For those cases, consider using fast and least integers . Some ruls of thumb: prefer int when integer size doesn't matter prefer std::int#_t when storing a quantity that needs a guaranteed range prefer std::uint#_t when doing bit manipulation or when some overflow/wrap-around behavior is desired. avoid using unsigned types for quantities avoid 8-bit fixed-width integer types which often get treated as chars avoid fast and least fixed-width types avoid compiler-specific fixed-width integers, such as VC++ __int8 Literal constants are fixed values explicitly specified. Examples: 5 // int literal 5u // unsigned int literal 30L // long literal 0214 // octal literal 0xF // hexadecimal literal 0b1001'1110 // binary literal, with ' as digit separator 3.0 // double literal 5.0f // float literal \"Helloworld\" // C-style string literal C++ will concatenate sequential string literals. The library <bitset> allows printing binary numbers. i.e. std::bitset<8> bin{ 0b1100'0100 }; Constant variables , aka symbolic constants, can be created using the const keyword, where the variable value can be determined in compile-time or runtime. Use constexpr to enforce compile-time constant evaluation. Compound types: Functions Arrays Pointers, to object or to function Reference, L-value reference or R-value reference Enums, scoped or unscoped Classes, Structs, Unions Casting and types \u00b6 A numeric promotion is the conversion of smaller numeric types to larger numeric type. Use explicit type conversion to explicitly tell the compiler to convert a value from one type to another type. The C-style casts is done via the (<new_type>) variable operator or with the function-like cast <new_type>(variable) . Avoid C-style casts since it risks misuse and not producing expected behavior. There is also static cast using static_cast<new_type>(<expression>) which allows carry out type conversions that may loss precision or cause overflow if done with implicit (automatic) type conversion. static_cast is best used to convert one fundamental type into another. The using or typedef keyword can be used to create type aliases , which creates an alias for an existing data type. Use a \"_t\" or \"_type\" suffix to help decrease the chance of naming collisions with other identifiers. i.e. using distance_t = double ; // define distance_t as an alias for type double typedef double distance_t ; // also does the same thing, but discouraged Use type aliases for platform independent coding. Because char, short, int, and long give no indication of their size, it is fairly common for cross-platform programs to use type aliases to define aliases that include the type's size in bits. Also use type aliases to make complex types simple, or help with code documentation and comprehension. i.e. using pairlist_t = std::vector<std::pair<std::string, int>>; Type deduction , aka type inference, is a feature that allows the compiler to deduce the type of an object from the object's non-empty initializer through the auto keyword. Type deduction can save a lot of typing and typos in some cases, and is generally safe to use for objects, and improves code readability. auto can also be used on function return type to let compiler infer return type from return statements. Favor explicit return types over function return type deduction. However, you can use the tailing return type syntax with the auto return type to make function names line up and improve code readability. i.e. in forward-declarations: auto add ( int x , int y ) -> int ; auto divide ( double x , double y ) -> double ; auto printSomething () -> void ; Read more about casting strings \u00b6 string type is not a fundamental type, but a compound type. strings are double quoted characters. Quoted text separated by nothing but whitespace (spaces, tabs, or newlines) will be concatenated. #include the <string> header to bring in the declarations for std::string . A string's length is optained with str.length() function call. The plain quote-surrounded strings are in fact C-style strings , not std::string. To specify different types of strings: C-style string is essentially an array of chars which implicitly adds a null character \\0 to end of the string. The strlen() function from cstring> returns the length of the C-style string without the null terminator. Other useful C-style functions: strcpy() -- copy a string to another string strcpy_s() strlcpy() -- string copy allowing a size parameter strcat() -- appends one string to another (dangerous) strncat() -- appends one string to another (with buffer length check) strcmp() -- compare two strings (returns 0 if equal) strncmp() -- compare two strings up to a specific number of characters (returns 0 if equal) As a rule of thumb, use std::string or std::string_view (next lesson) instead of C-style strings. Prefer std::string_view over std::string for read-only strings C++17 introduces std::string_view which provides a view of a string to avoid unnecessary copies of strings. It also contains functions for shrinking the view of the string which do not reflect on the underlying string. #include <string> #include <string_view> void foo () { auto s1 { \"Hello, world\" }; // c-style string of type const char* auto s2 { \"Hello\" s }; // std::string auto s3 { \"hi\" sv }; // std::string_view auto s4 { s1 }; // std::string auto s5 { s4 }; // std::string_view auto s6 { static_cast < std :: string > ( s5 ) }; // std::string auto s7 { s6 . c_str () }; // c-style string } Prefer passing strings using std::string_view (by value) instead of const std::string&, unless your function calls other functions that require C-style strings or std::string parameters. Operators \u00b6 Arithematical: + - * / % ++ -- Bitwise: & | ^ ~ << >> Logical: == != < > <= >= && || sizeof: sizeof (<type_or_variable>) Type casting: (<type>) <variable> Precedence of operators Variable scopes \u00b6 Variables declared within a function scope is local variable and only visible within the closure of the function. Arrays \u00b6 An array is an aggregate data type that holds many values of the same type through a single identifier. A fixed array is an array where the length is known at compile time. i.e. int prime[5]{}; or int array[]{ 0, 1, 2, 3, 4 }; Copying large arrays can be very expensive, C++ does not copy an array when an array is passed into a function. A fixed array identifier decay (inplicitly convert) to a pointer that points to the first element of the array. This makes void printSize(int array[]); and void printSize(int* array); declarations identical. In most cases, because the pointer doesn't know how large the array is, you'll need to pass in the array size as a separate parameter anyway. Favor the pointer syntax (*) over the array syntax ([]) for array function parameters. When using arrays, ensure that your indices are valid for the range of your array, as compiler does not check this. Array size can be accessed with std::size(<array>) . C++ multi-dimensional array can be declared as int array[3][5]{}; . Array indexing: int main () { int array []{ 9 , 7 , 5 , 3 , 1 }; std :: cout << & array [ 1 ] << '\\n' ; // print memory address of array element 1 std :: cout << array + 1 << '\\n' ; // print memory address of array pointer + 1 std :: cout << array [ 1 ] << '\\n' ; // prints 7 std :: cout << * ( array + 1 ) << '\\n' ; // prints 7 return 0 ; } std::array from <array> works like fixed arrays and makes array management easier. It does auto clean up after going out of scope, and offers convenient functions like size() . i.e. std::array<int, 3> myArray; creates an integer array of size 3. std::array allows assigning values to the array using an initializer list. Access std::array values with the subscript operator [] or the at() function. Always pass std::array by reference or const reference to avoid unnecessary copies. Favor std::array over built-in fixed arrays for any non-trivial array use. A dynamic array allows choosing an array length at runtime. Use new[] and delete[] with dynamic arrays. i.e. int* array{ new int[length]{} }; and delete[] array; . Dynamic arrays can be initialized using initializer lists, and benefits from auto type deduction. i.e. auto* array{ new int[5]{ 9, 7, 5, 3, 1 } }; Note that for-each loop do not work with pointer arrays since the array size is unknown. A pointer to a pointer can be used to create dynamical multi-dimensional arrays. i.e. void foo () { int ** array2d { new int * [ 10 ] }; for ( int i = 0 ; i < 10 ; ++ i ) { array2d [ i ] = new int [ 5 ]; } array2d [ 4 ][ 3 ]; // access // free up for ( int i = 0 ; i < 10 ; ++ i ) { delete [] array2d [ i ]; } delete [] array2d ; } std::vector from <vector> makes working with dynamic arrays safer and easier. It does auto clean up after going out of scope, and offers convenient functions like size(), resize() . i.e. std::vector<int> empty {}; std::vector allows assigning values to the array using an initializer list. Access std::vector values with the subscript operator [] or the at() function. An iterator is an object designed to traverse through a container (array, string) and provide access to each element along the way. The simplest kind of iterator is a pointer , which works for data stored sequentially in memory. void foo () { std :: array a { 0 , 1 , 2 , 3 , 4 , 5 , 6 }; auto begin { & a [ 0 ] }; // or { a.data() }, { a.begin() }, { std::begin(a) } auto end { begin + std :: size ( a ) }; // or { a.end() }, { std::end(a) } for ( auto ptr { begin }; ptr != end ; ++ ptr ) { std :: cout << * ptr << ' ' ; } } Functions \u00b6 A function must be either declared or defined before it is being used. Offically, a forward declaration allows us to tell the compiler about the existence of an identifier before actually defining the identifier. This is when header files come into play. return_type function_name ( arg_type arg1 , arg_type arg2 , ...); // forward declaration; the var names are optional but good to keep as it is self-documenting return_type function_name ( arg_type arg1 , arg_type arg2 , ...) { // function body } A default argument is a default value provided for a function parameter. When making a function call, the caller can optionally provide an argument for any function parameter that has a default argument. Default arguments can only be provided for the rightmost unspecified parameters, and can be declared in either the forward declaration or the function definition, but not both (best to do it in forward declaration). Overloading \u00b6 Function overloading allows us to create multiple functions with the same name, so long as each identically named function has different parameters (including ellipsis parameters i.e. void foo(int x, ...); ), or function-level qualifiers (const, volatile, ref-qualifiers). Templating \u00b6 Template system was designed to simplify the process of creating functions (or classes) that are able to work with different data types. A placeholder type represents some type that is not known at the time the template is written, but that will be provided later. Use a single capital letter (starting with T) to name the placeholder type. A function template is a function-like definition that is used to generate overloaded functions, each with a different set of actual types. i.e. template < typename T > // template parameter declaration T max ( T x , T y ) { // function template definition return ( x > y ) ? x : y ; } int main () { std :: cout << max < int > ( 1 , 2 ); // instantiates and calls function max<int>(int, int) std :: cout << max <> ( 1 , 2 ); // compiler deduced type from arguments std :: cout << max ( 1 , 2 ); // compiler deduced type from arguments return 0 ; } Favor the normal function call syntax over template argument deduction when using function templates. Inline \u00b6 Inline expansion is a process where a function call is replaced by the code from the called function\u2019s definition. Modern optimizing compilers are typically very good at determining which functions should be made inline to improve final executable performance. A function that is eligible to have its function calls expanded is called an inline function . Functions that are always expanded inline: defined inside a class, struct, or union. constexpr functions Namespacing \u00b6 C++ does not allow the same identifier to be defined within the same build context, aka naming collision . This can be happening more commonly when a program uses many libraries. A namespace is a region that allows you to declare names inside of it for the purpose of disambiguation. Any name declared inside the namespace won't be mistaken for identical names in other scopes. std is itself a namespace. :: is called the scope resolution operator. Any name that is not defined inside a class, function, or a namespace is considered to be part of an implicitly defined global namespace . You can define a namespace by enclosing the identifiers within a namespace block: namespace < namespace_name > { // namespace body } The same namespace can be created anywhere, as long as the identifiers within all namespaces of a kind do not clash. namespaces can also be nested , by enclosing one in another. One way to access identifiers inside a namespace is to use a using directive statement : using namespace std ; // at top of program after includes void some_function () { cout << \"Hello\" ; // directly access functions without namespace prefix } However, using namespace should be generally avoided since it increases the risk of causing naming collisions, especially for std namespace. You can also create namespace aliases and switch to a different namespace for everywhere it is referenced. Like so: namespace active = foo ; std :: out << active :: getMessage (); Namespace is designed primarily as a mechanism for preventing naming collisions. In applications, namespaces can be used to separate application-specific code from code that might be reusable later (e.g. math functions). When writing a library or code that you want to distribute to others, always place your code inside a namespace. The using declaration can also be used to allow using an unqualified name (with no scope) as an alias for a qualified name. i.e. using std :: cout ; cout << \"Hello\" ; // no qualified scope resolution is needed anymore An unnamed namespace (aka anonymous namespace) is a namespace that is defined without a name, which is treated as if it is part of the parent namespace. It is typically used when you have a lot of content that you want to ensure stays local to a given file. unnamed namespace does this and saves you the trouble to mark all declarations static . An inline namespace is one typically used to version content. Its content is also treated as if it is part of the parent namespace. Its advantage is that it preserves the function of existing programs while allowing newer programs to take advantage of newer/better variations by referencing with the newer namespace scope. i.e. inline namespace v1 { void doSomething () { // body } } namespace v2 { void doSomething () { // body } } int main () { doSomething (); // calls v1 version v1 :: doSomething (); // calls v1 version v2 :: doSomething (); // calls v2 version } Now when you decide to make v2 the official version, switch the inline keyword of the two versions. Ellipsis \u00b6 Ellipsis argument allows passing a variable number of parameters to a function. Ellipsis are potentially dangerous because it does not know how many parameters are actually passed, nor does it do type check for the passed data types. Functions that use ellipsis must have at least one non-ellipsis parameter. It is conceptually useful to think of the ellipsis as an array that holds any additional parameters beyond those in the argument_list. #include <cstdarg> // needed to use ellipsis int getSum ( int count , ...) { int sum { 0 }; // We access the ellipsis through a va_list, so let's declare one std :: va_list list ; // We initialize the va_list using va_start. va_start ( list , count ); for ( int i { 0 }; i < count ; ++ i ) { sum += va_arg ( list , int ); // auto advances the pointer } // cleanup va_list va_end ( list ); return sum ; } In general Ellipsis should be avoided unless there is a compelling reason not to. Lambda \u00b6 A lambda expression , aka function literal, allows us to define an anonymous function inside another function, and take advantage of the closure from naming conflicts. It is stored in the program as a functor object, which overloads the () operator. It takes the form: // captureClause, parameters, and returnType can be omitted if not required [ captureClause ] ( parameters ) -> returnType { // statements; } A lambda can be stored in a variable and passed later. These are valid ways: int main () { // A regular function pointer. Only works with an empty capture clause. double ( * addNumbers1 )( double , double ){ []( double a , double b ) { return ( a + b ); } }; // Using std::function. The lambda could have a non-empty capture clause. std :: function addNumbers2 { // note: pre-C++17, use std::function<double(double, double)> instead []( double a , double b ) { return ( a + b ); } }; // Using auto. Stores the lambda with its real type. auto addNumbers3 { []( double a , double b ) { return ( a + b ); } }; return 0 ; } Use auto when initializing variables with lambdas, and std::function if you can\u2019t initialize the variable with the lambda. Since C++14 it is allowed to use auto for lambda parameters to define generic lambdas. A unique lambda will be generated for each different type that auto resolves to. The capture clause is used to (indirectly) give a lambda access to variables available in the surrounding scope that it normally would not have access to, by enclosing the variable name (comma-separated) within the [] syntax. The captured variables of a lambda are constant clones of the outer scope variables, not the actual variables. The cloned variable can be made mutable with the mutable keyword added after the parameter list. Captured variables are members of the lambda object, their values are persisted across multiple calls to the lambda. Passing mutable lambdas can be dangerous as the passed lambda can be copies of its functor. To pass it as a reference, wrap it with std::ref() function which yields a std::reference_wrapper type to ensure the lambda does not make copies while being passed to another function. You can also capture variables by reference (prepending & ) to allow it affect the value of the variable within lambda calls. There is a chance for leaving dangling references in lambda so ensure captured variables outlive the lambda. Default capture can be used to capture all variables mentioned in the lambda. To capture by value, pass in = ; to capture by reference, pass in & . Default captures can be mixed with normal captures to capture some variables by value and some by reference, default capture operator must be the first in the list. You can define new variable from captured variables in the capture brackets using initialization braces. But it is best to do it outside and capture it. Function pointers \u00b6 C++ implicitly converts a function into a function pointer as needed. Functions used as arguments to another function are sometimes called callback functions . Function pointers can be initialized like so int (*fcnPtr)(int){ &foo }; which makes fcnPtr points to function foo that has return type of int and takes one int parameter. It can also be initialized with nullptr value. Calling with function pointer is like so (*fcnPtr)(10); or fcnPtr(10); . Make the function pointer type shorter with type alias: using FooFunction = int(*)(int); , or use auto type deduction. Alternatively, use std::function from <functional> , std::function<int(int)> fcn to declare a std::function object. Global variables \u00b6 Global variables are usually declared at the top of a file, below the includes, and have identifiers prefixed with g_ to help easily differentiate from local variables. Global variables are created when the program starts, and destroyed when it ends. They are also known as static variables. Non-constant global variables should be avoided. Global variables by default are visible from other files. You can limit non-constant global variable's visibility internal within a file (called internal linkage ), by using the static keyword, which is a storage class specifier, see also extern and mutable . Internal objects (and functions) that are defined in different files are considered to be independent entities. To use an external global variable defined in another file, you must do forward declaration of that variable, i.e. extern int <var_name>; . However, constexpr variables has no effect with extern keyword, as its type requires the value to be determined at compile time, so constexpr can only be limited to file internal use. // External global variable definitions: int g_x ; // non-initialized external global variable extern const int g_x { 1 }; // initialized const external global variable extern constexpr int g_x { 2 }; // initialized constexpr external global variable // Forward declarations extern int g_y ; // forward declaration for non-constant global variable extern const int g_y ; // forward declaration for const global variable extern constexpr int g_y ; // not allowed: constexpr variables can't be forward declared By default: functions have external linkage non-constant global variables have external linkage constant global variables have internal linkage inline variable is another way to avoid the same variables being copied into multiple files with includes. The linker ensures there is only one copy of each inlined variable that is shared by all files. You must ensure the inline definitions are the same across multiple places where it is defined (which is rare). You can define these variables in header files. Rule of thumb: avoid creating and using non-constant global variables, they can be changed by anything anywhere, making the program state unpredictable declare local variables as close to where they are used as possible pay attention to the order that global variables are initialized to make sure no variable is referenced before being initialized. avoid dynamic initialization of variables better not to directly use a global variable within a function body, pass it as argument instead. define your shared global constants in one source file, namespaced, and use extern to expose them to be accessed from other files. Then define its companion header file, with namespaced forward declarations of these constants this way constants are instantiated only once and you don't have to recompile other source files if only changing the constants values. trade off: the constants are not considered available at compile-time, which we lose some optimization from compiler, and need to worry about variable initialization order prefer defining inline constexpr global variables in a header file static local variable \u00b6 The static keyword on local variable changes its duration from automatic duration to static duration, which retain its value after it goes out of scope, and retain its previous value when it is back in scope. It's common to use \"s_\" to prefix static local variables. Do initialize static local variables. Static local variables are only initialized the first time the code is executed, not on subsequent calls. Static local variables can be made const, when creating or initializing an object is expensive. Avoid static local variables unless the variable never needs to be reset. Header files \u00b6 A header file contains declarations of functions and variables, which can be #include ed by other source files to pull in declarations. When including a stadnard library header, only the declarations are pulled in for successfuly compilation. During Linking, the standard library gets linked and the definitions are pulled in. A header file is typically paired with a source file of the same base name. The source file should include its paired header file. Use double quotes to include header files that you've written or are expected to be found in the current directory. Use angled brackets to include headers that come with your compiler, OS, or third-party libraries you've installed elsewhere on your system. A header file may #include other header files. To maximize the chance that missing includes will be flagged by compiler, order your #includes as follows: The paired header file Other headers from your project 3rd party library headers Standard library headers Header file rule of thumbs: always include header guards no definitions except global constants header files should group declarations for a specific job, no more every header should compile on its own and not rely on other headers to pull in required dependency Conditionals \u00b6 Typical if statements: if ( < condition > ) { // body } else if ( < condition > ) { // body } else { // body } switch statements, note the variable only allow integral/enumerated types: switch ( < variable > ) { case < val_1 >: // body break ; case < val_1 >: // body [[ fallthrough ]]; // special attribute to indicate fallthrough is expected, supress warning case < val_1 >: { // a case can define a block to limit variable scope and supress compile error, in case need to declare and initialize a variable // body break ; } default : //body break ; } A goto statement is used with its goto label to specify which line to jump to. The label must be used within the function where it is defined. However, it is best to avoid using the goto statments, except for some nested loops. void utility () { repeat : // do something repeat_2 : // do something else if (...) goto repeat ; // jump to after the repeat label else if (...) goto repeat_2 ; // jump to after the repeat_2 label } The shorthand of if statement exists <condition> ? <true_body> : <false_body> , which can be seen as an expression. loops statments, favor normal while or for loop over do-while loop: while ( < condition > ) { // body } do { // executes at least once } while ( < condition > ); for ( < init > ; < condition > ; < step > ) { // body } for ( < type > < variable > : < array > ) { // element will be a copy of the current array element } for ( < type >& < variable > : < array > ) { // reference avoids copying array element } Use continue and break in loops when it simplifies the logic. Reference types \u00b6 Prior to C++11, there were only two possible value categories: lvalue and rvalue . In C++11, three additional value categories ( glvalue, prvalue, and xvalue ) were added to support a new feature called move semantics . lvalue and rvalue \u00b6 lvalue is an expression that evaluates to a function or object that has an identity (identifier such as variable or function name, or identifiable memory address). Identifiable objects persist beyond the scope of the expression. It also has two subtypes: modifiable and non-modifiable lvalue rvalue is an expression that is not an lvalue. Commonly non-string literals and return by value of functions or operators, and only exist within the scope of the expression where they are used. lvalues can implicitly convert to rvalues, so an lvalue can be used wherever an rvalue is required. Dangling reference can be created when an object being referenced is destroyed before a reference to it is destroyed. Accessing a dangling reference leads to undefined behavior. int main () { int x { 5 }; // 5 is an rvalue expression const double d { 1.2 }; // 1.2 is an rvalue expression std :: cout << x << '\\n' ; // x is a modifiable lvalue expression std :: cout << d << '\\n' ; // d is a non-modifiable lvalue expression std :: cout << return5 (); // return5() is an rvalue expression (since the result is returned by value) std :: cout << x + 1 << '\\n' ; // x + 1 is a rvalue std :: cout << static_cast < int > ( d ) << '\\n' ; // the result of static casting d to an int is an rvalue return 0 ; } References \u00b6 An lvalue reference acts as an alias for an existing modifiable lvalue . Use & to declare an lvalue reference type; the type of the reference must match the type of the referent. i.e. int& ref { x ]; where ref is an lvalue reference to the non-reference variable x. You can use a reference to modify the value of the variable/object being referenced. References are not objects. When creating reference to a constant variable, the reference must be declared with const so it can be used to access but not to modify the referent. Favor const lvalue references when possible. When a const lvalue reference is bound to a temporary object (rvalue), the lifetime of the temporary object is extended to match the lifetime of the reference. To pass by reference , declare a function parameter as a reference type. When the function is called, lvalue reference parameter is bound to the argument passed in. Binding a reference is always inexpensive, and no copy of variable needs to be made. Passing values by non-const reference allows functions modify the value of arguments passed in, and can only accept modifiable lvalue arguments. Generally, prefer pass by value for objects that are cheap to copy, and pass by const reference for objects that are expensive to copy. Common types that are cheap to copy include all of the fundamental types, enumerated types, and std::string_view. Common types that are expensive to copy include std::array, std::string, std::vector, and std::ostream. Note that type deduction using auto drops the const qualifier and the reference. Pointers \u00b6 Variable memory addresses aren't exposed, but use the address-of operator & can access the memory address of the operand. i.e. &x gets variable x's mem address. The dereference operator * returns the value at a given memory address as an lvalue . i.e. *ref gets you the lvalue reference from the memory address ref. A pointer is an object that holds a memory address as its value. Pointer types are declared using an asterisk (*). i.e. int* ptr; . Always initialize your pointers. Pointers behave much like lvalue references. Some main differences between the two: References must be initialized, pointers are not required to (but should be) References are not objects, pointers are References can not be reseated, pointers can References must always point at an object, pointers can point to nothing References are safe from dangling references, pointers are not Favor references over pointers whenever possible . A dangling pointers is a pointer that is holding the address of an object that is no longer valid. A null pointer can be initialized with empty initializer or the nullptr keyword. Always verify non-null pointer before dereferencing it. Further, pointers implicitly convert to Boolean values, with null pointer a value of false. It is also acceptable to use assert(ptr); . However, there is NO convenient way to determine whether a non-null pointer is pointing to a valid object or dangling (pointing to an invalid object). Thus, ensure that any pointer that is not pointing at a valid object is set to nullptr . A pointer to a const value is a non-const pointer that points to a constant value. i.e. const int* ptr { &x }; given x is a const int. A const pointer is a pointer whose address cannot be changed after initialization. i.e. int* const ptr { &x }; Now it is possible to combine the two to have a const pointer to a const value. Three ways to pass data to functions: using string = std : string ; void passByValue ( string val ); void passByReference ( string & val ); void passByAddress ( string * val ); int main () { string str { \"Hello\" }; passByValue ( str ); passByReference ( str ); passByAddress ( & str ); } Return references \u00b6 It is allowed to return by reference or by address . Objects returned by reference must live beyond the scope of the function returning the reference, or a dangling reference will result. Never return a local variable by reference. Avoid returning references to non-const local static variables. Prefer return by reference over return by address unless the ability to return \u201cno object\u201d (using nullptr ) is important. Memory allocation \u00b6 Static memory allocation happens for static and global variables which is allocated once at program start and persists throughout program life time. Automatic memory allocation happens for function parameters and local variables which is allocated when the relevant block is entered, and freed when the block is exited. Dynamic memory allocation allows requesting heap memory from the os when needed, using the new operator. i.e. int* ptr{ new int { 6 } }; Dynamically allocated memory stays allocated until it is explicitly deallocated or until the program ends. Memory leaks happen when your program loses the address of some bit of dynamically allocated memory before giving it back to the operating system. The memory bing dynamically allocated to variable can be freed up when not needed, through the delete <ptr>; operator. Deleting a null pointer has no effect. Do delete the pointer before reassigning it a new value. Avoid having multiple pointers point at the same piece of dynamic memory. When deleting a pointer, set it to nullptr if it is not going out of scope immediately. Program-defined types \u00b6 Consider Enums, Structs, Classes as program-defined types to distinguish from standard C++ defined types. Whenever you create a new program-defined type, name it starting with a capital letter. A program-defined type used in only one code file should be defined in that code file as close to the first point of use as possible. A program-defined type used in multiple code files should be defined in a header file with the same name as the program-defined type and then #include into each code file as needed. Enumerations \u00b6 An enumeration ( enum ) is a compound data type where every possible value is defined as a symbolic constant. Enumerated types are considered part of the integer family of types. You can static_cast integers to a defined enumerator. enum Color { color_red , color_green , color_blue , }; An enumeration or enumerated type is the program-defined type itself. An enumerator is a symbolic constant that is a possible value for a given enumeration. Name your enumerated types starting with a capital letter. Name your enumerators starting with a lower case letter. Avoid assigning explicit values to your enumerators unless you have a compelling reason to do so. Unscoped enumerations are named such because they put their enumerator names into the same scope as the enumeration definition itself. If an unscoped enum is defined in the global scope, it pollutes the global scope and significantly raises the chance of naming collisions. One common way to reduce chances of naming collision is to prefix each enumerator with the name of the enumeration itself. A better way is to put the enumerated type definition in a namespace. It is also common to put enumerated types related to a class inside the scope region of the class. Scoped enumeration are strongly typed (no implicit integer convertion) and strongly scoped (enumerators scoped within the enumeration region). enum class Color { red , green , blue , }; Enumerators must be accessed with prefixing the enumeration type. i.e. Color c { Color::blue }; . static_cast can still be used to cast enumerators into integers. It is acceptable to use operator overloading to reduce the typing in conversions of scoped enumerators. Structs \u00b6 A struct allows bundling multiple variables together into a single type. The variables that are part of the struct are called data members (or member variables). struct Employee { int id {}; int age {}; double wage { 0 }; // explicit default value }; To access a specific member variable, use the member selection operator . on normal variables or references of structs. For pointers, use the pointer/arrow operator -> which does an implicit dereference of the pointer object before selecting the member. An aggregate data type is any type that can contain multiple data members. structs with only data members are aggregates. Aggregates use a form of initialization called aggregate initialization which is just a list of comma-separated initialization values. Each member in the struct is initialized in the order of declaration. When adding a new member to an aggregate, it's safest to add it to the bottom of the definition list so the initializers for other members don't shift. int main () { Employee frank = { 1 , 32 , 60000.0 }; // copy-list initialization Employee robert ( 3 , 45 , 62500.0 ); // direct initialization (C++20) Employee joe { 2 , 28 , 45000.0 }; // list initialization return 0 ; } Variables of a struct type can be const and must be initialized. Structs are generally passed by (const) reference to functions to avoid making copies. Classes \u00b6 By convention, class names should begin with an upper-case letter. The class name effectively acts like a namespace for the nested type. Type alias members make code easier to maintain and can reduce typing. Generally, nested types should only be used when the nested type is used exclusively within that class. class DateClass { private : // private members int m_year {}; // m_ prefix helps distinguish from local variables int m_month {}; int m_day {}; public : // public members // constructor DateClass () = default ; // tells compiler to create a default constructor DateClass () : m_year { 1999 }, m_month { 1 }, m_day { 1 } { // member initializer list } DateClass ( int year , int month , int day ) { m_year = year ; m_month = month ; m_day = day ; } DateClass ( int year , int month , int day ) // preferred, same form with initializer which can also initialize const variables : m_year { year }, m_month { month }, m_day { day } { } protected : // protected members }; When not specify any access specifiers, members are private by default. Member functions can be defined inside or outside of a class. When all members variables of a class (or struct) are public, we can use aggregate initialization to initialize the class (or struct) directly using list-initialization. Otherwise specify a constructor. Initialize variables in the initializer list in the same order in which they are declared in your class. Delegating constructors allows calling another constructor from one constructor. It is aka constructor chaining. To do so, put the other constructor call inside the initilization list, not in the body of the constructor. A class exposes the this pointer to its active instance to refer to its own object for accessing member variables and functions. It can also be reassigned to overwrite the implicit object. Like so: *this = Foo(); // Foo is the constructor . It can also be returned by functions to allow chaining calls. Destructors are like default constructors but has its name preceded by a ~ . Only one destructor per is class allowed. Destructors are called automatically when the object is destroyed. Note that if you use the exit() function, your program will terminate and no destructors will be called. Be wary if you\u2019re relying on your destructors to do necessary cleanup work. Class templates \u00b6 STD Libraries \u00b6 <algorithm> \u00b6 Includes functions like sort, fill, find, binary_search, reverse, shuffle, min/max, count_if. std::find(start_iterator, end_iterator, target) searches for the first occurrence of a value in a container. std::find_if works similarly but allows passing in a callable object (function pointer or lambda) that checks to see if a match is found. std::count, std::count_if works similarly and count all occurrences of element or elements fulfilling the condition. std::sort sorts an array to ascending with an optional custom comparing function. std::for_each applies a custom function to every element in an array, which is an alternative to writing a loop. It can also be parallelized for faster processing. Favor using functions from the algorithms library over writing your own functionality to do the same thing. <iostream> \u00b6 std::cout , allows print text to program STDOUT. Similarily for std::cerr for print text to STDERR. Put a line break and flushes output with std::endl . Prefer to use \\n for adding line breaks to avoid high frequency in flush. Use insertion operator << along with cout/cerr/endl , or extraction operator >> for cin . There is also std::getline for reading a full ine of text into a variable. i.e. std::getline(std::cin >> std::ws, input); The std::ws is a type of input manipulator to tell std::in ignore leading whitespace, which is necessary to continuously read user inputs in an interactive program. When doing extraction with std::cin , an extraction of user input can fail for many reasons, common one is the input causes an overflow, which leave cin in a failed state and cause any future extraction to be skipped. What to do in this case where bad user input can be anticipated is to use std::cin.fail() to check if the extraction failed, then std::cin.clear() to reset the failure flag and something like std::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\\n'); to skip what user entered (before hitting the enter key), any probably ask the user to enter again and tell them why it failed. <iterator> \u00b6 Contains functions like begin, end on arrays. <random> \u00b6 Provides PRNG (Pseudo Random Number Generator) function that generates fairly evenly distributed random numbers. std::mt19937 is a 32-bit Mersenne Twister random number generator function; std::mt19937_64 is the 64-bit version. Also to make sure it is truely random everytime the program is run, seed the PRNG with system clock ( <chrono> is needed). Use std::uniform_int_distribution<> die{ 1, 6 }; to limit the random number range. Like so: int rand_gen () { std :: mt19937 mt { static_cast < unsigned int > ( std :: chrono :: steady_clock :: now (). time_since_epoch (). count () ) }; // alternatively seed with the os implemented random_device std :: mt19937 mt { std :: random_device {}() }; std :: uniform_int_distribution <> die6 { 1 , 6 }; int rand = die6 ( mt ); return rand ; } Make a singleton of PRNG, and only seed a PRNG once, and use it throughout your program. Do so with a namespace and define a global PRNG, like so: namespace Random { // capital R to avoid conflicts with functions named random() std :: mt19937 mt { std :: random_device {}() }; int get ( int min , int max ) { std :: uniform_int_distribution die { min , max }; return die ( mt ); } } <utility> \u00b6 Provides useful types like Pairs, generic swap function <cassert> \u00b6 An assertion tests if an expression evaluates to true. If not, the program terminates with an error message. This is useful to do precondition check anywhere fits. It is best to use this format assert(<condition> && \"error message\"); so when this condition expression is false, the informative message is also printed. Generally use assertions to document cases that should be logically impossible; for other cases use proper exception handling methods. Asserts should never be encountered in production code. Asserts should be used only in cases where corruption isn't likely to occur if the program terminates unexpectedly. If the macro NDEBUG is defined, the assert macro gets disabled. static_assert(<condition>, \"diagnostic_message\"); to evaluate something at compile time. <cstdint> \u00b6 For defining fixed-width integers. <cstdlib> \u00b6 Halt functions: std::exit() can be used to explicitly exit the program. Do note that it does not clean up any local variables (either in the current function, or in functions up the call stack). Avoid using it unless you know how to properly clean up your program. std::atexit() can be used to pass in a cleanup function when exit() is called. In a multi-threaded program, it is better to use std::quick_exit() to exit the program (which does not immediately clean up static objects that may still be accessed by other threads), paired with std::at_quick_exit() for cleanup. Furthermore, std::abort() function causes your program to terminate abnormally. There is also std::terminate() which is typically used in conjunction with exceptions. Both does not do propery cleanups. Generally use exceptions for error handling. <cmath> \u00b6 Handy math functions. i.e. pow, abs, max, min","title":"C++ Language Reference"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#overview","text":"C++ is one of the popular high level languages and requires compiling source code into machine code for computer to run. C++ excels in situations where high performance and precise control over memory and other resources is needed. Some common places where C++ shines: Video games Real-time systems, i.e. transportation, manufacturing Financial applications, i.e. stock exchange Graphical applications, simulations Productivity applications Embedded systems Audio and video processing AI and neural networks C++ is designed to allow the programmer a high degree of freedom to do what they want, which is both wonderful and dangerous.","title":"Overview"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#basics","text":"","title":"Basics"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#compiler","text":"","title":"Compiler"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#preprocessor","text":"Prior to compilation, the code file goes through a phase known as translation. This phase involves the interpretation of preprocessor , which can be throught of as a separate program that manipulates the text in each code file. Preprocessor directives are instructions that start with a # symbol and end with a newline (no semicolon). Directives tell the preprocessor to perform specific particular text manipulation tasks on normal code text (non-directive lines). Directives are only valid from the point of definition to the end of the file in which they are defined. #include is one type of directive, which tells preprocessor to replace it with the contents of the included file, which gets preprocessed as well recursively. Avoid using relative paths in #include directives. Instead, pass in -I option to specify alternate directory to look for source files. In this way, changing directory structure do not require updating the include paths. The #define directive can be used to create a macro , a rule that defines how input text is converted into replacement output text . Two basic types of macros: object-like macros, and function-like macros. Object-like macros are traditionally typed in all capital letters, using underscores to represent spaces. They used to be used as a cheaper alternative to constant variables and is considered a legacy feature. A speical use case for Object-like macro with no substitution text is to remove occurrance of certain text, or serve as a marker for #ifdef . #ifdef, #ifndef, and #endif allows the preprocessor to check whether an identifier has been previously #defined , and skip redefining it if that is the case. This is known as the header guard . #ifndef HEADERFILENAME_H #define HEADERFILENAME_H // declarations #endif #pragma once serves as an alternate form of header guards but only supported by modern compilers, not old compilers. In this way, when this header file is being included by multiple files that end up being merged together, they won't cause repeated declarations error. #if 0, #endif is commonly used to exclude a block of code from being compiled (as if they are commented out).","title":"Preprocessor"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#compilation","text":"During compilation, C++ compiler sequentially goes through each source code ( .h .c .cpp ) file in your program and does two things: check the syntax correctness translate code into machine language and create the object file ( .o .obj ) After the object files are created, the linker comes in: resolve dependencies merge object files into a single executable program pull in library files referenced, which are collections of precompiled and prepackaged code for reuse Compile code with g++ <source_files> -o <output_program> . The output program can be run directly with ./<output_program> . C++ has evolved and has many language standards. Pass in compiler flags during compilation to change the language standard. i.e. g++ <language_standard_flag> <source_files> -o <output_program> , where language_standard_flag can be -std=c++1/c++14/c++17/c++2a","title":"Compilation"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#documenting-and-comments","text":"Single-line comment with // , multi-line comment with /* */ Indent with 4 spaces. Put the opening curly brace on the same line as the statement. Lines should keep a 80-char maximum and wrap to new line at appropriate place.","title":"Documenting and Comments"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#program-entry","text":"Every C++ program must have a main function or it will fail to link. int main () { // main function body return 0 ; } The main function must have int return type, which will be the exit code. An exit code of 0 typically signals the program execution was successful. To make the program take command line arguments, write the main function this way: int main ( int argc , char * argv []) { // main function body return 0 ; } argc is the count of arguments passed to the program and is >= 1, since the first argument is always the absolute path to the program. argv is an array of C-style strings and holds the command line arguments.","title":"Program entry"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#data-and-variables","text":"","title":"Data and Variables"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#declaration-initialization","text":"Variable declaration can be one line each or multiple variables of the same type declared in one line: int a ; // declared but uninitialized int b , c ; // declare multiple vars in one line double d = 1.0 ; // inline copy constant initialize int e ( 5 ); // inline initialize int f { 10 }; // inline brace initialize (preferred) int g = { 100 }; // inline copy brace initialize int h {}; // inline value initialize to 0, aka zero initialization (default is 0 for int) Some rules of thumb: initialize your variables upon creation, using one of the inline methods mentioned above initialize one variable on each line prefer inline brace initialize since in some circumstances it reports error when wrong variable type is being assigned, i.e. double assigned to int prefer value initialization when the variable value will be replaced later before use","title":"Declaration, Initialization"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#naming","text":"Reserved keywords must not be used as variable identifiers. Some naming rules of thumb for variables and function names: compose of letters, numbers, and underscore begin with a lower case letter use underscore to separate whole words, or use camelCase. Be consistent throught the program","title":"Naming"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#data-types","text":"Foundamental data types: Boolean: bool Character: char, wchar_t, char8_t (C++20), char16_t (C++11), char32_t (C++11) Integer: short, int, long, long long (C++11) Floating Point: float, double, long double Null Pointer: std::nullptr_t (C++11) Void: void Booleans are represented internally as integers. When printing you will get 1 and 0. To get true and false, set std::cout << std::boolalpha; , and similarly for stdin std::cin >> std::boolalpha; . Integers are signed by default. To declare unsigned integers, prefix variable type with unsigned keyword. i.e. unsigned int num; However, you should favor signed numbers over unsigned numbers for holding quantities (even quantities that should be non-negative) and mathematical operations. Avoid mixing signed and unsigned numbers. Use std::setprecision(<int>) to set the precision of floating numbers (default is 6). Although be careful changing this, since higher precision may introduce rounding errors. The _t suffix means type. The sizeof operator is a unary operator that takes either a type or a variable, and returns its size in bytes (of type std::size_t ). The integer types have unguaranteed size on different hardware. There is only a guaranteed minimal size. i.e. an int can be 2 bytes in old architecture but 4 bytes on modern architecture. Fixed-width integers are introduced by including the <cstdint> header to solve the issue of unguaranteed integer sizes on different hardware. They can be declared as: std :: int8_t std :: int16_t std :: int32_t std :: int64_t // unsigned std :: uint8_t std :: uint16_t std :: uint32_t std :: uint64_t Still, fixed-width integers are not guaranteed to be defined on all architectures. And for some modern architecture, using fixed-width integer might slow down the problem, as the CPU has to do extra work to process the smaller variables. For those cases, consider using fast and least integers . Some ruls of thumb: prefer int when integer size doesn't matter prefer std::int#_t when storing a quantity that needs a guaranteed range prefer std::uint#_t when doing bit manipulation or when some overflow/wrap-around behavior is desired. avoid using unsigned types for quantities avoid 8-bit fixed-width integer types which often get treated as chars avoid fast and least fixed-width types avoid compiler-specific fixed-width integers, such as VC++ __int8 Literal constants are fixed values explicitly specified. Examples: 5 // int literal 5u // unsigned int literal 30L // long literal 0214 // octal literal 0xF // hexadecimal literal 0b1001'1110 // binary literal, with ' as digit separator 3.0 // double literal 5.0f // float literal \"Helloworld\" // C-style string literal C++ will concatenate sequential string literals. The library <bitset> allows printing binary numbers. i.e. std::bitset<8> bin{ 0b1100'0100 }; Constant variables , aka symbolic constants, can be created using the const keyword, where the variable value can be determined in compile-time or runtime. Use constexpr to enforce compile-time constant evaluation. Compound types: Functions Arrays Pointers, to object or to function Reference, L-value reference or R-value reference Enums, scoped or unscoped Classes, Structs, Unions","title":"Data types"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#casting-and-types","text":"A numeric promotion is the conversion of smaller numeric types to larger numeric type. Use explicit type conversion to explicitly tell the compiler to convert a value from one type to another type. The C-style casts is done via the (<new_type>) variable operator or with the function-like cast <new_type>(variable) . Avoid C-style casts since it risks misuse and not producing expected behavior. There is also static cast using static_cast<new_type>(<expression>) which allows carry out type conversions that may loss precision or cause overflow if done with implicit (automatic) type conversion. static_cast is best used to convert one fundamental type into another. The using or typedef keyword can be used to create type aliases , which creates an alias for an existing data type. Use a \"_t\" or \"_type\" suffix to help decrease the chance of naming collisions with other identifiers. i.e. using distance_t = double ; // define distance_t as an alias for type double typedef double distance_t ; // also does the same thing, but discouraged Use type aliases for platform independent coding. Because char, short, int, and long give no indication of their size, it is fairly common for cross-platform programs to use type aliases to define aliases that include the type's size in bits. Also use type aliases to make complex types simple, or help with code documentation and comprehension. i.e. using pairlist_t = std::vector<std::pair<std::string, int>>; Type deduction , aka type inference, is a feature that allows the compiler to deduce the type of an object from the object's non-empty initializer through the auto keyword. Type deduction can save a lot of typing and typos in some cases, and is generally safe to use for objects, and improves code readability. auto can also be used on function return type to let compiler infer return type from return statements. Favor explicit return types over function return type deduction. However, you can use the tailing return type syntax with the auto return type to make function names line up and improve code readability. i.e. in forward-declarations: auto add ( int x , int y ) -> int ; auto divide ( double x , double y ) -> double ; auto printSomething () -> void ; Read more about casting","title":"Casting and types"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#strings","text":"string type is not a fundamental type, but a compound type. strings are double quoted characters. Quoted text separated by nothing but whitespace (spaces, tabs, or newlines) will be concatenated. #include the <string> header to bring in the declarations for std::string . A string's length is optained with str.length() function call. The plain quote-surrounded strings are in fact C-style strings , not std::string. To specify different types of strings: C-style string is essentially an array of chars which implicitly adds a null character \\0 to end of the string. The strlen() function from cstring> returns the length of the C-style string without the null terminator. Other useful C-style functions: strcpy() -- copy a string to another string strcpy_s() strlcpy() -- string copy allowing a size parameter strcat() -- appends one string to another (dangerous) strncat() -- appends one string to another (with buffer length check) strcmp() -- compare two strings (returns 0 if equal) strncmp() -- compare two strings up to a specific number of characters (returns 0 if equal) As a rule of thumb, use std::string or std::string_view (next lesson) instead of C-style strings. Prefer std::string_view over std::string for read-only strings C++17 introduces std::string_view which provides a view of a string to avoid unnecessary copies of strings. It also contains functions for shrinking the view of the string which do not reflect on the underlying string. #include <string> #include <string_view> void foo () { auto s1 { \"Hello, world\" }; // c-style string of type const char* auto s2 { \"Hello\" s }; // std::string auto s3 { \"hi\" sv }; // std::string_view auto s4 { s1 }; // std::string auto s5 { s4 }; // std::string_view auto s6 { static_cast < std :: string > ( s5 ) }; // std::string auto s7 { s6 . c_str () }; // c-style string } Prefer passing strings using std::string_view (by value) instead of const std::string&, unless your function calls other functions that require C-style strings or std::string parameters.","title":"strings"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#operators","text":"Arithematical: + - * / % ++ -- Bitwise: & | ^ ~ << >> Logical: == != < > <= >= && || sizeof: sizeof (<type_or_variable>) Type casting: (<type>) <variable> Precedence of operators","title":"Operators"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#variable-scopes","text":"Variables declared within a function scope is local variable and only visible within the closure of the function.","title":"Variable scopes"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#arrays","text":"An array is an aggregate data type that holds many values of the same type through a single identifier. A fixed array is an array where the length is known at compile time. i.e. int prime[5]{}; or int array[]{ 0, 1, 2, 3, 4 }; Copying large arrays can be very expensive, C++ does not copy an array when an array is passed into a function. A fixed array identifier decay (inplicitly convert) to a pointer that points to the first element of the array. This makes void printSize(int array[]); and void printSize(int* array); declarations identical. In most cases, because the pointer doesn't know how large the array is, you'll need to pass in the array size as a separate parameter anyway. Favor the pointer syntax (*) over the array syntax ([]) for array function parameters. When using arrays, ensure that your indices are valid for the range of your array, as compiler does not check this. Array size can be accessed with std::size(<array>) . C++ multi-dimensional array can be declared as int array[3][5]{}; . Array indexing: int main () { int array []{ 9 , 7 , 5 , 3 , 1 }; std :: cout << & array [ 1 ] << '\\n' ; // print memory address of array element 1 std :: cout << array + 1 << '\\n' ; // print memory address of array pointer + 1 std :: cout << array [ 1 ] << '\\n' ; // prints 7 std :: cout << * ( array + 1 ) << '\\n' ; // prints 7 return 0 ; } std::array from <array> works like fixed arrays and makes array management easier. It does auto clean up after going out of scope, and offers convenient functions like size() . i.e. std::array<int, 3> myArray; creates an integer array of size 3. std::array allows assigning values to the array using an initializer list. Access std::array values with the subscript operator [] or the at() function. Always pass std::array by reference or const reference to avoid unnecessary copies. Favor std::array over built-in fixed arrays for any non-trivial array use. A dynamic array allows choosing an array length at runtime. Use new[] and delete[] with dynamic arrays. i.e. int* array{ new int[length]{} }; and delete[] array; . Dynamic arrays can be initialized using initializer lists, and benefits from auto type deduction. i.e. auto* array{ new int[5]{ 9, 7, 5, 3, 1 } }; Note that for-each loop do not work with pointer arrays since the array size is unknown. A pointer to a pointer can be used to create dynamical multi-dimensional arrays. i.e. void foo () { int ** array2d { new int * [ 10 ] }; for ( int i = 0 ; i < 10 ; ++ i ) { array2d [ i ] = new int [ 5 ]; } array2d [ 4 ][ 3 ]; // access // free up for ( int i = 0 ; i < 10 ; ++ i ) { delete [] array2d [ i ]; } delete [] array2d ; } std::vector from <vector> makes working with dynamic arrays safer and easier. It does auto clean up after going out of scope, and offers convenient functions like size(), resize() . i.e. std::vector<int> empty {}; std::vector allows assigning values to the array using an initializer list. Access std::vector values with the subscript operator [] or the at() function. An iterator is an object designed to traverse through a container (array, string) and provide access to each element along the way. The simplest kind of iterator is a pointer , which works for data stored sequentially in memory. void foo () { std :: array a { 0 , 1 , 2 , 3 , 4 , 5 , 6 }; auto begin { & a [ 0 ] }; // or { a.data() }, { a.begin() }, { std::begin(a) } auto end { begin + std :: size ( a ) }; // or { a.end() }, { std::end(a) } for ( auto ptr { begin }; ptr != end ; ++ ptr ) { std :: cout << * ptr << ' ' ; } }","title":"Arrays"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#functions","text":"A function must be either declared or defined before it is being used. Offically, a forward declaration allows us to tell the compiler about the existence of an identifier before actually defining the identifier. This is when header files come into play. return_type function_name ( arg_type arg1 , arg_type arg2 , ...); // forward declaration; the var names are optional but good to keep as it is self-documenting return_type function_name ( arg_type arg1 , arg_type arg2 , ...) { // function body } A default argument is a default value provided for a function parameter. When making a function call, the caller can optionally provide an argument for any function parameter that has a default argument. Default arguments can only be provided for the rightmost unspecified parameters, and can be declared in either the forward declaration or the function definition, but not both (best to do it in forward declaration).","title":"Functions"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#overloading","text":"Function overloading allows us to create multiple functions with the same name, so long as each identically named function has different parameters (including ellipsis parameters i.e. void foo(int x, ...); ), or function-level qualifiers (const, volatile, ref-qualifiers).","title":"Overloading"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#templating","text":"Template system was designed to simplify the process of creating functions (or classes) that are able to work with different data types. A placeholder type represents some type that is not known at the time the template is written, but that will be provided later. Use a single capital letter (starting with T) to name the placeholder type. A function template is a function-like definition that is used to generate overloaded functions, each with a different set of actual types. i.e. template < typename T > // template parameter declaration T max ( T x , T y ) { // function template definition return ( x > y ) ? x : y ; } int main () { std :: cout << max < int > ( 1 , 2 ); // instantiates and calls function max<int>(int, int) std :: cout << max <> ( 1 , 2 ); // compiler deduced type from arguments std :: cout << max ( 1 , 2 ); // compiler deduced type from arguments return 0 ; } Favor the normal function call syntax over template argument deduction when using function templates.","title":"Templating"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#inline","text":"Inline expansion is a process where a function call is replaced by the code from the called function\u2019s definition. Modern optimizing compilers are typically very good at determining which functions should be made inline to improve final executable performance. A function that is eligible to have its function calls expanded is called an inline function . Functions that are always expanded inline: defined inside a class, struct, or union. constexpr functions","title":"Inline"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#namespacing","text":"C++ does not allow the same identifier to be defined within the same build context, aka naming collision . This can be happening more commonly when a program uses many libraries. A namespace is a region that allows you to declare names inside of it for the purpose of disambiguation. Any name declared inside the namespace won't be mistaken for identical names in other scopes. std is itself a namespace. :: is called the scope resolution operator. Any name that is not defined inside a class, function, or a namespace is considered to be part of an implicitly defined global namespace . You can define a namespace by enclosing the identifiers within a namespace block: namespace < namespace_name > { // namespace body } The same namespace can be created anywhere, as long as the identifiers within all namespaces of a kind do not clash. namespaces can also be nested , by enclosing one in another. One way to access identifiers inside a namespace is to use a using directive statement : using namespace std ; // at top of program after includes void some_function () { cout << \"Hello\" ; // directly access functions without namespace prefix } However, using namespace should be generally avoided since it increases the risk of causing naming collisions, especially for std namespace. You can also create namespace aliases and switch to a different namespace for everywhere it is referenced. Like so: namespace active = foo ; std :: out << active :: getMessage (); Namespace is designed primarily as a mechanism for preventing naming collisions. In applications, namespaces can be used to separate application-specific code from code that might be reusable later (e.g. math functions). When writing a library or code that you want to distribute to others, always place your code inside a namespace. The using declaration can also be used to allow using an unqualified name (with no scope) as an alias for a qualified name. i.e. using std :: cout ; cout << \"Hello\" ; // no qualified scope resolution is needed anymore An unnamed namespace (aka anonymous namespace) is a namespace that is defined without a name, which is treated as if it is part of the parent namespace. It is typically used when you have a lot of content that you want to ensure stays local to a given file. unnamed namespace does this and saves you the trouble to mark all declarations static . An inline namespace is one typically used to version content. Its content is also treated as if it is part of the parent namespace. Its advantage is that it preserves the function of existing programs while allowing newer programs to take advantage of newer/better variations by referencing with the newer namespace scope. i.e. inline namespace v1 { void doSomething () { // body } } namespace v2 { void doSomething () { // body } } int main () { doSomething (); // calls v1 version v1 :: doSomething (); // calls v1 version v2 :: doSomething (); // calls v2 version } Now when you decide to make v2 the official version, switch the inline keyword of the two versions.","title":"Namespacing"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#ellipsis","text":"Ellipsis argument allows passing a variable number of parameters to a function. Ellipsis are potentially dangerous because it does not know how many parameters are actually passed, nor does it do type check for the passed data types. Functions that use ellipsis must have at least one non-ellipsis parameter. It is conceptually useful to think of the ellipsis as an array that holds any additional parameters beyond those in the argument_list. #include <cstdarg> // needed to use ellipsis int getSum ( int count , ...) { int sum { 0 }; // We access the ellipsis through a va_list, so let's declare one std :: va_list list ; // We initialize the va_list using va_start. va_start ( list , count ); for ( int i { 0 }; i < count ; ++ i ) { sum += va_arg ( list , int ); // auto advances the pointer } // cleanup va_list va_end ( list ); return sum ; } In general Ellipsis should be avoided unless there is a compelling reason not to.","title":"Ellipsis"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#lambda","text":"A lambda expression , aka function literal, allows us to define an anonymous function inside another function, and take advantage of the closure from naming conflicts. It is stored in the program as a functor object, which overloads the () operator. It takes the form: // captureClause, parameters, and returnType can be omitted if not required [ captureClause ] ( parameters ) -> returnType { // statements; } A lambda can be stored in a variable and passed later. These are valid ways: int main () { // A regular function pointer. Only works with an empty capture clause. double ( * addNumbers1 )( double , double ){ []( double a , double b ) { return ( a + b ); } }; // Using std::function. The lambda could have a non-empty capture clause. std :: function addNumbers2 { // note: pre-C++17, use std::function<double(double, double)> instead []( double a , double b ) { return ( a + b ); } }; // Using auto. Stores the lambda with its real type. auto addNumbers3 { []( double a , double b ) { return ( a + b ); } }; return 0 ; } Use auto when initializing variables with lambdas, and std::function if you can\u2019t initialize the variable with the lambda. Since C++14 it is allowed to use auto for lambda parameters to define generic lambdas. A unique lambda will be generated for each different type that auto resolves to. The capture clause is used to (indirectly) give a lambda access to variables available in the surrounding scope that it normally would not have access to, by enclosing the variable name (comma-separated) within the [] syntax. The captured variables of a lambda are constant clones of the outer scope variables, not the actual variables. The cloned variable can be made mutable with the mutable keyword added after the parameter list. Captured variables are members of the lambda object, their values are persisted across multiple calls to the lambda. Passing mutable lambdas can be dangerous as the passed lambda can be copies of its functor. To pass it as a reference, wrap it with std::ref() function which yields a std::reference_wrapper type to ensure the lambda does not make copies while being passed to another function. You can also capture variables by reference (prepending & ) to allow it affect the value of the variable within lambda calls. There is a chance for leaving dangling references in lambda so ensure captured variables outlive the lambda. Default capture can be used to capture all variables mentioned in the lambda. To capture by value, pass in = ; to capture by reference, pass in & . Default captures can be mixed with normal captures to capture some variables by value and some by reference, default capture operator must be the first in the list. You can define new variable from captured variables in the capture brackets using initialization braces. But it is best to do it outside and capture it.","title":"Lambda"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#function-pointers","text":"C++ implicitly converts a function into a function pointer as needed. Functions used as arguments to another function are sometimes called callback functions . Function pointers can be initialized like so int (*fcnPtr)(int){ &foo }; which makes fcnPtr points to function foo that has return type of int and takes one int parameter. It can also be initialized with nullptr value. Calling with function pointer is like so (*fcnPtr)(10); or fcnPtr(10); . Make the function pointer type shorter with type alias: using FooFunction = int(*)(int); , or use auto type deduction. Alternatively, use std::function from <functional> , std::function<int(int)> fcn to declare a std::function object.","title":"Function pointers"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#global-variables","text":"Global variables are usually declared at the top of a file, below the includes, and have identifiers prefixed with g_ to help easily differentiate from local variables. Global variables are created when the program starts, and destroyed when it ends. They are also known as static variables. Non-constant global variables should be avoided. Global variables by default are visible from other files. You can limit non-constant global variable's visibility internal within a file (called internal linkage ), by using the static keyword, which is a storage class specifier, see also extern and mutable . Internal objects (and functions) that are defined in different files are considered to be independent entities. To use an external global variable defined in another file, you must do forward declaration of that variable, i.e. extern int <var_name>; . However, constexpr variables has no effect with extern keyword, as its type requires the value to be determined at compile time, so constexpr can only be limited to file internal use. // External global variable definitions: int g_x ; // non-initialized external global variable extern const int g_x { 1 }; // initialized const external global variable extern constexpr int g_x { 2 }; // initialized constexpr external global variable // Forward declarations extern int g_y ; // forward declaration for non-constant global variable extern const int g_y ; // forward declaration for const global variable extern constexpr int g_y ; // not allowed: constexpr variables can't be forward declared By default: functions have external linkage non-constant global variables have external linkage constant global variables have internal linkage inline variable is another way to avoid the same variables being copied into multiple files with includes. The linker ensures there is only one copy of each inlined variable that is shared by all files. You must ensure the inline definitions are the same across multiple places where it is defined (which is rare). You can define these variables in header files. Rule of thumb: avoid creating and using non-constant global variables, they can be changed by anything anywhere, making the program state unpredictable declare local variables as close to where they are used as possible pay attention to the order that global variables are initialized to make sure no variable is referenced before being initialized. avoid dynamic initialization of variables better not to directly use a global variable within a function body, pass it as argument instead. define your shared global constants in one source file, namespaced, and use extern to expose them to be accessed from other files. Then define its companion header file, with namespaced forward declarations of these constants this way constants are instantiated only once and you don't have to recompile other source files if only changing the constants values. trade off: the constants are not considered available at compile-time, which we lose some optimization from compiler, and need to worry about variable initialization order prefer defining inline constexpr global variables in a header file","title":"Global variables"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#static-local-variable","text":"The static keyword on local variable changes its duration from automatic duration to static duration, which retain its value after it goes out of scope, and retain its previous value when it is back in scope. It's common to use \"s_\" to prefix static local variables. Do initialize static local variables. Static local variables are only initialized the first time the code is executed, not on subsequent calls. Static local variables can be made const, when creating or initializing an object is expensive. Avoid static local variables unless the variable never needs to be reset.","title":"static local variable"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#header-files","text":"A header file contains declarations of functions and variables, which can be #include ed by other source files to pull in declarations. When including a stadnard library header, only the declarations are pulled in for successfuly compilation. During Linking, the standard library gets linked and the definitions are pulled in. A header file is typically paired with a source file of the same base name. The source file should include its paired header file. Use double quotes to include header files that you've written or are expected to be found in the current directory. Use angled brackets to include headers that come with your compiler, OS, or third-party libraries you've installed elsewhere on your system. A header file may #include other header files. To maximize the chance that missing includes will be flagged by compiler, order your #includes as follows: The paired header file Other headers from your project 3rd party library headers Standard library headers Header file rule of thumbs: always include header guards no definitions except global constants header files should group declarations for a specific job, no more every header should compile on its own and not rely on other headers to pull in required dependency","title":"Header files"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#conditionals","text":"Typical if statements: if ( < condition > ) { // body } else if ( < condition > ) { // body } else { // body } switch statements, note the variable only allow integral/enumerated types: switch ( < variable > ) { case < val_1 >: // body break ; case < val_1 >: // body [[ fallthrough ]]; // special attribute to indicate fallthrough is expected, supress warning case < val_1 >: { // a case can define a block to limit variable scope and supress compile error, in case need to declare and initialize a variable // body break ; } default : //body break ; } A goto statement is used with its goto label to specify which line to jump to. The label must be used within the function where it is defined. However, it is best to avoid using the goto statments, except for some nested loops. void utility () { repeat : // do something repeat_2 : // do something else if (...) goto repeat ; // jump to after the repeat label else if (...) goto repeat_2 ; // jump to after the repeat_2 label } The shorthand of if statement exists <condition> ? <true_body> : <false_body> , which can be seen as an expression. loops statments, favor normal while or for loop over do-while loop: while ( < condition > ) { // body } do { // executes at least once } while ( < condition > ); for ( < init > ; < condition > ; < step > ) { // body } for ( < type > < variable > : < array > ) { // element will be a copy of the current array element } for ( < type >& < variable > : < array > ) { // reference avoids copying array element } Use continue and break in loops when it simplifies the logic.","title":"Conditionals"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#reference-types","text":"Prior to C++11, there were only two possible value categories: lvalue and rvalue . In C++11, three additional value categories ( glvalue, prvalue, and xvalue ) were added to support a new feature called move semantics .","title":"Reference types"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#lvalue-and-rvalue","text":"lvalue is an expression that evaluates to a function or object that has an identity (identifier such as variable or function name, or identifiable memory address). Identifiable objects persist beyond the scope of the expression. It also has two subtypes: modifiable and non-modifiable lvalue rvalue is an expression that is not an lvalue. Commonly non-string literals and return by value of functions or operators, and only exist within the scope of the expression where they are used. lvalues can implicitly convert to rvalues, so an lvalue can be used wherever an rvalue is required. Dangling reference can be created when an object being referenced is destroyed before a reference to it is destroyed. Accessing a dangling reference leads to undefined behavior. int main () { int x { 5 }; // 5 is an rvalue expression const double d { 1.2 }; // 1.2 is an rvalue expression std :: cout << x << '\\n' ; // x is a modifiable lvalue expression std :: cout << d << '\\n' ; // d is a non-modifiable lvalue expression std :: cout << return5 (); // return5() is an rvalue expression (since the result is returned by value) std :: cout << x + 1 << '\\n' ; // x + 1 is a rvalue std :: cout << static_cast < int > ( d ) << '\\n' ; // the result of static casting d to an int is an rvalue return 0 ; }","title":"lvalue and rvalue"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#references","text":"An lvalue reference acts as an alias for an existing modifiable lvalue . Use & to declare an lvalue reference type; the type of the reference must match the type of the referent. i.e. int& ref { x ]; where ref is an lvalue reference to the non-reference variable x. You can use a reference to modify the value of the variable/object being referenced. References are not objects. When creating reference to a constant variable, the reference must be declared with const so it can be used to access but not to modify the referent. Favor const lvalue references when possible. When a const lvalue reference is bound to a temporary object (rvalue), the lifetime of the temporary object is extended to match the lifetime of the reference. To pass by reference , declare a function parameter as a reference type. When the function is called, lvalue reference parameter is bound to the argument passed in. Binding a reference is always inexpensive, and no copy of variable needs to be made. Passing values by non-const reference allows functions modify the value of arguments passed in, and can only accept modifiable lvalue arguments. Generally, prefer pass by value for objects that are cheap to copy, and pass by const reference for objects that are expensive to copy. Common types that are cheap to copy include all of the fundamental types, enumerated types, and std::string_view. Common types that are expensive to copy include std::array, std::string, std::vector, and std::ostream. Note that type deduction using auto drops the const qualifier and the reference.","title":"References"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#pointers","text":"Variable memory addresses aren't exposed, but use the address-of operator & can access the memory address of the operand. i.e. &x gets variable x's mem address. The dereference operator * returns the value at a given memory address as an lvalue . i.e. *ref gets you the lvalue reference from the memory address ref. A pointer is an object that holds a memory address as its value. Pointer types are declared using an asterisk (*). i.e. int* ptr; . Always initialize your pointers. Pointers behave much like lvalue references. Some main differences between the two: References must be initialized, pointers are not required to (but should be) References are not objects, pointers are References can not be reseated, pointers can References must always point at an object, pointers can point to nothing References are safe from dangling references, pointers are not Favor references over pointers whenever possible . A dangling pointers is a pointer that is holding the address of an object that is no longer valid. A null pointer can be initialized with empty initializer or the nullptr keyword. Always verify non-null pointer before dereferencing it. Further, pointers implicitly convert to Boolean values, with null pointer a value of false. It is also acceptable to use assert(ptr); . However, there is NO convenient way to determine whether a non-null pointer is pointing to a valid object or dangling (pointing to an invalid object). Thus, ensure that any pointer that is not pointing at a valid object is set to nullptr . A pointer to a const value is a non-const pointer that points to a constant value. i.e. const int* ptr { &x }; given x is a const int. A const pointer is a pointer whose address cannot be changed after initialization. i.e. int* const ptr { &x }; Now it is possible to combine the two to have a const pointer to a const value. Three ways to pass data to functions: using string = std : string ; void passByValue ( string val ); void passByReference ( string & val ); void passByAddress ( string * val ); int main () { string str { \"Hello\" }; passByValue ( str ); passByReference ( str ); passByAddress ( & str ); }","title":"Pointers"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#return-references","text":"It is allowed to return by reference or by address . Objects returned by reference must live beyond the scope of the function returning the reference, or a dangling reference will result. Never return a local variable by reference. Avoid returning references to non-const local static variables. Prefer return by reference over return by address unless the ability to return \u201cno object\u201d (using nullptr ) is important.","title":"Return references"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#memory-allocation","text":"Static memory allocation happens for static and global variables which is allocated once at program start and persists throughout program life time. Automatic memory allocation happens for function parameters and local variables which is allocated when the relevant block is entered, and freed when the block is exited. Dynamic memory allocation allows requesting heap memory from the os when needed, using the new operator. i.e. int* ptr{ new int { 6 } }; Dynamically allocated memory stays allocated until it is explicitly deallocated or until the program ends. Memory leaks happen when your program loses the address of some bit of dynamically allocated memory before giving it back to the operating system. The memory bing dynamically allocated to variable can be freed up when not needed, through the delete <ptr>; operator. Deleting a null pointer has no effect. Do delete the pointer before reassigning it a new value. Avoid having multiple pointers point at the same piece of dynamic memory. When deleting a pointer, set it to nullptr if it is not going out of scope immediately.","title":"Memory allocation"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#program-defined-types","text":"Consider Enums, Structs, Classes as program-defined types to distinguish from standard C++ defined types. Whenever you create a new program-defined type, name it starting with a capital letter. A program-defined type used in only one code file should be defined in that code file as close to the first point of use as possible. A program-defined type used in multiple code files should be defined in a header file with the same name as the program-defined type and then #include into each code file as needed.","title":"Program-defined types"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#enumerations","text":"An enumeration ( enum ) is a compound data type where every possible value is defined as a symbolic constant. Enumerated types are considered part of the integer family of types. You can static_cast integers to a defined enumerator. enum Color { color_red , color_green , color_blue , }; An enumeration or enumerated type is the program-defined type itself. An enumerator is a symbolic constant that is a possible value for a given enumeration. Name your enumerated types starting with a capital letter. Name your enumerators starting with a lower case letter. Avoid assigning explicit values to your enumerators unless you have a compelling reason to do so. Unscoped enumerations are named such because they put their enumerator names into the same scope as the enumeration definition itself. If an unscoped enum is defined in the global scope, it pollutes the global scope and significantly raises the chance of naming collisions. One common way to reduce chances of naming collision is to prefix each enumerator with the name of the enumeration itself. A better way is to put the enumerated type definition in a namespace. It is also common to put enumerated types related to a class inside the scope region of the class. Scoped enumeration are strongly typed (no implicit integer convertion) and strongly scoped (enumerators scoped within the enumeration region). enum class Color { red , green , blue , }; Enumerators must be accessed with prefixing the enumeration type. i.e. Color c { Color::blue }; . static_cast can still be used to cast enumerators into integers. It is acceptable to use operator overloading to reduce the typing in conversions of scoped enumerators.","title":"Enumerations"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#structs","text":"A struct allows bundling multiple variables together into a single type. The variables that are part of the struct are called data members (or member variables). struct Employee { int id {}; int age {}; double wage { 0 }; // explicit default value }; To access a specific member variable, use the member selection operator . on normal variables or references of structs. For pointers, use the pointer/arrow operator -> which does an implicit dereference of the pointer object before selecting the member. An aggregate data type is any type that can contain multiple data members. structs with only data members are aggregates. Aggregates use a form of initialization called aggregate initialization which is just a list of comma-separated initialization values. Each member in the struct is initialized in the order of declaration. When adding a new member to an aggregate, it's safest to add it to the bottom of the definition list so the initializers for other members don't shift. int main () { Employee frank = { 1 , 32 , 60000.0 }; // copy-list initialization Employee robert ( 3 , 45 , 62500.0 ); // direct initialization (C++20) Employee joe { 2 , 28 , 45000.0 }; // list initialization return 0 ; } Variables of a struct type can be const and must be initialized. Structs are generally passed by (const) reference to functions to avoid making copies.","title":"Structs"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#classes","text":"By convention, class names should begin with an upper-case letter. The class name effectively acts like a namespace for the nested type. Type alias members make code easier to maintain and can reduce typing. Generally, nested types should only be used when the nested type is used exclusively within that class. class DateClass { private : // private members int m_year {}; // m_ prefix helps distinguish from local variables int m_month {}; int m_day {}; public : // public members // constructor DateClass () = default ; // tells compiler to create a default constructor DateClass () : m_year { 1999 }, m_month { 1 }, m_day { 1 } { // member initializer list } DateClass ( int year , int month , int day ) { m_year = year ; m_month = month ; m_day = day ; } DateClass ( int year , int month , int day ) // preferred, same form with initializer which can also initialize const variables : m_year { year }, m_month { month }, m_day { day } { } protected : // protected members }; When not specify any access specifiers, members are private by default. Member functions can be defined inside or outside of a class. When all members variables of a class (or struct) are public, we can use aggregate initialization to initialize the class (or struct) directly using list-initialization. Otherwise specify a constructor. Initialize variables in the initializer list in the same order in which they are declared in your class. Delegating constructors allows calling another constructor from one constructor. It is aka constructor chaining. To do so, put the other constructor call inside the initilization list, not in the body of the constructor. A class exposes the this pointer to its active instance to refer to its own object for accessing member variables and functions. It can also be reassigned to overwrite the implicit object. Like so: *this = Foo(); // Foo is the constructor . It can also be returned by functions to allow chaining calls. Destructors are like default constructors but has its name preceded by a ~ . Only one destructor per is class allowed. Destructors are called automatically when the object is destroyed. Note that if you use the exit() function, your program will terminate and no destructors will be called. Be wary if you\u2019re relying on your destructors to do necessary cleanup work.","title":"Classes"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#class-templates","text":"","title":"Class templates"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#std-libraries","text":"","title":"STD Libraries"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#algorithm","text":"Includes functions like sort, fill, find, binary_search, reverse, shuffle, min/max, count_if. std::find(start_iterator, end_iterator, target) searches for the first occurrence of a value in a container. std::find_if works similarly but allows passing in a callable object (function pointer or lambda) that checks to see if a match is found. std::count, std::count_if works similarly and count all occurrences of element or elements fulfilling the condition. std::sort sorts an array to ascending with an optional custom comparing function. std::for_each applies a custom function to every element in an array, which is an alternative to writing a loop. It can also be parallelized for faster processing. Favor using functions from the algorithms library over writing your own functionality to do the same thing.","title":"&lt;algorithm&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#iostream","text":"std::cout , allows print text to program STDOUT. Similarily for std::cerr for print text to STDERR. Put a line break and flushes output with std::endl . Prefer to use \\n for adding line breaks to avoid high frequency in flush. Use insertion operator << along with cout/cerr/endl , or extraction operator >> for cin . There is also std::getline for reading a full ine of text into a variable. i.e. std::getline(std::cin >> std::ws, input); The std::ws is a type of input manipulator to tell std::in ignore leading whitespace, which is necessary to continuously read user inputs in an interactive program. When doing extraction with std::cin , an extraction of user input can fail for many reasons, common one is the input causes an overflow, which leave cin in a failed state and cause any future extraction to be skipped. What to do in this case where bad user input can be anticipated is to use std::cin.fail() to check if the extraction failed, then std::cin.clear() to reset the failure flag and something like std::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\\n'); to skip what user entered (before hitting the enter key), any probably ask the user to enter again and tell them why it failed.","title":"&lt;iostream&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#iterator","text":"Contains functions like begin, end on arrays.","title":"&lt;iterator&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#random","text":"Provides PRNG (Pseudo Random Number Generator) function that generates fairly evenly distributed random numbers. std::mt19937 is a 32-bit Mersenne Twister random number generator function; std::mt19937_64 is the 64-bit version. Also to make sure it is truely random everytime the program is run, seed the PRNG with system clock ( <chrono> is needed). Use std::uniform_int_distribution<> die{ 1, 6 }; to limit the random number range. Like so: int rand_gen () { std :: mt19937 mt { static_cast < unsigned int > ( std :: chrono :: steady_clock :: now (). time_since_epoch (). count () ) }; // alternatively seed with the os implemented random_device std :: mt19937 mt { std :: random_device {}() }; std :: uniform_int_distribution <> die6 { 1 , 6 }; int rand = die6 ( mt ); return rand ; } Make a singleton of PRNG, and only seed a PRNG once, and use it throughout your program. Do so with a namespace and define a global PRNG, like so: namespace Random { // capital R to avoid conflicts with functions named random() std :: mt19937 mt { std :: random_device {}() }; int get ( int min , int max ) { std :: uniform_int_distribution die { min , max }; return die ( mt ); } }","title":"&lt;random&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#utility","text":"Provides useful types like Pairs, generic swap function","title":"&lt;utility&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#cassert","text":"An assertion tests if an expression evaluates to true. If not, the program terminates with an error message. This is useful to do precondition check anywhere fits. It is best to use this format assert(<condition> && \"error message\"); so when this condition expression is false, the informative message is also printed. Generally use assertions to document cases that should be logically impossible; for other cases use proper exception handling methods. Asserts should never be encountered in production code. Asserts should be used only in cases where corruption isn't likely to occur if the program terminates unexpectedly. If the macro NDEBUG is defined, the assert macro gets disabled. static_assert(<condition>, \"diagnostic_message\"); to evaluate something at compile time.","title":"&lt;cassert&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#cstdint","text":"For defining fixed-width integers.","title":"&lt;cstdint&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#cstdlib","text":"Halt functions: std::exit() can be used to explicitly exit the program. Do note that it does not clean up any local variables (either in the current function, or in functions up the call stack). Avoid using it unless you know how to properly clean up your program. std::atexit() can be used to pass in a cleanup function when exit() is called. In a multi-threaded program, it is better to use std::quick_exit() to exit the program (which does not immediately clean up static objects that may still be accessed by other threads), paired with std::at_quick_exit() for cleanup. Furthermore, std::abort() function causes your program to terminate abnormally. There is also std::terminate() which is typically used in conjunction with exceptions. Both does not do propery cleanups. Generally use exceptions for error handling.","title":"&lt;cstdlib&gt;"},{"location":"Programming-Lang-Reference/CPP/CPlusPlus_Lang_Specs/#cmath","text":"Handy math functions. i.e. pow, abs, max, min","title":"&lt;cmath&gt;"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/","text":"This set of notes is taking from golang.org Thinking about the problems from a Go's perspective could produce a successful but quite different program. To write Go well, it's important to understand its properties and idioms, to know its conventions for programming. Formatting and Styles \u00b6 Formatting issues are the most contentious but the least consequential. In Go we can let the machine to take care of most formatting issues, by using gofmt program. gofmt is also available as command go fmt operates at the package level rather than source file level reads a Go program and emits the source in a standard style of indentation and vertical alignment while retaining and reformatting comments if necessary a few formatting details use tabs for indentation if a line feels too long, wrap it and indent with an extra tab use fewer parentheses; mathematic expression can be concise and clear without parentheses i.e. x<<8 + y<<16 Therefore, don't spend time on formatting the comments, let Go do it by running gofmt type T struct { name string // name of the object value int // its value } // after running gofmt type T struct { name string // name of the object value int // its value } github.com/uber-go offers a good style guide. Install some useful IDE plugins to help lint styles and errors quicker and sooner. Commentary \u00b6 Go provides C-style /* */ block comments and C++-style // line comments . Block comment is usually used for adding package comments. godoc is a program (and web server) that processes Go source files to extract documentation about the contents of the package. Comments that appear immediately before top-level declarations , with NO intervening newlines, are extracted along with the declaration to serve as explanatory text or doc comment for the declaration. every exported (capitalized) name in a program should have a doc comment doc comments work best as complete sentences first sentence should be a one-sentence summary that starts with the name being declared will allow docs search results to be more intuitive you can search docs to find a desired function by a command like go doc -all regexp | grep -i parse godoc displays indented text in a fixed-width font, suitable for program snippets // Compile parses a regular expression and returns, if successful, // a Regexp that can be used to match against text. func Compile ( str string ) ( * Regexp , error ) { // ... } Every package should have a package comment , a block comment preceding the package clause. However if the package is simple, the package comment can be brief and use line comments to serve the purpose /* Package regexp implements a simple library for regular expressions. The syntax of the regular expressions accepted is: regexp: concatenation { '|' concatenation } concatenation: { closure } closure: term [ '*' | '+' | '?' ] term: '^' '$' '.' character '[' [ '^' ] character-ranges ']' '(' regexp ')' */ package regexp Go's declaration syntax allows grouping of declarations. a single doc comment can introduce a group of related constants or variables grouping can also indicate relationships between items, such as the fact that a set of variables is protected by a mutex. // Error codes returned by failures to parse an expression. var ( ErrInternal = errors . New ( \"regexp: internal error\" ) ErrUnmatchedLpar = errors . New ( \"regexp: unmatched '('\" ) ErrUnmatchedRpar = errors . New ( \"regexp: unmatched ')'\" ) ... ) Names \u00b6 In Go, the visibility of a name outside a package is determined by whether its first character is upper case . And naming convention matters. Package Names When a package is imported, the package name becomes an accessor for the contents with the package. when import \"bytes\" , can access bytes.Buffer packages should be given lower case , single-word names and no need for underscores or mixedCaps package name is only the default name for imports; it need not be unique across all source code when there is a collision, a package can be given an alias for use locally package name should be the base name of its source directory the package in src/encoding/base64 is imported as \"encoding/base64\" but has name base64 , NOT encoding_base64 and NOT encodingBase64 don't use the import . notation importer of a package will use the name to refer to its contents give clear and concise names for things within a package i.e. bufio.Reader , so no need to name the Reader as BufReader i.e. ring.New , not need to name the constructor as NewRing long names don't automatically make things more readable. A helpful doc comment can often be more valuable than an extra long name Go doesn't provide automatic Getters or Setters , but when writing ones on your own, it is best to omit the \"Get\" part, say obj.Owner() should be concise and straight-forward. By convention, one-method interfaces are named by the method name plus an -er suffix to construct an agent noun. Read, Write, Close, Flush, String and so on have canonical signatures and meanings . To avoid confusion, don't give your method one of those names unless it has the same signature and meaning. The convention in Go is to use MixedCaps or mixedCaps rather than underscores to write multiword names Semicolons \u00b6 Go's formal grammar uses semicolons to terminate statements but they do NOT appear in the source. \u201cif the newline comes after a token that could end a statement, insert a semicolon\u201d a semicolon can also be omitted immediately before a closing brace idiomatic Go programs have semicolons only in places such as for loop clauses, to separate the initializer, condition, and continuation elements semicolons are also necessary to separate multiple statements on a line you CANNOT put the opening brace of a control structure ( if, for, switch, or select ) on the next line because of the semicolon insertion rule Control Structures \u00b6 Control structures of Go are related to those of C but differ in important ways: there is no do or while loop, only a slightly generalized for switch is more flexible if and switch accept an optional initialization statement like that of for mandatory braces encourage writing simple if statements on multiple lines when an if statement doesn't flow into the next statement\u2014that is, the body ends in break, continue, goto, or return , the unnecessary else is omitted break and continue statements take an optional label to identify what to break or continue there are new control structures including a type switch and a multi-way communications multiplexer, select there are no parentheses and the bodies must always be brace-delimited Three forms of for loops for init; condition; post {} for condition {} is like a while loop a range clause can manage the loop, i.e. for key, value := range oldMap {} for _, value := range array {} use _ (aka the blank identifier ) to discard UNWANTED return values for {} is like a while-true loop Go has NO comma operator and ++ and -- are statements , NOT expressions for i, j := 0, len(a)-1; i < j; i, j = i+1, j-1 {} Go's switch evaluates the cases top to bottom UNTIL a match is found if the switch has NO expression it switches on true it is idiomatic to write an if-else-if-else chain as a switch there is NO automatic fall through , but cases can be presented in comma-separated lists break statements can be used to terminate a switch early when trying to break out of a surrounding loop , not the switch , use a label on the loop and \"breaking\" to that label continue with label is specific to loops only a type switch can also be used to discover the dynamic type of an interface variable such a type switch uses the syntax of a type assertion with the keyword type inside the parentheses var t interface {} t = functionOfSomeType () switch t := t .( type ) { default : fmt . Printf ( \"unexpected type %T\\n\" , t ) // %T prints whatever type t has case bool : fmt . Printf ( \"boolean %t\\n\" , t ) // t has type bool case int : fmt . Printf ( \"integer %d\\n\" , t ) // t has type int case * bool : fmt . Printf ( \"pointer to boolean %t\\n\" , * t ) // t has type *bool case * int : fmt . Printf ( \"pointer to integer %d\\n\" , * t ) // t has type *int } In a := declaration a variable v may appear even if it has ALREADY been declared, when this declaration is in the same scope as the existing declaration of v if v is already declared in an outer scope, the declaration will create a NEW variable in Go the scope of function parameters and return values is the same as the function body the corresponding value in the initialization is assignable to v there is at least one other variable that is created by the declaration Functions \u00b6 In Go, functions and methods can return multiple values , which is convenient to report errors. i.e. func (file *File) Write(b []byte) (n int, err error) returns the number of bytes written and a non-nil error when n != len(b) i.e. a simple-minded function to grab a number from a position in a byte slice, returning the number and the next position func nextInt ( b [] byte , i int ) ( int , int ) { for ; i < len ( b ) && ! isDigit ( b [ i ]); i ++ { } x := 0 for ; i < len ( b ) && isDigit ( b [ i ]); i ++ { x = x * 10 + int ( b [ i ]) - '0' } return x , i } for i := 0 ; i < len ( b ); { x , i = nextInt ( b , i ) fmt . Println ( x ) } The return or result parameters of a Go function can be given names and used as regular variables, just like the incoming parameters when named, they are initialized to the zero values for their types when the function begins if the function executes a return statement with no arguments, the current values of the named result parameters are used as the returned values the names are not mandatory but they can make code shorter and clearer: they're documentation Go's defer statement schedules a function call to be run immediately before the function returns it is effective for closing or releasing resources it guarantees that you will never forget to close the file the close sits near the open , which is much clearer than placing it at the end of the function the arguments to the deferred function (including the receiver if it is a method) are evaluated when the defer executes if it is a variable, that variable value can change within the function body before the defer function is executed a function can defer multiple function executions deferred functions are executed in LIFO order // simple ways to add function traces for debugging func trace ( s string ) { fmt . Println ( \"entering:\" , s ) } func untrace ( s string ) { fmt . Println ( \"leaving:\" , s ) } func a () { trace ( \"a\" ) defer untrace ( \"a\" ) // do something.... } // simple ways to time a function for debugging func startTimer ( s string ) ( string , int64 ) { return ( s , time . Now (). UnixNano ()) } func timeIt ( s string , t int64 ) { now := time . Now () t2 := now . UnixNano () fmt . Println ( \"Function \" , s , \" run time is \" , t2 , \" nano seconds\" ) } func b () { defer timeIt ( startTimer ( \"a\" )) // do something.... } Data \u00b6 Go has two allocation primitives , the built-in functions new and make new(T) allocates memory and zeros it (uninitialized) and returns its address with type *T (pointer) it's helpful to arrange when designing your data structures that the zero value of each type can be used without further initialization p := new ( SyncedBuffer ) // type *SyncedBuffer var v SyncedBuffer // type SyncedBuffer Sometimes it is easier to use a composite literal , which is an expression that creates a new instance each time it is evaluated taking the address of a composite literal allocates a fresh instance each time it is evaluated fields of a composite literal are laid out in order and MUST ALL be present by labeling the elements explicitly as field:value pairs, the initializers can appear in any order and the missing ones left as their zero values if a composite literal contains no fields at all, it creates a zero value for the type in other words, the expressions new(File) and &File{} are equivalent composite literals can also be created for arrays, slices, and maps , with the field labels being indices or map keys as appropriate func NewFile ( fd int , name string ) * File { if fd < 0 { return nil } f := new ( File ) f . fd = fd f . name = name f . dirinfo = nil f . nepipe = 0 return f } // vs. func NewFile ( fd int , name string ) * File { if fd < 0 { return nil } return & File { fd , name , nil , 0 } } built-in function make(T, args) creates slices, maps, and channels only, and it returns an initialized (not zeroed) value of type T (not *T) these three types represent, under the covers, references to data structures that MUST be initialized before use i.e. make([]int, 10, 100) allocates an array of 100 ints and then creates a slice structure with length 10 and a capacity of 100 pointing at the first 10 elements of the array new([]int) returns a pointer to a newly allocated, zeroed slice structure, that is, a pointer to a nil slice value Arrays are useful when planning the detailed layout of memory - arrays are values and building block for slices - assigning one array to another copies ALL the elements - when passing an array to a function, it will receive a copy of the array - size of an array is part of its type - you can pass a pointer of an array - use slices whenever you can func Sum ( a * [ 3 ] float64 ) ( sum float64 ) { for _ , v := range * a { sum += v } return } array := [ ... ] float64 { 7.0 , 8.5 , 9.1 } x := Sum ( & array ) slices wrap arrays to give a more general, powerful, and convenient interface to sequences of data most array programming in Go is done with slices rather than simple arrays slices hold references to an underlying array if you assign one slice to another, both refer to the same array length within the slice sets an UPPER LIMIT of how much data to read length of a slice may be changed as long as it still fits within the limits of the underlying array do this by assigning it to a slice of itself capacity of a slice, accessible by the built-in function cap , reports the maximum length the slice may assume func Append ( slice , data [] byte ) [] byte { l := len ( slice ) if l + len ( data ) > cap ( slice ) { // reallocate // Allocate double what's needed, for future growth. newSlice := make ([] byte , ( l + len ( data )) * 2 ) // The copy function is predeclared and works for any slice type. copy ( newSlice , slice ) slice = newSlice } slice = slice [ 0 : l + len ( data )] copy ( slice [ l :], data ) return slice } slices are variable-length, it is possible to have each inner slice be a different length, when defining a 2D slice. type Transform [ 3 ][ 3 ] float64 // A 3x3 array, really an array of arrays. type LinesOfText [][] byte // A slice of byte slices. The built-in data structure map associate values of one type (the key) with values of another type (the element or value) key can be of any type for which the equality operator is defined integers, floating point and complex numbers, strings, pointers, interfaces (as long as the dynamic type supports equality), structs and arrays maps hold references to an underlying data structure maps can be constructed using composite literal syntax with colon-separated key-value pairs attempt to fetch a map value with a key that is not present in the map will return the zero value for the type of the entries in the map use delete to unset a map entry, like this delete(timeZone, \"PDT\") var timeZone = map [ string ] int { \"UTC\" : 0 * 60 * 60 , \"EST\" : - 5 * 60 * 60 , \"CST\" : - 6 * 60 * 60 , \"MST\" : - 7 * 60 * 60 , \"PST\" : - 8 * 60 * 60 , } if offset , isKeyDefined := timeZone [ \"EST\" ]; isKeyDefined { // do something with offset } formatted print functions fmt.Fprint and its friends take as a first argument any object that implements the io.Writer interface // prints same results fmt . Printf ( \"Hello %d\\n\" , 23 ) fmt . Fprint ( os . Stdout , \"Hello \" , 23 , \"\\n\" ) fmt . Println ( \"Hello\" , 23 ) fmt . Println ( fmt . Sprint ( \"Hello \" , 23 )) Attach a method such as String to any user-defined type makes it possible for arbitrary values to format themselves automatically for printing Initialization \u00b6 Complex structures can be built during initialization and the ordering issues among initialized objects, even among different packages, are handled correctly Go's constants are created at compile time and can only be numbers, characters (runes), strings or booleans ; expressions that define them must be constant expressions Variables can be initialized just like constants but the initializer can be a general expression computed at run time Each source file can define its own niladic init function(s) to set up whatever state is required init is called after all the variable declarations in the package have evaluated their initializers evaluated only after all the imported packages have been initialized init can also be used to verify or repair correctness of the program state before real execution begins Methods \u00b6 Methods can be defined for any named type (except a pointer or an interface) and the receiver does not have to be a struct . The rule about pointers vs. values for receivers is that value methods can be invoked on pointers and values, but pointer methods can ONLY be invoked on pointers this rule arises because pointer methods can modify the receiver; invoking them on a value would cause the method to receive a copy of the value, so any modifications would be discarded when the value is addressable, the language takes care of the common case of invoking a pointer method on a value by inserting the address operator automatically The idea of using Write on a slice of bytes is central to the implementation of bytes.Buffer func ( p * ByteSlice ) Append ( data [] byte ) { slice := * p // Body same as above Append function, without the return. * p = slice } // implements the io.Write interface func ( p * ByteSlice ) Write ( data [] byte ) ( n int , err error ) { slice := * p // Body same as above Append method * p = slice return len ( data ), nil } var b ByteSlice fmt . Fprintf ( & b , \"This hour has %d days\\n\" , 7 ) // because we implemented the Write method Interfaces and other types \u00b6 Interfaces in Go provide a way to specify the behavior of an object: if something can do this, then it can be used here . a type can implement multiple interfaces a collection can be sorted by the routines in package sort if it implements sort.Interface , which contains Len(), Less(i, j int) bool, and Swap(i, j int) , and it could also have a custom formatter Interfaces with only one or two methods are common in Go code, and are usually given a name derived from the method type Sequence [] int // Methods required by sort.Interface. func ( s Sequence ) Len () int { return len ( s ) } func ( s Sequence ) Less ( i , j int ) bool { return s [ i ] < s [ j ] } func ( s Sequence ) Swap ( i , j int ) { s [ i ], s [ j ] = s [ j ], s [ i ] } // Copy returns a copy of the Sequence. func ( s Sequence ) Copy () Sequence { copy := make ( Sequence , 0 , len ( s )) return append ( copy , s ... ) } // Method for printing - sorts the elements before printing. func ( s Sequence ) String () string { s = s . Copy () // Make a copy; don't overwrite argument. sort . Sort ( s ) str := \"[\" for i , elem := range s { // Loop is O(N\u00b2); will fix that in next example. if i > 0 { str += \" \" } str += fmt . Sprint ( elem ) } return str + \"]\" } If we convert the Sequence back to []int , we can call Sprint directly on s . The conversion doesn't create a new value, it just temporarily acts as though the existing value has a new type. func ( s Sequence ) String () string { s = s . Copy () sort . IntSlice ( s ). Sort () return fmt . Sprint ([] int ( s )) } It's an idiom in Go programs to convert the type of an expression to access a different set of methods. Type switches are a form of conversion: they take an interface and, for each case in the switch, in a sense convert it to the type of that case. type Stringer interface { String () string } var value interface {} // Value provided by caller. switch str := value .( type ) { case string : return str case Stringer : return str . String () } A type assertion takes an interface value and extracts from it a value of the specified explicit type and the result is a new value with the static type <typeName> that type must either be the concrete type held by the interface, or a second interface type that the value can be converted to. i.e. str := value.(string) if the value does not contain a string, the program will crash with a run-time error. can guard against it with a \"comma, ok\" idiom to test if assertion fails, str will still be a string with its zero value str , ok := value .( string ) if ok { fmt . Printf ( \"string value is: %q\\n\" , str ) } else { fmt . Printf ( \"value is not a string\\n\" ) } If a type exists only to implement an interface and will never have exported methods beyond that interface, there is no need to export the type itself its constructor should return an interface value rather than the implementing type in this way, similar types that implements the same interfaces can be easily replaced for one another by changing the constructor call and the rest of the code is unaffected Almost anything can have methods attached, almost anything can satisfy an interface. A simple http handler: type Counter int func ( ctr * Counter ) ServeHTTP ( w http . ResponseWriter , req * http . Request ) { * ctr ++ fmt . Fprintf ( w , \"counter = %d\\n\" , * ctr ) } ... import \"net/http\" ... ctr := new ( Counter ) http . Handle ( \"/counter\" , ctr ) The blank identifier \u00b6 It's a bit like writing to the Unix /dev/null file: it represents a write-only value to be used as a place-holder where a variable is needed but the actual value is irrelevant. Some common use cases are multiple variable assignment as placeholder to take up a variable's value assignment i.e. _, err := os.Stat(path) to silence compiler complaints about unused imports or variable assignments that would eventually be used i.e. fd, err := os.Open(\"test.go\"); _ = fd to rename a package that is imported only for its side effects i.e. import _ \"net/http/pprof\" to check whether a type implements an interface i.e. if _, ok := val.(json.Marshaler); ok {} Embedding \u00b6 Go allows a type to \u201cborrow\u201d pieces of an implementation by embedding types within a struct or interface you can directly include other interfaces in a type to form a new type, without specifying all of their methods only interfaces can be embedded within interfaces // ReadWriter is the interface that combines the Reader and Writer interfaces. type ReadWriter interface { Reader Writer } // ReadWriter stores pointers to a Reader and a Writer. // It implements io.ReadWriter. type ReadWriter struct { reader * Reader // *bufio.Reader writer * Writer // *bufio.Writer } When we embed a type, the methods of that type become methods of the outer type, but when they are invoked the receiver of the method is the inner type, not the outer one If we need to refer to an embedded field directly, the type name of the field, ignoring the package qualifier, can be served as a field name If embedding types introduces name conflicts, there are two rules if an embedded type introduces methods or fields that are defined by the same name at this struct, then the definition at this struct will dominate it (and suppress the conflicting methods/fields introduced by the embedded type) if embedded a type and also defined another type of method with the same name as the embedded type, it is an error unless neither one of them was used Concurrency \u00b6 Do not communicate by sharing memory; instead, share memory by communicating. The goal of concurrency, structuring a program as independently executing components and executing calculations in parallel for efficiency on multiple CPUs In Go, shared values are passed around on channels and never actively shared by separate threads of execution only one goroutine has access to the value at any given time data races cannot occur, by design A goroutine is a function executing concurrently with other goroutines in the same address space and when the call completes, the goroutine exits silently it is lightweight, costing little more than the allocation of stack space the stacks start small, cheap, and grow by allocating (and freeing) heap storage as required goroutines are multiplexed onto multiple OS threads so some can block and others can execute Unbuffered channels combine communication(the exchange of a value) with synchronization(guaranteeing that the two ends are in a known state) A buffered channel can be used like a semaphore , for instance to limit throughput. A common use of channels is to implement safe, parallel demultiplexing by creating \"channels of channels\". can be done by creating structs that contains channels and pass struct objects to a channel Errors \u00b6 Library routines must often return some sort of error indication to the caller errors have type error, a simple built-in interface. when successful the error will be nil; when there is a problem, it should hold an error when feasible, error strings should identify their origin, such as by having a prefix naming the operation or package that generated the error type error interface { Error () string } Caller can use a type switch or a type assertion to look for specific errors and extract details for try := 0 ; try < 2 ; try ++ { file , err = os . Create ( filename ) if err == nil { return } if e , ok := err .( * os . PathError ); ok && e . Err == syscall . ENOSPC { // if the err was of type *os.PathError, and then so is e deleteTempFiles () // Recover some space. continue } return } If an error is unrecoverable, it will be better to use panic to create a run-time error that will stop the program panic takes a single argument of arbitrary type (often a string) to be printed as the program dies when panic is called, it immediately stops execution of the current function and begins unwinding the stack of the goroutine, running any deferred functions along the way. if that unwinding reaches the top of the goroutine's stack, the program dies it is possible to use the built-in function recover to regain control of the goroutine and resume normal execution it's also a way to indicate that something impossible has happened real library functions should avoid using panic it's always better to let things continue to run rather than taking down the whole program one exception is during initialization, if the library truly cannot set itself up, it might be reasonable to panic without further damage A call to recover stops the unwinding and returns the argument passed to panic recover is only useful inside deferred functions because the only code that runs while undergoing unwinding is inside deferred functions one application of recover is to shut down a failing goroutine inside a server without killing the other executing goroutines deferred code can call library routines that themselves use panic and recover without failing deferred functions can modify named return values func server ( workChan <- chan * Work ) { for work := range workChan { go safelyDo ( work ) } } func safelyDo ( work * Work ) { defer func () { if err := recover (); err != nil { log . Println ( \"work failed:\" , err ) } }() do ( work ) } A more complete example, with deferred function modifying the named return values // Error is the type of a parse error; it satisfies the error interface. type Error string func ( e Error ) Error () string { return string ( e ) } // error is a method of *Regexp that reports parsing errors by // panicking with an Error. func ( regexp * Regexp ) error ( err string ) { panic ( Error ( err )) } // Compile returns a parsed representation of the regular expression. func Compile ( str string ) ( regexp * Regexp , err error ) { regexp = new ( Regexp ) // doParse will panic if there is a parse error. defer func () { if e := recover (); e != nil { regexp = nil // Clear return value. err = e .( Error ) // Will re-panic if not a parse error. } }() return regexp . doParse ( str ), nil } Web Server \u00b6 An simple example package main import ( \"flag\" \"html/template\" \"log\" \"net/http\" ) var addr = flag . String ( \"addr\" , \":1718\" , \"http service address\" ) // Q=17, R=18 var templ = template . Must ( template . New ( \"qr\" ). Parse ( templateStr )) func main () { flag . Parse () http . Handle ( \"/\" , http . HandlerFunc ( QR )) err := http . ListenAndServe ( * addr , nil ) if err != nil { log . Fatal ( \"ListenAndServe:\" , err ) } } func QR ( w http . ResponseWriter , req * http . Request ) { templ . Execute ( w , req . FormValue ( \"s\" )) } const templateStr = ` <html> <head> <title>QR Link Generator</title> </head> <body> {{if .}} <img src=\"http://chart.apis.google.com/chart?chs=300x300&cht=qr&choe=UTF-8&chl={{.}}\" /> <br> {{.}} <br> <br> {{end}} <form action=\"/\" name=f method=\"GET\"> <input maxLength=1024 size=70 name=s value=\"\" title=\"Text to QR Encode\"> <input type=submit value=\"Show QR\" name=qr> </form> </body> </html> ` The html/template package is very powerful. Documentation to it is here","title":"Effective Go"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#formatting-and-styles","text":"Formatting issues are the most contentious but the least consequential. In Go we can let the machine to take care of most formatting issues, by using gofmt program. gofmt is also available as command go fmt operates at the package level rather than source file level reads a Go program and emits the source in a standard style of indentation and vertical alignment while retaining and reformatting comments if necessary a few formatting details use tabs for indentation if a line feels too long, wrap it and indent with an extra tab use fewer parentheses; mathematic expression can be concise and clear without parentheses i.e. x<<8 + y<<16 Therefore, don't spend time on formatting the comments, let Go do it by running gofmt type T struct { name string // name of the object value int // its value } // after running gofmt type T struct { name string // name of the object value int // its value } github.com/uber-go offers a good style guide. Install some useful IDE plugins to help lint styles and errors quicker and sooner.","title":"Formatting and Styles"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#commentary","text":"Go provides C-style /* */ block comments and C++-style // line comments . Block comment is usually used for adding package comments. godoc is a program (and web server) that processes Go source files to extract documentation about the contents of the package. Comments that appear immediately before top-level declarations , with NO intervening newlines, are extracted along with the declaration to serve as explanatory text or doc comment for the declaration. every exported (capitalized) name in a program should have a doc comment doc comments work best as complete sentences first sentence should be a one-sentence summary that starts with the name being declared will allow docs search results to be more intuitive you can search docs to find a desired function by a command like go doc -all regexp | grep -i parse godoc displays indented text in a fixed-width font, suitable for program snippets // Compile parses a regular expression and returns, if successful, // a Regexp that can be used to match against text. func Compile ( str string ) ( * Regexp , error ) { // ... } Every package should have a package comment , a block comment preceding the package clause. However if the package is simple, the package comment can be brief and use line comments to serve the purpose /* Package regexp implements a simple library for regular expressions. The syntax of the regular expressions accepted is: regexp: concatenation { '|' concatenation } concatenation: { closure } closure: term [ '*' | '+' | '?' ] term: '^' '$' '.' character '[' [ '^' ] character-ranges ']' '(' regexp ')' */ package regexp Go's declaration syntax allows grouping of declarations. a single doc comment can introduce a group of related constants or variables grouping can also indicate relationships between items, such as the fact that a set of variables is protected by a mutex. // Error codes returned by failures to parse an expression. var ( ErrInternal = errors . New ( \"regexp: internal error\" ) ErrUnmatchedLpar = errors . New ( \"regexp: unmatched '('\" ) ErrUnmatchedRpar = errors . New ( \"regexp: unmatched ')'\" ) ... )","title":"Commentary"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#names","text":"In Go, the visibility of a name outside a package is determined by whether its first character is upper case . And naming convention matters. Package Names When a package is imported, the package name becomes an accessor for the contents with the package. when import \"bytes\" , can access bytes.Buffer packages should be given lower case , single-word names and no need for underscores or mixedCaps package name is only the default name for imports; it need not be unique across all source code when there is a collision, a package can be given an alias for use locally package name should be the base name of its source directory the package in src/encoding/base64 is imported as \"encoding/base64\" but has name base64 , NOT encoding_base64 and NOT encodingBase64 don't use the import . notation importer of a package will use the name to refer to its contents give clear and concise names for things within a package i.e. bufio.Reader , so no need to name the Reader as BufReader i.e. ring.New , not need to name the constructor as NewRing long names don't automatically make things more readable. A helpful doc comment can often be more valuable than an extra long name Go doesn't provide automatic Getters or Setters , but when writing ones on your own, it is best to omit the \"Get\" part, say obj.Owner() should be concise and straight-forward. By convention, one-method interfaces are named by the method name plus an -er suffix to construct an agent noun. Read, Write, Close, Flush, String and so on have canonical signatures and meanings . To avoid confusion, don't give your method one of those names unless it has the same signature and meaning. The convention in Go is to use MixedCaps or mixedCaps rather than underscores to write multiword names","title":"Names"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#semicolons","text":"Go's formal grammar uses semicolons to terminate statements but they do NOT appear in the source. \u201cif the newline comes after a token that could end a statement, insert a semicolon\u201d a semicolon can also be omitted immediately before a closing brace idiomatic Go programs have semicolons only in places such as for loop clauses, to separate the initializer, condition, and continuation elements semicolons are also necessary to separate multiple statements on a line you CANNOT put the opening brace of a control structure ( if, for, switch, or select ) on the next line because of the semicolon insertion rule","title":"Semicolons"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#control-structures","text":"Control structures of Go are related to those of C but differ in important ways: there is no do or while loop, only a slightly generalized for switch is more flexible if and switch accept an optional initialization statement like that of for mandatory braces encourage writing simple if statements on multiple lines when an if statement doesn't flow into the next statement\u2014that is, the body ends in break, continue, goto, or return , the unnecessary else is omitted break and continue statements take an optional label to identify what to break or continue there are new control structures including a type switch and a multi-way communications multiplexer, select there are no parentheses and the bodies must always be brace-delimited Three forms of for loops for init; condition; post {} for condition {} is like a while loop a range clause can manage the loop, i.e. for key, value := range oldMap {} for _, value := range array {} use _ (aka the blank identifier ) to discard UNWANTED return values for {} is like a while-true loop Go has NO comma operator and ++ and -- are statements , NOT expressions for i, j := 0, len(a)-1; i < j; i, j = i+1, j-1 {} Go's switch evaluates the cases top to bottom UNTIL a match is found if the switch has NO expression it switches on true it is idiomatic to write an if-else-if-else chain as a switch there is NO automatic fall through , but cases can be presented in comma-separated lists break statements can be used to terminate a switch early when trying to break out of a surrounding loop , not the switch , use a label on the loop and \"breaking\" to that label continue with label is specific to loops only a type switch can also be used to discover the dynamic type of an interface variable such a type switch uses the syntax of a type assertion with the keyword type inside the parentheses var t interface {} t = functionOfSomeType () switch t := t .( type ) { default : fmt . Printf ( \"unexpected type %T\\n\" , t ) // %T prints whatever type t has case bool : fmt . Printf ( \"boolean %t\\n\" , t ) // t has type bool case int : fmt . Printf ( \"integer %d\\n\" , t ) // t has type int case * bool : fmt . Printf ( \"pointer to boolean %t\\n\" , * t ) // t has type *bool case * int : fmt . Printf ( \"pointer to integer %d\\n\" , * t ) // t has type *int } In a := declaration a variable v may appear even if it has ALREADY been declared, when this declaration is in the same scope as the existing declaration of v if v is already declared in an outer scope, the declaration will create a NEW variable in Go the scope of function parameters and return values is the same as the function body the corresponding value in the initialization is assignable to v there is at least one other variable that is created by the declaration","title":"Control Structures"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#functions","text":"In Go, functions and methods can return multiple values , which is convenient to report errors. i.e. func (file *File) Write(b []byte) (n int, err error) returns the number of bytes written and a non-nil error when n != len(b) i.e. a simple-minded function to grab a number from a position in a byte slice, returning the number and the next position func nextInt ( b [] byte , i int ) ( int , int ) { for ; i < len ( b ) && ! isDigit ( b [ i ]); i ++ { } x := 0 for ; i < len ( b ) && isDigit ( b [ i ]); i ++ { x = x * 10 + int ( b [ i ]) - '0' } return x , i } for i := 0 ; i < len ( b ); { x , i = nextInt ( b , i ) fmt . Println ( x ) } The return or result parameters of a Go function can be given names and used as regular variables, just like the incoming parameters when named, they are initialized to the zero values for their types when the function begins if the function executes a return statement with no arguments, the current values of the named result parameters are used as the returned values the names are not mandatory but they can make code shorter and clearer: they're documentation Go's defer statement schedules a function call to be run immediately before the function returns it is effective for closing or releasing resources it guarantees that you will never forget to close the file the close sits near the open , which is much clearer than placing it at the end of the function the arguments to the deferred function (including the receiver if it is a method) are evaluated when the defer executes if it is a variable, that variable value can change within the function body before the defer function is executed a function can defer multiple function executions deferred functions are executed in LIFO order // simple ways to add function traces for debugging func trace ( s string ) { fmt . Println ( \"entering:\" , s ) } func untrace ( s string ) { fmt . Println ( \"leaving:\" , s ) } func a () { trace ( \"a\" ) defer untrace ( \"a\" ) // do something.... } // simple ways to time a function for debugging func startTimer ( s string ) ( string , int64 ) { return ( s , time . Now (). UnixNano ()) } func timeIt ( s string , t int64 ) { now := time . Now () t2 := now . UnixNano () fmt . Println ( \"Function \" , s , \" run time is \" , t2 , \" nano seconds\" ) } func b () { defer timeIt ( startTimer ( \"a\" )) // do something.... }","title":"Functions"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#data","text":"Go has two allocation primitives , the built-in functions new and make new(T) allocates memory and zeros it (uninitialized) and returns its address with type *T (pointer) it's helpful to arrange when designing your data structures that the zero value of each type can be used without further initialization p := new ( SyncedBuffer ) // type *SyncedBuffer var v SyncedBuffer // type SyncedBuffer Sometimes it is easier to use a composite literal , which is an expression that creates a new instance each time it is evaluated taking the address of a composite literal allocates a fresh instance each time it is evaluated fields of a composite literal are laid out in order and MUST ALL be present by labeling the elements explicitly as field:value pairs, the initializers can appear in any order and the missing ones left as their zero values if a composite literal contains no fields at all, it creates a zero value for the type in other words, the expressions new(File) and &File{} are equivalent composite literals can also be created for arrays, slices, and maps , with the field labels being indices or map keys as appropriate func NewFile ( fd int , name string ) * File { if fd < 0 { return nil } f := new ( File ) f . fd = fd f . name = name f . dirinfo = nil f . nepipe = 0 return f } // vs. func NewFile ( fd int , name string ) * File { if fd < 0 { return nil } return & File { fd , name , nil , 0 } } built-in function make(T, args) creates slices, maps, and channels only, and it returns an initialized (not zeroed) value of type T (not *T) these three types represent, under the covers, references to data structures that MUST be initialized before use i.e. make([]int, 10, 100) allocates an array of 100 ints and then creates a slice structure with length 10 and a capacity of 100 pointing at the first 10 elements of the array new([]int) returns a pointer to a newly allocated, zeroed slice structure, that is, a pointer to a nil slice value Arrays are useful when planning the detailed layout of memory - arrays are values and building block for slices - assigning one array to another copies ALL the elements - when passing an array to a function, it will receive a copy of the array - size of an array is part of its type - you can pass a pointer of an array - use slices whenever you can func Sum ( a * [ 3 ] float64 ) ( sum float64 ) { for _ , v := range * a { sum += v } return } array := [ ... ] float64 { 7.0 , 8.5 , 9.1 } x := Sum ( & array ) slices wrap arrays to give a more general, powerful, and convenient interface to sequences of data most array programming in Go is done with slices rather than simple arrays slices hold references to an underlying array if you assign one slice to another, both refer to the same array length within the slice sets an UPPER LIMIT of how much data to read length of a slice may be changed as long as it still fits within the limits of the underlying array do this by assigning it to a slice of itself capacity of a slice, accessible by the built-in function cap , reports the maximum length the slice may assume func Append ( slice , data [] byte ) [] byte { l := len ( slice ) if l + len ( data ) > cap ( slice ) { // reallocate // Allocate double what's needed, for future growth. newSlice := make ([] byte , ( l + len ( data )) * 2 ) // The copy function is predeclared and works for any slice type. copy ( newSlice , slice ) slice = newSlice } slice = slice [ 0 : l + len ( data )] copy ( slice [ l :], data ) return slice } slices are variable-length, it is possible to have each inner slice be a different length, when defining a 2D slice. type Transform [ 3 ][ 3 ] float64 // A 3x3 array, really an array of arrays. type LinesOfText [][] byte // A slice of byte slices. The built-in data structure map associate values of one type (the key) with values of another type (the element or value) key can be of any type for which the equality operator is defined integers, floating point and complex numbers, strings, pointers, interfaces (as long as the dynamic type supports equality), structs and arrays maps hold references to an underlying data structure maps can be constructed using composite literal syntax with colon-separated key-value pairs attempt to fetch a map value with a key that is not present in the map will return the zero value for the type of the entries in the map use delete to unset a map entry, like this delete(timeZone, \"PDT\") var timeZone = map [ string ] int { \"UTC\" : 0 * 60 * 60 , \"EST\" : - 5 * 60 * 60 , \"CST\" : - 6 * 60 * 60 , \"MST\" : - 7 * 60 * 60 , \"PST\" : - 8 * 60 * 60 , } if offset , isKeyDefined := timeZone [ \"EST\" ]; isKeyDefined { // do something with offset } formatted print functions fmt.Fprint and its friends take as a first argument any object that implements the io.Writer interface // prints same results fmt . Printf ( \"Hello %d\\n\" , 23 ) fmt . Fprint ( os . Stdout , \"Hello \" , 23 , \"\\n\" ) fmt . Println ( \"Hello\" , 23 ) fmt . Println ( fmt . Sprint ( \"Hello \" , 23 )) Attach a method such as String to any user-defined type makes it possible for arbitrary values to format themselves automatically for printing","title":"Data"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#initialization","text":"Complex structures can be built during initialization and the ordering issues among initialized objects, even among different packages, are handled correctly Go's constants are created at compile time and can only be numbers, characters (runes), strings or booleans ; expressions that define them must be constant expressions Variables can be initialized just like constants but the initializer can be a general expression computed at run time Each source file can define its own niladic init function(s) to set up whatever state is required init is called after all the variable declarations in the package have evaluated their initializers evaluated only after all the imported packages have been initialized init can also be used to verify or repair correctness of the program state before real execution begins","title":"Initialization"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#methods","text":"Methods can be defined for any named type (except a pointer or an interface) and the receiver does not have to be a struct . The rule about pointers vs. values for receivers is that value methods can be invoked on pointers and values, but pointer methods can ONLY be invoked on pointers this rule arises because pointer methods can modify the receiver; invoking them on a value would cause the method to receive a copy of the value, so any modifications would be discarded when the value is addressable, the language takes care of the common case of invoking a pointer method on a value by inserting the address operator automatically The idea of using Write on a slice of bytes is central to the implementation of bytes.Buffer func ( p * ByteSlice ) Append ( data [] byte ) { slice := * p // Body same as above Append function, without the return. * p = slice } // implements the io.Write interface func ( p * ByteSlice ) Write ( data [] byte ) ( n int , err error ) { slice := * p // Body same as above Append method * p = slice return len ( data ), nil } var b ByteSlice fmt . Fprintf ( & b , \"This hour has %d days\\n\" , 7 ) // because we implemented the Write method","title":"Methods"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#interfaces-and-other-types","text":"Interfaces in Go provide a way to specify the behavior of an object: if something can do this, then it can be used here . a type can implement multiple interfaces a collection can be sorted by the routines in package sort if it implements sort.Interface , which contains Len(), Less(i, j int) bool, and Swap(i, j int) , and it could also have a custom formatter Interfaces with only one or two methods are common in Go code, and are usually given a name derived from the method type Sequence [] int // Methods required by sort.Interface. func ( s Sequence ) Len () int { return len ( s ) } func ( s Sequence ) Less ( i , j int ) bool { return s [ i ] < s [ j ] } func ( s Sequence ) Swap ( i , j int ) { s [ i ], s [ j ] = s [ j ], s [ i ] } // Copy returns a copy of the Sequence. func ( s Sequence ) Copy () Sequence { copy := make ( Sequence , 0 , len ( s )) return append ( copy , s ... ) } // Method for printing - sorts the elements before printing. func ( s Sequence ) String () string { s = s . Copy () // Make a copy; don't overwrite argument. sort . Sort ( s ) str := \"[\" for i , elem := range s { // Loop is O(N\u00b2); will fix that in next example. if i > 0 { str += \" \" } str += fmt . Sprint ( elem ) } return str + \"]\" } If we convert the Sequence back to []int , we can call Sprint directly on s . The conversion doesn't create a new value, it just temporarily acts as though the existing value has a new type. func ( s Sequence ) String () string { s = s . Copy () sort . IntSlice ( s ). Sort () return fmt . Sprint ([] int ( s )) } It's an idiom in Go programs to convert the type of an expression to access a different set of methods. Type switches are a form of conversion: they take an interface and, for each case in the switch, in a sense convert it to the type of that case. type Stringer interface { String () string } var value interface {} // Value provided by caller. switch str := value .( type ) { case string : return str case Stringer : return str . String () } A type assertion takes an interface value and extracts from it a value of the specified explicit type and the result is a new value with the static type <typeName> that type must either be the concrete type held by the interface, or a second interface type that the value can be converted to. i.e. str := value.(string) if the value does not contain a string, the program will crash with a run-time error. can guard against it with a \"comma, ok\" idiom to test if assertion fails, str will still be a string with its zero value str , ok := value .( string ) if ok { fmt . Printf ( \"string value is: %q\\n\" , str ) } else { fmt . Printf ( \"value is not a string\\n\" ) } If a type exists only to implement an interface and will never have exported methods beyond that interface, there is no need to export the type itself its constructor should return an interface value rather than the implementing type in this way, similar types that implements the same interfaces can be easily replaced for one another by changing the constructor call and the rest of the code is unaffected Almost anything can have methods attached, almost anything can satisfy an interface. A simple http handler: type Counter int func ( ctr * Counter ) ServeHTTP ( w http . ResponseWriter , req * http . Request ) { * ctr ++ fmt . Fprintf ( w , \"counter = %d\\n\" , * ctr ) } ... import \"net/http\" ... ctr := new ( Counter ) http . Handle ( \"/counter\" , ctr )","title":"Interfaces and other types"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#the-blank-identifier","text":"It's a bit like writing to the Unix /dev/null file: it represents a write-only value to be used as a place-holder where a variable is needed but the actual value is irrelevant. Some common use cases are multiple variable assignment as placeholder to take up a variable's value assignment i.e. _, err := os.Stat(path) to silence compiler complaints about unused imports or variable assignments that would eventually be used i.e. fd, err := os.Open(\"test.go\"); _ = fd to rename a package that is imported only for its side effects i.e. import _ \"net/http/pprof\" to check whether a type implements an interface i.e. if _, ok := val.(json.Marshaler); ok {}","title":"The blank identifier"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#embedding","text":"Go allows a type to \u201cborrow\u201d pieces of an implementation by embedding types within a struct or interface you can directly include other interfaces in a type to form a new type, without specifying all of their methods only interfaces can be embedded within interfaces // ReadWriter is the interface that combines the Reader and Writer interfaces. type ReadWriter interface { Reader Writer } // ReadWriter stores pointers to a Reader and a Writer. // It implements io.ReadWriter. type ReadWriter struct { reader * Reader // *bufio.Reader writer * Writer // *bufio.Writer } When we embed a type, the methods of that type become methods of the outer type, but when they are invoked the receiver of the method is the inner type, not the outer one If we need to refer to an embedded field directly, the type name of the field, ignoring the package qualifier, can be served as a field name If embedding types introduces name conflicts, there are two rules if an embedded type introduces methods or fields that are defined by the same name at this struct, then the definition at this struct will dominate it (and suppress the conflicting methods/fields introduced by the embedded type) if embedded a type and also defined another type of method with the same name as the embedded type, it is an error unless neither one of them was used","title":"Embedding"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#concurrency","text":"Do not communicate by sharing memory; instead, share memory by communicating. The goal of concurrency, structuring a program as independently executing components and executing calculations in parallel for efficiency on multiple CPUs In Go, shared values are passed around on channels and never actively shared by separate threads of execution only one goroutine has access to the value at any given time data races cannot occur, by design A goroutine is a function executing concurrently with other goroutines in the same address space and when the call completes, the goroutine exits silently it is lightweight, costing little more than the allocation of stack space the stacks start small, cheap, and grow by allocating (and freeing) heap storage as required goroutines are multiplexed onto multiple OS threads so some can block and others can execute Unbuffered channels combine communication(the exchange of a value) with synchronization(guaranteeing that the two ends are in a known state) A buffered channel can be used like a semaphore , for instance to limit throughput. A common use of channels is to implement safe, parallel demultiplexing by creating \"channels of channels\". can be done by creating structs that contains channels and pass struct objects to a channel","title":"Concurrency"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#errors","text":"Library routines must often return some sort of error indication to the caller errors have type error, a simple built-in interface. when successful the error will be nil; when there is a problem, it should hold an error when feasible, error strings should identify their origin, such as by having a prefix naming the operation or package that generated the error type error interface { Error () string } Caller can use a type switch or a type assertion to look for specific errors and extract details for try := 0 ; try < 2 ; try ++ { file , err = os . Create ( filename ) if err == nil { return } if e , ok := err .( * os . PathError ); ok && e . Err == syscall . ENOSPC { // if the err was of type *os.PathError, and then so is e deleteTempFiles () // Recover some space. continue } return } If an error is unrecoverable, it will be better to use panic to create a run-time error that will stop the program panic takes a single argument of arbitrary type (often a string) to be printed as the program dies when panic is called, it immediately stops execution of the current function and begins unwinding the stack of the goroutine, running any deferred functions along the way. if that unwinding reaches the top of the goroutine's stack, the program dies it is possible to use the built-in function recover to regain control of the goroutine and resume normal execution it's also a way to indicate that something impossible has happened real library functions should avoid using panic it's always better to let things continue to run rather than taking down the whole program one exception is during initialization, if the library truly cannot set itself up, it might be reasonable to panic without further damage A call to recover stops the unwinding and returns the argument passed to panic recover is only useful inside deferred functions because the only code that runs while undergoing unwinding is inside deferred functions one application of recover is to shut down a failing goroutine inside a server without killing the other executing goroutines deferred code can call library routines that themselves use panic and recover without failing deferred functions can modify named return values func server ( workChan <- chan * Work ) { for work := range workChan { go safelyDo ( work ) } } func safelyDo ( work * Work ) { defer func () { if err := recover (); err != nil { log . Println ( \"work failed:\" , err ) } }() do ( work ) } A more complete example, with deferred function modifying the named return values // Error is the type of a parse error; it satisfies the error interface. type Error string func ( e Error ) Error () string { return string ( e ) } // error is a method of *Regexp that reports parsing errors by // panicking with an Error. func ( regexp * Regexp ) error ( err string ) { panic ( Error ( err )) } // Compile returns a parsed representation of the regular expression. func Compile ( str string ) ( regexp * Regexp , err error ) { regexp = new ( Regexp ) // doParse will panic if there is a parse error. defer func () { if e := recover (); e != nil { regexp = nil // Clear return value. err = e .( Error ) // Will re-panic if not a parse error. } }() return regexp . doParse ( str ), nil }","title":"Errors"},{"location":"Programming-Lang-Reference/Golang/Best-Practices/#web-server","text":"An simple example package main import ( \"flag\" \"html/template\" \"log\" \"net/http\" ) var addr = flag . String ( \"addr\" , \":1718\" , \"http service address\" ) // Q=17, R=18 var templ = template . Must ( template . New ( \"qr\" ). Parse ( templateStr )) func main () { flag . Parse () http . Handle ( \"/\" , http . HandlerFunc ( QR )) err := http . ListenAndServe ( * addr , nil ) if err != nil { log . Fatal ( \"ListenAndServe:\" , err ) } } func QR ( w http . ResponseWriter , req * http . Request ) { templ . Execute ( w , req . FormValue ( \"s\" )) } const templateStr = ` <html> <head> <title>QR Link Generator</title> </head> <body> {{if .}} <img src=\"http://chart.apis.google.com/chart?chs=300x300&cht=qr&choe=UTF-8&chl={{.}}\" /> <br> {{.}} <br> <br> {{end}} <form action=\"/\" name=f method=\"GET\"> <input maxLength=1024 size=70 name=s value=\"\" title=\"Text to QR Encode\"> <input type=submit value=\"Show QR\" name=qr> </form> </body> </html> ` The html/template package is very powerful. Documentation to it is here","title":"Web Server"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/","text":"This is set of notes is taking from https://golang.org/ref/spec and https://gobyexample.com/ This set of notes will show some use cases simple implementations using go's library modules. All golang packages: https://golang.org/pkg/ Good source of golang updates: https://blog.golang.org/ time package \u00b6 When requesting external resources or doing work that need to bound to an execution time limit , we can achieve it using the <-time.After function from the \"time\" package. This function awaits a value to be sent after the specified timeout. c1 := make ( chan string , 1 ) go func () { time . Sleep ( 2 * time . Second ) c1 <- \"result 1\" }() select { case res := <- c1 : fmt . Println ( res ) case <- time . After ( 1 * time . Second ): fmt . Println ( \"timeout 1\" ) } When trying to schedule a goroutine in a future time, or to schedule a function at some interval, use the built-in timer and ticker from the \"time\" package Timers represent a single event in the future. You tell the timer how long you want to wait, and it provides a channel that will be notified at that time. timer1 := time . NewTimer ( 2 * time . Second ) <- timer1 . C // blocks until the timer event is received fmt . Println ( \"Timer 1 fired\" ) // will print after 2 seconds time . Sleep ( 2 * time . Second ) // achieves the same thing above timer does Tickers can schedule a function repeatedly at regular intervals package main import ( \"fmt\" \"time\" ) func main () { ticker := time . NewTicker ( 500 * time . Millisecond ) done := make ( chan bool ) go func () { for { select { case <- done : return case t := <- ticker . C : fmt . Println ( \"Tick at\" , t ) } } }() time . Sleep ( 1600 * time . Millisecond ) ticker . Stop () done <- true fmt . Println ( \"Ticker stopped\" ) } Rate limit is a common requirement to consider when designing a service. In go this can be achieved using the tickers as a source of creating \"gaps\" between processing requests. package main import ( \"fmt\" \"time\" ) func main () { requests := make ( chan int , 5 ) for i := 1 ; i <= 5 ; i ++ { requests <- i } close ( requests ) limiter := time . Tick ( 200 * time . Millisecond ) for req := range requests { <- limiter fmt . Println ( \"request\" , req , time . Now ()) } burstyLimiter := make ( chan time . Time , 3 ) for i := 0 ; i < 3 ; i ++ { burstyLimiter <- time . Now () } go func () { for t := range time . Tick ( 200 * time . Millisecond ) { burstyLimiter <- t } }() burstyRequests := make ( chan int , 5 ) for i := 1 ; i <= 5 ; i ++ { burstyRequests <- i } close ( burstyRequests ) for req := range burstyRequests { <- burstyLimiter fmt . Println ( \"request\" , req , time . Now ()) } } Channel and goroutine with sync package \u00b6 A simple worker pool implementation using channels package main import ( \"fmt\" \"time\" ) func worker ( id int , jobs <- chan int , results chan <- int ) { for j := range jobs { fmt . Println ( \"worker\" , id , \"started job\" , j ) time . Sleep ( time . Second ) fmt . Println ( \"worker\" , id , \"finished job\" , j ) results <- j * 2 } } func main () { const numJobs = 5 jobs := make ( chan int , numJobs ) results := make ( chan int , numJobs ) for w := 1 ; w <= 3 ; w ++ { go worker ( w , jobs , results ) } for j := 1 ; j <= numJobs ; j ++ { jobs <- j } close ( jobs ) for a := 1 ; a <= numJobs ; a ++ { <- results } } To wait for multiple goroutines to finish, can use wait group from \"sync\" package package main import ( \"fmt\" \"sync\" \"time\" ) func worker ( id int , wg * sync . WaitGroup ) { defer wg . Done () fmt . Printf ( \"Worker %d starting\\n\" , id ) time . Sleep ( time . Second ) fmt . Printf ( \"Worker %d done\\n\" , id ) } func main () { var wg sync . WaitGroup for i := 1 ; i <= 5 ; i ++ { wg . Add ( 1 ) // increment wg's counter go worker ( i , & wg ) } wg . Wait () // wait until all works returned } \"sync/atomic\" package provides atomic counters that is thread-safe and can be accessed by multiple concurrent goroutines. package main import ( \"fmt\" \"sync\" \"sync/atomic\" ) func main () { var ops uint64 var wg sync . WaitGroup for i := 0 ; i < 50 ; i ++ { wg . Add ( 1 ) go func () { for c := 0 ; c < 1000 ; c ++ { atomic . AddUint64 ( & ops , 1 ) } wg . Done () }() } wg . Wait () fmt . Println ( \"ops:\" , ops ) // yields 50000 } mutex from the package \"sync\" provides a safe way for multiple goroutines to access the same data package main import ( \"fmt\" \"math/rand\" \"sync\" \"sync/atomic\" \"time\" ) func main () { var state = make ( map [ int ] int ) var mutex = & sync . Mutex {} var readOps uint64 var writeOps uint64 for r := 0 ; r < 100 ; r ++ { go func () { total := 0 for { key := rand . Intn ( 5 ) mutex . Lock () total += state [ key ] mutex . Unlock () atomic . AddUint64 ( & readOps , 1 ) time . Sleep ( time . Millisecond ) } }() } for w := 0 ; w < 10 ; w ++ { go func () { for { key := rand . Intn ( 5 ) val := rand . Intn ( 100 ) mutex . Lock () state [ key ] = val mutex . Unlock () atomic . AddUint64 ( & writeOps , 1 ) time . Sleep ( time . Millisecond ) } }() } time . Sleep ( time . Second ) readOpsFinal := atomic . LoadUint64 ( & readOps ) fmt . Println ( \"readOps:\" , readOpsFinal ) writeOpsFinal := atomic . LoadUint64 ( & writeOps ) fmt . Println ( \"writeOps:\" , writeOpsFinal ) } Stateful goroutines \u00b6 To have shared states owned by a single goroutine and use channel communications to coordinate read/write to the states. This will guarantee that the data is never corrupted with concurrent access. package main import ( \"fmt\" \"math/rand\" \"sync/atomic\" \"time\" ) type readOp struct { key int resp chan int } type writeOp struct { key int val int resp chan bool } func main () { var readOps uint64 var writeOps uint64 reads := make ( chan readOp ) writes := make ( chan writeOp ) go func () { var state = make ( map [ int ] int ) // shared state owned by this goroutine for { select { // act like a worker to process access requests case read := <- reads : read . resp <- state [ read . key ] case write := <- writes : state [ write . key ] = write . val write . resp <- true } } }() for r := 0 ; r < 100 ; r ++ { go func () { for { read := readOp { key : rand . Intn ( 5 ), resp : make ( chan int )} reads <- read <- read . resp atomic . AddUint64 ( & readOps , 1 ) time . Sleep ( time . Millisecond ) } }() } for w := 0 ; w < 10 ; w ++ { go func () { for { write := writeOp { key : rand . Intn ( 5 ), val : rand . Intn ( 100 ), resp : make ( chan bool )} writes <- write <- write . resp atomic . AddUint64 ( & writeOps , 1 ) time . Sleep ( time . Millisecond ) } }() } time . Sleep ( time . Second ) readOpsFinal := atomic . LoadUint64 ( & readOps ) fmt . Println ( \"readOps:\" , readOpsFinal ) writeOpsFinal := atomic . LoadUint64 ( & writeOps ) fmt . Println ( \"writeOps:\" , writeOpsFinal ) } Sorting \u00b6 Go\u2019s \"sort\" package implements sorting for built-ins and user-defined types. Note that sorting is in-place , so it changes the given slice and doesn't return a new one. We can also use sort to check if a slice is already in sorted order. strs := [] string { \"c\" , \"a\" , \"b\" } sort . Strings ( strs ) ints := [] int { 7 , 2 , 4 } sort . Ints ( ints ) Sorting by Functions allows sort something other than its natural order, by implementing the sort.Interface and provide methods for Len() , Less(i, j) and Swap(i, j) so that sort.Sort can be used on package main import ( \"fmt\" \"sort\" ) type byLength [] string // an alias for []string; alias creates a type that can implement an interface func ( s byLength ) Len () int { return len ( s ) } func ( s byLength ) Swap ( i , j int ) { s [ i ], s [ j ] = s [ j ], s [ i ] } func ( s byLength ) Less ( i , j int ) bool { return len ( s [ i ]) < len ( s [ j ]) } func main () { fruits := [] string { \"peach\" , \"banana\" , \"kiwi\" } sort . Sort ( byLength ( fruits )) // converting the slice into \"byLength\" type fmt . Println ( fruits ) } Work with Collection Functions \u00b6 Go does not support generics like other languages does; in Go it\u2019s common to provide collection functions when they are specifically needed for your program and data types. Note that in some cases it may be clearest to just inline the functions directly instead of creating and calling a helper function. func Index ( vs [] string , t string ) int { for i , v := range vs { if v == t { return i } } return - 1 } func Include ( vs [] string , t string ) bool { return Index ( vs , t ) >= 0 } func Any ( vs [] string , f func ( string ) bool ) bool { for _ , v := range vs { if f ( v ) { return true } } return false } func All ( vs [] string , f func ( string ) bool ) bool { for _ , v := range vs { if ! f ( v ) { return false } } return true } func Filter ( vs [] string , f func ( string ) bool ) [] string { vsf := make ([] string , 0 ) for _ , v := range vs { if f ( v ) { vsf = append ( vsf , v ) } } return vsf } func Map ( vs [] string , f func ( string ) string ) [] string { vsm := make ([] string , len ( vs )) for i , v := range vs { vsm [ i ] = f ( v ) } return vsm } Regex \u00b6 Go offers built-in support for regular expressions from the \"regexp\" package package main import ( \"bytes\" \"fmt\" \"regexp\" ) func main () { match , _ := regexp . MatchString ( \"p([a-z]+)ch\" , \"peach\" ) fmt . Println ( match ) r , _ := regexp . Compile ( \"p([a-z]+)ch\" ) // cache the pattern struct fmt . Println ( r . MatchString ( \"peach\" )) fmt . Println ( r . FindString ( \"peach punch\" )) fmt . Println ( r . FindStringIndex ( \"peach punch\" )) fmt . Println ( r . FindStringSubmatch ( \"peach punch\" )) fmt . Println ( r . FindStringSubmatchIndex ( \"peach punch\" )) fmt . Println ( r . FindAllString ( \"peach punch pinch\" , - 1 )) fmt . Println ( r . FindAllStringSubmatchIndex ( \"peach punch pinch\" , - 1 )) fmt . Println ( r . Match ([] byte ( \"peach\" ))) fmt . Println ( r . ReplaceAllString ( \"a peach\" , \"<fruit>\" )) in := [] byte ( \"a peach\" ) fmt . Println ( r . ReplaceAllFunc ( in , bytes . ToUpper )) } JSON \u00b6 Go offers built-in support for JSON encoding and decoding through the package \"encoding/json\" , including conversion to and from built-in and custom data types. package main import ( \"encoding/json\" \"fmt\" \"os\" ) type response1 struct { Page int Fruits [] string } // field tags contain directives for the encoder and decoder type response2 struct { Page int `json:\"page\"` Fruits [] string `json:\"fruits\"` } func main () { slcD := [] string { \"apple\" , \"peach\" , \"pear\" } slcB , _ := json . Marshal ( slcD ) // produces a json list string mapD := map [ string ] int { \"apple\" : 5 , \"lettuce\" : 7 } mapB , _ := json . Marshal ( mapD ) // produces a json object string res1D := & response1 { Page : 1 , Fruits : [] string { \"apple\" , \"peach\" , \"pear\" }} res1B , _ := json . Marshal ( res1D ) // produces a json object string with keys as-is res2D := & response2 { Page : 1 , Fruits : [] string { \"apple\" , \"peach\" , \"pear\" }} res2B , _ := json . Marshal ( res2D ) // produces a json object string with keys specified in the struct byt := [] byte ( `{\"num\":6.13,\"strs\":[\"a\",\"b\"]}` ) var dat map [ string ] interface {} // will hold a map of strings to arbitrary data types. if err := json . Unmarshal ( byt , & dat ); err != nil { panic ( err ) } num := dat [ \"num\" ].( float64 ) // cast values to the appropriate type strs := dat [ \"strs\" ].([] interface {}) str1 := strs [ 0 ].( string ) str := `{\"page\": 1, \"fruits\": [\"apple\", \"peach\"]}` res := response2 {} // creates a shell object json . Unmarshal ([] byte ( str ), & res ) // decode JSON string and save data into object enc := json . NewEncoder ( os . Stdout ) // stream JSON encoding to other things directly d := map [ string ] int { \"apple\" : 5 , \"lettuce\" : 7 } enc . Encode ( d ) } https://golang.org/pkg/encoding/json/ XML \u00b6 \"encoding.xml\" package offers built-in support for XML package main import ( \"encoding/xml\" \"fmt\" ) // field tags contain directives for the encoder and decoder type Plant struct { XMLName xml . Name `xml:\"plant\"` // the root element Id int `xml:\"id,attr\"` // an attribute of this element Name string `xml:\"name\"` // an element Origin [] string `xml:\"origin\"` // an element } func ( p Plant ) String () string { return fmt . Sprintf ( \"Plant id=%v, name=%v, origin=%v\" , p . Id , p . Name , p . Origin ) } func main () { coffee := & Plant { Id : 27 , Name : \"Coffee\" } coffee . Origin = [] string { \"Ethiopia\" , \"Brazil\" } out , _ := xml . MarshalIndent ( coffee , \" \" , \" \" ) // encode into a readable XML string fmt . Println ( string ( out )) fmt . Println ( xml . Header + string ( out )) var p Plant if err := xml . Unmarshal ( out , & p ); err != nil { // decode into a object panic ( err ) } fmt . Println ( p ) tomato := & Plant { Id : 81 , Name : \"Tomato\" } tomato . Origin = [] string { \"Mexico\" , \"California\" } type Nesting struct { XMLName xml . Name `xml:\"nesting\"` Plants [] * Plant `xml:\"parent>plant\"` // to nest all <plant>s within <parent> } nesting := & Nesting {} nesting . Plants = [] * Plant { coffee , tomato } out , _ = xml . MarshalIndent ( nesting , \" \" , \" \" ) fmt . Println ( string ( out )) } The time-related packages \u00b6 Go's \"time\" package offers handy functions for basic time operations. package main import ( \"fmt\" \"time\" ) func main () { p := fmt . Println // alias a function now := time . Now () secs := now . Unix () nanos := now . UnixNano () p ( now ) p ( time . Unix ( secs , 0 )) // converts seconds to time string p ( time . Unix ( 0 , nanos )) // converts nanoseconds to time string then := time . Date ( 2009 , 11 , 17 , 20 , 34 , 58 , 651387237 , time . UTC ) p ( then ) p ( then . Year ()) p ( then . Month ()) p ( then . Day ()) p ( then . Hour ()) p ( then . Minute ()) p ( then . Second ()) p ( then . Nanosecond ()) p ( then . Location ()) p ( then . Weekday ()) p ( then . Before ( now )) p ( then . After ( now )) p ( then . Equal ( now )) diff := now . Sub ( then ) p ( diff ) p ( then . Add ( diff )) p ( then . Add ( - diff )) // formatting time t := time . Now () p ( t . Format ( time . RFC3339 )) // 2014-04-15T18:00:15-07:00 t1 , e := time . Parse ( // parse to get a time object time . RFC3339 , // from this format \"2012-11-01T22:08:41+00:00\" ) // actual time string p ( t1 ) // given an example of the desired format, then time will do the formatting p ( t . Format ( \"3:04PM\" )) p ( t . Format ( \"Mon Jan _2 15:04:05 2006\" )) p ( t . Format ( \"2006-01-02T15:04:05.999999-07:00\" )) form := \"3 04 PM\" t2 , e := time . Parse ( form , \"8 41 PM\" ) p ( t2 ) } Random Numbers \u00b6 Go\u2019s \"math/rand\" package provides pseudorandom number generation. Use \"crypto/rand\" for random numbers for security related tasks. package main import ( \"fmt\" \"math/rand\" \"time\" ) func main () { fmt . Println ( rand . Intn ( 100 )) // generates an integer from [0,100) fmt . Println ( rand . Float64 ()) // generates a 64-bit floating-point from [0.0, 1.0) s1 := rand . NewSource ( time . Now (). UnixNano ()) // get a new seed object r1 := rand . New ( s1 ) // get a new generator from the seed fmt . Println ( r1 . Intn ( 100 )) } http://golang.org/pkg/math/rand/ String functions \u00b6 package main import ( \"fmt\" s \"strings\" ) var p = fmt . Println func main () { p ( \"Contains: \" , s . Contains ( \"test\" , \"es\" )) p ( \"Count: \" , s . Count ( \"test\" , \"t\" )) p ( \"HasPrefix: \" , s . HasPrefix ( \"test\" , \"te\" )) p ( \"HasSuffix: \" , s . HasSuffix ( \"test\" , \"st\" )) p ( \"Index: \" , s . Index ( \"test\" , \"e\" )) p ( \"Join: \" , s . Join ([] string { \"a\" , \"b\" }, \"-\" )) p ( \"Repeat: \" , s . Repeat ( \"a\" , 5 )) p ( \"Replace: \" , s . Replace ( \"foo\" , \"o\" , \"0\" , - 1 )) // replace all indexes p ( \"Replace: \" , s . Replace ( \"foo\" , \"o\" , \"0\" , 1 )) p ( \"Split: \" , s . Split ( \"a-b-c-d-e\" , \"-\" )) p ( \"ToLower: \" , s . ToLower ( \"TEST\" )) p ( \"ToUpper: \" , s . ToUpper ( \"test\" )) p ( \"Len: \" , len ( \"hello\" )) p ( \"Char:\" , \"hello\" [ 1 ]) // numerical char value is evaluated } http://golang.org/pkg/strings/ https://blog.golang.org/strings more on wide multi-byte characters \"fmt\" offers Printf that can help print out formatted results package main import ( \"fmt\" \"os\" ) type point struct { x , y int } func main () { p := point { 1 , 2 } fmt . Printf ( \"%v\\n\" , p ) // p's object values fmt . Printf ( \"%+v\\n\" , p ) // p's object values and field names if p is a struct fmt . Printf ( \"%#v\\n\" , p ) // the source code snippet that would produce p fmt . Printf ( \"%T\\n\" , p ) // the type of p fmt . Printf ( \"%t\\n\" , true ) // for boolean fmt . Printf ( \"%d\\n\" , 123 ) // for base-10 integer fmt . Printf ( \"%b\\n\" , 14 ) // for binary representation fmt . Printf ( \"%c\\n\" , 33 ) // for char value of the integer fmt . Printf ( \"%x\\n\" , 456 ) // for hex representation fmt . Printf ( \"%f\\n\" , 78.9 ) // for floating-point number fmt . Printf ( \"%e\\n\" , 123400000.0 ) // for scientific notation fmt . Printf ( \"%E\\n\" , 123400000.0 ) // for scientific notation fmt . Printf ( \"%s\\n\" , \"\\\"string\\\"\" ) // for string fmt . Printf ( \"%q\\n\" , \"string\" ) // add quotes around the string fmt . Printf ( \"%x\\n\" , \"hex this\" ) // renders string in base-16 fmt . Printf ( \"%p\\n\" , & p ) // for pointer representation fmt . Printf ( \"|%6d|%6d|\\n\" , 12 , 345 ) // control the width of the print fmt . Printf ( \"|%6.2f|%6.2f|\\n\" , 1.2 , 3.45 ) // control the precision after '.' fmt . Printf ( \"|%-6s|%-6s|\\n\" , \"foo\" , \"b\" ) // left-justify the print s := fmt . Sprintf ( \"a %s\" , \"string\" ) // saves formatted string as output fmt . Fprintf ( os . Stderr , \"an %s\\n\" , \"error\" ) // prints to other io.Writers } The package \"strconv\" offers functions to parse numbers from strings package main import ( \"fmt\" \"strconv\" ) func main () { f , _ := strconv . ParseFloat ( \"1.234\" , 64 ) // parse a 64-bit precision floating point i , _ := strconv . ParseInt ( \"123\" , 0 , 64 ) // 0 means auto-base, 64-bit integer k , _ := strconv . Atoi ( \"135\" ) // parse a base-10 integer _ , e := strconv . Atoi ( \"wat\" ) // error on bad input } URL Parsing \u00b6 The packages \"net\" and \"net/url\" provides functions to parse URLs package main import ( \"fmt\" \"net\" \"net/url\" ) func main () { s := \"postgres://user:pass@host.com:5432/path?k=v#f\" u , err := url . Parse ( s ) if err != nil { panic ( err ) } fmt . Println ( u . Scheme ) // postgres fmt . Println ( u . User ) // user:pass fmt . Println ( u . User . Username ()) // user p , _ := u . User . Password () // pass fmt . Println ( u . Host ) // host.com:5432 host , port , _ := net . SplitHostPort ( u . Host ) fmt . Println ( u . Path ) // /path fmt . Println ( u . Fragment ) // f fmt . Println ( u . RawQuery ) // k=v everything between '?' and '#' m , _ := url . ParseQuery ( u . RawQuery ) fmt . Println ( m ) // map[k:[v]] fmt . Println ( m [ \"k\" ][ 0 ]) // v } Encryption and Hashing Functions \u00b6 Go implements several hash functions in various \"crypto/*\" packages. Go provides built-in support for base64 encoding/decoding from the \"encoding/base64\" package, which can do URL encoding. package main import ( \"crypto/sha1\" // others like \"crypto/md5\" \"crypto/sha256\" has similar usage b64 \"encoding/base64\" \"fmt\" ) func main () { s := \"sha1 this string\" h := sha1 . New () h . Write ([] byte ( s )) // create a byte array from the string and pass in bs := h . Sum ( nil ) // creates the final hash; the argument will be appended another byte slice to the end of the existing byte slice (such as a salt) fmt . Println ( s ) fmt . Printf ( \"%x\\n\" , bs ) // sha values usually printed in hex data := \"abc123!?$*&()'-=@~\" stdEncode := b64 . StdEncoding . EncodeToString ([] byte ( data )) stdDecode , _ := b64 . StdEncoding . DecodeString ( sEnc ) urlEnc := b64 . URLEncoding . EncodeToString ([] byte ( data )) urlDec , _ := b64 . URLEncoding . DecodeString ( uEnc ) } File I/O \u00b6 Reading and writing files are basic tasks needed for many Go programs. Some related packages: \"bufio\" \"fmt\" \"io\" \"io/ioutil\" \"os\" package main import ( \"bufio\" \"fmt\" \"io\" \"io/ioutil\" \"os\" ) func check ( e error ) { // always check error for file operations if e != nil { panic ( e ) } } func main () { dat , err := ioutil . ReadFile ( \"/tmp/dat\" ) // reads entire file into memory check ( err ) f , err := os . Open ( \"/tmp/dat\" ) // open a file to gain more control over various file operations check ( err ) b1 := make ([] byte , 5 ) // make a byte array with a fixed size n1 , err := f . Read ( b1 ) // read bytes in, up to the size of the array; n1 is number of bytes actually read check ( err ) fmt . Printf ( \"%d bytes: %s\\n\" , n1 , string ( b1 [: n1 ])) o3 , err := f . Seek ( 6 , 0 ) // start next read from a byte position in the file check(err) b3 := make ([] byte , 2 ) n3 , err := io . ReadAtLeast ( f , b3 , 2 ) _ , err = f . Seek ( 0 , 0 ) // rewind to the beginning of this file r4 := bufio . NewReader ( f ) b4 , err := r4 . Peek ( 5 ) // directly returns the byte array for you check ( err ) fmt . Printf ( \"5 bytes: %s\\n\" , string ( b4 )) f . Close () d1 := [] byte ( \"hello\\ngo\\n\" ) err := ioutil . WriteFile ( \"/tmp/dat1\" , d1 , 0644 ) // write a byte array to a file directly check ( err ) f , err := os . Create ( \"/tmp/dat2\" ) // create an empty file check ( err ) defer f . Close () // will close the file when out of scope d2 := [] byte { 115 , 111 , 109 , 101 , 10 } n2 , err := f . Write ( d2 ) // write some bytes into the file check ( err ) n3 , err := f . WriteString ( \"writes\\n\" ) // write a string into the file check ( err ) f . Sync () // flush writes to the disk w := bufio . NewWriter ( f ) // buffered writer n4 , err := w . WriteString ( \"buffered\\n\" ) // write a string to the file check ( err ) w . Flush () // flush writes to the disk } A line filter is a common type of program that reads input on STDIN, processes it, and then prints some derived result to STDOUT. grep and sed are common line filters. This is an example line filter written with go. package main import ( \"bufio\" \"fmt\" \"os\" \"strings\" ) func main () { scanner := bufio . NewScanner ( os . Stdin ) for scanner . Scan () { ucl := strings . ToUpper ( scanner . Text ()) fmt . Println ( ucl ) } if err := scanner . Err (); err != nil { fmt . Fprintln ( os . Stderr , \"error:\" , err ) os . Exit ( 1 ) } } Files, Directories \u00b6 The \"path/filepath\" package provides lots of convenient functions to parse and construct file paths in a way that is portable between operating systems The \"os\" package offers functions for interacting with files and directories package main import ( \"fmt\" \"io/ioutil\" \"os\" \"path/filepath\" \"strings\" ) func main () { p := filepath . Join ( \"dir1\" , \"dir2\" , \"filename\" ) // prepare a path by concatenate tokens fmt . Println ( filepath . Join ( \"dir1//\" , \"filename\" )) // implicitly take care of extra chars fmt . Println ( filepath . Join ( \"dir1/../dir1\" , \"filename\" )) // implicitly evaluate '..' fmt . Println ( \"Dir(p):\" , filepath . Dir ( p )) // the path to the parent dir of the file fmt . Println ( \"Base(p):\" , filepath . Base ( p )) // the base of the path, could be a dir or file fmt . Println ( filepath . IsAbs ( \"dir/file\" )) // judge if it is an absolute path or relative path fmt . Println ( filepath . IsAbs ( \"/dir/file\" )) filename := \"config.json\" ext := filepath . Ext ( filename ) // gets the extension of the file fmt . Println ( strings . TrimSuffix ( filename , ext )) // rid of extension rel , err := filepath . Rel ( \"a/b\" , \"a/b/t/file\" ) // finds relative path between the two dirs (they must share a common root) if err != nil { panic ( err ) } err := os . Mkdir ( \"subdir\" , 0755 ) // creates a dir in current working directory check ( err ) defer os . RemoveAll ( \"subdir\" ) // remove a dir tree; it is good practice to defer removing temp dirs createEmptyFile := func ( name string ) { d := [] byte ( \"\" ) check ( ioutil . WriteFile ( name , d , 0644 )) // creates an empty file } createEmptyFile ( \"subdir/file1\" ) err = os . MkdirAll ( \"subdir/parent/child\" , 0755 ) // create a hierarchy of dirs check ( err ) c , err := ioutil . ReadDir ( \"subdir/parent\" ) // lists dir contents, returns a slice of `os.FileInfo` objects check ( err ) for _ , entry := range c { fmt . Println ( \" \" , entry . Name (), entry . IsDir ()) // some functions on the os.FileInfo } err = os . Chdir ( \"subdir/parent/child\" ) // change the current working directory check ( err ) c , err = ioutil . ReadDir ( \".\" ) // list current dir contents check ( err ) fmt . Println ( \"Visiting subdir\" ) err = filepath . Walk ( \"subdir\" , visit ) // accepts a callback function to handle every file or directory visited f , err := ioutil . TempFile ( \"\" , \"sample\" ) // creates and open a temp file for read/write; \"\" as first arg tells to create the file in OS default temp location check ( err ) defer os . Remove ( f . Name ()) dname , err := ioutil . TempDir ( \"\" , \"sampledir\" ) // similarly create a temp dir check ( err ) defer os . RemoveAll ( dname ) fname := filepath . Join ( dname , \"file1\" ) err = ioutil . WriteFile ( fname , [] byte { 1 , 2 }, 0666 ) check ( err ) } func visit ( p string , info os . FileInfo , err error ) error { if err != nil { return err } fmt . Println ( \" \" , p , info . IsDir ()) return nil } Testing \u00b6 The \"testing\" package provides the tools we need to write unit tests and the go test command runs tests. package main import ( \"fmt\" \"testing\" ) func IntMin ( a , b int ) int { if a < b { return a } return b } func TestIntMinTableDriven ( t * testing . T ) { var tests = [] struct { a , b int want int }{ // shortcut to initialize anonymous struct { 0 , 1 , 0 }, { 1 , 0 , 0 }, { 2 , - 2 , - 2 }, { 0 , - 1 , - 1 }, { - 1 , 0 , - 1 }, } for _ , tt := range tests { testname := fmt . Sprintf ( \"%d,%d\" , tt . a , tt . b ) t . Run ( testname , func ( t * testing . T ) { ans := IntMin ( tt . a , tt . b ) if ans != tt . want { t . Errorf ( \"got %d, want %d\" , ans , tt . want ) // report test failure and continue t . Fail ( \"fail this test\" ) // report test failure and stop this test immediately } }) } } Command, Environment Variables \u00b6 Use \"os\" package to access command-line args passed in when running the program. Need to build a binary using go build command first. It also provides functions to get/set environment variables. Go provides a \"flag\" package supporting basic command-line flag parsing. Using this package, it is easy to define subcommands that expect their own flags. package main import ( \"flag\" \"fmt\" \"os\" ) func main () { argsWithProg := os . Args // a slice of the arguments argsWithoutProg := os . Args [ 1 :] // first value is the path to the program os . Setenv ( \"FOO\" , \"1\" ) fmt . Println ( \"FOO:\" , os . Getenv ( \"FOO\" )) for _ , e := range os . Environ () { // os.Environ returns a list of env vars and values pair := strings . SplitN ( e , \"=\" , 2 ) } wordPtr := flag . String ( \"word\" , \"foo\" , \"a string\" ) // declare a flag for string with a default value; returns a pointer reference numbPtr := flag . Int ( \"numb\" , 42 , \"an int\" ) boolPtr := flag . Bool ( \"fork\" , false , \"a bool\" ) var svar string flag . StringVar ( & svar , \"svar\" , \"bar\" , \"a string var\" ) // declare an option that uses an existing var; pass-by-reference flag . Parse () // execute command-line parsing operation fmt . Println ( \"word:\" , * wordPtr ) fmt . Println ( \"numb:\" , * numbPtr ) fmt . Println ( \"fork:\" , * boolPtr ) fmt . Println ( \"svar:\" , svar ) fmt . Println ( \"tail:\" , flag . Args ()) // get all trailing tokens (flags should be passed before these tokens otherwise won't be picked up) // pass in -h/--help for a list of help text for each option supported // undefined flag option will also result in help text being printed fooCmd := flag . NewFlagSet ( \"foo\" , flag . ExitOnError ) // declare a subcommand fooEnable := fooCmd . Bool ( \"enable\" , false , \"enable\" ) // define option for this subcommand fooName := fooCmd . String ( \"name\" , \"\" , \"name\" ) barCmd := flag . NewFlagSet ( \"bar\" , flag . ExitOnError ) barLevel := barCmd . Int ( \"level\" , 0 , \"level\" ) switch os . Args [ 1 ] { case \"foo\" : fooCmd . Parse ( os . Args [ 2 :]) case \"bar\" : barCmd . Parse ( os . Args [ 2 :]) default : fmt . Println ( \"expected 'foo' or 'bar' subcommands\" ) os . Exit ( 1 ) } } Processes and Signals \u00b6 Sometimes it is useful to spawn other processes to handle work outside of this program using other programs. Then the package \"io/ioutil\" and \"os/exec\" will be helpful. While \"syscall\" package performs deep level process manipulation and allow us to replace current process with a new process. package main import ( \"fmt\" \"io/ioutil\" \"os/exec\" \"syscall\" ) func main () { dateCmd := exec . Command ( \"date\" ) // creates an object to represent an external process running this command dateOut , err := dateCmd . Output () // run the command string and collect its output if err != nil { panic ( err ) } grepCmd := exec . Command ( \"grep\" , \"hello\" ) // build command with arguments grepIn , _ := grepCmd . StdinPipe () // the channel for piping in stdin for the command grepOut , _ := grepCmd . StdoutPipe () // the channel for piping out stdout from the command grepCmd . Start () // run the command in a process grepIn . Write ([] byte ( \"hello grep\\ngoodbye grep\" )) grepIn . Close () grepBytes , _ := ioutil . ReadAll ( grepOut ) // read from the stdout grepCmd . Wait () // blocks until command finish execution fmt . Println ( \"> grep hello\" ) fmt . Println ( string ( grepBytes )) lsCmd := exec . Command ( \"bash\" , \"-c\" , \"ls -a -l -h\" ) // allows pass in a full command string lsOut , err := lsCmd . Output () if err != nil { panic ( err ) } binary , lookErr := exec . LookPath ( \"ls\" ) // get the absolute path to the binary if lookErr != nil { panic ( lookErr ) } args := [] string { \"ls\" , \"-a\" , \"-l\" , \"-h\" } // has to be in slice form env := os . Environ () execErr := syscall . Exec ( binary , args , env ) if execErr != nil { panic ( execErr ) } } Note that when spawning commands we need to provide an explicitly delineated command and argument array , vs. being able to just pass in one command-line string. Also note that Go does NOT offer a classic Unix fork function. Usually this isn't an issue though, since starting goroutines, spawning processes, and exec'ing processes covers most use cases for fork. It is possible a go program can receive Unix system signals during execution. We need to make sure the signals are handled correctly by using \"os/signal\" package. package main import ( \"fmt\" \"os\" \"os/signal\" \"syscall\" ) func main () { sigs := make ( chan os . Signal , 1 ) done := make ( chan bool , 1 ) signal . Notify ( sigs , syscall . SIGINT , syscall . SIGTERM ) // registers the given channel to receive notifications of the specified signals go func () { sig := <- sigs // blocks this routine until one of the above signals is received fmt . Println ( sig ) done <- true }() fmt . Println ( \"awaiting signal\" ) <- done fmt . Println ( \"exiting\" ) os . Exit ( 0 ) // exits the program and returns an exit-code } Note that defer won't run when os.Exit is called. HTTP Server/Client \u00b6 The \"net/http\" package provides good support for issuing HTTP requests as a client, or serving contents as a simple server. package main import ( \"bufio\" \"fmt\" \"net/http\" ) func main () { resp , err := http . Get ( \"http://gobyexample.com\" ) // a GET request if err != nil { panic ( err ) } defer resp . Body . Close () fmt . Println ( \"Response status:\" , resp . Status ) scanner := bufio . NewScanner ( resp . Body ) for scanner . Scan () { fmt . Println ( scanner . Text ()) } if err := scanner . Err (); err != nil { panic ( err ) } } Setting up a simple HTTP server can be simple as defining some handlers on some endpoints and serve on a port . A handler is an object implementing the http.Handler interface. package main import ( \"fmt\" \"net/http\" ) func hello ( w http . ResponseWriter , req * http . Request ) { ctx := req . Context () // a `context.Context` is created for each request fmt . Println ( \"server: hello handler started\" ) defer fmt . Println ( \"server: hello handler ended\" ) select { case <- time . After ( 10 * time . Second ): fmt . Fprintf ( w , \"hello\\n\" ) case <- ctx . Done (): err := ctx . Err () // returns an error that explains why the Done() channel was closed fmt . Println ( \"server:\" , err ) internalError := http . StatusInternalServerError http . Error ( w , err . Error (), internalError ) } } func headers ( w http . ResponseWriter , req * http . Request ) { for name , headers := range req . Header { for _ , h := range headers { fmt . Fprintf ( w , \"%v: %v\\n\" , name , h ) } } } func main () { http . HandleFunc ( \"/hello\" , hello ) // pass in functions to call for this endpoint http . HandleFunc ( \"/headers\" , headers ) http . ListenAndServe ( \":8090\" , nil ) // can specify a different router if desired }","title":"Go libraries and examples"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#time-package","text":"When requesting external resources or doing work that need to bound to an execution time limit , we can achieve it using the <-time.After function from the \"time\" package. This function awaits a value to be sent after the specified timeout. c1 := make ( chan string , 1 ) go func () { time . Sleep ( 2 * time . Second ) c1 <- \"result 1\" }() select { case res := <- c1 : fmt . Println ( res ) case <- time . After ( 1 * time . Second ): fmt . Println ( \"timeout 1\" ) } When trying to schedule a goroutine in a future time, or to schedule a function at some interval, use the built-in timer and ticker from the \"time\" package Timers represent a single event in the future. You tell the timer how long you want to wait, and it provides a channel that will be notified at that time. timer1 := time . NewTimer ( 2 * time . Second ) <- timer1 . C // blocks until the timer event is received fmt . Println ( \"Timer 1 fired\" ) // will print after 2 seconds time . Sleep ( 2 * time . Second ) // achieves the same thing above timer does Tickers can schedule a function repeatedly at regular intervals package main import ( \"fmt\" \"time\" ) func main () { ticker := time . NewTicker ( 500 * time . Millisecond ) done := make ( chan bool ) go func () { for { select { case <- done : return case t := <- ticker . C : fmt . Println ( \"Tick at\" , t ) } } }() time . Sleep ( 1600 * time . Millisecond ) ticker . Stop () done <- true fmt . Println ( \"Ticker stopped\" ) } Rate limit is a common requirement to consider when designing a service. In go this can be achieved using the tickers as a source of creating \"gaps\" between processing requests. package main import ( \"fmt\" \"time\" ) func main () { requests := make ( chan int , 5 ) for i := 1 ; i <= 5 ; i ++ { requests <- i } close ( requests ) limiter := time . Tick ( 200 * time . Millisecond ) for req := range requests { <- limiter fmt . Println ( \"request\" , req , time . Now ()) } burstyLimiter := make ( chan time . Time , 3 ) for i := 0 ; i < 3 ; i ++ { burstyLimiter <- time . Now () } go func () { for t := range time . Tick ( 200 * time . Millisecond ) { burstyLimiter <- t } }() burstyRequests := make ( chan int , 5 ) for i := 1 ; i <= 5 ; i ++ { burstyRequests <- i } close ( burstyRequests ) for req := range burstyRequests { <- burstyLimiter fmt . Println ( \"request\" , req , time . Now ()) } }","title":"time package"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#channel-and-goroutine-with-sync-package","text":"A simple worker pool implementation using channels package main import ( \"fmt\" \"time\" ) func worker ( id int , jobs <- chan int , results chan <- int ) { for j := range jobs { fmt . Println ( \"worker\" , id , \"started job\" , j ) time . Sleep ( time . Second ) fmt . Println ( \"worker\" , id , \"finished job\" , j ) results <- j * 2 } } func main () { const numJobs = 5 jobs := make ( chan int , numJobs ) results := make ( chan int , numJobs ) for w := 1 ; w <= 3 ; w ++ { go worker ( w , jobs , results ) } for j := 1 ; j <= numJobs ; j ++ { jobs <- j } close ( jobs ) for a := 1 ; a <= numJobs ; a ++ { <- results } } To wait for multiple goroutines to finish, can use wait group from \"sync\" package package main import ( \"fmt\" \"sync\" \"time\" ) func worker ( id int , wg * sync . WaitGroup ) { defer wg . Done () fmt . Printf ( \"Worker %d starting\\n\" , id ) time . Sleep ( time . Second ) fmt . Printf ( \"Worker %d done\\n\" , id ) } func main () { var wg sync . WaitGroup for i := 1 ; i <= 5 ; i ++ { wg . Add ( 1 ) // increment wg's counter go worker ( i , & wg ) } wg . Wait () // wait until all works returned } \"sync/atomic\" package provides atomic counters that is thread-safe and can be accessed by multiple concurrent goroutines. package main import ( \"fmt\" \"sync\" \"sync/atomic\" ) func main () { var ops uint64 var wg sync . WaitGroup for i := 0 ; i < 50 ; i ++ { wg . Add ( 1 ) go func () { for c := 0 ; c < 1000 ; c ++ { atomic . AddUint64 ( & ops , 1 ) } wg . Done () }() } wg . Wait () fmt . Println ( \"ops:\" , ops ) // yields 50000 } mutex from the package \"sync\" provides a safe way for multiple goroutines to access the same data package main import ( \"fmt\" \"math/rand\" \"sync\" \"sync/atomic\" \"time\" ) func main () { var state = make ( map [ int ] int ) var mutex = & sync . Mutex {} var readOps uint64 var writeOps uint64 for r := 0 ; r < 100 ; r ++ { go func () { total := 0 for { key := rand . Intn ( 5 ) mutex . Lock () total += state [ key ] mutex . Unlock () atomic . AddUint64 ( & readOps , 1 ) time . Sleep ( time . Millisecond ) } }() } for w := 0 ; w < 10 ; w ++ { go func () { for { key := rand . Intn ( 5 ) val := rand . Intn ( 100 ) mutex . Lock () state [ key ] = val mutex . Unlock () atomic . AddUint64 ( & writeOps , 1 ) time . Sleep ( time . Millisecond ) } }() } time . Sleep ( time . Second ) readOpsFinal := atomic . LoadUint64 ( & readOps ) fmt . Println ( \"readOps:\" , readOpsFinal ) writeOpsFinal := atomic . LoadUint64 ( & writeOps ) fmt . Println ( \"writeOps:\" , writeOpsFinal ) }","title":"Channel and goroutine with sync package"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#stateful-goroutines","text":"To have shared states owned by a single goroutine and use channel communications to coordinate read/write to the states. This will guarantee that the data is never corrupted with concurrent access. package main import ( \"fmt\" \"math/rand\" \"sync/atomic\" \"time\" ) type readOp struct { key int resp chan int } type writeOp struct { key int val int resp chan bool } func main () { var readOps uint64 var writeOps uint64 reads := make ( chan readOp ) writes := make ( chan writeOp ) go func () { var state = make ( map [ int ] int ) // shared state owned by this goroutine for { select { // act like a worker to process access requests case read := <- reads : read . resp <- state [ read . key ] case write := <- writes : state [ write . key ] = write . val write . resp <- true } } }() for r := 0 ; r < 100 ; r ++ { go func () { for { read := readOp { key : rand . Intn ( 5 ), resp : make ( chan int )} reads <- read <- read . resp atomic . AddUint64 ( & readOps , 1 ) time . Sleep ( time . Millisecond ) } }() } for w := 0 ; w < 10 ; w ++ { go func () { for { write := writeOp { key : rand . Intn ( 5 ), val : rand . Intn ( 100 ), resp : make ( chan bool )} writes <- write <- write . resp atomic . AddUint64 ( & writeOps , 1 ) time . Sleep ( time . Millisecond ) } }() } time . Sleep ( time . Second ) readOpsFinal := atomic . LoadUint64 ( & readOps ) fmt . Println ( \"readOps:\" , readOpsFinal ) writeOpsFinal := atomic . LoadUint64 ( & writeOps ) fmt . Println ( \"writeOps:\" , writeOpsFinal ) }","title":"Stateful goroutines"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#sorting","text":"Go\u2019s \"sort\" package implements sorting for built-ins and user-defined types. Note that sorting is in-place , so it changes the given slice and doesn't return a new one. We can also use sort to check if a slice is already in sorted order. strs := [] string { \"c\" , \"a\" , \"b\" } sort . Strings ( strs ) ints := [] int { 7 , 2 , 4 } sort . Ints ( ints ) Sorting by Functions allows sort something other than its natural order, by implementing the sort.Interface and provide methods for Len() , Less(i, j) and Swap(i, j) so that sort.Sort can be used on package main import ( \"fmt\" \"sort\" ) type byLength [] string // an alias for []string; alias creates a type that can implement an interface func ( s byLength ) Len () int { return len ( s ) } func ( s byLength ) Swap ( i , j int ) { s [ i ], s [ j ] = s [ j ], s [ i ] } func ( s byLength ) Less ( i , j int ) bool { return len ( s [ i ]) < len ( s [ j ]) } func main () { fruits := [] string { \"peach\" , \"banana\" , \"kiwi\" } sort . Sort ( byLength ( fruits )) // converting the slice into \"byLength\" type fmt . Println ( fruits ) }","title":"Sorting"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#work-with-collection-functions","text":"Go does not support generics like other languages does; in Go it\u2019s common to provide collection functions when they are specifically needed for your program and data types. Note that in some cases it may be clearest to just inline the functions directly instead of creating and calling a helper function. func Index ( vs [] string , t string ) int { for i , v := range vs { if v == t { return i } } return - 1 } func Include ( vs [] string , t string ) bool { return Index ( vs , t ) >= 0 } func Any ( vs [] string , f func ( string ) bool ) bool { for _ , v := range vs { if f ( v ) { return true } } return false } func All ( vs [] string , f func ( string ) bool ) bool { for _ , v := range vs { if ! f ( v ) { return false } } return true } func Filter ( vs [] string , f func ( string ) bool ) [] string { vsf := make ([] string , 0 ) for _ , v := range vs { if f ( v ) { vsf = append ( vsf , v ) } } return vsf } func Map ( vs [] string , f func ( string ) string ) [] string { vsm := make ([] string , len ( vs )) for i , v := range vs { vsm [ i ] = f ( v ) } return vsm }","title":"Work with Collection Functions"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#regex","text":"Go offers built-in support for regular expressions from the \"regexp\" package package main import ( \"bytes\" \"fmt\" \"regexp\" ) func main () { match , _ := regexp . MatchString ( \"p([a-z]+)ch\" , \"peach\" ) fmt . Println ( match ) r , _ := regexp . Compile ( \"p([a-z]+)ch\" ) // cache the pattern struct fmt . Println ( r . MatchString ( \"peach\" )) fmt . Println ( r . FindString ( \"peach punch\" )) fmt . Println ( r . FindStringIndex ( \"peach punch\" )) fmt . Println ( r . FindStringSubmatch ( \"peach punch\" )) fmt . Println ( r . FindStringSubmatchIndex ( \"peach punch\" )) fmt . Println ( r . FindAllString ( \"peach punch pinch\" , - 1 )) fmt . Println ( r . FindAllStringSubmatchIndex ( \"peach punch pinch\" , - 1 )) fmt . Println ( r . Match ([] byte ( \"peach\" ))) fmt . Println ( r . ReplaceAllString ( \"a peach\" , \"<fruit>\" )) in := [] byte ( \"a peach\" ) fmt . Println ( r . ReplaceAllFunc ( in , bytes . ToUpper )) }","title":"Regex"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#json","text":"Go offers built-in support for JSON encoding and decoding through the package \"encoding/json\" , including conversion to and from built-in and custom data types. package main import ( \"encoding/json\" \"fmt\" \"os\" ) type response1 struct { Page int Fruits [] string } // field tags contain directives for the encoder and decoder type response2 struct { Page int `json:\"page\"` Fruits [] string `json:\"fruits\"` } func main () { slcD := [] string { \"apple\" , \"peach\" , \"pear\" } slcB , _ := json . Marshal ( slcD ) // produces a json list string mapD := map [ string ] int { \"apple\" : 5 , \"lettuce\" : 7 } mapB , _ := json . Marshal ( mapD ) // produces a json object string res1D := & response1 { Page : 1 , Fruits : [] string { \"apple\" , \"peach\" , \"pear\" }} res1B , _ := json . Marshal ( res1D ) // produces a json object string with keys as-is res2D := & response2 { Page : 1 , Fruits : [] string { \"apple\" , \"peach\" , \"pear\" }} res2B , _ := json . Marshal ( res2D ) // produces a json object string with keys specified in the struct byt := [] byte ( `{\"num\":6.13,\"strs\":[\"a\",\"b\"]}` ) var dat map [ string ] interface {} // will hold a map of strings to arbitrary data types. if err := json . Unmarshal ( byt , & dat ); err != nil { panic ( err ) } num := dat [ \"num\" ].( float64 ) // cast values to the appropriate type strs := dat [ \"strs\" ].([] interface {}) str1 := strs [ 0 ].( string ) str := `{\"page\": 1, \"fruits\": [\"apple\", \"peach\"]}` res := response2 {} // creates a shell object json . Unmarshal ([] byte ( str ), & res ) // decode JSON string and save data into object enc := json . NewEncoder ( os . Stdout ) // stream JSON encoding to other things directly d := map [ string ] int { \"apple\" : 5 , \"lettuce\" : 7 } enc . Encode ( d ) } https://golang.org/pkg/encoding/json/","title":"JSON"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#xml","text":"\"encoding.xml\" package offers built-in support for XML package main import ( \"encoding/xml\" \"fmt\" ) // field tags contain directives for the encoder and decoder type Plant struct { XMLName xml . Name `xml:\"plant\"` // the root element Id int `xml:\"id,attr\"` // an attribute of this element Name string `xml:\"name\"` // an element Origin [] string `xml:\"origin\"` // an element } func ( p Plant ) String () string { return fmt . Sprintf ( \"Plant id=%v, name=%v, origin=%v\" , p . Id , p . Name , p . Origin ) } func main () { coffee := & Plant { Id : 27 , Name : \"Coffee\" } coffee . Origin = [] string { \"Ethiopia\" , \"Brazil\" } out , _ := xml . MarshalIndent ( coffee , \" \" , \" \" ) // encode into a readable XML string fmt . Println ( string ( out )) fmt . Println ( xml . Header + string ( out )) var p Plant if err := xml . Unmarshal ( out , & p ); err != nil { // decode into a object panic ( err ) } fmt . Println ( p ) tomato := & Plant { Id : 81 , Name : \"Tomato\" } tomato . Origin = [] string { \"Mexico\" , \"California\" } type Nesting struct { XMLName xml . Name `xml:\"nesting\"` Plants [] * Plant `xml:\"parent>plant\"` // to nest all <plant>s within <parent> } nesting := & Nesting {} nesting . Plants = [] * Plant { coffee , tomato } out , _ = xml . MarshalIndent ( nesting , \" \" , \" \" ) fmt . Println ( string ( out )) }","title":"XML"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#the-time-related-packages","text":"Go's \"time\" package offers handy functions for basic time operations. package main import ( \"fmt\" \"time\" ) func main () { p := fmt . Println // alias a function now := time . Now () secs := now . Unix () nanos := now . UnixNano () p ( now ) p ( time . Unix ( secs , 0 )) // converts seconds to time string p ( time . Unix ( 0 , nanos )) // converts nanoseconds to time string then := time . Date ( 2009 , 11 , 17 , 20 , 34 , 58 , 651387237 , time . UTC ) p ( then ) p ( then . Year ()) p ( then . Month ()) p ( then . Day ()) p ( then . Hour ()) p ( then . Minute ()) p ( then . Second ()) p ( then . Nanosecond ()) p ( then . Location ()) p ( then . Weekday ()) p ( then . Before ( now )) p ( then . After ( now )) p ( then . Equal ( now )) diff := now . Sub ( then ) p ( diff ) p ( then . Add ( diff )) p ( then . Add ( - diff )) // formatting time t := time . Now () p ( t . Format ( time . RFC3339 )) // 2014-04-15T18:00:15-07:00 t1 , e := time . Parse ( // parse to get a time object time . RFC3339 , // from this format \"2012-11-01T22:08:41+00:00\" ) // actual time string p ( t1 ) // given an example of the desired format, then time will do the formatting p ( t . Format ( \"3:04PM\" )) p ( t . Format ( \"Mon Jan _2 15:04:05 2006\" )) p ( t . Format ( \"2006-01-02T15:04:05.999999-07:00\" )) form := \"3 04 PM\" t2 , e := time . Parse ( form , \"8 41 PM\" ) p ( t2 ) }","title":"The time-related packages"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#random-numbers","text":"Go\u2019s \"math/rand\" package provides pseudorandom number generation. Use \"crypto/rand\" for random numbers for security related tasks. package main import ( \"fmt\" \"math/rand\" \"time\" ) func main () { fmt . Println ( rand . Intn ( 100 )) // generates an integer from [0,100) fmt . Println ( rand . Float64 ()) // generates a 64-bit floating-point from [0.0, 1.0) s1 := rand . NewSource ( time . Now (). UnixNano ()) // get a new seed object r1 := rand . New ( s1 ) // get a new generator from the seed fmt . Println ( r1 . Intn ( 100 )) } http://golang.org/pkg/math/rand/","title":"Random Numbers"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#string-functions","text":"package main import ( \"fmt\" s \"strings\" ) var p = fmt . Println func main () { p ( \"Contains: \" , s . Contains ( \"test\" , \"es\" )) p ( \"Count: \" , s . Count ( \"test\" , \"t\" )) p ( \"HasPrefix: \" , s . HasPrefix ( \"test\" , \"te\" )) p ( \"HasSuffix: \" , s . HasSuffix ( \"test\" , \"st\" )) p ( \"Index: \" , s . Index ( \"test\" , \"e\" )) p ( \"Join: \" , s . Join ([] string { \"a\" , \"b\" }, \"-\" )) p ( \"Repeat: \" , s . Repeat ( \"a\" , 5 )) p ( \"Replace: \" , s . Replace ( \"foo\" , \"o\" , \"0\" , - 1 )) // replace all indexes p ( \"Replace: \" , s . Replace ( \"foo\" , \"o\" , \"0\" , 1 )) p ( \"Split: \" , s . Split ( \"a-b-c-d-e\" , \"-\" )) p ( \"ToLower: \" , s . ToLower ( \"TEST\" )) p ( \"ToUpper: \" , s . ToUpper ( \"test\" )) p ( \"Len: \" , len ( \"hello\" )) p ( \"Char:\" , \"hello\" [ 1 ]) // numerical char value is evaluated } http://golang.org/pkg/strings/ https://blog.golang.org/strings more on wide multi-byte characters \"fmt\" offers Printf that can help print out formatted results package main import ( \"fmt\" \"os\" ) type point struct { x , y int } func main () { p := point { 1 , 2 } fmt . Printf ( \"%v\\n\" , p ) // p's object values fmt . Printf ( \"%+v\\n\" , p ) // p's object values and field names if p is a struct fmt . Printf ( \"%#v\\n\" , p ) // the source code snippet that would produce p fmt . Printf ( \"%T\\n\" , p ) // the type of p fmt . Printf ( \"%t\\n\" , true ) // for boolean fmt . Printf ( \"%d\\n\" , 123 ) // for base-10 integer fmt . Printf ( \"%b\\n\" , 14 ) // for binary representation fmt . Printf ( \"%c\\n\" , 33 ) // for char value of the integer fmt . Printf ( \"%x\\n\" , 456 ) // for hex representation fmt . Printf ( \"%f\\n\" , 78.9 ) // for floating-point number fmt . Printf ( \"%e\\n\" , 123400000.0 ) // for scientific notation fmt . Printf ( \"%E\\n\" , 123400000.0 ) // for scientific notation fmt . Printf ( \"%s\\n\" , \"\\\"string\\\"\" ) // for string fmt . Printf ( \"%q\\n\" , \"string\" ) // add quotes around the string fmt . Printf ( \"%x\\n\" , \"hex this\" ) // renders string in base-16 fmt . Printf ( \"%p\\n\" , & p ) // for pointer representation fmt . Printf ( \"|%6d|%6d|\\n\" , 12 , 345 ) // control the width of the print fmt . Printf ( \"|%6.2f|%6.2f|\\n\" , 1.2 , 3.45 ) // control the precision after '.' fmt . Printf ( \"|%-6s|%-6s|\\n\" , \"foo\" , \"b\" ) // left-justify the print s := fmt . Sprintf ( \"a %s\" , \"string\" ) // saves formatted string as output fmt . Fprintf ( os . Stderr , \"an %s\\n\" , \"error\" ) // prints to other io.Writers } The package \"strconv\" offers functions to parse numbers from strings package main import ( \"fmt\" \"strconv\" ) func main () { f , _ := strconv . ParseFloat ( \"1.234\" , 64 ) // parse a 64-bit precision floating point i , _ := strconv . ParseInt ( \"123\" , 0 , 64 ) // 0 means auto-base, 64-bit integer k , _ := strconv . Atoi ( \"135\" ) // parse a base-10 integer _ , e := strconv . Atoi ( \"wat\" ) // error on bad input }","title":"String functions"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#url-parsing","text":"The packages \"net\" and \"net/url\" provides functions to parse URLs package main import ( \"fmt\" \"net\" \"net/url\" ) func main () { s := \"postgres://user:pass@host.com:5432/path?k=v#f\" u , err := url . Parse ( s ) if err != nil { panic ( err ) } fmt . Println ( u . Scheme ) // postgres fmt . Println ( u . User ) // user:pass fmt . Println ( u . User . Username ()) // user p , _ := u . User . Password () // pass fmt . Println ( u . Host ) // host.com:5432 host , port , _ := net . SplitHostPort ( u . Host ) fmt . Println ( u . Path ) // /path fmt . Println ( u . Fragment ) // f fmt . Println ( u . RawQuery ) // k=v everything between '?' and '#' m , _ := url . ParseQuery ( u . RawQuery ) fmt . Println ( m ) // map[k:[v]] fmt . Println ( m [ \"k\" ][ 0 ]) // v }","title":"URL Parsing"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#encryption-and-hashing-functions","text":"Go implements several hash functions in various \"crypto/*\" packages. Go provides built-in support for base64 encoding/decoding from the \"encoding/base64\" package, which can do URL encoding. package main import ( \"crypto/sha1\" // others like \"crypto/md5\" \"crypto/sha256\" has similar usage b64 \"encoding/base64\" \"fmt\" ) func main () { s := \"sha1 this string\" h := sha1 . New () h . Write ([] byte ( s )) // create a byte array from the string and pass in bs := h . Sum ( nil ) // creates the final hash; the argument will be appended another byte slice to the end of the existing byte slice (such as a salt) fmt . Println ( s ) fmt . Printf ( \"%x\\n\" , bs ) // sha values usually printed in hex data := \"abc123!?$*&()'-=@~\" stdEncode := b64 . StdEncoding . EncodeToString ([] byte ( data )) stdDecode , _ := b64 . StdEncoding . DecodeString ( sEnc ) urlEnc := b64 . URLEncoding . EncodeToString ([] byte ( data )) urlDec , _ := b64 . URLEncoding . DecodeString ( uEnc ) }","title":"Encryption and Hashing Functions"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#file-io","text":"Reading and writing files are basic tasks needed for many Go programs. Some related packages: \"bufio\" \"fmt\" \"io\" \"io/ioutil\" \"os\" package main import ( \"bufio\" \"fmt\" \"io\" \"io/ioutil\" \"os\" ) func check ( e error ) { // always check error for file operations if e != nil { panic ( e ) } } func main () { dat , err := ioutil . ReadFile ( \"/tmp/dat\" ) // reads entire file into memory check ( err ) f , err := os . Open ( \"/tmp/dat\" ) // open a file to gain more control over various file operations check ( err ) b1 := make ([] byte , 5 ) // make a byte array with a fixed size n1 , err := f . Read ( b1 ) // read bytes in, up to the size of the array; n1 is number of bytes actually read check ( err ) fmt . Printf ( \"%d bytes: %s\\n\" , n1 , string ( b1 [: n1 ])) o3 , err := f . Seek ( 6 , 0 ) // start next read from a byte position in the file check(err) b3 := make ([] byte , 2 ) n3 , err := io . ReadAtLeast ( f , b3 , 2 ) _ , err = f . Seek ( 0 , 0 ) // rewind to the beginning of this file r4 := bufio . NewReader ( f ) b4 , err := r4 . Peek ( 5 ) // directly returns the byte array for you check ( err ) fmt . Printf ( \"5 bytes: %s\\n\" , string ( b4 )) f . Close () d1 := [] byte ( \"hello\\ngo\\n\" ) err := ioutil . WriteFile ( \"/tmp/dat1\" , d1 , 0644 ) // write a byte array to a file directly check ( err ) f , err := os . Create ( \"/tmp/dat2\" ) // create an empty file check ( err ) defer f . Close () // will close the file when out of scope d2 := [] byte { 115 , 111 , 109 , 101 , 10 } n2 , err := f . Write ( d2 ) // write some bytes into the file check ( err ) n3 , err := f . WriteString ( \"writes\\n\" ) // write a string into the file check ( err ) f . Sync () // flush writes to the disk w := bufio . NewWriter ( f ) // buffered writer n4 , err := w . WriteString ( \"buffered\\n\" ) // write a string to the file check ( err ) w . Flush () // flush writes to the disk } A line filter is a common type of program that reads input on STDIN, processes it, and then prints some derived result to STDOUT. grep and sed are common line filters. This is an example line filter written with go. package main import ( \"bufio\" \"fmt\" \"os\" \"strings\" ) func main () { scanner := bufio . NewScanner ( os . Stdin ) for scanner . Scan () { ucl := strings . ToUpper ( scanner . Text ()) fmt . Println ( ucl ) } if err := scanner . Err (); err != nil { fmt . Fprintln ( os . Stderr , \"error:\" , err ) os . Exit ( 1 ) } }","title":"File I/O"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#files-directories","text":"The \"path/filepath\" package provides lots of convenient functions to parse and construct file paths in a way that is portable between operating systems The \"os\" package offers functions for interacting with files and directories package main import ( \"fmt\" \"io/ioutil\" \"os\" \"path/filepath\" \"strings\" ) func main () { p := filepath . Join ( \"dir1\" , \"dir2\" , \"filename\" ) // prepare a path by concatenate tokens fmt . Println ( filepath . Join ( \"dir1//\" , \"filename\" )) // implicitly take care of extra chars fmt . Println ( filepath . Join ( \"dir1/../dir1\" , \"filename\" )) // implicitly evaluate '..' fmt . Println ( \"Dir(p):\" , filepath . Dir ( p )) // the path to the parent dir of the file fmt . Println ( \"Base(p):\" , filepath . Base ( p )) // the base of the path, could be a dir or file fmt . Println ( filepath . IsAbs ( \"dir/file\" )) // judge if it is an absolute path or relative path fmt . Println ( filepath . IsAbs ( \"/dir/file\" )) filename := \"config.json\" ext := filepath . Ext ( filename ) // gets the extension of the file fmt . Println ( strings . TrimSuffix ( filename , ext )) // rid of extension rel , err := filepath . Rel ( \"a/b\" , \"a/b/t/file\" ) // finds relative path between the two dirs (they must share a common root) if err != nil { panic ( err ) } err := os . Mkdir ( \"subdir\" , 0755 ) // creates a dir in current working directory check ( err ) defer os . RemoveAll ( \"subdir\" ) // remove a dir tree; it is good practice to defer removing temp dirs createEmptyFile := func ( name string ) { d := [] byte ( \"\" ) check ( ioutil . WriteFile ( name , d , 0644 )) // creates an empty file } createEmptyFile ( \"subdir/file1\" ) err = os . MkdirAll ( \"subdir/parent/child\" , 0755 ) // create a hierarchy of dirs check ( err ) c , err := ioutil . ReadDir ( \"subdir/parent\" ) // lists dir contents, returns a slice of `os.FileInfo` objects check ( err ) for _ , entry := range c { fmt . Println ( \" \" , entry . Name (), entry . IsDir ()) // some functions on the os.FileInfo } err = os . Chdir ( \"subdir/parent/child\" ) // change the current working directory check ( err ) c , err = ioutil . ReadDir ( \".\" ) // list current dir contents check ( err ) fmt . Println ( \"Visiting subdir\" ) err = filepath . Walk ( \"subdir\" , visit ) // accepts a callback function to handle every file or directory visited f , err := ioutil . TempFile ( \"\" , \"sample\" ) // creates and open a temp file for read/write; \"\" as first arg tells to create the file in OS default temp location check ( err ) defer os . Remove ( f . Name ()) dname , err := ioutil . TempDir ( \"\" , \"sampledir\" ) // similarly create a temp dir check ( err ) defer os . RemoveAll ( dname ) fname := filepath . Join ( dname , \"file1\" ) err = ioutil . WriteFile ( fname , [] byte { 1 , 2 }, 0666 ) check ( err ) } func visit ( p string , info os . FileInfo , err error ) error { if err != nil { return err } fmt . Println ( \" \" , p , info . IsDir ()) return nil }","title":"Files, Directories"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#testing","text":"The \"testing\" package provides the tools we need to write unit tests and the go test command runs tests. package main import ( \"fmt\" \"testing\" ) func IntMin ( a , b int ) int { if a < b { return a } return b } func TestIntMinTableDriven ( t * testing . T ) { var tests = [] struct { a , b int want int }{ // shortcut to initialize anonymous struct { 0 , 1 , 0 }, { 1 , 0 , 0 }, { 2 , - 2 , - 2 }, { 0 , - 1 , - 1 }, { - 1 , 0 , - 1 }, } for _ , tt := range tests { testname := fmt . Sprintf ( \"%d,%d\" , tt . a , tt . b ) t . Run ( testname , func ( t * testing . T ) { ans := IntMin ( tt . a , tt . b ) if ans != tt . want { t . Errorf ( \"got %d, want %d\" , ans , tt . want ) // report test failure and continue t . Fail ( \"fail this test\" ) // report test failure and stop this test immediately } }) } }","title":"Testing"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#command-environment-variables","text":"Use \"os\" package to access command-line args passed in when running the program. Need to build a binary using go build command first. It also provides functions to get/set environment variables. Go provides a \"flag\" package supporting basic command-line flag parsing. Using this package, it is easy to define subcommands that expect their own flags. package main import ( \"flag\" \"fmt\" \"os\" ) func main () { argsWithProg := os . Args // a slice of the arguments argsWithoutProg := os . Args [ 1 :] // first value is the path to the program os . Setenv ( \"FOO\" , \"1\" ) fmt . Println ( \"FOO:\" , os . Getenv ( \"FOO\" )) for _ , e := range os . Environ () { // os.Environ returns a list of env vars and values pair := strings . SplitN ( e , \"=\" , 2 ) } wordPtr := flag . String ( \"word\" , \"foo\" , \"a string\" ) // declare a flag for string with a default value; returns a pointer reference numbPtr := flag . Int ( \"numb\" , 42 , \"an int\" ) boolPtr := flag . Bool ( \"fork\" , false , \"a bool\" ) var svar string flag . StringVar ( & svar , \"svar\" , \"bar\" , \"a string var\" ) // declare an option that uses an existing var; pass-by-reference flag . Parse () // execute command-line parsing operation fmt . Println ( \"word:\" , * wordPtr ) fmt . Println ( \"numb:\" , * numbPtr ) fmt . Println ( \"fork:\" , * boolPtr ) fmt . Println ( \"svar:\" , svar ) fmt . Println ( \"tail:\" , flag . Args ()) // get all trailing tokens (flags should be passed before these tokens otherwise won't be picked up) // pass in -h/--help for a list of help text for each option supported // undefined flag option will also result in help text being printed fooCmd := flag . NewFlagSet ( \"foo\" , flag . ExitOnError ) // declare a subcommand fooEnable := fooCmd . Bool ( \"enable\" , false , \"enable\" ) // define option for this subcommand fooName := fooCmd . String ( \"name\" , \"\" , \"name\" ) barCmd := flag . NewFlagSet ( \"bar\" , flag . ExitOnError ) barLevel := barCmd . Int ( \"level\" , 0 , \"level\" ) switch os . Args [ 1 ] { case \"foo\" : fooCmd . Parse ( os . Args [ 2 :]) case \"bar\" : barCmd . Parse ( os . Args [ 2 :]) default : fmt . Println ( \"expected 'foo' or 'bar' subcommands\" ) os . Exit ( 1 ) } }","title":"Command, Environment Variables"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#processes-and-signals","text":"Sometimes it is useful to spawn other processes to handle work outside of this program using other programs. Then the package \"io/ioutil\" and \"os/exec\" will be helpful. While \"syscall\" package performs deep level process manipulation and allow us to replace current process with a new process. package main import ( \"fmt\" \"io/ioutil\" \"os/exec\" \"syscall\" ) func main () { dateCmd := exec . Command ( \"date\" ) // creates an object to represent an external process running this command dateOut , err := dateCmd . Output () // run the command string and collect its output if err != nil { panic ( err ) } grepCmd := exec . Command ( \"grep\" , \"hello\" ) // build command with arguments grepIn , _ := grepCmd . StdinPipe () // the channel for piping in stdin for the command grepOut , _ := grepCmd . StdoutPipe () // the channel for piping out stdout from the command grepCmd . Start () // run the command in a process grepIn . Write ([] byte ( \"hello grep\\ngoodbye grep\" )) grepIn . Close () grepBytes , _ := ioutil . ReadAll ( grepOut ) // read from the stdout grepCmd . Wait () // blocks until command finish execution fmt . Println ( \"> grep hello\" ) fmt . Println ( string ( grepBytes )) lsCmd := exec . Command ( \"bash\" , \"-c\" , \"ls -a -l -h\" ) // allows pass in a full command string lsOut , err := lsCmd . Output () if err != nil { panic ( err ) } binary , lookErr := exec . LookPath ( \"ls\" ) // get the absolute path to the binary if lookErr != nil { panic ( lookErr ) } args := [] string { \"ls\" , \"-a\" , \"-l\" , \"-h\" } // has to be in slice form env := os . Environ () execErr := syscall . Exec ( binary , args , env ) if execErr != nil { panic ( execErr ) } } Note that when spawning commands we need to provide an explicitly delineated command and argument array , vs. being able to just pass in one command-line string. Also note that Go does NOT offer a classic Unix fork function. Usually this isn't an issue though, since starting goroutines, spawning processes, and exec'ing processes covers most use cases for fork. It is possible a go program can receive Unix system signals during execution. We need to make sure the signals are handled correctly by using \"os/signal\" package. package main import ( \"fmt\" \"os\" \"os/signal\" \"syscall\" ) func main () { sigs := make ( chan os . Signal , 1 ) done := make ( chan bool , 1 ) signal . Notify ( sigs , syscall . SIGINT , syscall . SIGTERM ) // registers the given channel to receive notifications of the specified signals go func () { sig := <- sigs // blocks this routine until one of the above signals is received fmt . Println ( sig ) done <- true }() fmt . Println ( \"awaiting signal\" ) <- done fmt . Println ( \"exiting\" ) os . Exit ( 0 ) // exits the program and returns an exit-code } Note that defer won't run when os.Exit is called.","title":"Processes and Signals"},{"location":"Programming-Lang-Reference/Golang/Freq-Used-Libs/#http-serverclient","text":"The \"net/http\" package provides good support for issuing HTTP requests as a client, or serving contents as a simple server. package main import ( \"bufio\" \"fmt\" \"net/http\" ) func main () { resp , err := http . Get ( \"http://gobyexample.com\" ) // a GET request if err != nil { panic ( err ) } defer resp . Body . Close () fmt . Println ( \"Response status:\" , resp . Status ) scanner := bufio . NewScanner ( resp . Body ) for scanner . Scan () { fmt . Println ( scanner . Text ()) } if err := scanner . Err (); err != nil { panic ( err ) } } Setting up a simple HTTP server can be simple as defining some handlers on some endpoints and serve on a port . A handler is an object implementing the http.Handler interface. package main import ( \"fmt\" \"net/http\" ) func hello ( w http . ResponseWriter , req * http . Request ) { ctx := req . Context () // a `context.Context` is created for each request fmt . Println ( \"server: hello handler started\" ) defer fmt . Println ( \"server: hello handler ended\" ) select { case <- time . After ( 10 * time . Second ): fmt . Fprintf ( w , \"hello\\n\" ) case <- ctx . Done (): err := ctx . Err () // returns an error that explains why the Done() channel was closed fmt . Println ( \"server:\" , err ) internalError := http . StatusInternalServerError http . Error ( w , err . Error (), internalError ) } } func headers ( w http . ResponseWriter , req * http . Request ) { for name , headers := range req . Header { for _ , h := range headers { fmt . Fprintf ( w , \"%v: %v\\n\" , name , h ) } } } func main () { http . HandleFunc ( \"/hello\" , hello ) // pass in functions to call for this endpoint http . HandleFunc ( \"/headers\" , headers ) http . ListenAndServe ( \":8090\" , nil ) // can specify a different router if desired }","title":"HTTP Server/Client"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/","text":"This is set of notes is taking from golang.org/ref/spec and gobyexample.com Go is a general-purpose language designed with systems programming in mind. It is strongly typed and garbage-collected and has explicit support for concurrent programming. Go code organization \u00b6 Go organizes code into a collection of files that would be compiled together, called packages . Functions, types, variables, and constants defined in one source file are visible to all other source files within the same package. Go packages that are related and released together are organized within a module . A repository typically contains only one module located at the root of the repo. go.mod decalres the module path A module's path serves as an import path prefix for its packages and indicates where the go command should look to download it A package's import path is its module path joined with its subdirectory within the module. i.e. the module github.com/google/go-cmp contains a package in the directory cmp/ . That package's import path is github.com/google/go-cmp/cmp Go command \u00b6 Create from scratch module_path = <module_path> mkdir \" $module_path \" go mod init \" $module_path \" A module path can be in the form of example.com/user/hello , where the part before the first / is the organization domain name, then follows a variable-sized path, and the part after the last / will be the module name . A simple program package main import ( \"fmt\" ) func main () { fmt . Println ( \"Hello, world.\" ) } The first statement in a Go source file must be package name. When building Executable binaries, its starting point must always use package main . Build a program go install \"$module_path\" will build and install the executable binary to $HOME/go/bin/module_name has to be within the source directory of $module_path go commands accept paths relative to the working directory go build will just compile and build the code within current directory The install directory is controlled by the $GOPATH and $GOBIN environment variables. If $GOBIN is set, binaries are installed to that directory use go env -w GOBIN=/somewhere/else/bin to dynamically change its value use go env -u GOBIN to unset it If $GOPATH is set, binaries are installed to the bin subdirectory of the first directory in the $GOPATH list. Otherwise, binaries are installed to the bin subdirectory of the default $GOPATH ( $HOME/go ) Run the binary after the build, like you would do with a Linux command. Alternatively, can use go run <main_package>.go to preview running the program before building it. Use packages from other sources \u00b6 An import path can describe how to obtain the package source code using a revision control system such as Git. package main import ( \"fmt\" \"example.com/user/hello/morestrings\" \"github.com/google/go-cmp/cmp\" // load github.com/google/go-cmp/cmp in this program ) func main () { fmt . Println ( morestrings . ReverseRunes ( \"!oG ,olleH\" )) fmt . Println ( cmp . Diff ( \"Hello World\" , \"Hello Go\" )) // directly invoke functions defined in 'cmp' module } When you run commands like go install, go build, or go run , the go command will automatically download the remote module and record its version in your go.mod file. An interesting article about GO111MODULE. Module dependencies are automatically downloaded to the pkg/mod subdirectory of the directory indicated by the $GOPATH environment variable. To remove all downloaded modules, use go clean -modcache Testing \u00b6 Go has a lightweight test framework composed of the go test command and the testing package. A test file should be named ending with _test.go A test function should be named like TestXXX with signature like func (t *testing.T) If the function calls a failure function such as t.Error or t.Fail , the test is considered to have failed. package morestrings import \"testing\" func TestReverseRunes ( t * testing . T ) { cases := [] struct { in , want string }{ // shortcut to initialize anonymous struct { \"Hello, world\" , \"dlrow ,olleH\" }, { \"Hello, \u4e16\u754c\" , \"\u754c\u4e16 ,olleH\" }, { \"\" , \"\" }, } for _ , c := range cases { got := ReverseRunes ( c . in ) if got != c . want { t . Errorf ( \"ReverseRunes(%q) == %q, want %q\" , c . in , got , c . want ) } } } // run the tests go test go test - v // PASS // ok example.com/user/morestrings 0.165s Commenting \u00b6 Go provides C-style /* */ block comments and C++-style // line comments Type of Data \u00b6 A variable is a storage location for holding a value. The set of permissible values is determined by the variable's type. Go can infer the type of initialized variables so a type is optional var a = \"initial\" same as var a string = \"string value\" declare multiple variables in one line: var b, c int = 1, 2 shorthand for declaring and assigning value to a variable: f := \"apple\" A variable declaration or, for function parameters and results, the signature of a function declaration or function literal reserves storage for a named variable. Structured variables of array, slice, and struct types have elements and fields that may be addressed individually. Each such element acts like a variable. The static type of a variable is the type given in its declaration, the type provided in the new call or composite literal, or the type of an element of a structured variable. Variables of interface type also have a distinct dynamic type , which is the concrete type of the value assigned to the variable at run time var x interface {} // x is nil and has static type interface{} var v * T // v has value nil, static type *T x = 42 // x has value 42 and dynamic type int x = v // x has value (*T)(nil) and dynamic type *T type declarations: type ( A1 = string A2 = A1 ) Constants There are boolean constants, rune constants, integer constants, floating-point constants, complex constants, and string constants. There are no constants denoting the IEEE-754 negative zero, infinity, and not-a-number values A constant may be given a type explicitly by a constant declaration or conversion, or implicitly when used in a variable declaration or an assignment or as an operand in an expression. declares a constant with const s string = \"string constant\" const can be used anywhere var is used Numbers A numeric type represents sets of integer or floating-point values. uint8 the set of all unsigned 8 - bit integers ( 0 to 255 ) uint16 the set of all unsigned 16 - bit integers ( 0 to 65535 ) uint32 the set of all unsigned 32 - bit integers ( 0 to 4294967295 ) uint64 the set of all unsigned 64 - bit integers ( 0 to 18446744073709551615 ) int8 the set of all signed 8 - bit integers ( - 128 to 127 ) int16 the set of all signed 16 - bit integers ( - 32768 to 32767 ) int32 the set of all signed 32 - bit integers ( - 2147483648 to 2147483647 ) int64 the set of all signed 64 - bit integers ( - 9223372036854775808 to 9223372036854775807 ) float32 the set of all IEEE - 754 32 - bit floating - point numbers float64 the set of all IEEE - 754 64 - bit floating - point numbers complex64 the set of all complex numbers with float32 real and imaginary parts complex128 the set of all complex numbers with float64 real and imaginary parts byte alias for uint8 rune alias for int32 uint either 32 or 64 bits int same size as uint uintptr an unsigned integer large enough to store the uninterpreted bits of a pointer value Strings A string type represents the set of string values. String value is a sequence of bytes: \"go lang\" string length can be obtained from function len(str) chars within a string can be accessed with str[i] range can also be used on a string value to iterate through its chars Numbers<->Strings Operators and punctuation + & += &= && == != ( ) - | -= |= || < <= [ ] * ^ *= ^= <- > >= { } / << /= <<= ++ = := , ; % >> %= >>= -- ! ... . : &^ &^= Keywords reserved break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var Arrays is a numbered sequence of elements of a specific length. var a [ 5 ] int // declare an array a [ 4 ] = 100 // access specific index b := [ 5 ] int { 1 , 2 , 3 , 4 , 5 } // declare and assign with initialized list of elements var twoD [ 2 ][ 3 ] int // declare an 2D array p := [ 1000 ] * float64 // array of float64 pointers for _ , num := range nums { // use range on array sum += num } Slices is a descriptor for a contiguous segment of an underlying array and provides access to a numbered sequence of elements from that array. A slice can be taken from another slice or from an array b := [ 5 ] int { 11 , 22 , 33 , 44 , 55 } l := b [ 2 : 5 ] // in fact takes indexes from [start:end), so 2-4 in this case l = b [: 4 ] // takes indexes 0-3 in this case l = b [ 2 :] // takes indexes 2-4 in this case l = [] string { \"g\" , \"h\" } // declare and initialize a slice c := make ([] int , len ( b )) // make a slice with initial length copy ( c , b ) // copy values from another slice c = append ( c , 100 , 200 ) // can dynamically add elements to a slice Maps are Go\u2019s built-in associative data type aka hashes or dicts in other language m := make ( map [ string ] int ) // create a map with [key-type]value-type m [ \"key1\" ] = 1 // access key and modify value val , exists := m [ \"key2\" ] // 2nd value is a boolean indicates whether key2 defined len ( m ) // gives the number of key/value pairs delete ( m , \"key2\" ) // deletes a key/value pair for k , v := range m { // use range on map fmt . Printf ( \"%s -> %s\\n\" , k , v ) } Structs are mutable and typed collections of fields useful for grouping data together to form records. A field declaration may be followed by an optional string literal tag , which becomes an attribute for all the fields in the corresponding field declaration. An empty tag string is equivalent to an absent tag. The tags are made visible through a reflection interface and take part in type identity for structs but are otherwise ignored. Go supports methods defined on struct types. An embedded field must be specified as a type name T or as a pointer to a non-interface type name *T, and T itself may not be a pointer type. The unqualified type name acts as the field name. field names must be unique in a struct type, so cannot have more than one embedded field of the same type type person struct { name string age int \"tag string for age\" T1 // embedded field of type T1 } func newPerson ( name string ) * person { p := person { name : name } // specify values to the variables in the struct p . age = 42 // explicitly change the value return & p } Zero values When storage is allocated for a variable, either through a declaration or a call of new , or when a new value is created, either through a composite literal or a call of make , and no explicit initialization is provided, the variable or value is given a default value . false for booleans , 0 for numeric types, \"\" for strings , and nil for pointers, functions, interfaces, slices, channels, and maps . This initialization is done recursively. Conditions \u00b6 If-else condition block is just like other programming language. There is no ternary form, however. Note that you don\u2019t need parentheses around conditions in Go. This is true for loops as well. // A statement can precede conditionals; // any variables declared here are available in all branches if num := f (); num < 0 { fmt . Println ( num , \"is negative\" ) } else if num < 10 { fmt . Println ( num , \"has 1 digit\" ) } else { fmt . Println ( num , \"has multiple digits\" ) } Switch block conditions are evaluated left-to-right and top-to-bottom the first one that equals the switch expression triggers execution of the statements of the associated case; the other cases are skipped default condition can appear anywhere in the block use keyword fallthrough to fallthrough switch code := f (); { default : s3 () // can appear anywhere in this block case 0 , 1 , 2 , 3 : s1 () // there is a hidden break for each case case 4 , 5 , 6 , 7 : fallthrough // is necessary to fallthrough to next case case 8 , 9 , 10 , 11 : s2 () s3 () } Loops \u00b6 for is Go\u2019s only looping construct. for j := 7 ; j <= 9 ; j ++ { fmt . Println ( j ) } for loop used in a while fashion: i := 1 for i <= 30 { if i % 2 == 0 { continue } fmt . Println ( i ) i = i + 1 } while true fashion loop terminates with break for { fmt . Println ( \"loop\" ) break } for with range; like a for-each loop but with access to index var a [ 10 ] string for i , s := range a { // i is index; s is a[i] g ( i , s ) // do something for that values } Functions \u00b6 A few things special in golang's functions : A function can be assigned to a variable or invoked directly. A function can return another function A function can accept a variable number of arguments of the same type and saved as an array variable in the function body. it has to be the last argument type in the function signature There can be multiple return values from a function If no return values expected, can omit the return type func ( a , b int , z float64 ) bool { // return value is defined outside parentheses return a * b < int ( z ) } f := func ( x , y int ) ( int , int ) { return x + y , x - y } func ( s ... string ) { // variadic parameter list // can be invoked with zero or more arguments for that parameter. } Function literals are closures : they may refer to variables defined in a surrounding function. func intSeq () func () int { // returns the anonymous function i := 0 return func () int { i ++ return i } // this function closes over the variable i to form a closure // it captures its own i value, which will be updated each time it is called // until a new instance of this anonymous function is obtained from calling intSeq() } Special Statements \u00b6 A select statement chooses which of a set of possible send or receive operations will proceed. It looks similar to a switch statement but with the cases all referring to communication operations. Some rules: For all the cases in the statement, the channel operands of receive operations and the channel and right-hand-side expressions of send statements are evaluated exactly once evaluation will occur irrespective of which (if any) communication operation is selected to proceed. If one or more of the communications can proceed, a single one that can proceed is chosen via a uniform pseudo-random selection. Unless the selected case is the default case, the respective communication operation is executed. If the selected case is a RecvStmt with a short variable declaration or an assignment, the left-hand side expressions are evaluated and the received value (or values) are assigned. The statement list of the selected case is executed. Go\u2019s select lets you wait on multiple channel operations. Combining goroutines and channels with select is a powerful feature of Go. package main import ( \"fmt\" \"time\" ) func main () { c1 := make ( chan string ) c2 := make ( chan string ) go func () { time . Sleep ( 1 * time . Second ) c1 <- \"one\" }() go func () { time . Sleep ( 2 * time . Second ) c2 <- \"two\" }() for i := 0 ; i < 2 ; i ++ { select { // blocks until either one of the channels got a message case msg1 := <- c1 : fmt . Println ( \"received\" , msg1 ) case msg2 := <- c2 : fmt . Println ( \"received\" , msg2 ) } } } A defer statement is used to ensure that a function call is performed later at the end of the current function scope, usually for purposes of cleanup . Check for errors in a deferred function, if applicable. func main () { f := createFile ( \"/tmp/defer.txt\" ) defer closeFile ( f ) // will be executed at the end of the enclosing function (main) writeFile ( f ) } Methods \u00b6 Methods differ from functions in that it takes a receiver type The receiver is specified via an extra parameter section preceding the method name. receiver type can be a single defined type that is either pointer or value and non-variadic parameter type must be defined in the same package as the method the method is said to be bound to its receiver base type and the method name is visible only within selectors for type T or *T. the non-blank identifier (receiver name) must be unique in the method signature (against other arguments) method can still take in arguments like a function does Go automatically handles conversion between values and pointers for method calls The type of a method is the type of a function with the receiver as first argument i.e. func(r *rect) func ( r * rect ) area () int { // prefered for most cases return r . width * r . height } func ( r rect ) perim () int { return 2 * ( r . width + r . height ) } r = rect { width : 10 , height : 5 } r . area () // auto convert to pointer type and pass in `r` r . perim () A type may have a method set associated with it. In a method set, each method must have a unique non-blank method name. The method set of a type determines the interfaces that the type implements and the methods that can be called using a receiver of that type. The method set of an interface type is its interface. The method set of any other type T consists of all methods declared with receiver type T. The method set of the corresponding pointer type T is the set of all methods declared with receiver T or T Any other type has an empty method set. Interfaces \u00b6 Interfaces are named collections of method signatures type geometry interface { area () float64 perim () float64 } To implement an interface in Go, we just need to implement all the methods in the interface. type rect struct { width , height float64 } func ( r rect ) area () float64 { return r . width * r . height } func ( r rect ) perim () float64 { return 2 * r . width + 2 * r . height } If a variable has an interface type, then we can call methods that are in the named interface. func measure ( g geometry ) { fmt . Println ( g ) fmt . Println ( g . area ()) fmt . Println ( g . perim ()) } func main () { r := rect { width : 3 , height : 4 } measure ( r ) } The interface{} type is the interface that has no methods. So essentially all types implements this interface. If you write a function that takes an interface{} value as a parameter, you can supply that function with any value. Exceptions and Errors \u00b6 By convention, errors are the last return value and have type error , a built-in interface . type error interface { Error () string } Can construct a basic error value with errors.New(\"Error Message\") func foo ( arg int ) ( int , error ) { if arg < 0 { return - 1 , errors . New ( \"invalid input\" ) // constructs a basic error value with the given error message. } return arg + 3 , nil // A nil value in the error position indicates that there was no error. } It\u2019s possible to use custom types as errors by implementing the Error() method on them. type argError struct { arg int prob string } func ( e * argError ) Error () string { return fmt . Sprintf ( \"%d - %s\" , e . arg , e . prob ) } func foo2 ( arg int ) ( int , error ) { if arg < 0 { return - 1 , & argError { arg , \"can't work with it\" } // build a new struct } return arg + 3 , nil } // consume the error if v , e := foo ( - 5 ); e != nil { fmt . Println ( \"foo failed:\" , e ) } else { fmt . Println ( \"foo worked:\" , v ) } // or v , e := foo2 ( - 5 ) if errorInstance , ok := e .( * argError ); ok { // here used a syntax to dereference the error pointer back to its instance fmt . Println ( errorInstance . arg ) fmt . Println ( errorInstance . prob ) } Panics typically means something went unexpectedly wrong. The program should fail fast when this type of error happens. When calling panic(message) inside a function, it will immediate exit the current function, then waits for all other deferred functions in the callstack to run, then the program is terminated and error condition reported. A recover function allows a program to manage behavior of a panicking goroutine. func protect ( g func ()) { defer func () { log . Println ( \"done\" ) // Println executes normally even if there is a panic if x := recover (); x != nil { log . Printf ( \"run time panic: %v\" , x ) } }() log . Println ( \"start\" ) g () // run-time panics raised will be protected by the recover() in the deferred function } The return value of recover is nil if any of the following conditions holds: panic's argument was nil the goroutine is not panicking recover was not called directly by a deferred function. Note that unlike some languages which use exceptions for handling of many errors, in Go it is idiomatic to use error-indicating return values wherever possible. Run-time panics Execution errors such as attempting to index an array out of bounds trigger a run-time panic equivalent to a call of the built-in function panic with a value of the implementation-defined interface type runtime.Error . That type satisfies the predeclared interface type error . The exact error values that represent distinct run-time error conditions are unspecified. package runtime type Error interface { error // and perhaps other methods } Read more https://blog.golang.org/error-handling-and-go Threading and Goroutines \u00b6 A go statement starts the execution of a function call as an independent concurrent thread/process of control, or goroutine , within the same address space . Unlike with a regular function call, program execution does not wait for the invoked function to complete ( non-blocking ) by the goroutine. The expression must be a function or method call; it cannot be parenthesized. Cannot start goroutine on built-in functions and expression statements . f ( \"direct\" ) // invoke a function go f ( \"goroutine\" ) // start a goroutine calling a function go func ( msg string ) { // start a goroutine calling an anonymous function fmt . Println ( msg ) }( \"going\" ) Pipes and Channels \u00b6 Channels are the pipe-equivalent in golang that connect concurrent goroutines . You can send values into channels from one goroutine and receive those values into another goroutine, of a specified element type for the values passed. chan T // can be used to send and receive values of type T chan <- float64 // can only be used to send float64s <- chan int // can only be used to receive ints The optional <- operator specifies the channel direction, send or receive . If no direction is given, the channel is bidirectional . A channel may be constrained only to send or only to receive by assignment or explicit conversion . A new, initialized channel value can be made using the built-in function make , which takes the channel type and an optional capacity as arguments The capacity, in number of elements, sets the size of the buffer in the channel ( non-blocking unless the buffer is full). If the capacity is zero or absent, the channel is unbuffered and communication succeeds ONLY when both a sender and receiver are ready ( blocking ). A nil channel is never ready for communication and blocks forever. Channels act as first-in-first-out queues. A channel may be closed with the built-in function close . cap and len can be called on a channel anywhere without additional synchronization A receive operation on a closed channel can always proceed immediately , yielding the element type's zero value after any previously sent values have been received. make ( chan int , 100 ) // creates a non-blocking channel with buffer size of 100 int values messages := make ( chan string ) // creates a blocking channel of type string for values go func () { messages <- \"ping\" }() // sends a value to the channel msg := <- messages // receives a value to the channel msg , more := <- messages // more is a boolean specify whether the buffer is empty close ( messages ) // close a channel; then the `more` value will be false if the channel `messages` has been closed and all values in the channel have already been received Simple synchronization using blocking channel package main import ( \"fmt\" \"time\" ) func worker ( done chan bool ) { fmt . Print ( \"working...\" ) time . Sleep ( time . Second ) fmt . Println ( \"done\" ) done <- true } func main () { done := make ( chan bool , 1 ) go worker ( done ) <- done // blocks until a value is received } Use a range loop to drain the elements in a buffered channel queue := make ( chan string , 2 ) queue <- \"one\" queue <- \"two\" close ( queue ) // it\u2019s possible to close a non-empty channel but still have the remaining values be received from its buffer for elem := range queue { fmt . Println ( elem ) } Reference/Pointers \u00b6 A pointer type denotes the set of all pointers to variables of a given type, called the base type of the pointer. The value of an uninitialized pointer is nil. Pass a pointer to a function will ensure the changes on the variable referenced by the pointer will be reflected everywhere that variable is used. func zeroptr ( iptr * int ) { * iptr = 0 // dereference the pointer and change its value } z := 100 zeroptr ( & z ) // pass in the pointer of variable 'z' Recursion \u00b6 Nothing big regarding writing recursive functions in golang func fact ( n int ) int { if n == 0 { return 1 } return n * fact ( n - 1 ) }","title":"Golang Language Reference"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#go-code-organization","text":"Go organizes code into a collection of files that would be compiled together, called packages . Functions, types, variables, and constants defined in one source file are visible to all other source files within the same package. Go packages that are related and released together are organized within a module . A repository typically contains only one module located at the root of the repo. go.mod decalres the module path A module's path serves as an import path prefix for its packages and indicates where the go command should look to download it A package's import path is its module path joined with its subdirectory within the module. i.e. the module github.com/google/go-cmp contains a package in the directory cmp/ . That package's import path is github.com/google/go-cmp/cmp","title":"Go code organization"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#go-command","text":"Create from scratch module_path = <module_path> mkdir \" $module_path \" go mod init \" $module_path \" A module path can be in the form of example.com/user/hello , where the part before the first / is the organization domain name, then follows a variable-sized path, and the part after the last / will be the module name . A simple program package main import ( \"fmt\" ) func main () { fmt . Println ( \"Hello, world.\" ) } The first statement in a Go source file must be package name. When building Executable binaries, its starting point must always use package main . Build a program go install \"$module_path\" will build and install the executable binary to $HOME/go/bin/module_name has to be within the source directory of $module_path go commands accept paths relative to the working directory go build will just compile and build the code within current directory The install directory is controlled by the $GOPATH and $GOBIN environment variables. If $GOBIN is set, binaries are installed to that directory use go env -w GOBIN=/somewhere/else/bin to dynamically change its value use go env -u GOBIN to unset it If $GOPATH is set, binaries are installed to the bin subdirectory of the first directory in the $GOPATH list. Otherwise, binaries are installed to the bin subdirectory of the default $GOPATH ( $HOME/go ) Run the binary after the build, like you would do with a Linux command. Alternatively, can use go run <main_package>.go to preview running the program before building it.","title":"Go command"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#use-packages-from-other-sources","text":"An import path can describe how to obtain the package source code using a revision control system such as Git. package main import ( \"fmt\" \"example.com/user/hello/morestrings\" \"github.com/google/go-cmp/cmp\" // load github.com/google/go-cmp/cmp in this program ) func main () { fmt . Println ( morestrings . ReverseRunes ( \"!oG ,olleH\" )) fmt . Println ( cmp . Diff ( \"Hello World\" , \"Hello Go\" )) // directly invoke functions defined in 'cmp' module } When you run commands like go install, go build, or go run , the go command will automatically download the remote module and record its version in your go.mod file. An interesting article about GO111MODULE. Module dependencies are automatically downloaded to the pkg/mod subdirectory of the directory indicated by the $GOPATH environment variable. To remove all downloaded modules, use go clean -modcache","title":"Use packages from other sources"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#testing","text":"Go has a lightweight test framework composed of the go test command and the testing package. A test file should be named ending with _test.go A test function should be named like TestXXX with signature like func (t *testing.T) If the function calls a failure function such as t.Error or t.Fail , the test is considered to have failed. package morestrings import \"testing\" func TestReverseRunes ( t * testing . T ) { cases := [] struct { in , want string }{ // shortcut to initialize anonymous struct { \"Hello, world\" , \"dlrow ,olleH\" }, { \"Hello, \u4e16\u754c\" , \"\u754c\u4e16 ,olleH\" }, { \"\" , \"\" }, } for _ , c := range cases { got := ReverseRunes ( c . in ) if got != c . want { t . Errorf ( \"ReverseRunes(%q) == %q, want %q\" , c . in , got , c . want ) } } } // run the tests go test go test - v // PASS // ok example.com/user/morestrings 0.165s","title":"Testing"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#commenting","text":"Go provides C-style /* */ block comments and C++-style // line comments","title":"Commenting"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#type-of-data","text":"A variable is a storage location for holding a value. The set of permissible values is determined by the variable's type. Go can infer the type of initialized variables so a type is optional var a = \"initial\" same as var a string = \"string value\" declare multiple variables in one line: var b, c int = 1, 2 shorthand for declaring and assigning value to a variable: f := \"apple\" A variable declaration or, for function parameters and results, the signature of a function declaration or function literal reserves storage for a named variable. Structured variables of array, slice, and struct types have elements and fields that may be addressed individually. Each such element acts like a variable. The static type of a variable is the type given in its declaration, the type provided in the new call or composite literal, or the type of an element of a structured variable. Variables of interface type also have a distinct dynamic type , which is the concrete type of the value assigned to the variable at run time var x interface {} // x is nil and has static type interface{} var v * T // v has value nil, static type *T x = 42 // x has value 42 and dynamic type int x = v // x has value (*T)(nil) and dynamic type *T type declarations: type ( A1 = string A2 = A1 ) Constants There are boolean constants, rune constants, integer constants, floating-point constants, complex constants, and string constants. There are no constants denoting the IEEE-754 negative zero, infinity, and not-a-number values A constant may be given a type explicitly by a constant declaration or conversion, or implicitly when used in a variable declaration or an assignment or as an operand in an expression. declares a constant with const s string = \"string constant\" const can be used anywhere var is used Numbers A numeric type represents sets of integer or floating-point values. uint8 the set of all unsigned 8 - bit integers ( 0 to 255 ) uint16 the set of all unsigned 16 - bit integers ( 0 to 65535 ) uint32 the set of all unsigned 32 - bit integers ( 0 to 4294967295 ) uint64 the set of all unsigned 64 - bit integers ( 0 to 18446744073709551615 ) int8 the set of all signed 8 - bit integers ( - 128 to 127 ) int16 the set of all signed 16 - bit integers ( - 32768 to 32767 ) int32 the set of all signed 32 - bit integers ( - 2147483648 to 2147483647 ) int64 the set of all signed 64 - bit integers ( - 9223372036854775808 to 9223372036854775807 ) float32 the set of all IEEE - 754 32 - bit floating - point numbers float64 the set of all IEEE - 754 64 - bit floating - point numbers complex64 the set of all complex numbers with float32 real and imaginary parts complex128 the set of all complex numbers with float64 real and imaginary parts byte alias for uint8 rune alias for int32 uint either 32 or 64 bits int same size as uint uintptr an unsigned integer large enough to store the uninterpreted bits of a pointer value Strings A string type represents the set of string values. String value is a sequence of bytes: \"go lang\" string length can be obtained from function len(str) chars within a string can be accessed with str[i] range can also be used on a string value to iterate through its chars Numbers<->Strings Operators and punctuation + & += &= && == != ( ) - | -= |= || < <= [ ] * ^ *= ^= <- > >= { } / << /= <<= ++ = := , ; % >> %= >>= -- ! ... . : &^ &^= Keywords reserved break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var Arrays is a numbered sequence of elements of a specific length. var a [ 5 ] int // declare an array a [ 4 ] = 100 // access specific index b := [ 5 ] int { 1 , 2 , 3 , 4 , 5 } // declare and assign with initialized list of elements var twoD [ 2 ][ 3 ] int // declare an 2D array p := [ 1000 ] * float64 // array of float64 pointers for _ , num := range nums { // use range on array sum += num } Slices is a descriptor for a contiguous segment of an underlying array and provides access to a numbered sequence of elements from that array. A slice can be taken from another slice or from an array b := [ 5 ] int { 11 , 22 , 33 , 44 , 55 } l := b [ 2 : 5 ] // in fact takes indexes from [start:end), so 2-4 in this case l = b [: 4 ] // takes indexes 0-3 in this case l = b [ 2 :] // takes indexes 2-4 in this case l = [] string { \"g\" , \"h\" } // declare and initialize a slice c := make ([] int , len ( b )) // make a slice with initial length copy ( c , b ) // copy values from another slice c = append ( c , 100 , 200 ) // can dynamically add elements to a slice Maps are Go\u2019s built-in associative data type aka hashes or dicts in other language m := make ( map [ string ] int ) // create a map with [key-type]value-type m [ \"key1\" ] = 1 // access key and modify value val , exists := m [ \"key2\" ] // 2nd value is a boolean indicates whether key2 defined len ( m ) // gives the number of key/value pairs delete ( m , \"key2\" ) // deletes a key/value pair for k , v := range m { // use range on map fmt . Printf ( \"%s -> %s\\n\" , k , v ) } Structs are mutable and typed collections of fields useful for grouping data together to form records. A field declaration may be followed by an optional string literal tag , which becomes an attribute for all the fields in the corresponding field declaration. An empty tag string is equivalent to an absent tag. The tags are made visible through a reflection interface and take part in type identity for structs but are otherwise ignored. Go supports methods defined on struct types. An embedded field must be specified as a type name T or as a pointer to a non-interface type name *T, and T itself may not be a pointer type. The unqualified type name acts as the field name. field names must be unique in a struct type, so cannot have more than one embedded field of the same type type person struct { name string age int \"tag string for age\" T1 // embedded field of type T1 } func newPerson ( name string ) * person { p := person { name : name } // specify values to the variables in the struct p . age = 42 // explicitly change the value return & p } Zero values When storage is allocated for a variable, either through a declaration or a call of new , or when a new value is created, either through a composite literal or a call of make , and no explicit initialization is provided, the variable or value is given a default value . false for booleans , 0 for numeric types, \"\" for strings , and nil for pointers, functions, interfaces, slices, channels, and maps . This initialization is done recursively.","title":"Type of Data"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#conditions","text":"If-else condition block is just like other programming language. There is no ternary form, however. Note that you don\u2019t need parentheses around conditions in Go. This is true for loops as well. // A statement can precede conditionals; // any variables declared here are available in all branches if num := f (); num < 0 { fmt . Println ( num , \"is negative\" ) } else if num < 10 { fmt . Println ( num , \"has 1 digit\" ) } else { fmt . Println ( num , \"has multiple digits\" ) } Switch block conditions are evaluated left-to-right and top-to-bottom the first one that equals the switch expression triggers execution of the statements of the associated case; the other cases are skipped default condition can appear anywhere in the block use keyword fallthrough to fallthrough switch code := f (); { default : s3 () // can appear anywhere in this block case 0 , 1 , 2 , 3 : s1 () // there is a hidden break for each case case 4 , 5 , 6 , 7 : fallthrough // is necessary to fallthrough to next case case 8 , 9 , 10 , 11 : s2 () s3 () }","title":"Conditions"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#loops","text":"for is Go\u2019s only looping construct. for j := 7 ; j <= 9 ; j ++ { fmt . Println ( j ) } for loop used in a while fashion: i := 1 for i <= 30 { if i % 2 == 0 { continue } fmt . Println ( i ) i = i + 1 } while true fashion loop terminates with break for { fmt . Println ( \"loop\" ) break } for with range; like a for-each loop but with access to index var a [ 10 ] string for i , s := range a { // i is index; s is a[i] g ( i , s ) // do something for that values }","title":"Loops"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#functions","text":"A few things special in golang's functions : A function can be assigned to a variable or invoked directly. A function can return another function A function can accept a variable number of arguments of the same type and saved as an array variable in the function body. it has to be the last argument type in the function signature There can be multiple return values from a function If no return values expected, can omit the return type func ( a , b int , z float64 ) bool { // return value is defined outside parentheses return a * b < int ( z ) } f := func ( x , y int ) ( int , int ) { return x + y , x - y } func ( s ... string ) { // variadic parameter list // can be invoked with zero or more arguments for that parameter. } Function literals are closures : they may refer to variables defined in a surrounding function. func intSeq () func () int { // returns the anonymous function i := 0 return func () int { i ++ return i } // this function closes over the variable i to form a closure // it captures its own i value, which will be updated each time it is called // until a new instance of this anonymous function is obtained from calling intSeq() }","title":"Functions"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#special-statements","text":"A select statement chooses which of a set of possible send or receive operations will proceed. It looks similar to a switch statement but with the cases all referring to communication operations. Some rules: For all the cases in the statement, the channel operands of receive operations and the channel and right-hand-side expressions of send statements are evaluated exactly once evaluation will occur irrespective of which (if any) communication operation is selected to proceed. If one or more of the communications can proceed, a single one that can proceed is chosen via a uniform pseudo-random selection. Unless the selected case is the default case, the respective communication operation is executed. If the selected case is a RecvStmt with a short variable declaration or an assignment, the left-hand side expressions are evaluated and the received value (or values) are assigned. The statement list of the selected case is executed. Go\u2019s select lets you wait on multiple channel operations. Combining goroutines and channels with select is a powerful feature of Go. package main import ( \"fmt\" \"time\" ) func main () { c1 := make ( chan string ) c2 := make ( chan string ) go func () { time . Sleep ( 1 * time . Second ) c1 <- \"one\" }() go func () { time . Sleep ( 2 * time . Second ) c2 <- \"two\" }() for i := 0 ; i < 2 ; i ++ { select { // blocks until either one of the channels got a message case msg1 := <- c1 : fmt . Println ( \"received\" , msg1 ) case msg2 := <- c2 : fmt . Println ( \"received\" , msg2 ) } } } A defer statement is used to ensure that a function call is performed later at the end of the current function scope, usually for purposes of cleanup . Check for errors in a deferred function, if applicable. func main () { f := createFile ( \"/tmp/defer.txt\" ) defer closeFile ( f ) // will be executed at the end of the enclosing function (main) writeFile ( f ) }","title":"Special Statements"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#methods","text":"Methods differ from functions in that it takes a receiver type The receiver is specified via an extra parameter section preceding the method name. receiver type can be a single defined type that is either pointer or value and non-variadic parameter type must be defined in the same package as the method the method is said to be bound to its receiver base type and the method name is visible only within selectors for type T or *T. the non-blank identifier (receiver name) must be unique in the method signature (against other arguments) method can still take in arguments like a function does Go automatically handles conversion between values and pointers for method calls The type of a method is the type of a function with the receiver as first argument i.e. func(r *rect) func ( r * rect ) area () int { // prefered for most cases return r . width * r . height } func ( r rect ) perim () int { return 2 * ( r . width + r . height ) } r = rect { width : 10 , height : 5 } r . area () // auto convert to pointer type and pass in `r` r . perim () A type may have a method set associated with it. In a method set, each method must have a unique non-blank method name. The method set of a type determines the interfaces that the type implements and the methods that can be called using a receiver of that type. The method set of an interface type is its interface. The method set of any other type T consists of all methods declared with receiver type T. The method set of the corresponding pointer type T is the set of all methods declared with receiver T or T Any other type has an empty method set.","title":"Methods"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#interfaces","text":"Interfaces are named collections of method signatures type geometry interface { area () float64 perim () float64 } To implement an interface in Go, we just need to implement all the methods in the interface. type rect struct { width , height float64 } func ( r rect ) area () float64 { return r . width * r . height } func ( r rect ) perim () float64 { return 2 * r . width + 2 * r . height } If a variable has an interface type, then we can call methods that are in the named interface. func measure ( g geometry ) { fmt . Println ( g ) fmt . Println ( g . area ()) fmt . Println ( g . perim ()) } func main () { r := rect { width : 3 , height : 4 } measure ( r ) } The interface{} type is the interface that has no methods. So essentially all types implements this interface. If you write a function that takes an interface{} value as a parameter, you can supply that function with any value.","title":"Interfaces"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#exceptions-and-errors","text":"By convention, errors are the last return value and have type error , a built-in interface . type error interface { Error () string } Can construct a basic error value with errors.New(\"Error Message\") func foo ( arg int ) ( int , error ) { if arg < 0 { return - 1 , errors . New ( \"invalid input\" ) // constructs a basic error value with the given error message. } return arg + 3 , nil // A nil value in the error position indicates that there was no error. } It\u2019s possible to use custom types as errors by implementing the Error() method on them. type argError struct { arg int prob string } func ( e * argError ) Error () string { return fmt . Sprintf ( \"%d - %s\" , e . arg , e . prob ) } func foo2 ( arg int ) ( int , error ) { if arg < 0 { return - 1 , & argError { arg , \"can't work with it\" } // build a new struct } return arg + 3 , nil } // consume the error if v , e := foo ( - 5 ); e != nil { fmt . Println ( \"foo failed:\" , e ) } else { fmt . Println ( \"foo worked:\" , v ) } // or v , e := foo2 ( - 5 ) if errorInstance , ok := e .( * argError ); ok { // here used a syntax to dereference the error pointer back to its instance fmt . Println ( errorInstance . arg ) fmt . Println ( errorInstance . prob ) } Panics typically means something went unexpectedly wrong. The program should fail fast when this type of error happens. When calling panic(message) inside a function, it will immediate exit the current function, then waits for all other deferred functions in the callstack to run, then the program is terminated and error condition reported. A recover function allows a program to manage behavior of a panicking goroutine. func protect ( g func ()) { defer func () { log . Println ( \"done\" ) // Println executes normally even if there is a panic if x := recover (); x != nil { log . Printf ( \"run time panic: %v\" , x ) } }() log . Println ( \"start\" ) g () // run-time panics raised will be protected by the recover() in the deferred function } The return value of recover is nil if any of the following conditions holds: panic's argument was nil the goroutine is not panicking recover was not called directly by a deferred function. Note that unlike some languages which use exceptions for handling of many errors, in Go it is idiomatic to use error-indicating return values wherever possible. Run-time panics Execution errors such as attempting to index an array out of bounds trigger a run-time panic equivalent to a call of the built-in function panic with a value of the implementation-defined interface type runtime.Error . That type satisfies the predeclared interface type error . The exact error values that represent distinct run-time error conditions are unspecified. package runtime type Error interface { error // and perhaps other methods } Read more https://blog.golang.org/error-handling-and-go","title":"Exceptions and Errors"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#threading-and-goroutines","text":"A go statement starts the execution of a function call as an independent concurrent thread/process of control, or goroutine , within the same address space . Unlike with a regular function call, program execution does not wait for the invoked function to complete ( non-blocking ) by the goroutine. The expression must be a function or method call; it cannot be parenthesized. Cannot start goroutine on built-in functions and expression statements . f ( \"direct\" ) // invoke a function go f ( \"goroutine\" ) // start a goroutine calling a function go func ( msg string ) { // start a goroutine calling an anonymous function fmt . Println ( msg ) }( \"going\" )","title":"Threading and Goroutines"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#pipes-and-channels","text":"Channels are the pipe-equivalent in golang that connect concurrent goroutines . You can send values into channels from one goroutine and receive those values into another goroutine, of a specified element type for the values passed. chan T // can be used to send and receive values of type T chan <- float64 // can only be used to send float64s <- chan int // can only be used to receive ints The optional <- operator specifies the channel direction, send or receive . If no direction is given, the channel is bidirectional . A channel may be constrained only to send or only to receive by assignment or explicit conversion . A new, initialized channel value can be made using the built-in function make , which takes the channel type and an optional capacity as arguments The capacity, in number of elements, sets the size of the buffer in the channel ( non-blocking unless the buffer is full). If the capacity is zero or absent, the channel is unbuffered and communication succeeds ONLY when both a sender and receiver are ready ( blocking ). A nil channel is never ready for communication and blocks forever. Channels act as first-in-first-out queues. A channel may be closed with the built-in function close . cap and len can be called on a channel anywhere without additional synchronization A receive operation on a closed channel can always proceed immediately , yielding the element type's zero value after any previously sent values have been received. make ( chan int , 100 ) // creates a non-blocking channel with buffer size of 100 int values messages := make ( chan string ) // creates a blocking channel of type string for values go func () { messages <- \"ping\" }() // sends a value to the channel msg := <- messages // receives a value to the channel msg , more := <- messages // more is a boolean specify whether the buffer is empty close ( messages ) // close a channel; then the `more` value will be false if the channel `messages` has been closed and all values in the channel have already been received Simple synchronization using blocking channel package main import ( \"fmt\" \"time\" ) func worker ( done chan bool ) { fmt . Print ( \"working...\" ) time . Sleep ( time . Second ) fmt . Println ( \"done\" ) done <- true } func main () { done := make ( chan bool , 1 ) go worker ( done ) <- done // blocks until a value is received } Use a range loop to drain the elements in a buffered channel queue := make ( chan string , 2 ) queue <- \"one\" queue <- \"two\" close ( queue ) // it\u2019s possible to close a non-empty channel but still have the remaining values be received from its buffer for elem := range queue { fmt . Println ( elem ) }","title":"Pipes and Channels"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#referencepointers","text":"A pointer type denotes the set of all pointers to variables of a given type, called the base type of the pointer. The value of an uninitialized pointer is nil. Pass a pointer to a function will ensure the changes on the variable referenced by the pointer will be reflected everywhere that variable is used. func zeroptr ( iptr * int ) { * iptr = 0 // dereference the pointer and change its value } z := 100 zeroptr ( & z ) // pass in the pointer of variable 'z'","title":"Reference/Pointers"},{"location":"Programming-Lang-Reference/Golang/Golang-Specs/#recursion","text":"Nothing big regarding writing recursive functions in golang func fact ( n int ) int { if n == 0 { return 1 } return n * fact ( n - 1 ) }","title":"Recursion"},{"location":"Programming-Lang-Reference/Java/Best-Practices/","text":"Chapter 2. Creating and Destroying Objects \u00b6 Consider static factory methods instead of constructors A class can provide a public static factory method, which is simply a static method that returns an instance of the class. A static factory method is not the same as the Factory Method pattern from Design Patterns Advantages : can use well-chosen name to make code easier to read not required to create a new object each time invoked save resource on expensive-to-create objects can return an object of any subtype of their return type requires the client to refer to the returned object by interface rather than implementation class an API can return objects without making their classes public, hiding implementation classes the class of the returned object can vary from call to call as a function of the input parameters the class of the returned object need not exist when the class containing the method is written Disadvantages : classes without public or protected constructors cannot be subclassed static factory methods are hard for programmers to find do not stand out in API documentation Side note about service provider framework \u00b6 There are three essential components in a service provider framework: a service interface, which represents an implementation; a provider registration API, which providers use to register implementations; and a service access API, which clients use to obtain instances of the service. The service access API may allow clients to specify criteria for choosing an implementation. In the absence of such criteria, the API returns an instance of a default implementation, or allows the client to cycle through all available implementations. The service access API is the flexible static factory that forms the basis of the service provider framework. An optional fourth component of a service provider framework is a service provider interface, which describes a factory object that produces instances of the service interface. In the absence of a service provider interface, implementations must be instantiated reflectively. In the case of JDBC, Connection plays the part of the service interface, DriverManager.registerDriver is the provider registration API, DriverManager.getConnection is the service access API, and Driver is the service provider interface. Some common names for static factory methods \u00b6 from \u2014 A type-conversion method that takes a single parameter and returns a corresponding instance of this type. i.e. Date.from(instant) of \u2014 An aggregation method that takes multiple parameters and returns an instance of this type that incorporates them, i.e. EnumSet.of(JACK, QUEEN, KING) valueOf \u2014 A more verbose alternative to from and of , i.e. BigInteger.valueOf(Integer.MAX_VALUE) instance or getInstance \u2014 Returns an instance that is described by its parameters (if any) but cannot be said to have the same value. i.e. StackWalker.getInstance(options) create or newInstance \u2014 Like instance or getInstance , except that the method guarantees that each call returns a new instance, i.e. Array.newInstance(classObject, arrayLen) getType \u2014 Like getInstance , but used if the factory method is in a different class. Type is the type of object returned by the factory method. i.e. Files.getFileStore(path) newType \u2014 Like newInstance, but used if the factory method is in a different class. Type is the type of object returned by the factory method. i.e. Files.newBufferedReader(path) type \u2014 A concise alternative to getType and newType. i.e. Collections.list(legacyLitany) Consider a builder when faced with many constructor parameters Instead of making the desired object directly, the client calls a constructor (or static factory) with all of the required parameters and gets a builder object. Then the client calls setter-like methods on the builder object to set each optional parameter of interest. Finally, the client calls a parameterless build method to generate the object, which is typically immutable. To detect invalid parameters as soon as possible, check parameter validity in the builder\u2019s constructor and methods. Check invariants involving multiple parameters in the constructor invoked by the build method. To ensure these invariants against attack, do the checks on object fields after copying parameters from the builder. If a check fails, throw an IllegalArgumentException . The Builder pattern is well suited to class hierarchies. Use a parallel hierarchy of builders, each nested in the corresponding class. public abstract class Pizza { public enum Topping { HAM , MUSHROOM , ONION , PEPPER , SAUSAGE } final Set < Topping > toppings ; abstract static class Builder < T extends Builder < T >> { // generic type with a recursive type parameter EnumSet < Topping > toppings = EnumSet . noneOf ( Topping . class ); public T addTopping ( Topping topping ) { toppings . add ( Objects . requireNonNull ( topping )); return self (); } abstract Pizza build (); // Subclasses must override this method to return \"this\" protected abstract T self (); } Pizza ( Builder <?> builder ) { toppings = builder . toppings . clone (); // See Item 50 } } public class NyPizza extends Pizza { public enum Size { SMALL , MEDIUM , LARGE } private final Size size ; public static class Builder extends Pizza . Builder < Builder > { private final Size size ; public Builder ( Size size ) { this . size = Objects . requireNonNull ( size ); } @Override public NyPizza build () { return new NyPizza ( this ); } @Override protected Builder self () { return this ; } } private NyPizza ( Builder builder ) { super ( builder ); size = builder . size ; } } NyPizza pizza = new NyPizza . Builder ( SMALL ). addTopping ( SAUSAGE ). addTopping ( ONION ). build (); the Builder pattern is a good choice when designing classes whose constructors or static factories would have more than a handful of parameters, especially if many of the parameters are optional or of identical type. Enforce the singleton property with a private constructor or an enum type Making a class a singleton can make it difficult to test its clients because it\u2019s impossible to substitute a mock implementation for a singleton unless it implements an interface that serves as its type. public class Elvis { // exactly one Elvis instance will exist once the Elvis class is initialized\u2014no more, no less public static final Elvis INSTANCE = new Elvis (); private Elvis () { ... } ... } The above approach is simple and guarantees a singleton instance. // another approach public class Elvis { private static final Elvis INSTANCE = new Elvis (); private Elvis () { ... } // All calls to Elvis.getInstance return the same object reference, and no other Elvis instance will ever be created public static Elvis getInstance () { return INSTANCE ; } } The above approach gives the flexibility to change mind for singleton, allows us write generic singleton factory, and a method reference can be used as a supplier i.e. Elvis::instance // a third way public enum Elvis { INSTANCE ; ... } The above approach is very concise, with serialization. To make sure even after the object is deserialized still a singleton, public class Elvis implements Serializable { private static final transient Elvis INSTANCE = new Elvis (); ... private Object readResolve () { return INSTANCE ; } } A single-element enum type is often the best way to implement a singleton. Note that you can\u2019t use this approach if your singleton must extend a superclass. Enforce noninstantiability with a private constructor a class can be made noninstantiable by including a private constructor. Prefer dependency injection to hardwiring resources Static utility classes and singletons are inappropriate for classes whose behavior is parameterized by an underlying resource. Dependency Injection: Pass the resource into the constructor when creating a new instance. It preserves immutability, and is equally applicable to constructors, static factories, and builders. A useful variant of the pattern is to pass a resource factory to the constructor. The Supplier interface, introduced in Java 8, is perfect for representing factories. Methods that take a Supplier on input should typically constrain the factory\u2019s type parameter using a bounded wildcard type to allow the client to pass in a factory that creates any subtype of a specified type. i.e. Mosaic create(Supplier<? extends Tile> tileFactory) { ... } In summary, do not use a singleton or static utility class to implement a class that depends on one or more underlying resources whose behavior affects that of the class, and do not have the class create these resources directly. Instead, pass the resources, or factories to create them, into the constructor (or static factory or builder). Avoid creating unnecessary objects Reuse can be both faster and more stylish. An object can always be reused if it is immutable. Some object creations are much more expensive than others. If you\u2019re going to need such an \"expensive object\" repeatedly, it may be advisable to cache it for reuse. One good example: static boolean isRomanNumeral ( String s ) { return s . matches ( \"^(?=.)M*(C[MD]|D?C{0,3})\" + \"(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\" ); } is expensive to create a Pattern Object. As the method is called repeatedly, more Pattern Objects are created. Better: public class RomanNumerals { private static final Pattern ROMAN = Pattern . compile ( \"^(?=.)M*(C[MD]|D?C{0,3})\" + \"(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\" ); static boolean isRomanNumeral ( String s ) { return ROMAN . matcher ( s ). matches (); } } Aside from the performance improvement, making a static final field for the otherwise invisible Pattern instance allows us to give it a name, which is far more readable than the regular expression itself. Prefer primitives to boxed primitives, and watch out for unintentional autoboxing. Avoiding object creation by maintaining your own object pool is a bad idea unless the objects in the pool are extremely heavyweight. Generally speaking, however, maintaining your own object pools clutters your code, increases memory footprint, and harms performance. Modern JVM implementations have highly optimized garbage collectors that easily outperform such object pools on lightweight objects. Eliminate obsolete object references A prog. lang with garbage collector does not mean you don't need to worry about memory management. The best way to eliminate an obsolete reference is to let the variable that contained the reference fall out of scope. This occurs naturally if you define each variable in the narrowest possible scope. Generally speaking, whenever a class manages its own memory, the programmer should be alert for memory leaks. Whenever an element is freed, any object references contained in the element should be nulled out. Another common source of memory leaks is caches. A third common source of memory leaks is listeners and other callbacks, which happens when clients register callbacks but don\u2019t deregister them explicitly. One way to ensure that callbacks are garbage collected promptly is to store only weak references to them, for instance, by storing them only as keys in a WeakHashMap. Avoid finalizers and cleaners Finalizers are unpredictable, often dangerous, and generally unnecessary. Cleaners are less dangerous than finalizers, but still unpredictable, slow, and generally unnecessary. It can take arbitrarily long between the time that an object becomes unreachable and the time its finalizer or cleaner runs. This means that you should never do anything time-critical in a finalizer or cleaner. And you should never depend on a finalizer or cleaner to update persistent state. Just have your class implement AutoCloseable, and require its clients to invoke the close method on each instance when it is no longer needed, typically using try-with-resources to ensure termination even in the face of exceptions. One detail worth mentioning is that the instance must keep track of whether it has been closed: the close method must record in a field that the object is no longer valid, and other methods must check this field and throw an IllegalStateException if they are called after the object has been closed. So what, if anything, are cleaners and finalizers good for? They have perhaps two legitimate uses. One is to act as a safety net in case the owner of a resource neglects to call its close method. A second legitimate use of cleaners concerns objects with native peers. A native peer is a native (non-Java) object to which a normal object delegates via native methods. Because a native peer is not a normal object, the garbage collector doesn\u2019t know about it and can\u2019t reclaim it when its Java peer is reclaimed. A cleaner or finalizer may be an appropriate vehicle for this task. How to write a cleaner public class Room implements AutoCloseable { private static final Cleaner cleaner = Cleaner . create (); // Resource that requires cleaning. Must not refer to Room! private static class State implements Runnable { int numJunkPiles ; // Number of junk piles in this room State ( int numJunkPiles ) { this . numJunkPiles = numJunkPiles ; } // Invoked by close method or cleaner @Override public void run () { System . out . println ( \"Cleaning room\" ); numJunkPiles = 0 ; } } // The state of this room, shared with our cleanable private final State state ; // Our cleanable. Cleans the room when it\u2019s eligible for gc private final Cleaner . Cleanable cleanable ; public Room ( int numJunkPiles ) { state = new State ( numJunkPiles ); cleanable = cleaner . register ( this , state ); } @Override public void close () { cleanable . clean (); } } State implements Runnable, and its run method is called at most once, by the Cleanable that we get when we register our State instance with our cleaner in the Room constructor. The call to the run method will be triggered by one of two things: Usually it is triggered by a call to Room\u2019s close method calling Cleanable\u2019s clean method. If the client fails to call the close method by the time a Room instance is eligible for garbage collection, the cleaner will (hopefully) call State\u2019s run method. It is critical that a State instance does not refer to its Room instance. If it did, it would create a circularity that would prevent the Room instance from becoming eligible for garbage collection. If clients surround all Room instantiations in try-with-resource blocks, automatic cleaning will never be required. public class Adult { public static void main ( String [] args ) {} try ( Room myRoom = new Room ( 7 )) { System . out . println ( \"Goodbye\" ); } catch () { } } } In summary, don\u2019t use cleaners, or in releases prior to Java 9, finalizers, except as a safety net or to terminate noncritical native resources. Even then, beware the indeterminacy and performance consequences. Prefer try-with-resources to try-finally Historically, a try-finally statement was the best way to guarantee that a resource would be closed properly, even in the face of an exception or return. // try-finally - No longer the best way to close resources! static String firstLineOfFile ( String path ) throws IOException { BufferedReader br = new BufferedReader ( new FileReader ( path )); try { return br . readLine (); } finally { br . close (); } } However, the code in both the try block and the finally block is capable of throwing exceptions. Java 7 introduced the try-with-resources statement. To be usable with this construct, a resource must implement the AutoCloseable interface, which consists of a single void-returning close method. static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader(new FileReader(path))) { return br.readLine(); } } Not only are the try-with-resources versions shorter and more readable than the originals, but they provide far better diagnostics. Multiple exceptions may be suppressed in order to preserve the exception that you actually want to see. Chapter 3. Methods Common to All Objects \u00b6 ALTHOUGH Object is a concrete class, it is designed primarily for extension. All of its nonfinal methods (equals, hashCode, toString, clone, and finalize) have explicit general contracts because they are designed to be overridden. Obey the general contract when overriding EQUALS The easiest way to avoid problems is not to override the equals method, in which case each instance of the class is equal only to itself. This applies when: Each instance of the class is inherently unique. There is no need for the class to provide a \"logical equality\" test. A superclass has already overridden equals, and the superclass behavior is appropriate for this class. The class is private or package-private, and you are certain that its equals method will never be invoked. It is appropriate to override equals when a class has a notion of logical equality that differs from mere object identity and a superclass has not already overridden equals, generally the case for value classes. A value class is simply a class that represents a value, such as Integer or String. Overriding equals enables instances to serve as map keys or set elements with predictable, desirable behavior. One kind of value class that does not require the equals method to be overridden is a class that uses instance control to ensure that at most one object exists with each value. Enum types fall into this category. For these classes, logical equality is the same as object identity, so Object\u2019s equals method functions as a logical equals method. equals implements an equivalence relation. It has following properties: Reflexive: For any non-null reference value x, x.equals(x) must return true. Symmetric: For any non-null reference values x and y, x.equals(y) must return true if and only if y.equals(x) returns true. Transitive: For any non-null reference values x, y, z, if x.equals(y) returns true and y.equals(z) returns true, then x.equals(z) must return true. Consistent: For any non-null reference values x and y, multiple invocations of x.equals(y) must consistently return true or consistently return false, provided no information used in equals comparisons is modified. For any non-null reference value x, x.equals(null) must return false. Once you\u2019ve violated the equals contract, you simply don\u2019t know how other objects will behave when confronted with your object. There is no way to extend an instantiable class and add a value component while preserving the equals contract, unless you\u2019re willing to forgo the benefits of object-oriented abstraction. While there is no satisfactory way to extend an instantiable class and add a value component, there is a fine workaround: \"Favor composition over inheritance.\" Instead of having ColorPoint extend Point, give ColorPoint a private Point field and a public view method that returns the point at the same position as this color point. // Adds a value component without violating the equals contract public class ColorPoint { private final Point point ; private final Color color ; public ColorPoint ( int x , int y , Color color ) { point = new Point ( x , y ); this . color = Objects . requireNonNull ( color ); } public Point asPoint () { return point ; } @Override public boolean equals ( Object o ) { if ( ! ( o instanceof ColorPoint )) return false ; ColorPoint cp = ( ColorPoint ) o ; return cp . point . equals ( point ) && cp . color . equals ( color ); } ... // Remainder omitted } There are some classes in the Java platform libraries that do extend an instantiable class and add a value component. For example, java.sql.Timestamp extends java.util.Date and adds a nanoseconds field. The equals implementation for Timestamp does violate symmetry and can cause erratic behavior if Timestamp and Date objects are used in the same collection or are otherwise intermixed. Here is the recipe for a high-quality equals method: Use the == operator to check if the argument is a reference to this object Use the instanceof operator to check if the argument has the correct type Cast the argument to the correct type For each \"significant\" field in the class, check if that field of the argument matches the corresponding field of this object. For primitive fields whose type is not float or double, use the == operator for comparisons; for object reference fields, call the equals method recursively; for float fields, use the static Float.compare(float, float) method; and for double fields, use Double.compare(double, double). The special treatment of float and double fields is made necessary by the existence of Float.NaN, -0.0f and the analogous double values Some object reference fields may legitimately contain null. To avoid the possibility of a NullPointerException, check such fields for equality using the static method Objects.equals(Object, Object) For best performance, you should first compare fields that are more likely to differ, less expensive to compare, or, ideally, both. When you are finished writing your equals method, ask yourself three questions: Is it symmetric? Is it transitive? Is it consistent? Consistent use of the @Override annotation An excellent alternative to writing and testing these methods manually is to use Google\u2019s open source AutoValue framework, which automatically generates these methods for you, triggered by a single annotation on the class. Always override HASHCODE when you override EQUALS If you fail to do so, your class will violate the general contract for hashCode, which will prevent it from functioning properly in collections such as HashMap and HashSet: When the hashCode method is invoked on an object repeatedly during an execution of an application, it must consistently return the same value If two objects are equal according to the equals(Object) method, then calling hashCode on the two objects must produce the same integer result If two objects are unequal according to the equals(Object) method, it is not required that calling hashCode on each of the objects must produce distinct results (although it helps improve performance of hash tables) Receipe for fair good hashcode: Declare an int variable named result, and initialize it to the hash code c for the first significant field in your object For every remaining significant field f in your object, computer an int hash code c : if the field is a primitive type, compute Type.hashCode(f) if the field is an Object and this class\u2019s equals method compares the field by recursively invoking equals, recursively invoke hashCode on the field; if a more complex comparison is required, compute a \"canonical representation\" for this field and invoke hashCode on the canonical representation; if the value of the field is null, use 0 If the field is an array, treat it as if each significant element were a separate field. Do this for each hashcode c computed result = 31 * result + c; return result; Some other advices: If a class is immutable and the cost of computing the hash code is significant, you might consider caching the hash code in the object rather than recalculating it each time it is requested. Do not be tempted to exclude significant fields from the hash code computation to improve performance. Don\u2019t provide a detailed specification for the value returned by hashCode, so clients can\u2019t reasonably depend on it; this gives you the flexibility to change it. The AutoValue framework provides a fine alternative to writing equals and hashCode methods manually Always override TOSTRING The returned string of toString should be \"a concise but informative representation that is easy for a person to read.\" Providing a good toString implementation makes your class much more pleasant to use and makes systems using the class easier to debug. When practical, the toString method should return all of the interesting information contained in the object. It is recommended to specify the format of the return value for value classes. If you specify the format, it\u2019s usually a good idea to also provide a matching static factory or constructor so programmers can easily translate back and forth between the object and its string representation. Whether or not you decide to specify the format, you should clearly document your intentions. Whether or not you specify the format, provide programmatic access to the information contained in the value returned by toString. It makes no sense to write a toString method in a static utility class. Nor should you write a toString method in most enum types because Java provides a perfectly good one for you. You should, however, write a toString method in any abstract class whose subclasses share a common string representation. Override CLONE with Causion The Cloneable interface determines the behavior of Object\u2019s protected clone implementation: if a class implements Cloneable, Object\u2019s clone method returns a field-by-field copy of the object; otherwise it throws CloneNotSupportedException. Normally, implementing an interface says something about what a class can do for its clients. In this case, it modifies the behavior of a protected method on a superclass, which is very rare. A class implementing Cloneable is expected to provide a properly functioning public clone method. Note that immutable classes should never provide a clone method because it would merely encourage wasteful copying. @Override public PhoneNumber clone () { try { return ( PhoneNumber ) super . clone (); } catch ( CloneNotSupportedException e ) { throw new AssertionError (); // Can't happen } } In effect, the clone method functions as a constructor; you must ensure that it does no harm to the original object and that it properly establishes invariants on the clone. @Override public Stack clone () { try { Stack result = ( Stack ) super . clone (); result . elements = elements . clone (); return result ; } catch ( CloneNotSupportedException e ) { throw new AssertionError (); } } In order to make a class cloneable, it may be necessary to remove final modifiers from some fields. Java supports covariant return types. In other words, an overriding method\u2019s return type can be a subclass of the overridden method\u2019s return type. In effect, the clone method functions as a constructor; you must ensure that it does no harm to the original object and that it properly establishes invariants on the clone. Public clone methods should omit the throws clause, as methods that don\u2019t throw checked exceptions are easier to use. To prevent a subclass from implementing a working clone method, @Override protected final Object clone () throws CloneNotSupportedException { throw new CloneNotSupportedException (); } If you write a thread-safe class that implements Cloneable, remember that its clone method must be properly synchronized, just like any other method. A better approach to object copying is to provide a copy constructor or copy factory. A copy constructor is simply a constructor (or static method) that takes a single argument whose type is the class containing the constructor. Consider Implementing COMPARABLE It is the sole method in the Comparable interface but it permits order comparisons in addition to simple equality comparisons, and it is generic. By implementing Comparable, a class indicates that its instances have a natural ordering. Sorting an array of objects that implement Comparable is as simple as this: Arrays.sort(a); Returns a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. Throws ClassCastException if the specified object\u2019s type prevents it from being compared to this object. The implementor must ensure that sgn(x.compareTo(y)) == -sgn(y. compareTo(x)) for all x and y. (This implies that x.compareTo(y) must throw an exception if and only if y.compareTo(x) throws an exception.) The implementor must also ensure that the relation is transitive: (x. compareTo(y) > 0 && y.compareTo(z) > 0) implies x.compareTo(z) > 0. Finally, the implementor must ensure that x.compareTo(y) == 0 implies that sgn(x.compareTo(z)) == sgn(y.compareTo(z)), for all z. It is strongly recommended, but not required, that (x.compareTo(y) == 0) == (x.equals(y)) In Java 7 , static compare methods were added to all of Java\u2019s boxed primitive classes. Use of the relational operators < and > in compareTo methods is verbose and error-prone and no longer recommended. If a class has multiple significant fields, the order in which you compare them is critical. Start with the most significant field and work your way down. If a comparison results in anything other than zero (which represents equality), you\u2019re done. public int compareTo ( PhoneNumber pn ) { int result = Short . compare ( areaCode , pn . areaCode ); if ( result == 0 ) { result = Short . compare ( prefix , pn . prefix ); if ( result == 0 ) result = Short . compare ( lineNum , pn . lineNum ); } return result ; } In Java 8 , the Comparator interface was outfitted with a set of comparator construction methods, which enable fluent construction of comparators. These comparators can then be used to implement a compareTo method private static final Comparator < PhoneNumber > COMPARATOR = comparingInt (( PhoneNumber pn ) -> pn . areaCode ) . thenComparingInt ( pn -> pn . prefix ) . thenComparingInt ( pn -> pn . lineNum ); public int compareTo ( PhoneNumber pn ) { return COMPARATOR . compare ( this , pn ); } // alternative static Comparator < PhoneNumber > hashCodeOrder = new Comparator <> () { public int compare ( PhoneNumber o1 , PhoneNumber o2 ) { int result = Short . compare ( areaCode , o2 . areaCode ); if ( result == 0 ) { result = Short . compare ( prefix , pn . prefix ); if ( result == 0 ) result = Short . compare ( lineNum , pn . lineNum ); } return result ; } }; The method comparingInt is a static method that takes a key extractor function that maps an object reference to a key of type int and returns a comparator that orders instances according to that key. The method thenComparingInt is an instance method on Comparator that takes an int key extractor function, and returns a comparator that first applies the original comparator and then uses the extracted key to break ties. Classes implement the Comparable interface produce instances that can be easily sorted, searched, and used in comparison-based collections. When comparing field values in the implementations of the compareTo methods, avoid the use of the < and > operators. Instead, use the static compare methods in the boxed primitive classes or the comparator construction methods in the Comparator interface. Chapter 4 Classes and Interfaces \u00b6 CLASSES and interfaces lie at the heart of the Java programming language. They are its basic units of abstraction. 15: Minimize the accessibility of classes and members A well-designed component hides all its implementation details, cleanly separating its API from its implementation. This concept, known as information hiding or encapsulation , is a fundamental tenet of software design that it decouples the components that comprise a system, allowing them to be developed, tested, optimized, used, understood, and modified in isolation. The rule of thumb is simple: make each class or member as inaccessible as possible. For top-level (non-nested) classes and interfaces, there are only two possible access levels: package-private and public. - By making it package-private, you make it part of the implementation rather than the exported API, and you can modify it, replace it, or eliminate it in a subsequent release without fear of harming existing clients. - If you make it public, you are obligated to support it forever to maintain compatibility. - If a package-private top-level class or interface is used by only one class, consider making the top-level class a private static nested class of the sole class that uses it. - If a method overrides a superclass method, it cannot have a more restrictive access level in the subclass than in the superclass. - Instance fields of public classes should rarely be public; classes with public mutable fields are not generally thread-safe. - It is wrong for a class to have a public static final array field, or an accessor that returns such a field, since clients will be able to modify the contents of the array. Access levels: private\u2014The member is accessible only from the top-level class where it is declared. package-private\u2014The member is accessible from any class in the package where it is declared. Technically known as default access, this is the access level you get if no access modifier is specified (except for interface members, which are public by default). protected\u2014The member is accessible from subclasses of the class where it is declared (subject to a few restrictions [JLS, 6.6.2]) and from any class in the package where it is declared. public\u2014The member is accessible from anywhere. As of Java 9 , there are two additional, implicit access levels introduced as part of the module system. A module is a grouping of packages, like a package is a grouping of classes. A module may explicitly export some of its packages via export declarations in its module declaration. A module may explicitly export some of its packages via export declarations in its module declaration (which is by convention contained in a source file named module-info.java). Public and protected members of unexported packages in a module are inaccessible outside the module; within the module, accessibility is unaffected by export declarations. Using the module system allows you to share classes among packages within a module without making them visible to the entire world. After carefully designing a minimal public API, you should prevent any stray classes, interfaces, or members from becoming part of the API. With the exception of public static final fields, which serve as constants, public classes should have no public fields. Ensure that objects referenced by public static final fields are immutable. 16: In public classes, use accessor methods, not public fields if a class is accessible outside its package, provide accessor methods to preserve the flexibility to change the class\u2019s internal representation. if a class is package-private or is a private nested class, there is nothing inherently wrong with exposing its data fields, assuming they do an adequate job of describing the abstraction provided by the class. Public classes should never expose mutable fields. It is less harmful, though still questionable, for public classes to expose immutable fields. It is, however, sometimes desirable for package-private or private nested classes to expose fields, whether mutable or immutable. 17: Minimize mutability Immutable classes are easier to design, implement, and use than mutable classes. They are less prone to error and are more secure. Five rules of thumb to make a class immutable: Don\u2019t provide methods that modify the object\u2019s state (known as mutators). Ensure that the class can\u2019t be extended by making the class final. Make all fields final. Make all fields private. Ensure exclusive access to any mutable components. Immutable objects are simple and inherently thread-safe; they require no synchronization and can be shared freely. Immutable objects provide failure atomicity for free; their state never changes, so there is no possibility of a temporary inconsistency. The major disadvantage of immutable classes is that they require a separate object for each distinct value. public class Complex { private final double re ; private final double im ; private Complex ( double re , double im ) { this . re = re ; this . im = im ; } public static Complex valueOf ( double re , double im ) { // sanity checks here return new Complex ( re , im ); } ... // Remainder unchanged } If you choose to have your immutable class implement Serializable and it contains one or more fields that refer to mutable objects, you must provide an explicit readObject or readResolve method, or use the ObjectOutputStream.writeUnshared and ObjectInputStream.readUnshared methods. Constructors should create fully initialized objects with all of their invariants established. 18: Favor composition over inheritance Unlike method invocation, inheritance violates encapsulation. A subclass depends on the implementation details of its superclass for its proper function. The superclass\u2019s implementation may change from release to release, and if it does, the subclass may break, even though its code has not been touched. One caveat is that wrapper classes are not suited for use in callback frameworks, wherein objects pass self-references to other objects for subsequent invocations (\"callbacks\"). Because a wrapped object doesn\u2019t know of its wrapper, it passes a reference to itself (this) and callbacks elude the wrapper. This is known as the SELF problem. Inheritance is appropriate only when a genuine subtype relationship exists between the subclass and the superclass. Even then, inheritance may lead to fragility if the subclass is in a different package from the superclass and the superclass is not designed for inheritance. Use composition and forwarding instead of inheritance, especially if an appropriate interface to implement a wrapper class exists. 19: Design and document for inheritance or else prohibit it What does it mean for a class to be designed and documented for inheritance? A class must document its self-use of overridable methods. A class may have to provide hooks into its internal workings in the form of judiciously chosen protected methods or, in rare instances, protected fields. Constructors must not invoke overridable methods Designing a class for inheritance requires great effort and places substantial limitations on the class. 20: Prefer interfaces to abstract classes Here is why: To implement the type defined by an abstract class, a class must be a subclass of the abstract class. Not for interfaces. Existing classes can easily be retrofitted to implement a new interface. Interfaces are ideal for defining mixins. A mixin is a type that a class can implement in addition to its \"primary type,\" to declare that it provides some optional behavior. It is called a mixin because it allows the optional functionality to be \"mixed in\" to the type\u2019s primary functionality. Interfaces allow for the construction of nonhierarchical type frameworks. Type hierarchies are great for organizing some things, but other things don\u2019t fall neatly into a rigid hierarchy. i.e. class Singer and class SongWriter. A singer can also be a songwriter. A class can implement both Singer and SongWriter, but not extending both if they are abstract classes. Interfaces enable safe, powerful functionality enhancements via the wrapper class idiom An interface is generally the best way to define a type that permits multiple implementations. If you export a nontrivial interface, you should strongly consider providing a skeletal implementation to go with it. To the extent possible, you should provide the skeletal implementation via default method. 21: Design interfaces for posterity In Java 8, the default method construct was added, with the intent of allowing the addition of methods to existing interfaces. Default methods are \"injected\" into existing implementations without the knowledge or consent of their implementors. It is not always possible to write a default method that maintains all invariants of every conceivable implementation. 22: Use interfaces only to define types When a class implements an interface, the interface serves as a type that can be used to refer to instances of the class. The constant interface pattern is a poor use of interfaces. Avoid making interfaces for the sake of defining constants. 23: Prefer class hierarchies to tagged classes Occasionally you may run across a class whose instances come in two or more flavors and contain a tag field indicating the flavor of the instance. In short, tagged classes are verbose, error-prone, and inefficient. A tagged class is just a pallid imitation of a class hierarchy. If you\u2019re tempted to write a class with an explicit tag field, think about whether the tag could be eliminated and the class replaced by a hierarchy. 24: Favor static member classes over nonstatic A nested class should exist only to serve its enclosing class. If a nested class would be useful in some other context, then it should be a top-level class. There are four kinds of nested classes: static member classes, nonstatic member classes, anonymous classes, and local classes. All but the first kind are known as inner classes. One common use of a static member class is as a public helper class, useful only in conjunction with its outer class. Despite the syntactic similarity, static member classes and nonstatic member classes are very different. Each instance of a nonstatic member class is implicitly associated with an enclosing instance of its containing class. If an instance of a nested class can exist in isolation from an instance of its enclosing class, then the nested class must be a static member class: it is impossible to create an instance of a nonstatic member class without an enclosing instance. If you declare a member class that does not require access to an enclosing instance, always put the static modifier in its declaration A common use of private static member classes is to represent components of the object represented by their enclosing class. i.e. many Map implementations have an internal Entry object for each key-value pair in the map. There are many limitations on the applicability of anonymous classes. You can\u2019t instantiate them except at the point they\u2019re declared. You can\u2019t perform instanceof tests or do anything else that requires you to name the class. You can\u2019t declare an anonymous class to implement multiple interfaces or to extend a class and implement an interface at the same time. Clients of an anonymous class can\u2019t invoke any members except those it inherits from its supertype. Because anonymous classes occur in the midst of expressions, they must be kept short\u2014about ten lines or fewer\u2014or readability will suffer. If a nested class needs to be visible outside of a single method or is too long to fit comfortably inside a method, use a member class. If each instance of a member class needs a reference to its enclosing instance, make it nonstatic; otherwise, make it static. Assuming the class belongs inside a method, if you need to create instances from only one location and there is a preexisting type that characterizes the class, make it an anonymous class; otherwise, make it a local class. 25: Limit source files to a single top-level class Never put multiple top-level classes or interfaces in a single source file. Keep each source file one class (nested classes are okay to have). Chapter 5. Generics \u00b6 With generics, you tell the compiler what types of objects are permitted in each collection. This results in programs that are both safer and clearer, but these benefits, which are not limited to collections, come at a price. 26: Don\u2019t use raw types A class or interface whose declaration has one or more type parameters is a generic class or interface. Generic classes and interfaces are collectively known as generic types. Each generic type defines a set of parameterized types, i.e. E in List . Each generic type defines a raw type, which is the name of the generic type used without any accompanying type parameters. i.e. List is a raw type for List . If you use raw types, you lose all the safety and expressiveness benefits of generics. But it is fine to use types that are parameterized to allow insertion of arbitrary objects, such as List . If you want to use a generic type but you don\u2019t know or care what the actual type parameter is, you can use a question mark instead (unbounded wildcard type). It is the most general parameterized type, capable of holding anything. You can put any element into a collection with a raw type, easily corrupting the collection\u2019s type invariant; you can\u2019t put any element (other than null) into a Collection<?>. You must use raw types in class literals. i.e. List.class, String[].class, and int.class are all legal, but List .class and List<?>.class are not. This is the preferred way to use the instanceof operator with generic types: if ( o instanceof Set ) { // Raw type Set <?> s = ( Set <?> ) o ; // Wildcard type ... } 27: Eliminate unchecked warnings When you program with generics, you will see many compiler warnings: unchecked cast warnings, unchecked method invocation warnings, unchecked parameterized vararg type warnings, and unchecked conversion warnings. Many unchecked warnings are easy to eliminate. Eliminate every unchecked warning that you can. Unchecked warnings are important. Don\u2019t ignore them. Every unchecked warning represents the potential for a ClassCastException at runtime. If you can\u2019t eliminate a warning, but you can prove that the code that provoked the warning is typesafe, then (and only then) suppress the warning with an @SuppressWarnings(\"unchecked\") annotation. The SuppressWarnings annotation can be used on any declaration, from an individual local variable declaration to an entire class. Always use the SuppressWarnings annotation on the smallest scope possible. If you find yourself using the SuppressWarnings annotation on a method or constructor that\u2019s more than one line long, you may be able to move it onto a local variable declaration. Every time you use a @SuppressWarnings(\"unchecked\") annotation, add a comment saying why it is safe to do so. 28: Prefer lists to arrays Arrays differ from generic types in two important ways. First, arrays are covariant. This scary-sounding word means simply that if Sub is a subtype of Super, then the array type Sub[] is a subtype of the array type Super[]. Generics, by contrast, are invariant: for any two distinct types Type1 and Type2, List is neither a subtype nor a supertype of List . // Fails at runtime Object [] objectArray = new Long [ 1 ] ; objectArray [ 0 ] = \"I don't fit in\" ; // Throws ArrayStoreException // Fails at compile time List < Object > ol = new ArrayList < Long > (); // Incompatible types ol . add ( \"I don't fit in\" ); Second major difference between arrays and generics is that arrays are reified. Means arrays know and enforce their element type at runtime. Generics, by contrast, are implemented by erasure, means that they enforce their type constraints only at compile time and discard (or erase) their element type information at runtime. Because of these fundamental differences, arrays and generics do not mix well. For example, it is illegal to create an array of a generic type, a parameterized type, or a type parameter. 29: Favor generic types Writing your own generic types is a bit more difficult, but it\u2019s worth the effort to learn how. The first step in generifying a class is to add one or more type parameters to its declaration. The next step is to replace all the uses of the type Object with the appropriate type parameter and then try to compile. public class Stack < E > { private E [] elements ; // you can\u2019t create an array of a non-reifiable type, such as E private int size = 0 ; private static final int DEFAULT_INITIAL_CAPACITY = 16 ; @SuppressWarnings ( \"unchecked\" ) public Stack () { elements = ( E [] ) new Object [ DEFAULT_INITIAL_CAPACITY ] ; } public void push ( E e ) { ensureCapacity (); elements [ size ++] = e ; } public E pop () { if ( size == 0 ) throw new EmptyStackException (); E result = elements [-- size ] ; elements [ size ] = null ; // Eliminate obsolete reference return result ; } ... // no changes in isEmpty or ensureCapacity } Generic types are safer and easier to use than types that require casts in client code. When you design new types, make sure that they can be used without such casts. This will often mean making the types generic. 30: Favor generic methods Just as classes can be generic, so can methods. Static utility methods that operate on parameterized types are usually generic. All of the \"algorithm\" methods in Collections (such as binarySearch and sort) are generic. // Uses raw types - unacceptable! Compiles with warnings: unchecked call public static Set union ( Set s1 , Set s2 ) { Set result = new HashSet ( s1 ); result . addAll ( s2 ); return result ; } To fix these warnings and make the method typesafe, modify its declaration to declare a type parameter representing the element type for the three sets. The type parameter list, which declares the type parameters, goes between a method\u2019s modifiers and its return type. public static < E > Set < E > union ( Set < E > s1 , Set < E > s2 ) { Set < E > result = new HashSet <> ( s1 ); result . addAll ( s2 ); return result ; } On occasion, you will need to create an object that is immutable but applicable to many different types. Because generics are implemented by erasure, you can use a single object for all required type parameterizations, but you need to write a static factory method to repeatedly dole out the object for each requested type parameterization. // Generic singleton factory pattern private static UnaryOperator < Object > IDENTITY_FN = ( t ) -> t ; @SuppressWarnings ( \"unchecked\" ) public static < T > UnaryOperator < T > identityFunction () { return ( UnaryOperator < T > ) IDENTITY_FN ; } The cast of IDENTITY_FN to (UnaryFunction ) generates an unchecked cast warning, as UnaryOperator is not a UnaryOperator for every T. But the identity function is special: it returns its argument unmodified, so we know that it is typesafe to use it as a UnaryFunction , whatever the value of T. // Sample program to exercise generic singleton public static void main ( String [] args ) { String [] strings = { \"jute\" , \"hemp\" , \"nylon\" }; UnaryOperator < String > sameString = identityFunction (); for ( String s : strings ) System . out . println ( sameString . apply ( s )); Number [] numbers = { 1 , 2.0 , 3L }; UnaryOperator < Number > sameNumber = identityFunction (); for ( Number n : numbers ) System . out . println ( sameNumber . apply ( n )); } It is permissible, though relatively rare, for a type parameter to be bounded by some expression involving that type parameter itself. This is what\u2019s known as a recursive type bound. A common use of recursive type bounds is in connection with the Comparable interface, which defines a type\u2019s natural ordering. Many methods take a collection of elements implementing Comparable to sort it, search within it, calculate its minimum or maximum, and the like. To do these things, it is required that every element in the collection be comparable to every other element in it. // Using a recursive type bound to express mutual comparability public static < E extends Comparable < E >> E max ( Collection < E > c ); The type bound > may be read as \"any type E that can be compared to itself\". In summary, generic methods, like generic types, are safer and easier to use than methods requiring their clients to put explicit casts on input parameters and return values. 31: Use bounded wildcards to increase API flexibility Sometimes you need more flexibility than invariant typing can provide. // Wildcard type for a parameter that serves as an E producer public void pushAll ( Iterable <? extends E > src ) { for ( E e : src ) push ( e ); } // Two possible declarations for the swap method public static < E > void swap ( List < E > list , int i , int j ); public static void swap ( List <?> list , int i , int j ); For maximum flexibility, use wildcard types on input parameters that represent producers or consumers. If an input parameter is both a producer and a consumer, then wildcard types will do you no good: you need an exact type match, which is what you get without any wildcards. As a rule, if a type parameter appears only once in a method declaration, replace it with a wildcard. It doesn\u2019t seem right that we can\u2019t put an element back into the list that we just took it out of. The problem is that the type of list is List<?>, and you can\u2019t put any value except null into a List<?>. public static void swap ( List <?> list , int i , int j ) { swapHelper ( list , i , j ); } // Private helper method for wildcard capture private static < E > void swapHelper ( List < E > list , int i , int j ) { list . set ( i , list . set ( j , list . get ( i ))); } In summary, using wildcard types in your APIs, while tricky, makes the APIs far more flexible. If you write a library that will be widely used, the proper use of wildcard types should be considered mandatory. Remember the basic rule: producer-extends, consumer-super (PECS). In other words, if a parameterized type represents a T producer, use <? extends T>; if it represents a T consumer, use <? super T>. In our Stack example, pushAll\u2019s src parameter produces E instances for use by the Stack, so the appropriate type for src is Iterable<? extends E>; popAll\u2019s dst parameter consumes E instances from the Stack, so the appropriate type for dst is Collection<? super E>. 32: Combine generics and varargs judiciously The purpose of varargs is to allow clients to pass a variable number of arguments to a method, but it is a leaky abstraction: when you invoke a varargs method, an array is created to hold the varargs parameters; that array, which should be an implementation detail, is visible. As a consequence, you get confusing compiler warnings when varargs parameters have generic or parameterized types. If a method declares its varargs parameter to be of a non-reifiable type, the compiler generates a warning on the declaration. If the method is invoked on varargs parameters whose inferred type is non-reifiable, the compiler generates a warning on the invocation too. Recall that non-reifiable type is one whose runtime representation has less information than its compile-time representation, and that nearly all generic and parameterized types are non-reifiable. Heap pollution occurs when a variable of a parameterized type refers to an object that is not of that type. // Mixing generics and varargs can violate type safety! static void dangerous ( List < String > ... stringLists ) { List < Integer > intList = List . of ( 42 ); Object [] objects = stringLists ; objects [ 0 ] = intList ; // Heap pollution String s = stringLists [ 0 ] . get ( 0 ); // ClassCastException } It is unsafe to store a value in a generic varargs array parameter The SafeVarargs annotation constitutes a promise by the author of a method that it is typesafe. It is critical that you do not annotate a method with @SafeVarargs unless it actually is safe. It is unsafe to give another method access to a generic varargs parameter array, with two exceptions: it is safe to pass the array to another varargs method that is correctly annotated with @SafeVarargs, and it is safe to pass the array to a non-varargs method that merely computes some function of the contents of the array. 33: Consider typesafe heterogeneous containers Common uses of generics include collections, such as Set and Map , and single-element containers, such as ThreadLocal and AtomicReference . In all of these uses, it is the container that is parameterized. This limits you to a fixed number of type parameters per container. // Typesafe heterogeneous container pattern - API public class Favorites { public < T > void putFavorite ( Class < T > type , T instance ); public < T > T getFavorite ( Class < T > type ); } // Typesafe heterogeneous container pattern - client public static void main ( String [] args ) { Favorites f = new Favorites (); f . putFavorite ( String . class , \"Java\" ); f . putFavorite ( Integer . class , 0xcafebabe ); f . putFavorite ( Class . class , Favorites . class ); String favoriteString = f . getFavorite ( String . class ); int favoriteInteger = f . getFavorite ( Integer . class ); Class <?> favoriteClass = f . getFavorite ( Class . class ); System . out . printf ( \"%s %x %s%n\" , favoriteString , favoriteInteger , favoriteClass . getName ()); // prints \"Java cafebabe Favorites\" } A Favorites instance is typesafe: it will never return an Integer when you ask it for a String. It is also heterogeneous: unlike an ordinary map, all the keys are of different types. Therefore, we call Favorites a typesafe heterogeneous container . // Typesafe heterogeneous container pattern - implementation public class Favorites { private Map < Class <?> , Object > favorites = new HashMap <> (); public < T > void putFavorite ( Class < T > type , T instance ) { favorites . put ( Objects . requireNonNull ( type ), instance ); } public < T > T getFavorite ( Class < T > type ) { return type . cast ( favorites . get ( type )); } } Each Favorites instance is backed by a private Map<Class<?>, Object> called favorites. The thing to notice is that the wildcard type is nested: it\u2019s not the type of the map that\u2019s a wildcard type but the type of its key. This means that every key can have a different parameterized type: one can be Class , the next Class , and so on. That\u2019s where the heterogeneity comes from. The next thing to notice is that the value type of the favorites Map is simply Object. In other words, the Map does not guarantee the type relationship between keys and values, which is that every value is of the type represented by its key. The cast method is the dynamic analogue of Java\u2019s cast operator. It simply checks that its argument is an instance of the type represented by the Class object. If so, it returns the argument; otherwise it throws a ClassCastException. In summary, the normal use of generics, exemplified by the collections APIs, restricts you to a fixed number of type parameters per container. You can get around this restriction by placing the type parameter on the key rather than the container. You can use Class objects as keys for such typesafe heterogeneous containers. A Class object used in this fashion is called a type token. You can also use a custom key type. Chapter 6. Enums and Annotations \u00b6 JAVA supports two special-purpose families of reference types: a kind of class called an enum type, and a kind of interface called an annotation type. 34: Use enums instead of int constants An enumerated type is a type whose legal values consist of a fixed set of constants. Java\u2019s enum types are full-fledged classes, far more powerful than their counterparts in these other languages, where enums are essentially int values. They are classes that export one instance for each enumeration constant via a public static final field. Enum types are effectively final, by virtue of having no accessible constructors. Because clients can neither create instances of an enum type nor extend it, there can be no instances but the declared enum constants. They are a generalization of singletons (Item 3), which are essentially single-element enums. Enums provide compile-time type safety. If you declare a parameter to be of type Apple, you are guaranteed that any non-null object reference passed to the parameter is one of the three valid Apple values. Enum types with identically named constants coexist peacefully because each type has its own namespace. You can translate enums into printable strings by calling their toString method. Enum types let you add arbitrary methods and fields and implement arbitrary interfaces. They provide high-quality implementations of all the Object methods, they implement Comparable and Serializable, and their serialized form is designed to withstand most changes to the enum type. You should add methods or fields to an enum type when you need to associate data with its constants. An enum type can start life as a simple collection of enum constants and evolve over time into a full-featured abstraction. // Enum type with data and behavior public enum Planet { MERCURY ( 3.302e+23 , 2.439e6 ), VENUS ( 4.869e+24 , 6.052e6 ), EARTH ( 5.975e+24 , 6.378e6 ), MARS ( 6.419e+23 , 3.393e6 ), JUPITER ( 1.899e+27 , 7.149e7 ), SATURN ( 5.685e+26 , 6.027e7 ), URANUS ( 8.683e+25 , 2.556e7 ), NEPTUNE ( 1.024e+26 , 2.477e7 ); private final double mass ; // In kilograms private final double radius ; // In meters private final double surfaceGravity ; // In m / s^2 // Universal gravitational constant in m^3 / kg s^2 private static final double G = 6.67300E-11 ; // Constructor Planet ( double mass , double radius ) { this . mass = mass ; this . radius = radius ; surfaceGravity = G * mass / ( radius * radius ); } public double mass () { return mass ; } public double radius () { return radius ; } public double surfaceGravity () { return surfaceGravity ; } public double surfaceWeight ( double mass ) { return mass * surfaceGravity ; // F = ma } } public class WeightTable { public static void main ( String [] args ) { double earthWeight = Double . parseDouble ( args [ 0 ] ); double mass = earthWeight / Planet . EARTH . surfaceGravity (); for ( Planet p : Planet . values ()) System . out . printf ( \"Weight on %s is %f%n\" , p , p . surfaceWeight ( mass )); } } To associate data with enum constants, declare instance fields and write a constructor that takes the data and stores it in the fields. Enums are by their nature immutable, so all fields should be final. Fields can be public, but it is better to make them private and provide public accessors. If an enum is generally useful, it should be a top-level class; if its use is tied to a specific top-level class, it should be a member class of that top-level class. // Enum type that switches on its own value - questionable public enum Operation { PLUS , MINUS , TIMES , DIVIDE ; // Do the arithmetic operation represented by this constant public double apply ( double x , double y ) { switch ( this ) { case PLUS : return x + y ; case MINUS : return x - y ; case TIMES : return x * y ; case DIVIDE : return x / y ; } throw new AssertionError ( \"Unknown op: \" + this ); } } // Enum type with constant-specific method implementations public enum Operation { PLUS { public double apply ( double x , double y ){ return x + y ;}}, MINUS { public double apply ( double x , double y ){ return x - y ;}}, TIMES { public double apply ( double x , double y ){ return x * y ;}}, DIVIDE { public double apply ( double x , double y ){ return x / y ;}}; public abstract double apply ( double x , double y ); } It is better to declare an abstract apply method in the enum type, and override it with a concrete method for each constant in a constant-specific class body. Such methods are known as constant-specific method implementations. Enum types have an automatically generated valueOf(String) method that translates a constant\u2019s name into the constant itself. If you override the toString method in an enum type, consider writing a fromString method to translate the custom string representation back to the corresponding enum. The following code (with the type name changed appropriately) will do the trick for any enum, so long as each constant has a unique string representation: // Implementing a fromString method on an enum type private static final Map < String , Operation > stringToEnum = Stream . of ( values ()). collect ( toMap ( Object :: toString , e -> e )); // Returns Operation for string, if any public static Optional < Operation > fromString ( String symbol ) { return Optional . ofNullable ( stringToEnum . get ( symbol )); } Note that the Operation constants are put into the stringToEnum map from a static field initialization that runs after the enum constants have been created. Also note that the fromString method returns an Optional . This allows the method to indicate that the string that was passed in does not represent a valid operation, and it forces the client to confront this possibility. A disadvantage of constant-specific method implementations is that they make it harder to share code among enum constants. Switches on enums are good for augmenting enum types with constant-specific behavior. Enums are, generally speaking, comparable in performance to int constants. A minor performance disadvantage of enums is that there is a space and time cost to load and initialize enum types, but it is unlikely to be noticeable in practice. Use enums any time you need a set of constants whose members are known at compile time. It is not necessary that the set of constants in an enum type stay fixed for all time. 35: Use instance fields instead of ordinals All enums have an ordinal method, which returns the numerical position of each enum constant in its type. Never derive a value associated with an enum from its ordinal; store it in an instance field instead. public enum Ensemble { SOLO ( 1 ), DUET ( 2 ), TRIO ( 3 ), QUARTET ( 4 ), QUINTET ( 5 ), SEXTET ( 6 ), SEPTET ( 7 ), OCTET ( 8 ), DOUBLE_QUARTET ( 8 ), NONET ( 9 ), DECTET ( 10 ), TRIPLE_QUARTET ( 12 ); private final int numberOfMusicians ; Ensemble ( int size ) { this . numberOfMusicians = size ; } public int numberOfMusicians () { return numberOfMusicians ; } } 36: Use ENUMSET instead of bit fields If the elements of an enumerated type are used primarily in sets, it is traditional to use the int enum pattern. This representation lets you use the bitwise OR operation to combine several constants into a set, known as a bit field. This is called Bit Field enumeration constants. Obsolete! The java.util package provides the EnumSet class to efficiently represent sets of values drawn from a single enum type. This class implements the Set interface, providing all of the richness, type safety, and interoperability you get with any other Set implementation. // EnumSet - a modern replacement for bit fields public class Text { public enum Style { BOLD , ITALIC , UNDERLINE , STRIKETHROUGH } // Any Set could be passed in, but EnumSet is clearly best public void applyStyles ( Set < Style > styles ) { ... } } text . applyStyles ( EnumSet . of ( Style . BOLD , Style . ITALIC )); Just because an enumerated type will be used in sets, there is no reason to represent it with bit fields. The EnumSet class combines the conciseness and performance of bit fields with all the many advantages of enum types. 37: Use ENUMMAP instead of ordinal indexing It is rarely appropriate to use ordinals to index into arrays: use EnumMap instead. If the relationship you are representing is multidimensional, use EnumMap<..., EnumMap<...>>. // Using an EnumMap to associate data with an enum Map < Plant . LifeCycle , Set < Plant >> plantsByLifeCycle = new EnumMap <> ( Plant . LifeCycle . class ); for ( Plant . LifeCycle lc : Plant . LifeCycle . values ()) plantsByLifeCycle . put ( lc , new HashSet <> ()); for ( Plant p : garden ) plantsByLifeCycle . get ( p . lifeCycle ). add ( p ); System . out . println ( plantsByLifeCycle ); // Using a stream and an EnumMap to associate data with an enum System . out . println ( Arrays . stream ( garden ) . collect ( groupingBy ( p -> p . lifeCycle , () -> new EnumMap <> ( LifeCycle . class ), toSet ()))); 38: Emulate extensible enums with interfaces Enum types can implement arbitrary interfaces by defining an interface for the opcode type and an enum that is the standard implementation of the interface. In summary, while you cannot write an extensible enum type, you can emulate it by writing an interface to accompany a basic enum type that implements the interface. 39: Prefer annotations to naming patterns Historically, it was common to use naming patterns to indicate that some program elements demanded special treatment by a tool or framework. // Marker annotation type declaration import java.lang.annotation.* ; /** * Indicates that the annotated method is a test method. * Use only on parameterless static methods. */ @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface Test { ... } // annotation type with a parameter @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface ExceptionTest { Class <? extends Throwable > value (); } // Program containing marker annotations /** * Indicates that the annotated method is a test method that * must throw the designated exception to succeed. */ public class Sample { @Test public static void m1 () { } // Test should pass public static void m2 () { } @Test public static void m3 () { // Test should fail throw new RuntimeException ( \"Boom\" ); } public static void m4 () { } @Test public void m5 () { } // INVALID USE: nonstatic method public static void m6 () { } @Test public static void m7 () { // Test should fail throw new RuntimeException ( \"Crash\" ); } public static void m8 () { } } The declaration for the Test annotation type is itself annotated with Retention and Target annotations. Such annotations on annotation type declarations are known as meta-annotations. More generally, annotations don\u2019t change the semantics of the annotated code but enable it for special treatment by tools. As of Java 8, you can annotate the declaration of an annotation with the @Repeatable meta-annotation, to indicate that the annotation may be applied repeatedly to a single element. // Repeatable annotation type @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) @Repeatable ( ExceptionTestContainer . class ) public @interface ExceptionTest { Class <? extends Exception > value (); } @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface ExceptionTestContainer { ExceptionTest [] value (); } // Code containing a repeated annotation @ExceptionTest ( IndexOutOfBoundsException . class ) @ExceptionTest ( NullPointerException . class ) public static void doublyBad () { ... } All programmers should use the predefined annotation types that Java provides. Also, consider using the annotations provided by your IDE or static analysis tools. Such annotations can improve the quality of the diagnostic information provided by these tools. 40: Consistently use the OVERRIDE annotation @Override indicates that the annotated method declaration overrides a declaration in a supertype. Use the Override annotation on every method declaration that you believe to override a superclass declaration. 41: Use marker interfaces to define types A marker interface is an interface that contains no method declarations but merely designates (or \"marks\") a class that implements the interface as having some property. Marker interfaces have two advantages over marker annotations. First and foremost, marker interfaces define a type that is implemented by instances of the marked class; marker annotations do not. Another advantage of marker interfaces over marker annotations is that they can be targeted more precisely. The chief advantage of marker annotations over marker interfaces is that they are part of the larger annotation facility. Therefore, marker annotations allow for consistency in annotation-based frameworks. Ask yourself the question \"Might I want to write one or more methods that accept only objects that have this marking?\" If so, you should use a marker interface in preference to an annotation. This will make it possible for you to use the interface as a parameter type for the methods in question, which will result in the benefit of compile-time type checking. If you can convince yourself that you\u2019ll never want to write a method that accepts only objects with the marking, then you\u2019re probably better off using a marker annotation. If, additionally, the marking is part of a framework that makes heavy use of annotations, then a marker annotation is the clear choice. If you want to define a type that does not have any new methods associated with it, a marker interface is the way to go. If you want to mark program elements other than classes and interfaces or to fit the marker into a framework that already makes heavy use of annotation types, then a marker annotation is the correct choice. If you find yourself writing a marker annotation type whose target is ElementType.TYPE, take the time to figure out whether it really should be an annotation type or whether a marker interface would be more appropriate. Chapter 7. Lambdas and Streams \u00b6 42: Prefer lambdas to anonymous classes Historically, interfaces (or, rarely, abstract classes) with a single abstract method were used as function types. Their instances, known as function objects, represent functions or actions. // Anonymous class instance as a function object - obsolete! Collections . sort ( words , new Comparator < String > () { public int compare ( String s1 , String s2 ) { return Integer . compare ( s1 . length (), s2 . length ()); } }); Anonymous classes were adequate for the classic objected-oriented design patterns requiring function objects, notably the Strategy pattern. The Comparator interface represents an abstract strategy for sorting; the anonymous class above is a concrete strategy for sorting strings. In Java 8, the language formalized the notion that interfaces with a single abstract method are special and deserve special treatment. These interfaces are now known as functional interfaces, and the language allows you to create instances of these interfaces using lambda expressions , or lambdas for short. // Lambda expression as function object (replaces anonymous class) Collections . sort ( words , ( s1 , s2 ) -> Integer . compare ( s1 . length (), s2 . length ())); Note that the types of the lambda (Comparator ), of its parameters (s1 and s2, both String), and of its return value (int) are not present in the code. The compiler deduces these types from context, using a process known as type inference. Omit the types of all lambda parameters unless their presence makes your program clearer. If the compiler generates an error telling you it can\u2019t infer the type of a lambda parameter, then specify it. The addition of lambdas to the language makes it practical to use function objects where it would not previously have made sense. Lambdas make it easy to implement constant-specific behavior using the former instead of the latter. Lambdas lack names and documentation; if a computation isn\u2019t self-explanatory, or exceeds a few lines, don\u2019t put it in a lambda. One line is ideal for a lambda, and three lines is a reasonable maximum. There are a few things you can do with anonymous classes that you can\u2019t do with lambdas. Lambdas are limited to functional interfaces. If you want to create an instance of an abstract class, you can do it with an anonymous class, but not a lambda. Finally, a lambda cannot obtain a reference to itself. In a lambda, the this keyword refers to the enclosing instance, which is typically what you want. In an anonymous class, the this keyword refers to the anonymous class instance. 43: Prefer method references to lambdas Java provides a way to generate function objects even more succinct than lambdas: method references. map . merge ( key , 1 , ( count , incr ) -> count + incr ); map . merge ( key , 1 , Integer :: sum ); The parameters count and incr don\u2019t add much value, and they take up a fair amount of space. Really, all the lambda tells you is that the function returns the sum of its two arguments. service . execute ( GoshThisClassNameIsHumongous :: action ); service . execute (() -> action ()); Along similar lines, the Function interface provides a generic static factory method to return the identity function, Function.identity(). It\u2019s typically shorter and cleaner not to use this method but to code the equivalent lambda inline: x -> x. In summary, method references often provide a more succinct alternative to lambdas. Where method references are shorter and clearer, use them; where they aren\u2019t, stick with lambdas. 44: Favor the use of standard functional interfaces If one of the standard functional interfaces does the job, you should generally use it in preference to a purpose-built functional interface. There are forty-three interfaces in java.util.Function. Remember six basic interfaces, you can derive the rest. The basic interfaces operate on object reference types. The Operator interfaces represent functions whose result and argument types are the same. The Predicate interface represents a function that takes an argument and returns a boolean. The Function interface represents a function whose argument and return types differ. The Supplier interface represents a function that takes no arguments and returns (or \"supplies\") a value. Finally, Consumer represents a function that takes an argument and returns nothing, essentially consuming its argument. Interface Function Signature Example UnaryOperator T apply(T t) String::toLowerCase BinaryOperator T apply(T t1, T t2) BigInteger::add Predicate boolean test(T t) Collection::isEmpty Function R apply(T t) Arrays::asList Supplier T get() Instant::now Consumer void accept(T t) System.out::println Don\u2019t be tempted to use basic functional interfaces with boxed primitives instead of primitive functional interfaces. The performance consequences of using boxed primitives for bulk operations can be deadly. You should seriously consider writing a purpose-built functional interface in preference to using a standard one if you need a functional interface that shares one or more of the following characteristics with Comparator: - It will be commonly used and could benefit from a descriptive name. - It has a strong contract associated with it. - It would benefit from custom default methods. Notice that the EldestEntryRemovalFunction interface is labeled with the @FunctionalInterface annotation. It is a statement of programmer intent that serves three purposes: it tells readers of the class and its documentation that the interface was designed to enable lambdas; it keeps you honest because the interface won\u2019t compile unless it has exactly one abstract method; and it prevents maintainers from accidentally adding abstract methods to the interface as it evolves. Always annotate your functional interfaces with the @FunctionalInterface annotation. Do not provide a method with multiple overloadings that take different functional interfaces in the same argument position if it could create a possible ambiguity in the client. The submit method of ExecutorService can take either a Callable or a Runnable, and it is possible to write a client program that requires a cast to indicate the correct overloading. The easiest way to avoid this problem is not to write overloadings that take different functional interfaces in the same argument position. Now that Java has lambdas, it is imperative that you design your APIs with lambdas in mind. Accept functional interface types on input and return them on output. It is generally best to use the standard interfaces provided in java.util.function.Function, but keep your eyes open for the relatively rare cases where you would be better off writing your own functional interface. 45: Use STREAMS judiciously The streams API was added in Java 8 to ease the task of performing bulk operations, sequentially or in parallel. This API provides two key abstractions: the stream, which represents a finite or infinite sequence of data elements, and the stream pipeline, which represents a multistage computation on these elements. The elements in a stream can come from anywhere. Common sources include collections, arrays, files, regular expression pattern matchers, pseudorandom number generators, and other streams. The data elements in a stream can be object references or primitive values. Three primitive types are supported: int, long, and double. A stream pipeline consists of a source stream followed by zero or more intermediate operations and one terminal operation. Each intermediate operation transforms the stream in some way, such as mapping each element to a function of that element or filtering out all elements that do not satisfy some condition. Intermediate operations all transform one stream into another, whose element type may be the same as the input stream or different from it. The terminal operation performs a final computation on the stream resulting from the last intermediate operation, such as storing its elements into a collection, returning a certain element, or printing all of its elements. Stream pipelines are evaluated lazily: evaluation doesn\u2019t start until the terminal operation is invoked, and data elements that aren\u2019t required in order to complete the terminal operation are never computed. This lazy evaluation is what makes it possible to work with infinite streams. Note that a stream pipeline without a terminal operation is a silent no-op. The streams API is fluent: it is designed to allow all of the calls that comprise a pipeline to be chained into a single expression. In fact, multiple pipelines can be chained together into a single expression. By default, stream pipelines run sequentially. Making a pipeline execute in parallel is as simple as invoking the parallel method on any stream in the pipeline, but it is seldom appropriate to do so. The streams API is sufficiently versatile that practically any computation can be performed using streams, but just because you can doesn\u2019t mean you should. When used inappropriately, they can make programs difficult to read and maintain. // Prints all large anagram groups in a dictionary iteratively public class Anagrams { public static void main(String[] args) throws IOException { File dictionary = new File(args[0]); int minGroupSize = Integer.parseInt(args[1]); Map<String, Set<String>> groups = new HashMap<>(); try (Scanner s = new Scanner(dictionary)) { while (s.hasNext()) { String word = s.next(); groups.computeIfAbsent(alphabetize(word), (unused) -> new TreeSet<>()).add(word); } } for (Set<String> group : groups.values()) if (group.size() >= minGroupSize) System.out.println(group.size() + \": \" + group); } private static String alphabetize(String s) { char[] a = s.toCharArray(); Arrays.sort(a); return new String(a); } } // Overuse of streams - don't do this! public class Anagrams { public static void main(String[] args) throws IOException { Path dictionary = Paths.get(args[0]); int minGroupSize = Integer.parseInt(args[1]); try (Stream<String> words = Files.lines(dictionary)) { words.collect( groupingBy(word -> word.chars().sorted() .collect(StringBuilder::new, (sb, c) -> sb.append((char) c), StringBuilder::append).toString())) .values().stream() .filter(group -> group.size() >= minGroupSize) .map(group -> group.size() + \": \" + group) .forEach(System.out::println); } } } Overusing streams makes programs hard to read and maintain. // Tasteful use of streams enhances clarity and conciseness public class Anagrams { public static void main(String[] args) throws IOException { Path dictionary = Paths.get(args[0]); int minGroupSize = Integer.parseInt(args[1]); try (Stream<String> words = Files.lines(dictionary)) { words.collect(groupingBy(word -> alphabetize(word))) .values().stream() .filter(group -> group.size() >= minGroupSize) .forEach(g -> System.out.println(g.size() + \": \" + g)); } } // alphabetize method is the same as in original version } In the absence of explicit types, careful naming of lambda parameters is essential to the readability of stream pipelines. Using helper methods is even more important for readability in stream pipelines than in iterative code because pipelines lack explicit type information and named temporary variables. Refactor existing code to use streams and use them in new code only where it makes sense to do so. Stream pipelines express repeated computation using function objects (typically lambdas or method references), while iterative code expresses repeated computation using code blocks. There are some things you can do from code blocks that you can\u2019t do from function objects: From a code block, you can read or modify any local variable in scope; from a lambda, you can only read final or effectively final variables From a code block, you can return from the enclosing method, break or continue an enclosing loop, or throw any checked exception that this method is declared to throw; from a lambda you can do none of these things. Conversely, streams make it very easy to do some things: Uniformly transform sequences of elements Filter sequences of elements Combine sequences of elements using a single operation (for example to add them, concatenate them, or compute their minimum) Accumulate sequences of elements into a collection, perhaps grouping them by some common attribute Search a sequence of elements for an element satisfying some criterion static Stream < BigInteger > primes () { return Stream . iterate ( TWO , BigInteger :: nextProbablePrime ); } public static void main ( String [] args ) { primes (). map ( p -> TWO . pow ( p . intValueExact ()). subtract ( ONE )) . filter ( mersenne -> mersenne . isProbablePrime ( 50 )) . limit ( 20 ) . forEach ( System . out :: println ); } This program starts with the primes, computes the corresponding Mersenne numbers, filters out all but the primes (the magic number 50 controls the probabilistic primality test), limits the resulting stream to twenty elements, and prints them out. // Iterative Cartesian product computation private static List < Card > newDeck () { List < Card > result = new ArrayList <> (); for ( Suit suit : Suit . values ()) for ( Rank rank : Rank . values ()) result . add ( new Card ( suit , rank )); return result ; } // Stream-based Cartesian product computation private static List < Card > newDeck () { return Stream . of ( Suit . values ()) . flatMap ( suit -> Stream . of ( Rank . values ()) . map ( rank -> new Card ( suit , rank ))) . collect ( toList ()); } And here is a stream-based implementation that makes use of the intermediate operation flatMap. This operation maps each element in a stream to a stream and then concatenates all of these new streams into a single stream. If you\u2019re not sure whether a task is better served by streams or iteration, try both and see which works better. 46: Prefer side-effect-free functions in streams Streams isn\u2019t just an API, it\u2019s a paradigm based on functional programming. In order to obtain the expressiveness, speed, and in some cases parallelizability that streams have to offer, you have to adopt the paradigm as well as the API. The most important part of the streams paradigm is to structure your computation as a sequence of transformations where the result of each stage is as close as possible to a pure function of the result of the previous stage. A pure function is one whose result depends only on its input: it does not depend on any mutable state, nor does it update any state. In order to achieve this, any function objects that you pass into stream operations, both intermediate and terminal, should be free of side-effects. // Uses the streams API but not the paradigm--Don't do this! Map<String, Long> freq = new HashMap<>(); try (Stream<String> words = new Scanner(file).tokens()) { words.forEach(word -> { freq.merge(word.toLowerCase(), 1L, Long::sum); }); } Simply put, it\u2019s not streams code at all; it\u2019s iterative code masquerading as streams code. This code is doing all its work in a terminal forEach operation, using a lambda that mutates external state (the frequency table). A forEach operation that does anything more than present the result of the computation performed by a stream is a \"bad smell in code,\" as is a lambda that mutates state. // Proper use of streams to initialize a frequency table Map<String, Long> freq; try (Stream<String> words = new Scanner(file).tokens()) { freq = words .collect(groupingBy(String::toLowerCase, counting())); } This snippet does the same thing as the previous one but makes proper use of the streams API. It\u2019s shorter and clearer. So why would anyone write it the other way? Because it uses tools they\u2019re already familiar with. The forEach operation should be used only to report the result of a stream computation, not to perform the computation. You can ignore the Collector interface and think of a collector as an opaque object that encapsulates a reduction strategy. Reduction means combining the elements of a stream into a single object. The object produced by a collector is typically a collection. There are three such collectors: toList(), toSet(), and toCollection(collectionFactory). They return, respectively, a set, a list, and a programmer-specified collection type. // Pipeline to get a top-ten list of words from a frequency table List < String > topTen = freq . keySet (). stream () . sorted ( comparing ( freq :: get ). reversed ()) . limit ( 10 ) . collect ( toList ()); It is customary and wise to statically import all members of Collectors because it makes stream pipelines more readable. The comparing method is a comparator construction method (Item 14) that takes a key extraction function. The function takes a word, and the \"extraction\" is actually a table lookup: the bound method reference freq::get looks up the word in the frequency table and returns the number of times the word appears in the file. Finally, we call reversed on the comparator, so we\u2019re sorting the words from most frequent to least frequent. The simplest map collector is toMap(keyMapper, valueMapper), which takes two functions, one of which maps a stream element to a key, the other, to a value. The more complicated forms of toMap, as well as the groupingBy method, give you various ways to provide strategies for dealing with such collisions. One way is to provide the toMap method with a merge function in addition to its key and value mappers. The merge function is a BinaryOperator , where V is the value type of the map. Any additional values associated with a key are combined with the existing value using the merge function. // Collector to generate a map from key to chosen element for key Map < Artist , Album > topHits = albums . collect ( // convert the stream of albums to a map, mapping each artist to the album // that has the best album by sales toMap ( Album :: artist , a -> a , maxBy ( comparing ( Album :: sales )))); In addition to the toMap method, the Collectors API provides the groupingBy method, which returns collectors to produce maps that group elements into categories based on a classifier function. The classifier function takes an element and returns the category into which it falls. This category serves as the element\u2019s map key. If you want groupingBy to return a collector that produces a map with values other than lists, you can specify a downstream collector in addition to a classifier. A downstream collector produces a value from a stream containing all the elements in a category. Alternatively, you can pass toCollection(collectionFactory), which lets you create the collections into which each category of elements is placed. Another simple use of the two-argument form of groupingBy is to pass counting() as the downstream collector. This results in a map that associates each category with the number of elements in the category. List < String > words = words . collect ( groupingBy ( word -> alphabetize ( word ))); Map < String , Long > freq = words . collect ( groupingBy ( String :: toLowerCase , counting ())); The collectors returned by the counting method are intended only for use as downstream collectors. The same functionality is available directly on Stream, via the count method, so there is never a reason to say collect(counting()). In summary, the essence of programming stream pipelines is side-effect-free function objects. This applies to all of the many function objects passed to streams and related objects. The terminal operation forEach should only be used to report the result of a computation performed by a stream, not to perform the computation. In order to use streams properly, you have to know about collectors. The most important collector factories are toList, toSet, toMap, groupingBy, and joining. 47: Prefer COLLECTION to STREAM as a return type Streams do not make iteration obsolete: writing good code requires combining streams and iteration judiciously. Stream fails to extend the iterator method. But it is easy to write an adapter method to enable stream iterators. // Adapter from Stream<E> to Iterable<E> public static < E > Iterable < E > iterableOf ( Stream < E > stream ) { return stream :: iterator ; } // Adapter from Iterable<E> to Stream<E> public static < E > Stream < E > streamOf ( Iterable < E > iterable ) { return StreamSupport . stream ( iterable . spliterator (), false ); } for ( ProcessHandle p : iterableOf ( ProcessHandle . allProcesses ())) { // Process the process } The Collection interface is a subtype of Iterable and has a stream method, so it provides for both iteration and stream access. Therefore, Collection or an appropriate subtype is generally the best return type for a public, sequence-returning method. Arrays also provide for easy iteration and stream access with the Arrays.asList and Stream.of methods. If the sequence you\u2019re returning is small enough to fit easily in memory, you\u2019re probably best off returning one of the standard collection implementations, such as ArrayList or HashSet. But do not store a large sequence in memory just to return it as a collection. In summary, when writing a method that returns a sequence of elements, remember that some of your users may want to process them as a stream while others may want to iterate over them. Try to accommodate both groups. If it\u2019s feasible to return a collection, do so. If you already have the elements in a collection or the number of elements in the sequence is small enough to justify creating a new one, return a standard collection such as ArrayList. Otherwise, consider implementing a custom collection. 48: Use caution when making STREAMs parallel Safety and liveness violations are a fact of life in concurrent programming, and parallel stream pipelines are no exception. Even under the best of circumstances, parallelizing a pipeline is unlikely to increase its performance if the source is from Stream.iterate, or the intermediate operation limit is used. As a rule, performance gains from parallelism are best on streams over ArrayList, HashMap, HashSet, and ConcurrentHashMap instances; arrays; int ranges; and long ranges. What these data structures have in common is that they can all be accurately and cheaply split into subranges of any desired sizes, which makes it easy to divide work among parallel threads. Another important factor that all of these data structures have in common is that they provide good-to-excellent locality of reference when processed sequentially: sequential element references are stored together in memory. Not only can parallelizing a stream lead to poor performance, including liveness failures; it can lead to incorrect results and unpredictable behavior. Under the right circumstances, it is possible to achieve near-linear speedup in the number of processor cores simply by adding a parallel call to a stream pipeline. Chapter 8. Methods \u00b6 49: Check parameters for validity You should clearly document all restrictions and enforce them with checks at the beginning of the method body. For public and protected methods, use the Javadoc @throws tag to document the exception that will be thrown if a restriction on parameter values is violated. The Objects.requireNonNull method, added in Java 7, is flexible and convenient, so there\u2019s no reason to perform null checks manually anymore. In Java 9, a range-checking facility was added to java.util.Objects. This facility consists of three methods: checkFromIndexSize, checkFromToIndex, and checkIndex . Nonpublic methods can check their parameters using assertions, as shown below: // Private helper function for a recursive sort private static void sort ( long a [] , int offset , int length ) { assert a != null ; assert offset >= 0 && offset <= a . length ; assert length >= 0 && length <= a . length - offset ; ... // Do the computation } Assertions throw AssertionError if they fail. They have no effect and essentially no cost unless you enable them, which you do by passing the -ea (or -enableassertions) flag to the java command. 50: Make defensive copies when needed Even java is a safe language, you must program defensively, with the assumption that clients of your class will do their best to destroy its invariants. i.e. Date is a mutable class. If you implement something using Date that is intended to be immutable, it is still vulnerable. // Attack the internals of a Period instance Date start = new Date (); Date end = new Date (); Period p = new Period ( start , end ); end . setYear ( 78 ); // Modifies internals of p! As of Java 8, the obvious way to fix this problem is to use Instant (or Local-DateTime or ZonedDateTime) in place of a Date because Instant (and the other java.time classes) are immutable. Date is obsolete and should no longer be used in new code. It is essential to make a defensive copy of each mutable parameter to the constructor and to use the copies as components of the Period instance in place of the originals: // Repaired constructor - makes defensive copies of parameters public Period ( Date start , Date end ) { this . start = new Date ( start . getTime ()); this . end = new Date ( end . getTime ()); if ( this . start . compareTo ( this . end ) > 0 ) throw new IllegalArgumentException ( this . start + \" after \" + this . end ); } Note that defensive copies are made before checking the validity of the parameters, and the validity check is performed on the copies rather than on the originals. It protects the class against changes to the parameters from another thread during the window of vulnerability between the time the parameters are checked and the time they are copied. This is known as a time-of-check/time-of-use or TOCTOU attack . Do not use the clone method to make a defensive copy of a parameter whose type is subclassable by untrusted parties. While the replacement constructor successfully defends against the previous attack, it is still possible to mutate a Period instance, because its accessors offer access to its mutable internals: // Second attack on the internals of a Period instance Date start = new Date (); Date end = new Date (); Period p = new Period ( start , end ); p . end (). setYear ( 78 ); // Modifies internals of p! To defend against the second attack, merely modify the accessors to return defensive copies of mutable internal fields. // Repaired accessors - make defensive copies of internal fields public Date start () { return new Date ( start . getTime ()); } public Date end () { return new Date ( end . getTime ()); } In the accessors, unlike the constructor, it would be permissible to use the clone method to make the defensive copies. This is so because we know that the class of Period\u2019s internal Date objects is java.util.Date, and not some untrusted subclass. That said, you are generally better off using a constructor or static factory to copy an instance. Arguably, the real lesson in all of this is that you should, where possible, use immutable objects as components of your objects so that you that don\u2019t have to worry about defensive copying. In summary, if a class has mutable components that it gets from or returns to its clients, the class must defensively copy these components. If the cost of the copy would be prohibitive and the class trusts its clients not to modify the components inappropriately, then the defensive copy may be replaced by documentation outlining the client\u2019s responsibility not to modify the affected components. 51: Design method signatures carefully Choose method names carefully. Choose names that are understandable and consistent with other names in the same package. Avoid long method names. Don\u2019t go overboard in providing convenience methods. Too many methods make a class difficult to learn, use, document, test, and maintain. Avoid long parameter lists. Long sequences of identically typed parameters are especially harmful. Some techniques for shortening overly long parameter lists: break the method up into multiple methods, each of which requires only a subset of the parameters create helper classes to hold groups of parameters, typically static member classes adapt the Builder pattern For parameter types, favor interfaces over classes. i.e. there is no reason to ever write a method that takes HashMap on input\u2014use Map instead. Prefer two-element enum types to boolean parameters, unless the meaning of the boolean is clear from the method name. Enums make your code easier to read and to write. Also, they make it easy to add more options later. public enum TemperatureScale { FAHRENHEIT , CELSIUS } Thermometer . newInstance ( TemperatureScale . CELSIUS ) 52: Use overloading judiciously Because the following classify method is overloaded, and the choice of which overloading to invoke is made at compile time, it prints \"Unknown Collection\" three times. // Broken! public class CollectionClassifier { public static String classify ( Set <?> s ) { return \"Set\" ; } public static String classify ( List <?> lst ) { return \"List\" ; } public static String classify ( Collection <?> c ) { return \"Unknown Collection\" ; } public static void main ( String [] args ) { Collection <?>[] collections = { new HashSet < String > (), new ArrayList < BigInteger > (), new HashMap < String , String > (). values () }; for ( Collection <?> c : collections ) System . out . println ( classify ( c )); } } For all three iterations of the loop, the compile-time type of the parameter is the same: Collection<?>. The runtime type is different in each iteration, but this does not affect the choice of overloading. The best way to fix the CollectionClassifier program is to replace all three overloadings of classify with a single method that does explicit instanceof tests. Selection among overloaded methods is static, while selection among overridden methods is dynamic. A safe, conservative policy is never to export two overloadings with the same number of parameters. You can always give methods different names instead of overloading them. Do not overload methods to take different functional interfaces in the same argument position. 53: Use varargs judiciously Varargs methods, formally known as variable arity methods, accept zero or more arguments of a specified type. The varargs facility works by first creating an array whose size is the number of arguments passed at the call site, then putting the argument values into the array, and finally passing the array to the method. // The WRONG way to use varargs to pass one or more arguments! static int min ( int ... args ) { if ( args . length == 0 ) throw new IllegalArgumentException ( \"Too few arguments\" ); int min = args [ 0 ] ; for ( int i = 1 ; i < args . length ; i ++ ) if ( args [ i ] < min ) min = args [ i ] ; return min ; } // The right way to use varargs to pass one or more arguments static int min ( int firstArg , int ... remainingArgs ) { int min = firstArg ; for ( int arg : remainingArgs ) if ( arg < min ) min = arg ; return min ; } Exercise care when using varargs in performance-critical situations. Every invocation of a varargs method causes an array allocation and initialization. Suppose you\u2019ve determined that 95 percent of the calls to a method have three or fewer parameters. public void foo () { } public void foo ( int a1 ) { } public void foo ( int a1 , int a2 ) { } public void foo ( int a1 , int a2 , int a3 ) { } public void foo ( int a1 , int a2 , int a3 , int ... rest ) { } Like most performance optimizations, this technique usually isn\u2019t appropriate, but when it is, it\u2019s a lifesaver. In summary, varargs are invaluable when you need to define methods with a variable number of arguments. Precede the varargs parameter with any required parameters, and be aware of the performance consequences of using varargs. 54: Return empty COLLECTIONs or ARRAYs, not nulls In the unlikely event that you have evidence suggesting that allocating empty collections is harming performance, you can avoid the allocations by returning the same immutable empty collection repeatedly, as immutable objects may be shared freely. //The right way to return a possibly empty collection public List < Cheese > getCheeses () { return new ArrayList <> ( cheesesInStock ); } // Optimization - avoids allocating empty collections public List < Cheese > getCheeses () { return cheesesInStock . isEmpty () ? Collections . emptyList () : new ArrayList <> ( cheesesInStock ); } //The right way to return a possibly empty array public Cheese [] getCheeses () { return cheesesInStock . toArray ( new Cheese [ 0 ] ); } // Optimization - avoids allocating empty arrays private static final Cheese [] EMPTY_CHEESE_ARRAY = new Cheese [ 0 ] ; public Cheese [] getCheeses () { return cheesesInStock . toArray ( EMPTY_CHEESE_ARRAY ); } // Don\u2019t do this - preallocating the array harms performance! return cheesesInStock . toArray ( new Cheese [ cheesesInStock . size () ] ); In summary, never return null in place of an empty array or collection. It makes your API more difficult to use and more prone to error, and it has no performance advantages. 55: Return OPTIONALs judiciously Prior to Java 8, there were two approaches you could take when writing a method that was unable to return a value under certain circumstances. Either you could throw an exception, or you could return null. Exceptions should be reserved for exceptional conditions (Item 69), and throwing an exception is expensive because the entire stack trace is captured when an exception is created. If a method returns null, clients must contain special-case code to deal with the possibility of a null return. In Java 8, the Optional class represents an immutable container that can hold either a single non-null T reference or nothing at all. An optional that contains nothing is said to be empty. A value is said to be present in an optional that is not empty. An optional is essentially an immutable collection that can hold at most one element. Optional does not implement Collection , but it could in principle. A method that conceptually returns a T but may be unable to do so under certain circumstances can instead be declared to return an Optional . This allows the method to return an empty result to indicate that it couldn\u2019t return a valid result. An Optional-returning method is more flexible and easier to use than one that throws an exception, and it is less error-prone than one that returns null. // Returns maximum value in collection - throws exception if empty public static < E extends Comparable < E >> E max ( Collection < E > c ) { if ( c . isEmpty ()) throw new IllegalArgumentException ( \"Empty collection\" ); E result = null ; for ( E e : c ) if ( result == null || e . compareTo ( result ) > 0 ) result = Objects . requireNonNull ( e ); return result ; } // Returns maximum value in collection as an Optional<E> public static < E extends Comparable < E >> Optional < E > max ( Collection < E > c ) { if ( c . isEmpty ()) return Optional . empty (); E result = null ; for ( E e : c ) if ( result == null || e . compareTo ( result ) > 0 ) result = Objects . requireNonNull ( e ); return Optional . of ( result ); } If a method returns an optional, the client gets to choose what action to take if the method can\u2019t return a value. You can specify a default value. // Using an optional to provide a chosen default value String lastWordInLexicon = max ( words ). orElse ( \"No words...\" ); // Using an optional to throw a chosen exception Toy myToy = max ( toys ). orElseThrow ( TemperTantrumException :: new ); // Using optional when you know there\u2019s a return value Element lastNobleGas = max ( Elements . NOBLE_GASES ). get (); Occasionally you may be faced with a situation where it\u2019s expensive to get the default value, and you want to avoid that cost unless it\u2019s necessary. For these situations, Optional provides a method that takes a Supplier and invokes it only when necessary. Optional < ProcessHandle > parentProcess = ph . parent (); System . out . println ( \"Parent PID: \" + ( parentProcess . isPresent () ? String . valueOf ( parentProcess . get (). pid ()) : \"N/A\" )); // can be replaced by this one, which uses Optional\u2019s map function\\ System . out . println ( \"Parent PID: \" + ph . parent (). map ( h -> String . valueOf ( h . pid ())). orElse ( \"N/A\" )); Container types, including collections, maps, streams, arrays, and optionals should not be wrapped in optionals. Rather than returning an empty Optional >, you should simply return an empty List . As a rule, you should declare a method to return Optional if it might not be able to return a result and clients will have to perform special processing if no result is returned. You should never return an optional of a boxed primitive type, with the possible exception of the \"minor primitive types,\" Boolean, Byte, Character, Short, and Float. There are OptionalInt, OptionalLong, and OptionalDouble available for use. It is almost never appropriate to use an optional as a key, value, or element in a collection or array. In summary, if you find yourself writing a method that can\u2019t always return a value and you believe it is important that users of the method consider this possibility every time they call it, then you should probably return an optional. You should, however, be aware that there are real performance consequences associated with returning optionals; for performance-critical methods, it may be better to return a null or throw an exception. Finally, you should rarely use an optional in any other capacity than as a return value. 56: Write doc comments for all exposed API elements Javadoc generates API documentation automatically from source code with specially formatted documentation comments, more commonly known as doc comments. To document your API properly, you must precede every exported class, interface, constructor, method, and field declaration with a doc comment. The doc comment for a method should describe succinctly the contract between the method and its client. The contract should say what the method does rather than how it does its job. The doc comment should enumerate all of the method\u2019s preconditions. Methods should document any side effects. A side effect is an observable change in the state of the system that is not obviously required in order to achieve the postcondition. i.e. if a method starts new threads, it should be documented. To describe a method\u2019s contract fully, the doc comment should have an @param tag for every parameter, an @return tag unless the method has a void return type, and an @throws tag for every exception thrown by the method. By convention, the text following an @param tag or @return tag should be a noun phrase describing the value represented by the parameter or return value. The text following an @throws tag should consist of the word \"if,\" followed by a clause describing the conditions under which the exception is thrown. /** * Returns the element at the specified position in this list. * * <p>This method is <i>not</i> guaranteed to run in constant * time. In some implementations it may run in time proportional * to the element position. * * @param index index of element to return; must be * non-negative and less than the size of this list * @return the element at the specified position in this list * @throws IndexOutOfBoundsException if the index is out of range * ({@code index < 0 || index >= this.size()}) */ E get ( int index ); The Javadoc utility translates doc comments into HTML, and arbitrary HTML elements in doc comments end up in the resulting HTML document. The Javadoc {@code} tag causes the code fragment to be rendered in code font, and it suppresses processing of HTML markup and nested Javadoc tags in the code fragment. {@literal} tag has similar effect. When you design a class for inheritance, you must document its self-use patterns, so programmers know the semantics of overriding its methods. Should be documented using the @implSpec tag. As of Java 9, the Javadoc utility still ignores the @implSpec tag unless you pass the command line switch -tag \"implSpec Implementation Requirements:\". Doc comments should be readable both in the source code and in the generated documentation. To avoid confusion, no two members or constructors in a class or interface should have the same summary description. Be careful if the intended summary description contains a period, because the period can prematurely terminate the description. In Java 9, a client-side index was added to the HTML generated by Javadoc. This index, which eases the task of navigating large API documentation sets, takes the form of a search box in the upper-right corner of the page. API elements, such as classes, methods, and fields, are indexed automatically. To add index, use {@index} tag: * This method complies with the {@index IEEE 754} standard. Generics, enums, and annotations require special care in doc comments. When documenting a generic type or method, be sure to document all type parameters When documenting an enum type, be sure to document the constants as well as the type and any public methods When documenting an annotation type, be sure to document any members as well as the type itself. /** * (omitted) * @param <K> the type of keys maintained by this map * @param <V> the type of mapped values */ public interface Map < K , V > { ... } /** * An instrument section of a symphony orchestra. */ public enum OrchestraSection { /** Woodwinds, such as flute, clarinet, and oboe. */ WOODWIND , /** Brass instruments, such as french horn and trumpet. */ BRASS , /** Percussion instruments, such as timpani and cymbals. */ PERCUSSION , /** Stringed instruments, such as violin and cello. */ STRING ; } /** * Indicates that the annotated method is a test method that * must throw the designated exception to pass. */ @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface ExceptionTest { /** * The exception that the annotated test method must throw * in order to pass. (The test is permitted to throw any * subtype of the type described by this class object.) */ Class <? extends Throwable > value (); } Package-level doc comments should be placed in a file named package-info.java and it must contain a package declaration and may contain annotations on this declaration. Similarly, if you elect to use the module system, module-level comments should be placed in the module-info.java file. Whether or not a class or static method is thread-safe, you should document its thread-safety level. If a class is serializable, you should document its serialized form. To summarize, documentation comments are the best, most effective way to document your API. Their use should be considered mandatory for all exported API elements. Adopt a consistent style that adheres to standard conventions. Remember that arbitrary HTML is permissible in documentation comments and that HTML metacharacters must be escaped. Chapter 9. General Programming \u00b6 57: Minimize the scope of local variables The most powerful technique for minimizing the scope of a local variable is to declare it where it is first used. Nearly every local variable declaration should contain an initializer. A final technique to minimize the scope of local variables is to keep methods small and focused. If you combine two activities in the same method, local variables relevant to one activity may be in the scope of the code performing the other activity. 58: Prefer for-each loops to traditional for loops Unfortunately, there are three common situations where you can\u2019t use for-each: - Destructive filtering\u2014If you need to traverse a collection removing selected elements, then you need to use an explicit iterator so that you can call its remove method. - You can often avoid explicit traversal by using Collection\u2019s removeIf method, added in Java 8. - Transforming\u2014If you need to traverse a list or array and replace some or all of the values of its elements, then you need the list iterator or array index in order to replace the value of an element. - Parallel iteration\u2014If you need to traverse multiple collections in parallel, then you need explicit control over the iterator or index variable so that all iterators or index variables can be advanced in lockstep. In summary, the for-each loop provides compelling advantages over the traditional for loop in clarity, flexibility, and bug prevention, with no performance penalty. Use for-each loops in preference to for loops wherever you can. 59: Know and use the libraries By using a standard library, you take advantage of the knowledge of the experts who wrote it and the experience of those who used it before you. As of Java 7, you should no longer use Random. For most uses, the random number generator of choice is now ThreadLocalRandom. It produces higher quality random numbers, and it\u2019s very fast. A second advantage of using the libraries is that you don\u2019t have to waste your time writing ad hoc solutions to problems that are only marginally related to your work. A third advantage of using standard libraries is that their performance tends to improve over time, with no effort on your part. A fourth advantage of using libraries is that they tend to gain functionality over time. A final advantage of using the standard libraries is that you place your code in the mainstream. Such code is more easily readable, maintainable, and reusable by the multitude of developers. Numerous features are added to the libraries in every major release, and it pays to keep abreast of these additions. Each time there is a major release of the Java platform, a web page is published describing its new features. These pages are well worth reading. The libraries are too big to study all the documentation, but every programmer should be familiar with the basics of java.lang, java.util, and java.io, and their subpackages. If you can\u2019t find what you need in Java platform libraries, your next choice should be to look in high-quality third-party libraries, such as Google\u2019s excellent, open source Guava library To summarize, don\u2019t reinvent the wheel. If you need to do something that seems like it should be reasonably common, there may already be a facility in the libraries that does what you want. If there is, use it; if you don\u2019t know, check. 60: Avoid float and double if exact answers are required The float and double types are designed primarily for scientific and engineering calculations. They do not, however, provide exact results and should not be used where exact results are required. The float and double types are particularly ill-suited for monetary calculations because it is impossible to represent 0.1 (or any other negative power of ten) as a float or double exactly. The right way to solve this problem is to use BigDecimal, int, or long for monetary calculations. There are, however, two disadvantages to using BigDecimal: it\u2019s a lot less convenient than using a primitive arithmetic type, and it\u2019s a lot slower. 61: Prefer primitive types to boxed primitives There are three major differences between primitives and boxed primitives. primitives have only their values, whereas boxed primitives have identities distinct from their values. primitive types have only fully functional values, whereas each boxed primitive type has one nonfunctional value, which is null. primitives are more time- and space-efficient than boxed primitives. Applying the == operator to boxed primitives is almost always wrong. In practice, if you need a comparator to describe a type\u2019s natural order, you should simply call Comparator.naturalOrder(), and if you write a comparator yourself, you should use the comparator construction methods, or the static compare methods on primitive types. In nearly every case when you mix primitives and boxed primitives in an operation, the boxed primitive is auto-unboxed. In summary, use primitives in preference to boxed primitives whenever you have the choice. Primitive types are simpler and faster. Autoboxing reduces the verbosity, but not the danger, of using boxed primitives. When your program compares two boxed primitives with the == operator, it does an identity comparison, which is almost certainly not what you want. When your program does mixed-type computations involving boxed and unboxed primitives, it does unboxing, and when your program does unboxing, it can throw a NullPointerException. Finally, when your program boxes primitive values, it can result in costly and unnecessary object creations. 62: Avoid STRINGs where other types are more appropriate Strings are poor substitutes for other value types. When a piece of data comes into a program from a file, from the network, or from keyboard input, it is often in string form. If there\u2019s an appropriate value type, whether primitive or object reference, you should use it; if there isn\u2019t, you should write one. Strings are poor substitutes for enum types. Strings are poor substitutes for aggregate types. If an entity has multiple components, it is usually a bad idea to represent it as a single string. Strings are poor substitutes for capabilities. To summarize, avoid the natural tendency to represent objects as strings when better data types exist or can be written. Used inappropriately, strings are more cumbersome, less flexible, slower, and more error-prone than other types. Types for which strings are commonly misused include primitive types, enums, and aggregate types. 63: Beware the performance of STRING concatenation The string concatenation operator (+) is a convenient way to combine a few strings into one. It is fine for generating a single line of output or constructing the string representation of a small, fixed-size object, but it does not scale. Using the string concatenation operator repeatedly to concatenate n strings requires time quadratic in n. Strings are immutable. When two strings are concatenated, the contents of both are copied. To achieve acceptable performance, use a StringBuilder in place of a String. 64: Refer to objects by their interfaces If appropriate interface types exist, then parameters, return values, variables, and fields should all be declared using interface types. It will make your program much more flexible. The only time you really need to refer to an object\u2019s class is when you\u2019re creating it with a constructor. // Good - uses interface as type Set < Son > sonSet = new LinkedHashSet <> (); // Bad - uses class as type! LinkedHashSet < Son > sonSet = new LinkedHashSet <> (); If you decide that you want to switch implementations, all you have to do is change the class name in the constructor (or use a different static factory). It is entirely appropriate to refer to an object by a class rather than an interface if no appropriate interface exists. Value classes are rarely written with multiple implementations in mind. Objects belonging to a framework whose fundamental types are classes rather than interfaces. Classes that implement an interface but also provide extra methods not found in the interface. If there is no appropriate interface, just use the least specific class in the class hierarchy that provides the required functionality. 65: Prefer interfaces to reflection The core reflection facility, java.lang.reflect, offers programmatic access to arbitrary classes. Given a Class object, you can obtain Constructor, Method, and Field instances representing the constructors, methods, and fields of the class represented by the Class instance. These objects provide programmatic access to the class\u2019s member names, field types, method signatures, and so on. Moreover, Constructor, Method, and Field instances let you manipulate their underlying counterparts reflectively: you can construct instances, invoke methods, and access fields of the underlying class by invoking methods on the Constructor, Method, and Field instances. Reflection allows one class to use another, even if the latter class did not exist when the former was compiled. This power, however, comes at a price: You lose all the benefits of compile-time type checking, including exception checking. The code required to perform reflective access is clumsy and verbose. Performance suffers. You can obtain many of the benefits of reflection while incurring few of its costs by using it only in a very limited form. For many programs that must use a class that is unavailable at compile time, there exists at compile time an appropriate interface or superclass by which to refer to the class. If this is the case, you can create instances reflectively and access them normally via their interface or superclass. i.e. here is a program that creates a Set instance whose class is specified by the first command line argument. The program inserts the remaining command line arguments into the set and prints it. // Reflective instantiation with interface access public static void main ( String [] args ) { // Translate the class name into a Class object Class <? extends Set < String >> cl = null ; try { cl = ( Class <? extends Set < String >> ) // Unchecked cast! Class . forName ( args [ 0 ] ); } catch ( ClassNotFoundException e ) { fatalError ( \"Class not found.\" ); } // Get the constructor Constructor <? extends Set < String >> cons = null ; try { cons = cl . getDeclaredConstructor (); } catch ( NoSuchMethodException e ) { fatalError ( \"No parameterless constructor\" ); } // Instantiate the set Set < String > s = null ; try { s = cons . newInstance (); } catch ( IllegalAccessException e ) { fatalError ( \"Constructor not accessible\" ); } catch ( InstantiationException e ) { fatalError ( \"Class not instantiable.\" ); } catch ( InvocationTargetException e ) { fatalError ( \"Constructor threw \" + e . getCause ()); } catch ( ClassCastException e ) { fatalError ( \"Class doesn't implement Set\" ); } // Exercise the set s . addAll ( Arrays . asList ( args ). subList ( 1 , args . length )); System . out . println ( s ); } private static void fatalError ( String msg ) { System . err . println ( msg ); System . exit ( 1 ); } Usually, this technique is all that you need in the way of reflection. This example demonstrates two disadvantages of reflection: the example can generate six different exceptions at runtime, all of which would have been compile-time errors if reflective instantiation were not used; it takes twenty-five lines of tedious code to generate an instance of the class from its name, whereas a constructor invocation would fit neatly on a single line. If you are writing a program that has to work with classes unknown at compile time, you should, if at all possible, use reflection only to instantiate objects, and access the objects using some interface or superclass that is known at compile time. 66: Use native methods judiciously The Java Native Interface (JNI) allows Java programs to call native methods, which are methods written in native programming languages such as C or C++. Historically, native methods have had three main uses. They provide access to platform-specific facilities such as registries. They provide access to existing libraries of native code, including legacy libraries that provide access to legacy data. Finally, native methods are used to write performance-critical parts of applications in native languages for improved performance. It is legitimate to use native methods to access platform-specific facilities, but it is seldom necessary; it is also legitimate to use native methods to use native libraries when no equivalent libraries are available in Java. It is rarely advisable to use native methods for improved performance. The use of native methods has serious disadvantages. Because native languages are not safe, applications using native methods are no longer immune to memory corruption errors. Native languages are more platform-dependent than Java, programs using native methods are less portable. They are harder to debug too. 67: Optimize judiciously There are three aphorisms concerning optimization that everyone should know: More computing sins are committed in the name of efficiency (without necessarily achieving it) than for any other single reason\u2014including blind stupidity. \u2014William A. Wulf We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. \u2014Donald E. Knuth We follow two rules in the matter of optimization: Rule 1. Don\u2019t do it. Rule 2 (for experts only). Don\u2019t do it yet\u2014that is, not until you have a perfectly clear and unoptimized solution. Rule 3 (added by us). Measure performance before and after each attempted optimization. \u2014M. A. Jackson It is easy to do more harm than good, especially if you optimize prematurely. In the process, you may produce software that is neither fast nor correct and cannot easily be fixed. Strive to write good programs rather than fast ones. If a good program is not fast enough, its architecture will allow it to be optimized. Good programs embody the principle of information hiding: where possible, they localize design decisions within individual components, so individual decisions can be changed without affecting the remainder of the system. Therefore you must think about performance during the design process. Strive to avoid design decisions that limit performance. Consider the performance consequences of your API design decisions. Luckily, it is generally the case that good API design is consistent with good performance. It is a very bad idea to warp an API to achieve good performance. The performance issue that caused you to warp the API may go away in a future release of the platform or other underlying software, but the warped API and the support headaches that come with it will be with you forever. Common wisdom says that programs spend 90 percent of their time in 10 percent of their code. Profiling tools can help you decide where to focus your optimization efforts. These tools give you runtime information, such as roughly how much time each method is consuming and how many times it is invoked. In addition to focusing your tuning efforts, this can alert you to the need for algorithmic changes. The more code in the system, the more important it is to use a profiler. The need to measure the effects of attempted optimization is even greater in Java than in more traditional languages such as C and C++, because Java has a weaker performance model: The relative cost of the various primitive operations is less well defined. To summarize, do not strive to write fast programs\u2014strive to write good ones; speed will follow. But do think about performance while you\u2019re designing systems, especially while you\u2019re designing APIs, wire-level protocols, and persistent data formats. When you\u2019ve finished building the system, measure its performance. 68: Adhere to generally accepted naming conventions Loosely speaking, naming conventions fall into two categories: typographical and grammatical. Details can be found in the The Java Language Specification . Typographical Package and module names should be hierarchical with the components separated by periods. Components should consist of lowercase alphabetic characters and, rarely, digits. The name of any package that will be used outside your organization should begin with your organization\u2019s Internet domain name with the components reversed The remainder of a package name should consist of one or more components describing the package. Components should be short, generally eight or fewer characters. Meaningful abbreviations are encouraged. Components should generally consist of a single word or abbreviation. Class and interface names, including enum and annotation type names, should consist of one or more words, with the first letter of each word capitalized. Method and field names follow the same typographical conventions as class and interface names, except that the first letter of a method or field name should be lowercase. Constant fields names should consist of one or more uppercase words separated by the underscore character. Constant fields constitute the only recommended use of underscores. Local variable names have similar typographical naming conventions to member names, except that abbreviations are permitted, as are individual characters and short sequences of characters whose meaning depends on the context in which they occur. Input parameters are a special kind of local variable. They should be named much more carefully than ordinary local variables, as their names are an integral part of their method\u2019s documentation. Type parameter names usually consist of a single letter. Most commonly it is one of these five: T for an arbitrary type, E for the element type of a collection, K and V for the key and value types of a map, and X for an exception. The return type of a function is usually R. A sequence of arbitrary types can be T, U, V or T1, T2, T3. Grammatical There are no grammatical naming conventions to speak of for packages. Instantiable classes, including enum types, are generally named with a singular noun or noun phrase, such as Thread, PriorityQueue, or ChessPiece. Non-instantiable utility classes are often named with a plural noun, such as Collectors or Collections. Interfaces are named like classes, for example, Collection or Comparator, or with an adjective ending in able or ible, for example, Runnable, Iterable, or Accessible. Annotation types have so many uses, no part of speech predominates. Methods that perform some action are generally named with a verb or verb phrase (including object), for example, append or drawImage. Methods that return a boolean value usually have names that begin with the word is or, less commonly, has, followed by a noun, noun phrase, or any word or phrase that functions as an adjective, for example, isDigit. Methods that return a non-boolean function or attribute of the object on which they\u2019re invoked are usually named with a noun, a noun phrase, or a verb phrase beginning with the verb get, for example, size, hashCode, or getTime. Instance methods that convert the type of an object, returning an independent object of a different type, are often called toType, for example, toString or toArray. Methods that return a view whose type differs from that of the receiving object are often called asType, for example, asList. Methods that return a primitive with the same value as the object on which they\u2019re invoked are often called typeValue, for example, intValue. Common names for static factories include from, of, valueOf, instance, getInstance, newInstance, getType, and newType. To summarize, internalize the standard naming conventions and learn to use them as second nature. The typographical conventions are straightforward and largely unambiguous; the grammatical conventions are more complex and looser. Use common sense. Chapter 10. Exceptions \u00b6 69: Use EXCEPTIONs only for exceptional conditions When used to best advantage, exceptions can improve a program\u2019s readability, reliability, and maintainability. When used improperly, they can have the opposite effect. Exceptions are to be used only for exceptional conditions; they should never be used for ordinary control flow. A well-designed API must not force its clients to use exceptions for ordinary control flow. 70: Use checked exceptions for recoverable conditions and runtime exceptions for programming errors Java provides three kinds of throwables: checked exceptions, runtime exceptions, and errors. Use checked exceptions for conditions from which the caller can reasonably be expected to recover. By throwing a checked exception, you force the caller to handle the exception in a catch clause or to propagate it outward. Each checked exception declared to throw indicates to the user that it is a possible outcome. There are two kinds of unchecked throwables: runtime exceptions and errors. They both are throwables that needn\u2019t, and generally shouldn\u2019t, be caught. If a program throws an unchecked exception or an error, it is generally the case that recovery is impossible and continued execution would do more harm than good. Use runtime exceptions to indicate programming errors. The great majority of runtime exceptions indicate precondition violations. A precondition violation is simply a failure by the client of an API to adhere to the contract established by the API specification. i.e. the contract for array access specifies that the array index must be between zero and the array length minus one, inclusive. ArrayIndexOutOfBoundsException indicates that this precondition was violated. If you believe a condition is likely to allow for recovery, use a checked exception; if not, use a runtime exception. If it isn\u2019t clear whether recovery is possible, you\u2019re probably better off using an unchecked exception. All of the unchecked throwables you implement should subclass RuntimeException all of the unchecked throwables you implement should subclass RuntimeException. Not only shouldn\u2019t you define Error subclasses, but with the exception of AssertionError, you shouldn\u2019t throw them either. To summarize, throw checked exceptions for recoverable conditions and unchecked exceptions for programming errors. When in doubt, throw unchecked exceptions. Don\u2019t define any throwables that are neither checked exceptions nor runtime exceptions. Provide methods on your checked exceptions to aid in recovery. 71: Avoid unnecessary use of checked exceptions Used properly, they can improve APIs and programs. Unlike return codes and unchecked exceptions, checked exceptions force programmers to deal with problems, enhancing reliability. That said, overuse of checked exceptions in APIs can make them far less pleasant to use. This burden on the user may be justified if the exceptional condition cannot be prevented by proper use of the API and the programmer using the API can take some useful action once confronted with the exception. Unless both of these conditions are met, an unchecked exception is appropriate. The easiest way to eliminate a checked exception is to return an optional of the desired result type. Instead of throwing a checked exception, the method simply returns an empty optional. The disadvantage of this technique is that the method can\u2019t return any additional information detailing its inability to perform the desired computation. 72: Favor the use of standard exceptions The Java libraries provide a set of exceptions that covers most of the exception-throwing needs of most APIs. Reusing standard exceptions makes your API easier to learn and use because it matches the established conventions that programmers are already familiar with. Programs using your API are easier to read because they aren\u2019t cluttered with unfamiliar exceptions. Fewer exception classes means a smaller memory footprint and less time spent loading classes. IllegalArgumentException is generally thrown when the caller passes in an argument whose value is inappropriate. IllegalStateException is generally thrown if the invocation is illegal because of the state of the receiving object. NullPointerException should be thrown rather than IllegalArgumentException if a null value is passed to a method where an instance is expected. Similarly, if a caller passes an out-of-range value in a parameter representing an index into a sequence, IndexOutOfBoundsException should be thrown rather than IllegalArgumentException. ConcurrentModificationException should be thrown if an object that was designed for use by a single thread (or with external synchronization) detects that it is being modified concurrently. UnsupportedOperationException should be thrown if an object does not support an attempted operation. This exception is used by classes that fail to implement one or more optional operations defined by an interface they implement. Do not reuse Exception, RuntimeException, Throwable, or Error directly. Treat these classes as if they were abstract. If an exception fits your needs, go ahead and use it, but only if the conditions under which you would throw it are consistent with the exception\u2019s documentation: reuse must be based on documented semantics, not just on name. Also, feel free to subclass a standard exception if you want to add more detail, but remember that exceptions are serializable. That alone is reason not to write your own exception class without good reason. 73: Throw exceptions appropriate to the abstraction Higher layers should catch lower-level exceptions and, in their place, throw exceptions that can be explained in terms of the higher-level abstraction. // Exception Translation try { ... // Use lower-level abstraction to do our bidding } catch ( LowerLevelException e ) { throw new HigherLevelException (...); } // Exception Chaining try { ... // Use lower-level abstraction to do our bidding } catch ( LowerLevelException cause ) { throw new HigherLevelException ( cause ); } While exception translation is superior to mindless propagation of exceptions from lower layers, it should not be overused. Where possible, the best way to deal with exceptions from lower layers is to avoid them, by ensuring that lower-level methods succeed. Sometimes you can do this by checking the validity of the higher-level method\u2019s parameters before passing them on to lower layers. In summary, if it isn\u2019t feasible to prevent or to handle exceptions from lower layers, use exception translation, unless the lower-level method happens to guarantee that all of its exceptions are appropriate to the higher level. Chaining provides the best of both worlds: it allows you to throw an appropriate higher-level exception, while capturing the underlying cause for failure analysis. 74: Document all exceptions thrown by each method Always declare checked exceptions individually, and document precisely the conditions under which each one is thrown using the Javadoc @throws tag. Don\u2019t take the shortcut of declaring that a method throws some superclass of multiple exception classes that it can throw. As an extreme example, don\u2019t declare that a public method throws Exception or, worse, throws Throwable. It is particularly important that methods in interfaces document the unchecked exceptions they may throw. This documentation forms a part of the interface\u2019s general contract and enables common behavior among multiple implementations of the interface. Use the Javadoc @throws tag to document each exception that a method can throw, but do not use the throws keyword on unchecked exceptions. If an exception is thrown by many methods in a class for the same reason, you can document the exception in the class\u2019s documentation comment rather than documenting it individually for each method. 75: Include failure-capture information in detail messages To capture a failure, the detail message of an exception should contain the values of all parameters and fields that contributed to the exception. Do not include passwords, encryption keys, and the like in detail messages. One way to ensure that exceptions contain adequate failure-capture information in their detail messages is to require this information in their constructors instead of a string detail message. The detail message can then be generated automatically to include the information. 76: Strive for failure atomicity Generally speaking, a failed method invocation should leave the object in the state that it was in prior to the invocation. A method with this property is said to be failure-atomic. There are several ways to achieve this effect. The simplest is to design immutable objects. If an object is immutable, failure atomicity is free. For methods that operate on mutable objects, the most common way to achieve failure atomicity is to check parameters for validity before performing the operation. This causes most exceptions to get thrown before object modification commences. public Object pop () { if ( size == 0 ) throw new EmptyStackException (); Object result = elements [-- size ] ; elements [ size ] = null ; // Eliminate obsolete reference return result ; } A third approach to achieving failure atomicity is to perform the operation on a temporary copy of the object and to replace the contents of the object with the temporary copy once the operation is complete. A last and far less common approach to achieving failure atomicity is to write recovery code that intercepts a failure that occurs in the midst of an operation, and causes the object to roll back its state to the point before the operation began. In summary, as a rule, any generated exception that is part of a method\u2019s specification should leave the object in the same state it was in prior to the method invocation. Where this rule is violated, the API documentation should clearly indicate what state the object will be left in. 77: Don\u2019t ignore exceptions An empty catch block defeats the purpose of exceptions, which is to force you to handle exceptional conditions. If you choose to ignore an exception, the catch block should contain a comment explaining why it is appropriate to do so, and the variable should be named ignored. The advice in this item applies equally to checked and unchecked exceptions. Whether an exception represents a predictable exceptional condition or a programming error, ignoring it with an empty catch block will result in a program that continues silently in the face of error. Properly handling an exception can avert failure entirely. Merely letting an exception propagate outward can at least cause the program to fail swiftly, preserving information to aid in debugging the failure. Chapter 11. Concurrency \u00b6 Synchronize access to shared mutable data \u00b6 The synchronized keyword ensures that only a single thread can execute a method or block at one time. Many programmers think of synchronization solely as a means of mutual exclusion, to prevent an object from being seen in an inconsistent state by one thread while it\u2019s being modified by another. In this view, an object is created in a consistent state and locked by the methods that access it. These methods observe the state and optionally cause a state transition, transforming the object from one consistent state to another. Not only does synchronization prevent threads from observing an object in an inconsistent state, but it ensures that each thread entering a synchronized method or block sees the effects of all previous modifications that were guarded by the same lock. The language specification guarantees that reading or writing a variable is atomic unless the variable is of type long or double. Synchronization is required for reliable communication between threads as well as for mutual exclusion. The consequences of failing to synchronize access to shared mutable data can be dire even if the data is atomically readable and writable. // Broken! - How long would you expect this program to run? public class StopThread { private static boolean stopRequested ; public static void main ( String [] args ) throws InterruptedException { Thread backgroundThread = new Thread (() -> { int i = 0 ; while ( ! stopRequested ) // ends up running forever i ++ ; }); backgroundThread . start (); TimeUnit . SECONDS . sleep ( 1 ); stopRequested = true ; } } The problem is that in the absence of synchronization, there is no guarantee as to when, if ever, the background thread will see the change in the value of stopRequested made by the main thread. // added methods to make it work private static synchronized void requestStop () { stopRequested = true ; } private static synchronized boolean stopRequested () { return stopRequested ; } Synchronization is not guaranteed to work unless both read and write operations are synchronized. The above solution works, but the locking in the second version of StopThread can be omitted if stopRequested is declared volatile . While the volatile modifier performs no mutual exclusion, it guarantees that any thread that reads the field will see the most recently written value. // Cooperative thread termination with a volatile field public class StopThread { private static volatile boolean stopRequested ; public static void main ( String [] args ) throws InterruptedException { Thread backgroundThread = new Thread (() -> { int i = 0 ; while ( ! stopRequested ) i ++ ; }); backgroundThread . start (); TimeUnit . SECONDS . sleep ( 1 ); stopRequested = true ; } } However you need to be careful when using volatile . // Broken - requires synchronization! private static volatile int nextSerialNumber = 0 ; public static int generateSerialNumber () { return nextSerialNumber ++ ; } The method\u2019s state consists of a single atomically accessible field, nextSerialNumber, and all possible values of this field are legal. Therefore, no synchronization is necessary to protect its invariants. Still, the method won\u2019t work properly without synchronization. The problem is that the increment operator (++) is not atomic. It performs two operations on the nextSerialNumber field: first it reads the value, and then it writes back a new value. If a second thread reads the field between the time a thread reads the old value and writes back a new one, the second thread will see the same value as the first and return the same serial number. This is a safety failure: the program computes the wrong results. One way to fix generateSerialNumber is to add the synchronized modifier to its declaration and remove the volatile modifier from nextSerialNumber. Finally, the best solution is to use existing package: // Lock-free synchronization with java.util.concurrent.atomic private static final AtomicLong nextSerialNum = new AtomicLong (); public static long generateSerialNumber () { return nextSerialNum . getAndIncrement (); } It is acceptable for one thread to modify a data object for a while and then to share it with other threads, synchronizing only the act of sharing the object reference. Other threads can then read the object without further synchronization, so long as it isn\u2019t modified again. Such objects are said to be effectively immutable. Transferring such an object reference from one thread to others is called safe publication. There are many ways to safely publish an object reference: you can store it in a static field as part of class initialization; you can store it in a volatile field, a final field, or a field that is accessed with normal locking; or you can put it into a concurrent collection. In summary, when multiple threads share mutable data, each thread that reads or writes the data must perform synchronization. If you need only inter-thread communication, and not mutual exclusion, the volatile modifier is an acceptable form of synchronization, but it can be tricky to use correctly. 79: Avoid excessive synchronization Excessive synchronization can cause reduced performance, deadlock, or even nondeterministic behavior. To avoid liveness and safety failures, never cede control to the client within a synchronized method or block. In other words, inside a synchronized region, do not invoke a method that is designed to be overridden, or one provided by a client in the form of a function object. // Broken - invokes alien method from synchronized block! public class ObservableSet < E > extends ForwardingSet < E > { public ObservableSet ( Set < E > set ) { super ( set ); } private final List < SetObserver < E >> observers = new ArrayList <> (); public void addObserver ( SetObserver < E > observer ) { synchronized ( observers ) { observers . add ( observer ); } } public boolean removeObserver ( SetObserver < E > observer ) { synchronized ( observers ) { return observers . remove ( observer ); } } private void notifyElementAdded ( E element ) { synchronized ( observers ) { for ( SetObserver < E > observer : observers ) observer . added ( this , element ); } } @Override public boolean add ( E element ) { boolean added = super . add ( element ); if ( added ) notifyElementAdded ( element ); return added ; } @Override public boolean addAll ( Collection <? extends E > c ) { boolean result = false ; for ( E element : c ) result |= add ( element ); // Calls notifyElementAdded return result ; } } // runs and prints 0-99 public static void main ( String [] args ) { ObservableSet < Integer > set = new ObservableSet <> ( new HashSet <> ()); set . addObserver (( s , e ) -> System . out . println ( e )); for ( int i = 0 ; i < 100 ; i ++ ) set . add ( i ); } Observers subscribe to notifications by invoking the addObserver method and unsubscribe by invoking the removeObserver method. In both cases, an instance of this callback interface is passed to the method. // structurally identical to BiConsumer<ObservableSet<E>,E> @FunctionalInterface public interface SetObserver < E > { // Invoked when an element is added to the observable set void added ( ObservableSet < E > set , E element ); } Now if we make some changes to addObserver set . addObserver ( new SetObserver <> () { public void added ( ObservableSet < Integer > s , Integer e ) { System . out . println ( e ); if ( e == 23 ) s . removeObserver ( this ); } }); It throws ConcurrentModificationException after it prints 0-23. We are trying to remove an element from a list in the midst of iterating over it, which is illegal. // Observer that uses a background thread needlessly set . addObserver ( new SetObserver <> () { public void added ( ObservableSet < Integer > s , Integer e ) { System . out . println ( e ); if ( e == 23 ) { ExecutorService exec = Executors . newSingleThreadExecutor (); try { exec . submit (() -> s . removeObserver ( this )). get (); } catch ( ExecutionException | InterruptedException ex ) { throw new AssertionError ( ex ); } finally { exec . shutdown (); } } } }); When we run this program now, we don\u2019t get an exception; we get a deadlock. The background thread calls s.removeObserver, which attempts to lock observers, but it can\u2019t acquire the lock, because the main thread already has the lock. All the while, the main thread is waiting for the background thread to finish removing the observer. // Alien method moved outside of synchronized block - open calls private void notifyElementAdded ( E element ) { List < SetObserver < E >> snapshot = null ; synchronized ( observers ) { snapshot = new ArrayList <> ( observers ); } for ( SetObserver < E > observer : snapshot ) observer . added ( this , element ); } We can fix above two issues by taking a \"snapshot\" of the observers list that can then be safely traversed without a lock. With this change, both of the previous examples run without exception or deadlock. There is also a better way to move the alien method invocations out of the synchronized block. The libraries provide a concurrent collection known as CopyOnWriteArrayList that is tailor-made for this purpose. This List implementation is a variant of ArrayList in which all modification operations are implemented by making a fresh copy of the entire underlying array. Because the internal array is never modified, iteration requires no locking and is very fast. For most uses, the performance of CopyOnWriteArrayList would be atrocious, but it\u2019s perfect for observer lists, which are rarely modified and often traversed. // Thread-safe observable set with CopyOnWriteArrayList private final List < SetObserver < E >> observers = new CopyOnWriteArrayList <> (); public void addObserver ( SetObserver < E > observer ) { observers . add ( observer ); } public boolean removeObserver ( SetObserver < E > observer ) { return observers . remove ( observer ); } private void notifyElementAdded ( E element ) { for ( SetObserver < E > observer : observers ) observer . added ( this , element ); } An alien method invoked outside of a synchronized region is known as an open call. Besides preventing failures, open calls can greatly increase concurrency. An alien method might run for an arbitrarily long period. If the alien method were invoked from a synchronized region, other threads would be denied access to the protected resource unnecessarily. As a rule, you should do as little work as possible inside synchronized regions. Obtain the lock, examine the shared data, transform it as necessary, and drop the lock. If you must perform some time-consuming activity, find a way to move it out of the synchronized region. In a multicore world, the real cost of excessive synchronization is not the CPU time spent getting locks; it is contention: the lost opportunities for parallelism and the delays imposed by the need to ensure that every core has a consistent view of memory. Another hidden cost of oversynchronization is that it can limit the VM\u2019s ability to optimize code execution. If you are writing a mutable class, you have two options: you can omit all synchronization and allow the client to synchronize externally if concurrent use is desired, or you can synchronize internally, making the class thread-safe. - You should choose the latter option only if you can achieve significantly higher concurrency with internal synchronization than you could by having the client lock the entire object externally. - The collections in java.util (with the exception of the obsolete Vector and Hashtable) take the former approach, while those in java.util.concurrent take the latter. - When in doubt, do not synchronize your class, but document that it is not thread-safe. - If you do synchronize your class internally, you can use various techniques to achieve high concurrency, such as lock splitting, lock striping, and nonblocking concurrency control. If a method modifies a static field and there is any possibility that the method will be called from multiple threads, you must synchronize access to the field internally. The field is essentially a global variable even if it is private because it can be read and modified by unrelated clients. The nextSerialNumber field used by the method generateSerialNumber exemplifies this situation. In summary, to avoid deadlock and data corruption, never call an alien method from within a synchronized region. More generally, keep the amount of work that you do from within synchronized regions to a minimum. When you are designing a mutable class, think about whether it should do its own synchronization. In the multicore era, it is more important than ever not to oversynchronize. Synchronize your class internally only if there is a good reason to do so, and document your decision clearly. 80: Prefer executors, tasks, and streams to threads The Java Executor Framework in java.util.concurrent is a flexible interface-based task execution facility. ExecutorService exec = Executors . newSingleThreadExecutor (); // create exec . execute ( runnable ); // submit work exec . shutdown (); // terminate gracefully You can do many more things with an executor service. wait for a particular task to complete (with the get method) wait for any or all of a collection of tasks to complete (using the invokeAny or invokeAll methods) wait for the executor service to terminate (using the awaitTermination method) retrieve the results of tasks one by one as they complete (using an ExecutorCompletionService) schedule tasks to run at a particular time or to run periodically (using a ScheduledThreadPoolExecutor) create a thread pool with a fixed or variable number of threads use the ThreadPoolExecutor class directly to configure nearly every aspect of a thread pool\u2019s operation. A cached thread pool is not a good choice for a heavily loaded production server. Submitted tasks are not queued but immediately handed off to a thread for execution. If no threads are available, a new one is created. If a server is so heavily loaded that all of its CPUs are fully utilized and more tasks arrive, more threads will be created, which will only make matters worse. In the executor framework, the unit of work and the execution mechanism are separate. The key abstraction is the unit of work, which is the task. There are two kinds of tasks: Runnable and Callable (which is like Runnable, except that it returns a value and can throw arbitrary exceptions). In Java 7, the Executor Framework was extended to support fork-join tasks, which are run by a special kind of executor service known as a fork-join pool. A fork-join task, represented by a ForkJoinTask instance, may be split up into smaller subtasks, and the threads comprising a ForkJoinPool not only process these tasks but \"steal\" tasks from one another to ensure that all threads remain busy, resulting in higher CPU utilization, higher throughput, and lower latency. Writing and tuning fork-join tasks is tricky. Parallel streams are written atop fork join pools and allow you to take advantage of their performance benefits with little effort, assuming they are appropriate for the task at hand. 81: Prefer concurrency utilities to wait and notify Given the difficulty of using wait and notify correctly, you should use the higher-level concurrency utilities instead. The higher-level utilities in java.util.concurrent fall into three categories: the Executor Framework; concurrent collections; and synchronizers. The concurrent collections are high-performance concurrent implementations of standard collection interfaces such as List, Queue, and Map, which managing their own synchronization internally. It is impossible to exclude concurrent activity from a concurrent collection; locking it will only slow the program. Because you can\u2019t exclude concurrent activity on concurrent collections, you can\u2019t atomically compose method invocations on them either. Therefore, concurrent collection interfaces were outfitted with state-dependent modify operations, which combine several primitives into a single atomic operation. // Concurrent canonicalizing map atop ConcurrentMap - not optimal private static final ConcurrentMap < String , String > map = new ConcurrentHashMap <> (); public static String intern ( String s ) { String previousValue = map . putIfAbsent ( s , s ); return previousValue == null ? s : previousValue ; } ConcurrentHashMap is optimized for retrieval operations, such as get. Therefore, it is worth invoking get initially and calling putIfAbsent only if get indicates that it is necessary. // Concurrent canonicalizing map atop ConcurrentMap - faster! public static String intern ( String s ) { String result = map . get ( s ); if ( result == null ) { result = map . putIfAbsent ( s , s ); if ( result == null ) result = s ; } return result ; } Concurrent collections make synchronized collections largely obsolete. For example, use ConcurrentHashMap in preference to Collections.synchronizedMap. Simply replacing synchronized maps with concurrent maps can dramatically increase the performance of concurrent applications. Synchronizers are objects that enable threads to wait for one another, allowing them to coordinate their activities. The most commonly used synchronizers are CountDownLatch and Semaphore. Less commonly used are CyclicBarrier and Exchanger. The most powerful synchronizer is Phaser. Countdown latches are single-use barriers that allow one or more threads to wait for one or more other threads to do something. The sole constructor for CountDownLatch takes an int that is the number of times the countDown method must be invoked on the latch before all waiting threads are allowed to proceed. // Simple framework for timing concurrent execution public static long time ( Executor executor , int concurrency , Runnable action ) throws InterruptedException { CountDownLatch ready = new CountDownLatch ( concurrency ); CountDownLatch start = new CountDownLatch ( 1 ); CountDownLatch done = new CountDownLatch ( concurrency ); for ( int i = 0 ; i < concurrency ; i ++ ) { executor . execute (() -> { ready . countDown (); // Tell timer we're ready try { start . await (); // Wait till peers are ready action . run (); } catch ( InterruptedException e ) { Thread . currentThread (). interrupt (); } finally { done . countDown (); // Tell timer we're done } }); } ready . await (); // Wait for all workers to be ready long startNanos = System . nanoTime (); start . countDown (); // And they're off! done . await (); // Wait for all workers to finish return System . nanoTime () - startNanos ; } The first, ready, is used by worker threads to tell the timer thread when they\u2019re ready. The worker threads then wait on the second latch, which is start. When the last worker thread invokes ready.countDown, the timer thread records the start time and invokes start.countDown, allowing all of the worker threads to proceed. Then the timer thread waits on the third latch, done, until the last of the worker threads finishes running the action and calls done.countDown. As soon as this happens, the timer thread awakens and records the end time. A few more details bear noting. The executor passed to the time method must allow for the creation of at least as many threads as the given concurrency level, or the test will never complete. This is known as a thread starvation deadlock. For interval timing, always use System.nanoTime rather than System.currentTimeMillis. System.nanoTime is both more accurate and more precise and is unaffected by adjustments to the system\u2019s real-time clock. Note that the code in this example won\u2019t yield accurate timings unless action does a fair amount of work. Accurate microbenchmarking is notoriously hard and is best done with the aid of a specialized framework such as jmh the three countdown latches in the previous example could be replaced by a single CyclicBarrier or Phaser instance. The resulting code would be a bit more concise but perhaps more difficult to understand. In summary, using wait and notify directly is like programming in \"concurrency assembly language,\" as compared to the higher-level language provided by java.util.concurrent. There is seldom, if ever, a reason to use wait and notify in new code. If you maintain code that uses wait and notify, make sure that it always invokes wait from within a while loop using the standard idiom. The notifyAll method should generally be used in preference to notify. If notify is used, great care must be taken to ensure liveness. 82: Document thread safety The presence of the synchronized modifier in a method declaration is an implementation detail, not a part of its API. It does not reliably indicate that a method is thread-safe. To enable safe concurrent use, a class must clearly document what level of thread safety it supports: Immutable\u2014Instances of this class appear constant. No external synchronization is necessary. Unconditionally thread-safe\u2014Instances of this class are mutable, but the class has sufficient internal synchronization that its instances can be used concurrently without the need for any external synchronization. Conditionally thread-safe\u2014Like unconditionally thread-safe, except that some methods require external synchronization for safe concurrent use. Not thread-safe\u2014Instances of this class are mutable. To use them concurrently, clients must surround each method invocation (or invocation sequence) with external synchronization of the clients\u2019 choosing. Thread-hostile\u2014This class is unsafe for concurrent use even if every method invocation is surrounded by external synchronization. Thread hostility usually results from modifying static data without synchronization. When a class or method is found to be thread-hostile, it is typically fixed or deprecated. There are some thread safety annotations in Java Concurrency in Practice, which are @Immutable, @ThreadSafe, and @NotThreadSafe. Documenting a conditionally thread-safe class requires care. You must indicate which invocation sequences require external synchronization, and which lock (or in rare cases, locks) must be acquired to execute these sequences. The description of a class\u2019s thread safety generally belongs in the class\u2019s doc comment, but methods with special thread safety properties should describe these properties in their own documentation comments. Lock fields should always be declared final. This is true whether you use an ordinary monitor lock (as shown above) or a lock from the java.util.concurrent.locks package. 83: Use lazy initialization judiciously Lazy initialization is the act of delaying the initialization of a field until its value is needed. If the value is never needed, the field is never initialized. This technique is applicable to both static and instance fields. While lazy initialization is primarily an optimization, it can also be used to break harmful circularities in class and instance initialization. The best advice for lazy initialization is \"don\u2019t do it unless you need to\" because it decreases the cost of initializing a class or creating an instance, at the expense of increasing the cost of accessing the lazily initialized field. Under most circumstances, normal initialization is preferable to lazy initialization. If you use lazy initialization to break an initialization circularity, use a synchronized accessor because it is the simplest, clearest alternative. // Lazy initialization of instance field - synchronized accessor private FieldType field ; private synchronized FieldType getField () { if ( field == null ) field = computeFieldValue (); return field ; } If you need to use lazy initialization for performance on a static field, use the lazy initialization holder class idiom. This idiom exploits the guarantee that a class will not be initialized until it is used. // Lazy initialization holder class idiom for static fields private static class FieldHolder { static final FieldType field = computeFieldValue (); } private static FieldType getField () { return FieldHolder . field ; } The beauty of this idiom is that the getField method is not synchronized and performs only a field access, so lazy initialization adds practically nothing to the cost of access. A typical VM will synchronize field access only to initialize the class. If you need to use lazy initialization for performance on an instance field, use the double-check idiom. // Double-check idiom for lazy initialization of instance fields // needs volatile because there is no locking once the field is initialized private volatile FieldType field ; private FieldType getField () { // this local variable declaration is to ensure that field is read only // once in the common case where it\u2019s already initialized // thus increases performance FieldType result = field ; if ( result == null ) { // First check (no locking) synchronized ( this ) { if ( field == null ) // Second check (with locking) field = result = computeFieldValue (); } } return result ; } In summary, you should initialize most fields normally, not lazily. If you must initialize a field lazily in order to achieve your performance goals or to break a harmful initialization circularity, then use the appropriate lazy initialization technique. For instance fields, it is the double-check idiom; for static fields, the lazy initialization holder class idiom. For instance fields that can tolerate repeated initialization, you may also consider the single-check idiom. 84: Don\u2019t depend on the thread scheduler Any program that relies on the thread scheduler for correctness or performance is likely to be nonportable. The best way to write a robust, responsive, portable program is to ensure that the average number of runnable threads is not significantly greater than the number of processors. This leaves the thread scheduler with little choice: it simply runs the runnable threads till they\u2019re no longer runnable. The program\u2019s behavior doesn\u2019t vary too much, even under radically different thread-scheduling policies. Note that the number of runnable threads isn\u2019t the same as the total number of threads, which can be much higher. Threads that are waiting are not runnable. The main technique for keeping the number of runnable threads low is to have each thread do some useful work, and then wait for more. Threads should not run if they aren\u2019t doing useful work. Threads should not busy-wait, repeatedly checking a shared object waiting for its state to change. // Awful CountDownLatch implementation - busy-waits incessantly! public class SlowCountDownLatch { private int count ; public SlowCountDownLatch ( int count ) { if ( count < 0 ) throw new IllegalArgumentException ( count + \" < 0\" ); this . count = count ; } public void await () { while ( true ) { synchronized ( this ) { if ( count == 0 ) return ; } } } public synchronized void countDown () { if ( count != 0 ) count -- ; } } When faced with a program that barely works because some threads aren\u2019t getting enough CPU time relative to others, resist the temptation to \"fix\" the program by putting in calls to Thread.yield. A better course of action is to restructure the application to reduce the number of concurrently runnable threads. Thread priorities are among the least portable features of Java. It is not unreasonable to tune the responsiveness of an application by tweaking a few thread priorities, but it is rarely necessary and is not portable. In summary, do not depend on the thread scheduler for the correctness of your program. The resulting program will be neither robust nor portable. As a corollary, do not rely on Thread.yield or thread priorities. These facilities are merely hints to the scheduler. Thread priorities may be used sparingly to improve the quality of service of an already working program, but they should never be used to \"fix\" a program that barely works. Chapter 12. Serialization \u00b6 85: Prefer alternatives to Java serialization Java deserialization is a clear and present danger as it is widely used both directly by applications and indirectly by Java subsystems such as RMI (Remote Method Invocation), JMX (Java Management Extension), and JMS (Java Messaging System). Deserialization of untrusted streams can result in remote code execution (RCE), denial-of-service (DoS), and a range of other exploits. Applications can be vulnerable to these attacks even if they did nothing wrong. -- Robert Seacord You open yourself up to attack whenever you deserialize a byte stream that you don\u2019t trust. The best way to avoid serialization exploits is never to deserialize anything. 86: Implement Serializable with great caution A major cost of implementing Serializable is that it decreases the flexibility to change a class\u2019s implementation once it has been released. When a class implements Serializable, its byte-stream encoding (or serialized form) becomes part of its exported API. Once you distribute a class widely, you are generally required to support the serialized form forever, just as you are required to support all other parts of the exported API. A second cost of implementing Serializable is that it increases the likelihood of bugs and security holes. Serialization is an extralinguistic mechanism for creating objects. Relying on the default deserialization mechanism can easily leave objects open to invariant corruption and illegal access. A third cost of implementing Serializable is that it increases the testing burden associated with releasing a new version of a class. You must ensure both that the serialization-deserialization process succeeds and that it results in a faithful replica of the original object. Classes representing active entities, such as thread pools, should rarely implement Serializable. Classes designed for inheritance should rarely implement Serializable, and interfaces should rarely extend it. Inner classes should not implement Serializable. 87: Consider using a custom serialized form Do not accept the default serialized form without first considering whether it is appropriate. The default serialized form is likely to be appropriate if an object\u2019s physical representation is identical to its logical content. Even if you decide that the default serialized form is appropriate, you often must provide a readObject method to ensure invariants and security. Using the default serialized form when an object\u2019s physical representation differs substantially from its logical data content has four disadvantages: It permanently ties the exported API to the current internal representation. It can consume excessive space. It can consume excessive time. It can cause stack overflows. Whether or not you use the default serialized form, you must impose any synchronization on object serialization that you would impose on any other method that reads the entire state of the object. Regardless of what serialized form you choose, declare an explicit serial version UID in every serializable class you write. Do not change the serial version UID unless you want to break compatibility with all existing serialized instances of a class. 88: Write readObject methods defensively Just as a constructor must check its arguments for validity and make defensive copies of parameters where appropriate, so must a readObject method. Loosely speaking, readObject is a constructor that takes a byte stream as its sole parameter. In normal use, the byte stream is generated by serializing a normally constructed instance. The problem arises when readObject is presented with a byte stream that is artificially constructed to generate an object that violates the invariants of its class. To summarize, anytime you write a readObject method, adopt the mind-set that you are writing a public constructor that must produce a valid instance regardless of what byte stream it is given. Do not assume that the byte stream represents an actual serialized instance. 89: For instance control, prefer enum types to readResolve To summarize, use enum types to enforce instance control invariants wherever possible. If this is not possible and you need a class to be both serializable and instance-controlled, you must provide a readResolve method and ensure that all of the class\u2019s instance fields are either primitive or transient. 90: Consider serialization proxies instead of serialized instances The serialization proxy pattern is reasonably straightforward. First, design a private static nested class that concisely represents the logical state of an instance of the enclosing class. It should have a single constructor, whose parameter type is the enclosing class. This constructor merely copies the data from its argument: it need not do any consistency checking or defensive copying. By design, the default serialized form of the serialization proxy is the perfect serialized form of the enclosing class. Both the enclosing class and its serialization proxy must be declared to implement Serializable. // Serialization proxy for Period class private static class SerializationProxy implements Serializable { private final Date start ; private final Date end ; SerializationProxy ( Period p ) { this . start = p . start ; this . end = p . end ; } private static final long serialVersionUID = 234098243823485285L ; // Any number will do (Item 87) } Next, add the following writeReplace method to the enclosing class. This method can be copied verbatim into any class with a serialization proxy: // writeReplace method for the serialization proxy pattern private Object writeReplace () { return new SerializationProxy ( this ); } The presence of this method on the enclosing class causes the serialization system to emit a SerializationProxy instance instead of an instance of the enclosing class. With this writeReplace method in place, the serialization system will never generate a serialized instance of the enclosing class, but an attacker might fabricate one in an attempt to violate the class\u2019s invariants. To guarantee that such an attack would fail, merely add this readObject method to the enclosing class: // readObject method for the serialization proxy pattern private void readObject ( ObjectInputStream stream ) throws InvalidObjectException { throw new InvalidObjectException ( \"Proxy required\" ); } Finally, provide a readResolve method on the SerializationProxy class that returns a logically equivalent instance of the enclosing class. The presence of this method causes the serialization system to translate the serialization proxy back into an instance of the enclosing class upon deserialization. // readResolve method for Period.SerializationProxy private Object readResolve () { return new Period ( start , end ); // Uses public constructor } In summary, consider the serialization proxy pattern whenever you find yourself having to write a readObject or writeObject method on a class that is not extendable by its clients. This pattern is perhaps the easiest way to robustly serialize objects with nontrivial invariants.","title":"Effective Java"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-2-creating-and-destroying-objects","text":"Consider static factory methods instead of constructors A class can provide a public static factory method, which is simply a static method that returns an instance of the class. A static factory method is not the same as the Factory Method pattern from Design Patterns Advantages : can use well-chosen name to make code easier to read not required to create a new object each time invoked save resource on expensive-to-create objects can return an object of any subtype of their return type requires the client to refer to the returned object by interface rather than implementation class an API can return objects without making their classes public, hiding implementation classes the class of the returned object can vary from call to call as a function of the input parameters the class of the returned object need not exist when the class containing the method is written Disadvantages : classes without public or protected constructors cannot be subclassed static factory methods are hard for programmers to find do not stand out in API documentation","title":"Chapter 2. Creating and Destroying Objects"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#side-note-about-service-provider-framework","text":"There are three essential components in a service provider framework: a service interface, which represents an implementation; a provider registration API, which providers use to register implementations; and a service access API, which clients use to obtain instances of the service. The service access API may allow clients to specify criteria for choosing an implementation. In the absence of such criteria, the API returns an instance of a default implementation, or allows the client to cycle through all available implementations. The service access API is the flexible static factory that forms the basis of the service provider framework. An optional fourth component of a service provider framework is a service provider interface, which describes a factory object that produces instances of the service interface. In the absence of a service provider interface, implementations must be instantiated reflectively. In the case of JDBC, Connection plays the part of the service interface, DriverManager.registerDriver is the provider registration API, DriverManager.getConnection is the service access API, and Driver is the service provider interface.","title":"Side note about service provider framework"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#some-common-names-for-static-factory-methods","text":"from \u2014 A type-conversion method that takes a single parameter and returns a corresponding instance of this type. i.e. Date.from(instant) of \u2014 An aggregation method that takes multiple parameters and returns an instance of this type that incorporates them, i.e. EnumSet.of(JACK, QUEEN, KING) valueOf \u2014 A more verbose alternative to from and of , i.e. BigInteger.valueOf(Integer.MAX_VALUE) instance or getInstance \u2014 Returns an instance that is described by its parameters (if any) but cannot be said to have the same value. i.e. StackWalker.getInstance(options) create or newInstance \u2014 Like instance or getInstance , except that the method guarantees that each call returns a new instance, i.e. Array.newInstance(classObject, arrayLen) getType \u2014 Like getInstance , but used if the factory method is in a different class. Type is the type of object returned by the factory method. i.e. Files.getFileStore(path) newType \u2014 Like newInstance, but used if the factory method is in a different class. Type is the type of object returned by the factory method. i.e. Files.newBufferedReader(path) type \u2014 A concise alternative to getType and newType. i.e. Collections.list(legacyLitany) Consider a builder when faced with many constructor parameters Instead of making the desired object directly, the client calls a constructor (or static factory) with all of the required parameters and gets a builder object. Then the client calls setter-like methods on the builder object to set each optional parameter of interest. Finally, the client calls a parameterless build method to generate the object, which is typically immutable. To detect invalid parameters as soon as possible, check parameter validity in the builder\u2019s constructor and methods. Check invariants involving multiple parameters in the constructor invoked by the build method. To ensure these invariants against attack, do the checks on object fields after copying parameters from the builder. If a check fails, throw an IllegalArgumentException . The Builder pattern is well suited to class hierarchies. Use a parallel hierarchy of builders, each nested in the corresponding class. public abstract class Pizza { public enum Topping { HAM , MUSHROOM , ONION , PEPPER , SAUSAGE } final Set < Topping > toppings ; abstract static class Builder < T extends Builder < T >> { // generic type with a recursive type parameter EnumSet < Topping > toppings = EnumSet . noneOf ( Topping . class ); public T addTopping ( Topping topping ) { toppings . add ( Objects . requireNonNull ( topping )); return self (); } abstract Pizza build (); // Subclasses must override this method to return \"this\" protected abstract T self (); } Pizza ( Builder <?> builder ) { toppings = builder . toppings . clone (); // See Item 50 } } public class NyPizza extends Pizza { public enum Size { SMALL , MEDIUM , LARGE } private final Size size ; public static class Builder extends Pizza . Builder < Builder > { private final Size size ; public Builder ( Size size ) { this . size = Objects . requireNonNull ( size ); } @Override public NyPizza build () { return new NyPizza ( this ); } @Override protected Builder self () { return this ; } } private NyPizza ( Builder builder ) { super ( builder ); size = builder . size ; } } NyPizza pizza = new NyPizza . Builder ( SMALL ). addTopping ( SAUSAGE ). addTopping ( ONION ). build (); the Builder pattern is a good choice when designing classes whose constructors or static factories would have more than a handful of parameters, especially if many of the parameters are optional or of identical type. Enforce the singleton property with a private constructor or an enum type Making a class a singleton can make it difficult to test its clients because it\u2019s impossible to substitute a mock implementation for a singleton unless it implements an interface that serves as its type. public class Elvis { // exactly one Elvis instance will exist once the Elvis class is initialized\u2014no more, no less public static final Elvis INSTANCE = new Elvis (); private Elvis () { ... } ... } The above approach is simple and guarantees a singleton instance. // another approach public class Elvis { private static final Elvis INSTANCE = new Elvis (); private Elvis () { ... } // All calls to Elvis.getInstance return the same object reference, and no other Elvis instance will ever be created public static Elvis getInstance () { return INSTANCE ; } } The above approach gives the flexibility to change mind for singleton, allows us write generic singleton factory, and a method reference can be used as a supplier i.e. Elvis::instance // a third way public enum Elvis { INSTANCE ; ... } The above approach is very concise, with serialization. To make sure even after the object is deserialized still a singleton, public class Elvis implements Serializable { private static final transient Elvis INSTANCE = new Elvis (); ... private Object readResolve () { return INSTANCE ; } } A single-element enum type is often the best way to implement a singleton. Note that you can\u2019t use this approach if your singleton must extend a superclass. Enforce noninstantiability with a private constructor a class can be made noninstantiable by including a private constructor. Prefer dependency injection to hardwiring resources Static utility classes and singletons are inappropriate for classes whose behavior is parameterized by an underlying resource. Dependency Injection: Pass the resource into the constructor when creating a new instance. It preserves immutability, and is equally applicable to constructors, static factories, and builders. A useful variant of the pattern is to pass a resource factory to the constructor. The Supplier interface, introduced in Java 8, is perfect for representing factories. Methods that take a Supplier on input should typically constrain the factory\u2019s type parameter using a bounded wildcard type to allow the client to pass in a factory that creates any subtype of a specified type. i.e. Mosaic create(Supplier<? extends Tile> tileFactory) { ... } In summary, do not use a singleton or static utility class to implement a class that depends on one or more underlying resources whose behavior affects that of the class, and do not have the class create these resources directly. Instead, pass the resources, or factories to create them, into the constructor (or static factory or builder). Avoid creating unnecessary objects Reuse can be both faster and more stylish. An object can always be reused if it is immutable. Some object creations are much more expensive than others. If you\u2019re going to need such an \"expensive object\" repeatedly, it may be advisable to cache it for reuse. One good example: static boolean isRomanNumeral ( String s ) { return s . matches ( \"^(?=.)M*(C[MD]|D?C{0,3})\" + \"(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\" ); } is expensive to create a Pattern Object. As the method is called repeatedly, more Pattern Objects are created. Better: public class RomanNumerals { private static final Pattern ROMAN = Pattern . compile ( \"^(?=.)M*(C[MD]|D?C{0,3})\" + \"(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\" ); static boolean isRomanNumeral ( String s ) { return ROMAN . matcher ( s ). matches (); } } Aside from the performance improvement, making a static final field for the otherwise invisible Pattern instance allows us to give it a name, which is far more readable than the regular expression itself. Prefer primitives to boxed primitives, and watch out for unintentional autoboxing. Avoiding object creation by maintaining your own object pool is a bad idea unless the objects in the pool are extremely heavyweight. Generally speaking, however, maintaining your own object pools clutters your code, increases memory footprint, and harms performance. Modern JVM implementations have highly optimized garbage collectors that easily outperform such object pools on lightweight objects. Eliminate obsolete object references A prog. lang with garbage collector does not mean you don't need to worry about memory management. The best way to eliminate an obsolete reference is to let the variable that contained the reference fall out of scope. This occurs naturally if you define each variable in the narrowest possible scope. Generally speaking, whenever a class manages its own memory, the programmer should be alert for memory leaks. Whenever an element is freed, any object references contained in the element should be nulled out. Another common source of memory leaks is caches. A third common source of memory leaks is listeners and other callbacks, which happens when clients register callbacks but don\u2019t deregister them explicitly. One way to ensure that callbacks are garbage collected promptly is to store only weak references to them, for instance, by storing them only as keys in a WeakHashMap. Avoid finalizers and cleaners Finalizers are unpredictable, often dangerous, and generally unnecessary. Cleaners are less dangerous than finalizers, but still unpredictable, slow, and generally unnecessary. It can take arbitrarily long between the time that an object becomes unreachable and the time its finalizer or cleaner runs. This means that you should never do anything time-critical in a finalizer or cleaner. And you should never depend on a finalizer or cleaner to update persistent state. Just have your class implement AutoCloseable, and require its clients to invoke the close method on each instance when it is no longer needed, typically using try-with-resources to ensure termination even in the face of exceptions. One detail worth mentioning is that the instance must keep track of whether it has been closed: the close method must record in a field that the object is no longer valid, and other methods must check this field and throw an IllegalStateException if they are called after the object has been closed. So what, if anything, are cleaners and finalizers good for? They have perhaps two legitimate uses. One is to act as a safety net in case the owner of a resource neglects to call its close method. A second legitimate use of cleaners concerns objects with native peers. A native peer is a native (non-Java) object to which a normal object delegates via native methods. Because a native peer is not a normal object, the garbage collector doesn\u2019t know about it and can\u2019t reclaim it when its Java peer is reclaimed. A cleaner or finalizer may be an appropriate vehicle for this task. How to write a cleaner public class Room implements AutoCloseable { private static final Cleaner cleaner = Cleaner . create (); // Resource that requires cleaning. Must not refer to Room! private static class State implements Runnable { int numJunkPiles ; // Number of junk piles in this room State ( int numJunkPiles ) { this . numJunkPiles = numJunkPiles ; } // Invoked by close method or cleaner @Override public void run () { System . out . println ( \"Cleaning room\" ); numJunkPiles = 0 ; } } // The state of this room, shared with our cleanable private final State state ; // Our cleanable. Cleans the room when it\u2019s eligible for gc private final Cleaner . Cleanable cleanable ; public Room ( int numJunkPiles ) { state = new State ( numJunkPiles ); cleanable = cleaner . register ( this , state ); } @Override public void close () { cleanable . clean (); } } State implements Runnable, and its run method is called at most once, by the Cleanable that we get when we register our State instance with our cleaner in the Room constructor. The call to the run method will be triggered by one of two things: Usually it is triggered by a call to Room\u2019s close method calling Cleanable\u2019s clean method. If the client fails to call the close method by the time a Room instance is eligible for garbage collection, the cleaner will (hopefully) call State\u2019s run method. It is critical that a State instance does not refer to its Room instance. If it did, it would create a circularity that would prevent the Room instance from becoming eligible for garbage collection. If clients surround all Room instantiations in try-with-resource blocks, automatic cleaning will never be required. public class Adult { public static void main ( String [] args ) {} try ( Room myRoom = new Room ( 7 )) { System . out . println ( \"Goodbye\" ); } catch () { } } } In summary, don\u2019t use cleaners, or in releases prior to Java 9, finalizers, except as a safety net or to terminate noncritical native resources. Even then, beware the indeterminacy and performance consequences. Prefer try-with-resources to try-finally Historically, a try-finally statement was the best way to guarantee that a resource would be closed properly, even in the face of an exception or return. // try-finally - No longer the best way to close resources! static String firstLineOfFile ( String path ) throws IOException { BufferedReader br = new BufferedReader ( new FileReader ( path )); try { return br . readLine (); } finally { br . close (); } } However, the code in both the try block and the finally block is capable of throwing exceptions. Java 7 introduced the try-with-resources statement. To be usable with this construct, a resource must implement the AutoCloseable interface, which consists of a single void-returning close method. static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader(new FileReader(path))) { return br.readLine(); } } Not only are the try-with-resources versions shorter and more readable than the originals, but they provide far better diagnostics. Multiple exceptions may be suppressed in order to preserve the exception that you actually want to see.","title":"Some common names for static factory methods"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-3-methods-common-to-all-objects","text":"ALTHOUGH Object is a concrete class, it is designed primarily for extension. All of its nonfinal methods (equals, hashCode, toString, clone, and finalize) have explicit general contracts because they are designed to be overridden. Obey the general contract when overriding EQUALS The easiest way to avoid problems is not to override the equals method, in which case each instance of the class is equal only to itself. This applies when: Each instance of the class is inherently unique. There is no need for the class to provide a \"logical equality\" test. A superclass has already overridden equals, and the superclass behavior is appropriate for this class. The class is private or package-private, and you are certain that its equals method will never be invoked. It is appropriate to override equals when a class has a notion of logical equality that differs from mere object identity and a superclass has not already overridden equals, generally the case for value classes. A value class is simply a class that represents a value, such as Integer or String. Overriding equals enables instances to serve as map keys or set elements with predictable, desirable behavior. One kind of value class that does not require the equals method to be overridden is a class that uses instance control to ensure that at most one object exists with each value. Enum types fall into this category. For these classes, logical equality is the same as object identity, so Object\u2019s equals method functions as a logical equals method. equals implements an equivalence relation. It has following properties: Reflexive: For any non-null reference value x, x.equals(x) must return true. Symmetric: For any non-null reference values x and y, x.equals(y) must return true if and only if y.equals(x) returns true. Transitive: For any non-null reference values x, y, z, if x.equals(y) returns true and y.equals(z) returns true, then x.equals(z) must return true. Consistent: For any non-null reference values x and y, multiple invocations of x.equals(y) must consistently return true or consistently return false, provided no information used in equals comparisons is modified. For any non-null reference value x, x.equals(null) must return false. Once you\u2019ve violated the equals contract, you simply don\u2019t know how other objects will behave when confronted with your object. There is no way to extend an instantiable class and add a value component while preserving the equals contract, unless you\u2019re willing to forgo the benefits of object-oriented abstraction. While there is no satisfactory way to extend an instantiable class and add a value component, there is a fine workaround: \"Favor composition over inheritance.\" Instead of having ColorPoint extend Point, give ColorPoint a private Point field and a public view method that returns the point at the same position as this color point. // Adds a value component without violating the equals contract public class ColorPoint { private final Point point ; private final Color color ; public ColorPoint ( int x , int y , Color color ) { point = new Point ( x , y ); this . color = Objects . requireNonNull ( color ); } public Point asPoint () { return point ; } @Override public boolean equals ( Object o ) { if ( ! ( o instanceof ColorPoint )) return false ; ColorPoint cp = ( ColorPoint ) o ; return cp . point . equals ( point ) && cp . color . equals ( color ); } ... // Remainder omitted } There are some classes in the Java platform libraries that do extend an instantiable class and add a value component. For example, java.sql.Timestamp extends java.util.Date and adds a nanoseconds field. The equals implementation for Timestamp does violate symmetry and can cause erratic behavior if Timestamp and Date objects are used in the same collection or are otherwise intermixed. Here is the recipe for a high-quality equals method: Use the == operator to check if the argument is a reference to this object Use the instanceof operator to check if the argument has the correct type Cast the argument to the correct type For each \"significant\" field in the class, check if that field of the argument matches the corresponding field of this object. For primitive fields whose type is not float or double, use the == operator for comparisons; for object reference fields, call the equals method recursively; for float fields, use the static Float.compare(float, float) method; and for double fields, use Double.compare(double, double). The special treatment of float and double fields is made necessary by the existence of Float.NaN, -0.0f and the analogous double values Some object reference fields may legitimately contain null. To avoid the possibility of a NullPointerException, check such fields for equality using the static method Objects.equals(Object, Object) For best performance, you should first compare fields that are more likely to differ, less expensive to compare, or, ideally, both. When you are finished writing your equals method, ask yourself three questions: Is it symmetric? Is it transitive? Is it consistent? Consistent use of the @Override annotation An excellent alternative to writing and testing these methods manually is to use Google\u2019s open source AutoValue framework, which automatically generates these methods for you, triggered by a single annotation on the class. Always override HASHCODE when you override EQUALS If you fail to do so, your class will violate the general contract for hashCode, which will prevent it from functioning properly in collections such as HashMap and HashSet: When the hashCode method is invoked on an object repeatedly during an execution of an application, it must consistently return the same value If two objects are equal according to the equals(Object) method, then calling hashCode on the two objects must produce the same integer result If two objects are unequal according to the equals(Object) method, it is not required that calling hashCode on each of the objects must produce distinct results (although it helps improve performance of hash tables) Receipe for fair good hashcode: Declare an int variable named result, and initialize it to the hash code c for the first significant field in your object For every remaining significant field f in your object, computer an int hash code c : if the field is a primitive type, compute Type.hashCode(f) if the field is an Object and this class\u2019s equals method compares the field by recursively invoking equals, recursively invoke hashCode on the field; if a more complex comparison is required, compute a \"canonical representation\" for this field and invoke hashCode on the canonical representation; if the value of the field is null, use 0 If the field is an array, treat it as if each significant element were a separate field. Do this for each hashcode c computed result = 31 * result + c; return result; Some other advices: If a class is immutable and the cost of computing the hash code is significant, you might consider caching the hash code in the object rather than recalculating it each time it is requested. Do not be tempted to exclude significant fields from the hash code computation to improve performance. Don\u2019t provide a detailed specification for the value returned by hashCode, so clients can\u2019t reasonably depend on it; this gives you the flexibility to change it. The AutoValue framework provides a fine alternative to writing equals and hashCode methods manually Always override TOSTRING The returned string of toString should be \"a concise but informative representation that is easy for a person to read.\" Providing a good toString implementation makes your class much more pleasant to use and makes systems using the class easier to debug. When practical, the toString method should return all of the interesting information contained in the object. It is recommended to specify the format of the return value for value classes. If you specify the format, it\u2019s usually a good idea to also provide a matching static factory or constructor so programmers can easily translate back and forth between the object and its string representation. Whether or not you decide to specify the format, you should clearly document your intentions. Whether or not you specify the format, provide programmatic access to the information contained in the value returned by toString. It makes no sense to write a toString method in a static utility class. Nor should you write a toString method in most enum types because Java provides a perfectly good one for you. You should, however, write a toString method in any abstract class whose subclasses share a common string representation. Override CLONE with Causion The Cloneable interface determines the behavior of Object\u2019s protected clone implementation: if a class implements Cloneable, Object\u2019s clone method returns a field-by-field copy of the object; otherwise it throws CloneNotSupportedException. Normally, implementing an interface says something about what a class can do for its clients. In this case, it modifies the behavior of a protected method on a superclass, which is very rare. A class implementing Cloneable is expected to provide a properly functioning public clone method. Note that immutable classes should never provide a clone method because it would merely encourage wasteful copying. @Override public PhoneNumber clone () { try { return ( PhoneNumber ) super . clone (); } catch ( CloneNotSupportedException e ) { throw new AssertionError (); // Can't happen } } In effect, the clone method functions as a constructor; you must ensure that it does no harm to the original object and that it properly establishes invariants on the clone. @Override public Stack clone () { try { Stack result = ( Stack ) super . clone (); result . elements = elements . clone (); return result ; } catch ( CloneNotSupportedException e ) { throw new AssertionError (); } } In order to make a class cloneable, it may be necessary to remove final modifiers from some fields. Java supports covariant return types. In other words, an overriding method\u2019s return type can be a subclass of the overridden method\u2019s return type. In effect, the clone method functions as a constructor; you must ensure that it does no harm to the original object and that it properly establishes invariants on the clone. Public clone methods should omit the throws clause, as methods that don\u2019t throw checked exceptions are easier to use. To prevent a subclass from implementing a working clone method, @Override protected final Object clone () throws CloneNotSupportedException { throw new CloneNotSupportedException (); } If you write a thread-safe class that implements Cloneable, remember that its clone method must be properly synchronized, just like any other method. A better approach to object copying is to provide a copy constructor or copy factory. A copy constructor is simply a constructor (or static method) that takes a single argument whose type is the class containing the constructor. Consider Implementing COMPARABLE It is the sole method in the Comparable interface but it permits order comparisons in addition to simple equality comparisons, and it is generic. By implementing Comparable, a class indicates that its instances have a natural ordering. Sorting an array of objects that implement Comparable is as simple as this: Arrays.sort(a); Returns a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. Throws ClassCastException if the specified object\u2019s type prevents it from being compared to this object. The implementor must ensure that sgn(x.compareTo(y)) == -sgn(y. compareTo(x)) for all x and y. (This implies that x.compareTo(y) must throw an exception if and only if y.compareTo(x) throws an exception.) The implementor must also ensure that the relation is transitive: (x. compareTo(y) > 0 && y.compareTo(z) > 0) implies x.compareTo(z) > 0. Finally, the implementor must ensure that x.compareTo(y) == 0 implies that sgn(x.compareTo(z)) == sgn(y.compareTo(z)), for all z. It is strongly recommended, but not required, that (x.compareTo(y) == 0) == (x.equals(y)) In Java 7 , static compare methods were added to all of Java\u2019s boxed primitive classes. Use of the relational operators < and > in compareTo methods is verbose and error-prone and no longer recommended. If a class has multiple significant fields, the order in which you compare them is critical. Start with the most significant field and work your way down. If a comparison results in anything other than zero (which represents equality), you\u2019re done. public int compareTo ( PhoneNumber pn ) { int result = Short . compare ( areaCode , pn . areaCode ); if ( result == 0 ) { result = Short . compare ( prefix , pn . prefix ); if ( result == 0 ) result = Short . compare ( lineNum , pn . lineNum ); } return result ; } In Java 8 , the Comparator interface was outfitted with a set of comparator construction methods, which enable fluent construction of comparators. These comparators can then be used to implement a compareTo method private static final Comparator < PhoneNumber > COMPARATOR = comparingInt (( PhoneNumber pn ) -> pn . areaCode ) . thenComparingInt ( pn -> pn . prefix ) . thenComparingInt ( pn -> pn . lineNum ); public int compareTo ( PhoneNumber pn ) { return COMPARATOR . compare ( this , pn ); } // alternative static Comparator < PhoneNumber > hashCodeOrder = new Comparator <> () { public int compare ( PhoneNumber o1 , PhoneNumber o2 ) { int result = Short . compare ( areaCode , o2 . areaCode ); if ( result == 0 ) { result = Short . compare ( prefix , pn . prefix ); if ( result == 0 ) result = Short . compare ( lineNum , pn . lineNum ); } return result ; } }; The method comparingInt is a static method that takes a key extractor function that maps an object reference to a key of type int and returns a comparator that orders instances according to that key. The method thenComparingInt is an instance method on Comparator that takes an int key extractor function, and returns a comparator that first applies the original comparator and then uses the extracted key to break ties. Classes implement the Comparable interface produce instances that can be easily sorted, searched, and used in comparison-based collections. When comparing field values in the implementations of the compareTo methods, avoid the use of the < and > operators. Instead, use the static compare methods in the boxed primitive classes or the comparator construction methods in the Comparator interface.","title":"Chapter 3. Methods Common to All Objects"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-4-classes-and-interfaces","text":"CLASSES and interfaces lie at the heart of the Java programming language. They are its basic units of abstraction. 15: Minimize the accessibility of classes and members A well-designed component hides all its implementation details, cleanly separating its API from its implementation. This concept, known as information hiding or encapsulation , is a fundamental tenet of software design that it decouples the components that comprise a system, allowing them to be developed, tested, optimized, used, understood, and modified in isolation. The rule of thumb is simple: make each class or member as inaccessible as possible. For top-level (non-nested) classes and interfaces, there are only two possible access levels: package-private and public. - By making it package-private, you make it part of the implementation rather than the exported API, and you can modify it, replace it, or eliminate it in a subsequent release without fear of harming existing clients. - If you make it public, you are obligated to support it forever to maintain compatibility. - If a package-private top-level class or interface is used by only one class, consider making the top-level class a private static nested class of the sole class that uses it. - If a method overrides a superclass method, it cannot have a more restrictive access level in the subclass than in the superclass. - Instance fields of public classes should rarely be public; classes with public mutable fields are not generally thread-safe. - It is wrong for a class to have a public static final array field, or an accessor that returns such a field, since clients will be able to modify the contents of the array. Access levels: private\u2014The member is accessible only from the top-level class where it is declared. package-private\u2014The member is accessible from any class in the package where it is declared. Technically known as default access, this is the access level you get if no access modifier is specified (except for interface members, which are public by default). protected\u2014The member is accessible from subclasses of the class where it is declared (subject to a few restrictions [JLS, 6.6.2]) and from any class in the package where it is declared. public\u2014The member is accessible from anywhere. As of Java 9 , there are two additional, implicit access levels introduced as part of the module system. A module is a grouping of packages, like a package is a grouping of classes. A module may explicitly export some of its packages via export declarations in its module declaration. A module may explicitly export some of its packages via export declarations in its module declaration (which is by convention contained in a source file named module-info.java). Public and protected members of unexported packages in a module are inaccessible outside the module; within the module, accessibility is unaffected by export declarations. Using the module system allows you to share classes among packages within a module without making them visible to the entire world. After carefully designing a minimal public API, you should prevent any stray classes, interfaces, or members from becoming part of the API. With the exception of public static final fields, which serve as constants, public classes should have no public fields. Ensure that objects referenced by public static final fields are immutable. 16: In public classes, use accessor methods, not public fields if a class is accessible outside its package, provide accessor methods to preserve the flexibility to change the class\u2019s internal representation. if a class is package-private or is a private nested class, there is nothing inherently wrong with exposing its data fields, assuming they do an adequate job of describing the abstraction provided by the class. Public classes should never expose mutable fields. It is less harmful, though still questionable, for public classes to expose immutable fields. It is, however, sometimes desirable for package-private or private nested classes to expose fields, whether mutable or immutable. 17: Minimize mutability Immutable classes are easier to design, implement, and use than mutable classes. They are less prone to error and are more secure. Five rules of thumb to make a class immutable: Don\u2019t provide methods that modify the object\u2019s state (known as mutators). Ensure that the class can\u2019t be extended by making the class final. Make all fields final. Make all fields private. Ensure exclusive access to any mutable components. Immutable objects are simple and inherently thread-safe; they require no synchronization and can be shared freely. Immutable objects provide failure atomicity for free; their state never changes, so there is no possibility of a temporary inconsistency. The major disadvantage of immutable classes is that they require a separate object for each distinct value. public class Complex { private final double re ; private final double im ; private Complex ( double re , double im ) { this . re = re ; this . im = im ; } public static Complex valueOf ( double re , double im ) { // sanity checks here return new Complex ( re , im ); } ... // Remainder unchanged } If you choose to have your immutable class implement Serializable and it contains one or more fields that refer to mutable objects, you must provide an explicit readObject or readResolve method, or use the ObjectOutputStream.writeUnshared and ObjectInputStream.readUnshared methods. Constructors should create fully initialized objects with all of their invariants established. 18: Favor composition over inheritance Unlike method invocation, inheritance violates encapsulation. A subclass depends on the implementation details of its superclass for its proper function. The superclass\u2019s implementation may change from release to release, and if it does, the subclass may break, even though its code has not been touched. One caveat is that wrapper classes are not suited for use in callback frameworks, wherein objects pass self-references to other objects for subsequent invocations (\"callbacks\"). Because a wrapped object doesn\u2019t know of its wrapper, it passes a reference to itself (this) and callbacks elude the wrapper. This is known as the SELF problem. Inheritance is appropriate only when a genuine subtype relationship exists between the subclass and the superclass. Even then, inheritance may lead to fragility if the subclass is in a different package from the superclass and the superclass is not designed for inheritance. Use composition and forwarding instead of inheritance, especially if an appropriate interface to implement a wrapper class exists. 19: Design and document for inheritance or else prohibit it What does it mean for a class to be designed and documented for inheritance? A class must document its self-use of overridable methods. A class may have to provide hooks into its internal workings in the form of judiciously chosen protected methods or, in rare instances, protected fields. Constructors must not invoke overridable methods Designing a class for inheritance requires great effort and places substantial limitations on the class. 20: Prefer interfaces to abstract classes Here is why: To implement the type defined by an abstract class, a class must be a subclass of the abstract class. Not for interfaces. Existing classes can easily be retrofitted to implement a new interface. Interfaces are ideal for defining mixins. A mixin is a type that a class can implement in addition to its \"primary type,\" to declare that it provides some optional behavior. It is called a mixin because it allows the optional functionality to be \"mixed in\" to the type\u2019s primary functionality. Interfaces allow for the construction of nonhierarchical type frameworks. Type hierarchies are great for organizing some things, but other things don\u2019t fall neatly into a rigid hierarchy. i.e. class Singer and class SongWriter. A singer can also be a songwriter. A class can implement both Singer and SongWriter, but not extending both if they are abstract classes. Interfaces enable safe, powerful functionality enhancements via the wrapper class idiom An interface is generally the best way to define a type that permits multiple implementations. If you export a nontrivial interface, you should strongly consider providing a skeletal implementation to go with it. To the extent possible, you should provide the skeletal implementation via default method. 21: Design interfaces for posterity In Java 8, the default method construct was added, with the intent of allowing the addition of methods to existing interfaces. Default methods are \"injected\" into existing implementations without the knowledge or consent of their implementors. It is not always possible to write a default method that maintains all invariants of every conceivable implementation. 22: Use interfaces only to define types When a class implements an interface, the interface serves as a type that can be used to refer to instances of the class. The constant interface pattern is a poor use of interfaces. Avoid making interfaces for the sake of defining constants. 23: Prefer class hierarchies to tagged classes Occasionally you may run across a class whose instances come in two or more flavors and contain a tag field indicating the flavor of the instance. In short, tagged classes are verbose, error-prone, and inefficient. A tagged class is just a pallid imitation of a class hierarchy. If you\u2019re tempted to write a class with an explicit tag field, think about whether the tag could be eliminated and the class replaced by a hierarchy. 24: Favor static member classes over nonstatic A nested class should exist only to serve its enclosing class. If a nested class would be useful in some other context, then it should be a top-level class. There are four kinds of nested classes: static member classes, nonstatic member classes, anonymous classes, and local classes. All but the first kind are known as inner classes. One common use of a static member class is as a public helper class, useful only in conjunction with its outer class. Despite the syntactic similarity, static member classes and nonstatic member classes are very different. Each instance of a nonstatic member class is implicitly associated with an enclosing instance of its containing class. If an instance of a nested class can exist in isolation from an instance of its enclosing class, then the nested class must be a static member class: it is impossible to create an instance of a nonstatic member class without an enclosing instance. If you declare a member class that does not require access to an enclosing instance, always put the static modifier in its declaration A common use of private static member classes is to represent components of the object represented by their enclosing class. i.e. many Map implementations have an internal Entry object for each key-value pair in the map. There are many limitations on the applicability of anonymous classes. You can\u2019t instantiate them except at the point they\u2019re declared. You can\u2019t perform instanceof tests or do anything else that requires you to name the class. You can\u2019t declare an anonymous class to implement multiple interfaces or to extend a class and implement an interface at the same time. Clients of an anonymous class can\u2019t invoke any members except those it inherits from its supertype. Because anonymous classes occur in the midst of expressions, they must be kept short\u2014about ten lines or fewer\u2014or readability will suffer. If a nested class needs to be visible outside of a single method or is too long to fit comfortably inside a method, use a member class. If each instance of a member class needs a reference to its enclosing instance, make it nonstatic; otherwise, make it static. Assuming the class belongs inside a method, if you need to create instances from only one location and there is a preexisting type that characterizes the class, make it an anonymous class; otherwise, make it a local class. 25: Limit source files to a single top-level class Never put multiple top-level classes or interfaces in a single source file. Keep each source file one class (nested classes are okay to have).","title":"Chapter 4 Classes and Interfaces"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-5-generics","text":"With generics, you tell the compiler what types of objects are permitted in each collection. This results in programs that are both safer and clearer, but these benefits, which are not limited to collections, come at a price. 26: Don\u2019t use raw types A class or interface whose declaration has one or more type parameters is a generic class or interface. Generic classes and interfaces are collectively known as generic types. Each generic type defines a set of parameterized types, i.e. E in List . Each generic type defines a raw type, which is the name of the generic type used without any accompanying type parameters. i.e. List is a raw type for List . If you use raw types, you lose all the safety and expressiveness benefits of generics. But it is fine to use types that are parameterized to allow insertion of arbitrary objects, such as List . If you want to use a generic type but you don\u2019t know or care what the actual type parameter is, you can use a question mark instead (unbounded wildcard type). It is the most general parameterized type, capable of holding anything. You can put any element into a collection with a raw type, easily corrupting the collection\u2019s type invariant; you can\u2019t put any element (other than null) into a Collection<?>. You must use raw types in class literals. i.e. List.class, String[].class, and int.class are all legal, but List .class and List<?>.class are not. This is the preferred way to use the instanceof operator with generic types: if ( o instanceof Set ) { // Raw type Set <?> s = ( Set <?> ) o ; // Wildcard type ... } 27: Eliminate unchecked warnings When you program with generics, you will see many compiler warnings: unchecked cast warnings, unchecked method invocation warnings, unchecked parameterized vararg type warnings, and unchecked conversion warnings. Many unchecked warnings are easy to eliminate. Eliminate every unchecked warning that you can. Unchecked warnings are important. Don\u2019t ignore them. Every unchecked warning represents the potential for a ClassCastException at runtime. If you can\u2019t eliminate a warning, but you can prove that the code that provoked the warning is typesafe, then (and only then) suppress the warning with an @SuppressWarnings(\"unchecked\") annotation. The SuppressWarnings annotation can be used on any declaration, from an individual local variable declaration to an entire class. Always use the SuppressWarnings annotation on the smallest scope possible. If you find yourself using the SuppressWarnings annotation on a method or constructor that\u2019s more than one line long, you may be able to move it onto a local variable declaration. Every time you use a @SuppressWarnings(\"unchecked\") annotation, add a comment saying why it is safe to do so. 28: Prefer lists to arrays Arrays differ from generic types in two important ways. First, arrays are covariant. This scary-sounding word means simply that if Sub is a subtype of Super, then the array type Sub[] is a subtype of the array type Super[]. Generics, by contrast, are invariant: for any two distinct types Type1 and Type2, List is neither a subtype nor a supertype of List . // Fails at runtime Object [] objectArray = new Long [ 1 ] ; objectArray [ 0 ] = \"I don't fit in\" ; // Throws ArrayStoreException // Fails at compile time List < Object > ol = new ArrayList < Long > (); // Incompatible types ol . add ( \"I don't fit in\" ); Second major difference between arrays and generics is that arrays are reified. Means arrays know and enforce their element type at runtime. Generics, by contrast, are implemented by erasure, means that they enforce their type constraints only at compile time and discard (or erase) their element type information at runtime. Because of these fundamental differences, arrays and generics do not mix well. For example, it is illegal to create an array of a generic type, a parameterized type, or a type parameter. 29: Favor generic types Writing your own generic types is a bit more difficult, but it\u2019s worth the effort to learn how. The first step in generifying a class is to add one or more type parameters to its declaration. The next step is to replace all the uses of the type Object with the appropriate type parameter and then try to compile. public class Stack < E > { private E [] elements ; // you can\u2019t create an array of a non-reifiable type, such as E private int size = 0 ; private static final int DEFAULT_INITIAL_CAPACITY = 16 ; @SuppressWarnings ( \"unchecked\" ) public Stack () { elements = ( E [] ) new Object [ DEFAULT_INITIAL_CAPACITY ] ; } public void push ( E e ) { ensureCapacity (); elements [ size ++] = e ; } public E pop () { if ( size == 0 ) throw new EmptyStackException (); E result = elements [-- size ] ; elements [ size ] = null ; // Eliminate obsolete reference return result ; } ... // no changes in isEmpty or ensureCapacity } Generic types are safer and easier to use than types that require casts in client code. When you design new types, make sure that they can be used without such casts. This will often mean making the types generic. 30: Favor generic methods Just as classes can be generic, so can methods. Static utility methods that operate on parameterized types are usually generic. All of the \"algorithm\" methods in Collections (such as binarySearch and sort) are generic. // Uses raw types - unacceptable! Compiles with warnings: unchecked call public static Set union ( Set s1 , Set s2 ) { Set result = new HashSet ( s1 ); result . addAll ( s2 ); return result ; } To fix these warnings and make the method typesafe, modify its declaration to declare a type parameter representing the element type for the three sets. The type parameter list, which declares the type parameters, goes between a method\u2019s modifiers and its return type. public static < E > Set < E > union ( Set < E > s1 , Set < E > s2 ) { Set < E > result = new HashSet <> ( s1 ); result . addAll ( s2 ); return result ; } On occasion, you will need to create an object that is immutable but applicable to many different types. Because generics are implemented by erasure, you can use a single object for all required type parameterizations, but you need to write a static factory method to repeatedly dole out the object for each requested type parameterization. // Generic singleton factory pattern private static UnaryOperator < Object > IDENTITY_FN = ( t ) -> t ; @SuppressWarnings ( \"unchecked\" ) public static < T > UnaryOperator < T > identityFunction () { return ( UnaryOperator < T > ) IDENTITY_FN ; } The cast of IDENTITY_FN to (UnaryFunction ) generates an unchecked cast warning, as UnaryOperator is not a UnaryOperator for every T. But the identity function is special: it returns its argument unmodified, so we know that it is typesafe to use it as a UnaryFunction , whatever the value of T. // Sample program to exercise generic singleton public static void main ( String [] args ) { String [] strings = { \"jute\" , \"hemp\" , \"nylon\" }; UnaryOperator < String > sameString = identityFunction (); for ( String s : strings ) System . out . println ( sameString . apply ( s )); Number [] numbers = { 1 , 2.0 , 3L }; UnaryOperator < Number > sameNumber = identityFunction (); for ( Number n : numbers ) System . out . println ( sameNumber . apply ( n )); } It is permissible, though relatively rare, for a type parameter to be bounded by some expression involving that type parameter itself. This is what\u2019s known as a recursive type bound. A common use of recursive type bounds is in connection with the Comparable interface, which defines a type\u2019s natural ordering. Many methods take a collection of elements implementing Comparable to sort it, search within it, calculate its minimum or maximum, and the like. To do these things, it is required that every element in the collection be comparable to every other element in it. // Using a recursive type bound to express mutual comparability public static < E extends Comparable < E >> E max ( Collection < E > c ); The type bound > may be read as \"any type E that can be compared to itself\". In summary, generic methods, like generic types, are safer and easier to use than methods requiring their clients to put explicit casts on input parameters and return values. 31: Use bounded wildcards to increase API flexibility Sometimes you need more flexibility than invariant typing can provide. // Wildcard type for a parameter that serves as an E producer public void pushAll ( Iterable <? extends E > src ) { for ( E e : src ) push ( e ); } // Two possible declarations for the swap method public static < E > void swap ( List < E > list , int i , int j ); public static void swap ( List <?> list , int i , int j ); For maximum flexibility, use wildcard types on input parameters that represent producers or consumers. If an input parameter is both a producer and a consumer, then wildcard types will do you no good: you need an exact type match, which is what you get without any wildcards. As a rule, if a type parameter appears only once in a method declaration, replace it with a wildcard. It doesn\u2019t seem right that we can\u2019t put an element back into the list that we just took it out of. The problem is that the type of list is List<?>, and you can\u2019t put any value except null into a List<?>. public static void swap ( List <?> list , int i , int j ) { swapHelper ( list , i , j ); } // Private helper method for wildcard capture private static < E > void swapHelper ( List < E > list , int i , int j ) { list . set ( i , list . set ( j , list . get ( i ))); } In summary, using wildcard types in your APIs, while tricky, makes the APIs far more flexible. If you write a library that will be widely used, the proper use of wildcard types should be considered mandatory. Remember the basic rule: producer-extends, consumer-super (PECS). In other words, if a parameterized type represents a T producer, use <? extends T>; if it represents a T consumer, use <? super T>. In our Stack example, pushAll\u2019s src parameter produces E instances for use by the Stack, so the appropriate type for src is Iterable<? extends E>; popAll\u2019s dst parameter consumes E instances from the Stack, so the appropriate type for dst is Collection<? super E>. 32: Combine generics and varargs judiciously The purpose of varargs is to allow clients to pass a variable number of arguments to a method, but it is a leaky abstraction: when you invoke a varargs method, an array is created to hold the varargs parameters; that array, which should be an implementation detail, is visible. As a consequence, you get confusing compiler warnings when varargs parameters have generic or parameterized types. If a method declares its varargs parameter to be of a non-reifiable type, the compiler generates a warning on the declaration. If the method is invoked on varargs parameters whose inferred type is non-reifiable, the compiler generates a warning on the invocation too. Recall that non-reifiable type is one whose runtime representation has less information than its compile-time representation, and that nearly all generic and parameterized types are non-reifiable. Heap pollution occurs when a variable of a parameterized type refers to an object that is not of that type. // Mixing generics and varargs can violate type safety! static void dangerous ( List < String > ... stringLists ) { List < Integer > intList = List . of ( 42 ); Object [] objects = stringLists ; objects [ 0 ] = intList ; // Heap pollution String s = stringLists [ 0 ] . get ( 0 ); // ClassCastException } It is unsafe to store a value in a generic varargs array parameter The SafeVarargs annotation constitutes a promise by the author of a method that it is typesafe. It is critical that you do not annotate a method with @SafeVarargs unless it actually is safe. It is unsafe to give another method access to a generic varargs parameter array, with two exceptions: it is safe to pass the array to another varargs method that is correctly annotated with @SafeVarargs, and it is safe to pass the array to a non-varargs method that merely computes some function of the contents of the array. 33: Consider typesafe heterogeneous containers Common uses of generics include collections, such as Set and Map , and single-element containers, such as ThreadLocal and AtomicReference . In all of these uses, it is the container that is parameterized. This limits you to a fixed number of type parameters per container. // Typesafe heterogeneous container pattern - API public class Favorites { public < T > void putFavorite ( Class < T > type , T instance ); public < T > T getFavorite ( Class < T > type ); } // Typesafe heterogeneous container pattern - client public static void main ( String [] args ) { Favorites f = new Favorites (); f . putFavorite ( String . class , \"Java\" ); f . putFavorite ( Integer . class , 0xcafebabe ); f . putFavorite ( Class . class , Favorites . class ); String favoriteString = f . getFavorite ( String . class ); int favoriteInteger = f . getFavorite ( Integer . class ); Class <?> favoriteClass = f . getFavorite ( Class . class ); System . out . printf ( \"%s %x %s%n\" , favoriteString , favoriteInteger , favoriteClass . getName ()); // prints \"Java cafebabe Favorites\" } A Favorites instance is typesafe: it will never return an Integer when you ask it for a String. It is also heterogeneous: unlike an ordinary map, all the keys are of different types. Therefore, we call Favorites a typesafe heterogeneous container . // Typesafe heterogeneous container pattern - implementation public class Favorites { private Map < Class <?> , Object > favorites = new HashMap <> (); public < T > void putFavorite ( Class < T > type , T instance ) { favorites . put ( Objects . requireNonNull ( type ), instance ); } public < T > T getFavorite ( Class < T > type ) { return type . cast ( favorites . get ( type )); } } Each Favorites instance is backed by a private Map<Class<?>, Object> called favorites. The thing to notice is that the wildcard type is nested: it\u2019s not the type of the map that\u2019s a wildcard type but the type of its key. This means that every key can have a different parameterized type: one can be Class , the next Class , and so on. That\u2019s where the heterogeneity comes from. The next thing to notice is that the value type of the favorites Map is simply Object. In other words, the Map does not guarantee the type relationship between keys and values, which is that every value is of the type represented by its key. The cast method is the dynamic analogue of Java\u2019s cast operator. It simply checks that its argument is an instance of the type represented by the Class object. If so, it returns the argument; otherwise it throws a ClassCastException. In summary, the normal use of generics, exemplified by the collections APIs, restricts you to a fixed number of type parameters per container. You can get around this restriction by placing the type parameter on the key rather than the container. You can use Class objects as keys for such typesafe heterogeneous containers. A Class object used in this fashion is called a type token. You can also use a custom key type.","title":"Chapter 5. Generics"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-6-enums-and-annotations","text":"JAVA supports two special-purpose families of reference types: a kind of class called an enum type, and a kind of interface called an annotation type. 34: Use enums instead of int constants An enumerated type is a type whose legal values consist of a fixed set of constants. Java\u2019s enum types are full-fledged classes, far more powerful than their counterparts in these other languages, where enums are essentially int values. They are classes that export one instance for each enumeration constant via a public static final field. Enum types are effectively final, by virtue of having no accessible constructors. Because clients can neither create instances of an enum type nor extend it, there can be no instances but the declared enum constants. They are a generalization of singletons (Item 3), which are essentially single-element enums. Enums provide compile-time type safety. If you declare a parameter to be of type Apple, you are guaranteed that any non-null object reference passed to the parameter is one of the three valid Apple values. Enum types with identically named constants coexist peacefully because each type has its own namespace. You can translate enums into printable strings by calling their toString method. Enum types let you add arbitrary methods and fields and implement arbitrary interfaces. They provide high-quality implementations of all the Object methods, they implement Comparable and Serializable, and their serialized form is designed to withstand most changes to the enum type. You should add methods or fields to an enum type when you need to associate data with its constants. An enum type can start life as a simple collection of enum constants and evolve over time into a full-featured abstraction. // Enum type with data and behavior public enum Planet { MERCURY ( 3.302e+23 , 2.439e6 ), VENUS ( 4.869e+24 , 6.052e6 ), EARTH ( 5.975e+24 , 6.378e6 ), MARS ( 6.419e+23 , 3.393e6 ), JUPITER ( 1.899e+27 , 7.149e7 ), SATURN ( 5.685e+26 , 6.027e7 ), URANUS ( 8.683e+25 , 2.556e7 ), NEPTUNE ( 1.024e+26 , 2.477e7 ); private final double mass ; // In kilograms private final double radius ; // In meters private final double surfaceGravity ; // In m / s^2 // Universal gravitational constant in m^3 / kg s^2 private static final double G = 6.67300E-11 ; // Constructor Planet ( double mass , double radius ) { this . mass = mass ; this . radius = radius ; surfaceGravity = G * mass / ( radius * radius ); } public double mass () { return mass ; } public double radius () { return radius ; } public double surfaceGravity () { return surfaceGravity ; } public double surfaceWeight ( double mass ) { return mass * surfaceGravity ; // F = ma } } public class WeightTable { public static void main ( String [] args ) { double earthWeight = Double . parseDouble ( args [ 0 ] ); double mass = earthWeight / Planet . EARTH . surfaceGravity (); for ( Planet p : Planet . values ()) System . out . printf ( \"Weight on %s is %f%n\" , p , p . surfaceWeight ( mass )); } } To associate data with enum constants, declare instance fields and write a constructor that takes the data and stores it in the fields. Enums are by their nature immutable, so all fields should be final. Fields can be public, but it is better to make them private and provide public accessors. If an enum is generally useful, it should be a top-level class; if its use is tied to a specific top-level class, it should be a member class of that top-level class. // Enum type that switches on its own value - questionable public enum Operation { PLUS , MINUS , TIMES , DIVIDE ; // Do the arithmetic operation represented by this constant public double apply ( double x , double y ) { switch ( this ) { case PLUS : return x + y ; case MINUS : return x - y ; case TIMES : return x * y ; case DIVIDE : return x / y ; } throw new AssertionError ( \"Unknown op: \" + this ); } } // Enum type with constant-specific method implementations public enum Operation { PLUS { public double apply ( double x , double y ){ return x + y ;}}, MINUS { public double apply ( double x , double y ){ return x - y ;}}, TIMES { public double apply ( double x , double y ){ return x * y ;}}, DIVIDE { public double apply ( double x , double y ){ return x / y ;}}; public abstract double apply ( double x , double y ); } It is better to declare an abstract apply method in the enum type, and override it with a concrete method for each constant in a constant-specific class body. Such methods are known as constant-specific method implementations. Enum types have an automatically generated valueOf(String) method that translates a constant\u2019s name into the constant itself. If you override the toString method in an enum type, consider writing a fromString method to translate the custom string representation back to the corresponding enum. The following code (with the type name changed appropriately) will do the trick for any enum, so long as each constant has a unique string representation: // Implementing a fromString method on an enum type private static final Map < String , Operation > stringToEnum = Stream . of ( values ()). collect ( toMap ( Object :: toString , e -> e )); // Returns Operation for string, if any public static Optional < Operation > fromString ( String symbol ) { return Optional . ofNullable ( stringToEnum . get ( symbol )); } Note that the Operation constants are put into the stringToEnum map from a static field initialization that runs after the enum constants have been created. Also note that the fromString method returns an Optional . This allows the method to indicate that the string that was passed in does not represent a valid operation, and it forces the client to confront this possibility. A disadvantage of constant-specific method implementations is that they make it harder to share code among enum constants. Switches on enums are good for augmenting enum types with constant-specific behavior. Enums are, generally speaking, comparable in performance to int constants. A minor performance disadvantage of enums is that there is a space and time cost to load and initialize enum types, but it is unlikely to be noticeable in practice. Use enums any time you need a set of constants whose members are known at compile time. It is not necessary that the set of constants in an enum type stay fixed for all time. 35: Use instance fields instead of ordinals All enums have an ordinal method, which returns the numerical position of each enum constant in its type. Never derive a value associated with an enum from its ordinal; store it in an instance field instead. public enum Ensemble { SOLO ( 1 ), DUET ( 2 ), TRIO ( 3 ), QUARTET ( 4 ), QUINTET ( 5 ), SEXTET ( 6 ), SEPTET ( 7 ), OCTET ( 8 ), DOUBLE_QUARTET ( 8 ), NONET ( 9 ), DECTET ( 10 ), TRIPLE_QUARTET ( 12 ); private final int numberOfMusicians ; Ensemble ( int size ) { this . numberOfMusicians = size ; } public int numberOfMusicians () { return numberOfMusicians ; } } 36: Use ENUMSET instead of bit fields If the elements of an enumerated type are used primarily in sets, it is traditional to use the int enum pattern. This representation lets you use the bitwise OR operation to combine several constants into a set, known as a bit field. This is called Bit Field enumeration constants. Obsolete! The java.util package provides the EnumSet class to efficiently represent sets of values drawn from a single enum type. This class implements the Set interface, providing all of the richness, type safety, and interoperability you get with any other Set implementation. // EnumSet - a modern replacement for bit fields public class Text { public enum Style { BOLD , ITALIC , UNDERLINE , STRIKETHROUGH } // Any Set could be passed in, but EnumSet is clearly best public void applyStyles ( Set < Style > styles ) { ... } } text . applyStyles ( EnumSet . of ( Style . BOLD , Style . ITALIC )); Just because an enumerated type will be used in sets, there is no reason to represent it with bit fields. The EnumSet class combines the conciseness and performance of bit fields with all the many advantages of enum types. 37: Use ENUMMAP instead of ordinal indexing It is rarely appropriate to use ordinals to index into arrays: use EnumMap instead. If the relationship you are representing is multidimensional, use EnumMap<..., EnumMap<...>>. // Using an EnumMap to associate data with an enum Map < Plant . LifeCycle , Set < Plant >> plantsByLifeCycle = new EnumMap <> ( Plant . LifeCycle . class ); for ( Plant . LifeCycle lc : Plant . LifeCycle . values ()) plantsByLifeCycle . put ( lc , new HashSet <> ()); for ( Plant p : garden ) plantsByLifeCycle . get ( p . lifeCycle ). add ( p ); System . out . println ( plantsByLifeCycle ); // Using a stream and an EnumMap to associate data with an enum System . out . println ( Arrays . stream ( garden ) . collect ( groupingBy ( p -> p . lifeCycle , () -> new EnumMap <> ( LifeCycle . class ), toSet ()))); 38: Emulate extensible enums with interfaces Enum types can implement arbitrary interfaces by defining an interface for the opcode type and an enum that is the standard implementation of the interface. In summary, while you cannot write an extensible enum type, you can emulate it by writing an interface to accompany a basic enum type that implements the interface. 39: Prefer annotations to naming patterns Historically, it was common to use naming patterns to indicate that some program elements demanded special treatment by a tool or framework. // Marker annotation type declaration import java.lang.annotation.* ; /** * Indicates that the annotated method is a test method. * Use only on parameterless static methods. */ @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface Test { ... } // annotation type with a parameter @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface ExceptionTest { Class <? extends Throwable > value (); } // Program containing marker annotations /** * Indicates that the annotated method is a test method that * must throw the designated exception to succeed. */ public class Sample { @Test public static void m1 () { } // Test should pass public static void m2 () { } @Test public static void m3 () { // Test should fail throw new RuntimeException ( \"Boom\" ); } public static void m4 () { } @Test public void m5 () { } // INVALID USE: nonstatic method public static void m6 () { } @Test public static void m7 () { // Test should fail throw new RuntimeException ( \"Crash\" ); } public static void m8 () { } } The declaration for the Test annotation type is itself annotated with Retention and Target annotations. Such annotations on annotation type declarations are known as meta-annotations. More generally, annotations don\u2019t change the semantics of the annotated code but enable it for special treatment by tools. As of Java 8, you can annotate the declaration of an annotation with the @Repeatable meta-annotation, to indicate that the annotation may be applied repeatedly to a single element. // Repeatable annotation type @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) @Repeatable ( ExceptionTestContainer . class ) public @interface ExceptionTest { Class <? extends Exception > value (); } @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface ExceptionTestContainer { ExceptionTest [] value (); } // Code containing a repeated annotation @ExceptionTest ( IndexOutOfBoundsException . class ) @ExceptionTest ( NullPointerException . class ) public static void doublyBad () { ... } All programmers should use the predefined annotation types that Java provides. Also, consider using the annotations provided by your IDE or static analysis tools. Such annotations can improve the quality of the diagnostic information provided by these tools. 40: Consistently use the OVERRIDE annotation @Override indicates that the annotated method declaration overrides a declaration in a supertype. Use the Override annotation on every method declaration that you believe to override a superclass declaration. 41: Use marker interfaces to define types A marker interface is an interface that contains no method declarations but merely designates (or \"marks\") a class that implements the interface as having some property. Marker interfaces have two advantages over marker annotations. First and foremost, marker interfaces define a type that is implemented by instances of the marked class; marker annotations do not. Another advantage of marker interfaces over marker annotations is that they can be targeted more precisely. The chief advantage of marker annotations over marker interfaces is that they are part of the larger annotation facility. Therefore, marker annotations allow for consistency in annotation-based frameworks. Ask yourself the question \"Might I want to write one or more methods that accept only objects that have this marking?\" If so, you should use a marker interface in preference to an annotation. This will make it possible for you to use the interface as a parameter type for the methods in question, which will result in the benefit of compile-time type checking. If you can convince yourself that you\u2019ll never want to write a method that accepts only objects with the marking, then you\u2019re probably better off using a marker annotation. If, additionally, the marking is part of a framework that makes heavy use of annotations, then a marker annotation is the clear choice. If you want to define a type that does not have any new methods associated with it, a marker interface is the way to go. If you want to mark program elements other than classes and interfaces or to fit the marker into a framework that already makes heavy use of annotation types, then a marker annotation is the correct choice. If you find yourself writing a marker annotation type whose target is ElementType.TYPE, take the time to figure out whether it really should be an annotation type or whether a marker interface would be more appropriate.","title":"Chapter 6. Enums and Annotations"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-7-lambdas-and-streams","text":"42: Prefer lambdas to anonymous classes Historically, interfaces (or, rarely, abstract classes) with a single abstract method were used as function types. Their instances, known as function objects, represent functions or actions. // Anonymous class instance as a function object - obsolete! Collections . sort ( words , new Comparator < String > () { public int compare ( String s1 , String s2 ) { return Integer . compare ( s1 . length (), s2 . length ()); } }); Anonymous classes were adequate for the classic objected-oriented design patterns requiring function objects, notably the Strategy pattern. The Comparator interface represents an abstract strategy for sorting; the anonymous class above is a concrete strategy for sorting strings. In Java 8, the language formalized the notion that interfaces with a single abstract method are special and deserve special treatment. These interfaces are now known as functional interfaces, and the language allows you to create instances of these interfaces using lambda expressions , or lambdas for short. // Lambda expression as function object (replaces anonymous class) Collections . sort ( words , ( s1 , s2 ) -> Integer . compare ( s1 . length (), s2 . length ())); Note that the types of the lambda (Comparator ), of its parameters (s1 and s2, both String), and of its return value (int) are not present in the code. The compiler deduces these types from context, using a process known as type inference. Omit the types of all lambda parameters unless their presence makes your program clearer. If the compiler generates an error telling you it can\u2019t infer the type of a lambda parameter, then specify it. The addition of lambdas to the language makes it practical to use function objects where it would not previously have made sense. Lambdas make it easy to implement constant-specific behavior using the former instead of the latter. Lambdas lack names and documentation; if a computation isn\u2019t self-explanatory, or exceeds a few lines, don\u2019t put it in a lambda. One line is ideal for a lambda, and three lines is a reasonable maximum. There are a few things you can do with anonymous classes that you can\u2019t do with lambdas. Lambdas are limited to functional interfaces. If you want to create an instance of an abstract class, you can do it with an anonymous class, but not a lambda. Finally, a lambda cannot obtain a reference to itself. In a lambda, the this keyword refers to the enclosing instance, which is typically what you want. In an anonymous class, the this keyword refers to the anonymous class instance. 43: Prefer method references to lambdas Java provides a way to generate function objects even more succinct than lambdas: method references. map . merge ( key , 1 , ( count , incr ) -> count + incr ); map . merge ( key , 1 , Integer :: sum ); The parameters count and incr don\u2019t add much value, and they take up a fair amount of space. Really, all the lambda tells you is that the function returns the sum of its two arguments. service . execute ( GoshThisClassNameIsHumongous :: action ); service . execute (() -> action ()); Along similar lines, the Function interface provides a generic static factory method to return the identity function, Function.identity(). It\u2019s typically shorter and cleaner not to use this method but to code the equivalent lambda inline: x -> x. In summary, method references often provide a more succinct alternative to lambdas. Where method references are shorter and clearer, use them; where they aren\u2019t, stick with lambdas. 44: Favor the use of standard functional interfaces If one of the standard functional interfaces does the job, you should generally use it in preference to a purpose-built functional interface. There are forty-three interfaces in java.util.Function. Remember six basic interfaces, you can derive the rest. The basic interfaces operate on object reference types. The Operator interfaces represent functions whose result and argument types are the same. The Predicate interface represents a function that takes an argument and returns a boolean. The Function interface represents a function whose argument and return types differ. The Supplier interface represents a function that takes no arguments and returns (or \"supplies\") a value. Finally, Consumer represents a function that takes an argument and returns nothing, essentially consuming its argument. Interface Function Signature Example UnaryOperator T apply(T t) String::toLowerCase BinaryOperator T apply(T t1, T t2) BigInteger::add Predicate boolean test(T t) Collection::isEmpty Function R apply(T t) Arrays::asList Supplier T get() Instant::now Consumer void accept(T t) System.out::println Don\u2019t be tempted to use basic functional interfaces with boxed primitives instead of primitive functional interfaces. The performance consequences of using boxed primitives for bulk operations can be deadly. You should seriously consider writing a purpose-built functional interface in preference to using a standard one if you need a functional interface that shares one or more of the following characteristics with Comparator: - It will be commonly used and could benefit from a descriptive name. - It has a strong contract associated with it. - It would benefit from custom default methods. Notice that the EldestEntryRemovalFunction interface is labeled with the @FunctionalInterface annotation. It is a statement of programmer intent that serves three purposes: it tells readers of the class and its documentation that the interface was designed to enable lambdas; it keeps you honest because the interface won\u2019t compile unless it has exactly one abstract method; and it prevents maintainers from accidentally adding abstract methods to the interface as it evolves. Always annotate your functional interfaces with the @FunctionalInterface annotation. Do not provide a method with multiple overloadings that take different functional interfaces in the same argument position if it could create a possible ambiguity in the client. The submit method of ExecutorService can take either a Callable or a Runnable, and it is possible to write a client program that requires a cast to indicate the correct overloading. The easiest way to avoid this problem is not to write overloadings that take different functional interfaces in the same argument position. Now that Java has lambdas, it is imperative that you design your APIs with lambdas in mind. Accept functional interface types on input and return them on output. It is generally best to use the standard interfaces provided in java.util.function.Function, but keep your eyes open for the relatively rare cases where you would be better off writing your own functional interface. 45: Use STREAMS judiciously The streams API was added in Java 8 to ease the task of performing bulk operations, sequentially or in parallel. This API provides two key abstractions: the stream, which represents a finite or infinite sequence of data elements, and the stream pipeline, which represents a multistage computation on these elements. The elements in a stream can come from anywhere. Common sources include collections, arrays, files, regular expression pattern matchers, pseudorandom number generators, and other streams. The data elements in a stream can be object references or primitive values. Three primitive types are supported: int, long, and double. A stream pipeline consists of a source stream followed by zero or more intermediate operations and one terminal operation. Each intermediate operation transforms the stream in some way, such as mapping each element to a function of that element or filtering out all elements that do not satisfy some condition. Intermediate operations all transform one stream into another, whose element type may be the same as the input stream or different from it. The terminal operation performs a final computation on the stream resulting from the last intermediate operation, such as storing its elements into a collection, returning a certain element, or printing all of its elements. Stream pipelines are evaluated lazily: evaluation doesn\u2019t start until the terminal operation is invoked, and data elements that aren\u2019t required in order to complete the terminal operation are never computed. This lazy evaluation is what makes it possible to work with infinite streams. Note that a stream pipeline without a terminal operation is a silent no-op. The streams API is fluent: it is designed to allow all of the calls that comprise a pipeline to be chained into a single expression. In fact, multiple pipelines can be chained together into a single expression. By default, stream pipelines run sequentially. Making a pipeline execute in parallel is as simple as invoking the parallel method on any stream in the pipeline, but it is seldom appropriate to do so. The streams API is sufficiently versatile that practically any computation can be performed using streams, but just because you can doesn\u2019t mean you should. When used inappropriately, they can make programs difficult to read and maintain. // Prints all large anagram groups in a dictionary iteratively public class Anagrams { public static void main(String[] args) throws IOException { File dictionary = new File(args[0]); int minGroupSize = Integer.parseInt(args[1]); Map<String, Set<String>> groups = new HashMap<>(); try (Scanner s = new Scanner(dictionary)) { while (s.hasNext()) { String word = s.next(); groups.computeIfAbsent(alphabetize(word), (unused) -> new TreeSet<>()).add(word); } } for (Set<String> group : groups.values()) if (group.size() >= minGroupSize) System.out.println(group.size() + \": \" + group); } private static String alphabetize(String s) { char[] a = s.toCharArray(); Arrays.sort(a); return new String(a); } } // Overuse of streams - don't do this! public class Anagrams { public static void main(String[] args) throws IOException { Path dictionary = Paths.get(args[0]); int minGroupSize = Integer.parseInt(args[1]); try (Stream<String> words = Files.lines(dictionary)) { words.collect( groupingBy(word -> word.chars().sorted() .collect(StringBuilder::new, (sb, c) -> sb.append((char) c), StringBuilder::append).toString())) .values().stream() .filter(group -> group.size() >= minGroupSize) .map(group -> group.size() + \": \" + group) .forEach(System.out::println); } } } Overusing streams makes programs hard to read and maintain. // Tasteful use of streams enhances clarity and conciseness public class Anagrams { public static void main(String[] args) throws IOException { Path dictionary = Paths.get(args[0]); int minGroupSize = Integer.parseInt(args[1]); try (Stream<String> words = Files.lines(dictionary)) { words.collect(groupingBy(word -> alphabetize(word))) .values().stream() .filter(group -> group.size() >= minGroupSize) .forEach(g -> System.out.println(g.size() + \": \" + g)); } } // alphabetize method is the same as in original version } In the absence of explicit types, careful naming of lambda parameters is essential to the readability of stream pipelines. Using helper methods is even more important for readability in stream pipelines than in iterative code because pipelines lack explicit type information and named temporary variables. Refactor existing code to use streams and use them in new code only where it makes sense to do so. Stream pipelines express repeated computation using function objects (typically lambdas or method references), while iterative code expresses repeated computation using code blocks. There are some things you can do from code blocks that you can\u2019t do from function objects: From a code block, you can read or modify any local variable in scope; from a lambda, you can only read final or effectively final variables From a code block, you can return from the enclosing method, break or continue an enclosing loop, or throw any checked exception that this method is declared to throw; from a lambda you can do none of these things. Conversely, streams make it very easy to do some things: Uniformly transform sequences of elements Filter sequences of elements Combine sequences of elements using a single operation (for example to add them, concatenate them, or compute their minimum) Accumulate sequences of elements into a collection, perhaps grouping them by some common attribute Search a sequence of elements for an element satisfying some criterion static Stream < BigInteger > primes () { return Stream . iterate ( TWO , BigInteger :: nextProbablePrime ); } public static void main ( String [] args ) { primes (). map ( p -> TWO . pow ( p . intValueExact ()). subtract ( ONE )) . filter ( mersenne -> mersenne . isProbablePrime ( 50 )) . limit ( 20 ) . forEach ( System . out :: println ); } This program starts with the primes, computes the corresponding Mersenne numbers, filters out all but the primes (the magic number 50 controls the probabilistic primality test), limits the resulting stream to twenty elements, and prints them out. // Iterative Cartesian product computation private static List < Card > newDeck () { List < Card > result = new ArrayList <> (); for ( Suit suit : Suit . values ()) for ( Rank rank : Rank . values ()) result . add ( new Card ( suit , rank )); return result ; } // Stream-based Cartesian product computation private static List < Card > newDeck () { return Stream . of ( Suit . values ()) . flatMap ( suit -> Stream . of ( Rank . values ()) . map ( rank -> new Card ( suit , rank ))) . collect ( toList ()); } And here is a stream-based implementation that makes use of the intermediate operation flatMap. This operation maps each element in a stream to a stream and then concatenates all of these new streams into a single stream. If you\u2019re not sure whether a task is better served by streams or iteration, try both and see which works better. 46: Prefer side-effect-free functions in streams Streams isn\u2019t just an API, it\u2019s a paradigm based on functional programming. In order to obtain the expressiveness, speed, and in some cases parallelizability that streams have to offer, you have to adopt the paradigm as well as the API. The most important part of the streams paradigm is to structure your computation as a sequence of transformations where the result of each stage is as close as possible to a pure function of the result of the previous stage. A pure function is one whose result depends only on its input: it does not depend on any mutable state, nor does it update any state. In order to achieve this, any function objects that you pass into stream operations, both intermediate and terminal, should be free of side-effects. // Uses the streams API but not the paradigm--Don't do this! Map<String, Long> freq = new HashMap<>(); try (Stream<String> words = new Scanner(file).tokens()) { words.forEach(word -> { freq.merge(word.toLowerCase(), 1L, Long::sum); }); } Simply put, it\u2019s not streams code at all; it\u2019s iterative code masquerading as streams code. This code is doing all its work in a terminal forEach operation, using a lambda that mutates external state (the frequency table). A forEach operation that does anything more than present the result of the computation performed by a stream is a \"bad smell in code,\" as is a lambda that mutates state. // Proper use of streams to initialize a frequency table Map<String, Long> freq; try (Stream<String> words = new Scanner(file).tokens()) { freq = words .collect(groupingBy(String::toLowerCase, counting())); } This snippet does the same thing as the previous one but makes proper use of the streams API. It\u2019s shorter and clearer. So why would anyone write it the other way? Because it uses tools they\u2019re already familiar with. The forEach operation should be used only to report the result of a stream computation, not to perform the computation. You can ignore the Collector interface and think of a collector as an opaque object that encapsulates a reduction strategy. Reduction means combining the elements of a stream into a single object. The object produced by a collector is typically a collection. There are three such collectors: toList(), toSet(), and toCollection(collectionFactory). They return, respectively, a set, a list, and a programmer-specified collection type. // Pipeline to get a top-ten list of words from a frequency table List < String > topTen = freq . keySet (). stream () . sorted ( comparing ( freq :: get ). reversed ()) . limit ( 10 ) . collect ( toList ()); It is customary and wise to statically import all members of Collectors because it makes stream pipelines more readable. The comparing method is a comparator construction method (Item 14) that takes a key extraction function. The function takes a word, and the \"extraction\" is actually a table lookup: the bound method reference freq::get looks up the word in the frequency table and returns the number of times the word appears in the file. Finally, we call reversed on the comparator, so we\u2019re sorting the words from most frequent to least frequent. The simplest map collector is toMap(keyMapper, valueMapper), which takes two functions, one of which maps a stream element to a key, the other, to a value. The more complicated forms of toMap, as well as the groupingBy method, give you various ways to provide strategies for dealing with such collisions. One way is to provide the toMap method with a merge function in addition to its key and value mappers. The merge function is a BinaryOperator , where V is the value type of the map. Any additional values associated with a key are combined with the existing value using the merge function. // Collector to generate a map from key to chosen element for key Map < Artist , Album > topHits = albums . collect ( // convert the stream of albums to a map, mapping each artist to the album // that has the best album by sales toMap ( Album :: artist , a -> a , maxBy ( comparing ( Album :: sales )))); In addition to the toMap method, the Collectors API provides the groupingBy method, which returns collectors to produce maps that group elements into categories based on a classifier function. The classifier function takes an element and returns the category into which it falls. This category serves as the element\u2019s map key. If you want groupingBy to return a collector that produces a map with values other than lists, you can specify a downstream collector in addition to a classifier. A downstream collector produces a value from a stream containing all the elements in a category. Alternatively, you can pass toCollection(collectionFactory), which lets you create the collections into which each category of elements is placed. Another simple use of the two-argument form of groupingBy is to pass counting() as the downstream collector. This results in a map that associates each category with the number of elements in the category. List < String > words = words . collect ( groupingBy ( word -> alphabetize ( word ))); Map < String , Long > freq = words . collect ( groupingBy ( String :: toLowerCase , counting ())); The collectors returned by the counting method are intended only for use as downstream collectors. The same functionality is available directly on Stream, via the count method, so there is never a reason to say collect(counting()). In summary, the essence of programming stream pipelines is side-effect-free function objects. This applies to all of the many function objects passed to streams and related objects. The terminal operation forEach should only be used to report the result of a computation performed by a stream, not to perform the computation. In order to use streams properly, you have to know about collectors. The most important collector factories are toList, toSet, toMap, groupingBy, and joining. 47: Prefer COLLECTION to STREAM as a return type Streams do not make iteration obsolete: writing good code requires combining streams and iteration judiciously. Stream fails to extend the iterator method. But it is easy to write an adapter method to enable stream iterators. // Adapter from Stream<E> to Iterable<E> public static < E > Iterable < E > iterableOf ( Stream < E > stream ) { return stream :: iterator ; } // Adapter from Iterable<E> to Stream<E> public static < E > Stream < E > streamOf ( Iterable < E > iterable ) { return StreamSupport . stream ( iterable . spliterator (), false ); } for ( ProcessHandle p : iterableOf ( ProcessHandle . allProcesses ())) { // Process the process } The Collection interface is a subtype of Iterable and has a stream method, so it provides for both iteration and stream access. Therefore, Collection or an appropriate subtype is generally the best return type for a public, sequence-returning method. Arrays also provide for easy iteration and stream access with the Arrays.asList and Stream.of methods. If the sequence you\u2019re returning is small enough to fit easily in memory, you\u2019re probably best off returning one of the standard collection implementations, such as ArrayList or HashSet. But do not store a large sequence in memory just to return it as a collection. In summary, when writing a method that returns a sequence of elements, remember that some of your users may want to process them as a stream while others may want to iterate over them. Try to accommodate both groups. If it\u2019s feasible to return a collection, do so. If you already have the elements in a collection or the number of elements in the sequence is small enough to justify creating a new one, return a standard collection such as ArrayList. Otherwise, consider implementing a custom collection. 48: Use caution when making STREAMs parallel Safety and liveness violations are a fact of life in concurrent programming, and parallel stream pipelines are no exception. Even under the best of circumstances, parallelizing a pipeline is unlikely to increase its performance if the source is from Stream.iterate, or the intermediate operation limit is used. As a rule, performance gains from parallelism are best on streams over ArrayList, HashMap, HashSet, and ConcurrentHashMap instances; arrays; int ranges; and long ranges. What these data structures have in common is that they can all be accurately and cheaply split into subranges of any desired sizes, which makes it easy to divide work among parallel threads. Another important factor that all of these data structures have in common is that they provide good-to-excellent locality of reference when processed sequentially: sequential element references are stored together in memory. Not only can parallelizing a stream lead to poor performance, including liveness failures; it can lead to incorrect results and unpredictable behavior. Under the right circumstances, it is possible to achieve near-linear speedup in the number of processor cores simply by adding a parallel call to a stream pipeline.","title":"Chapter 7. Lambdas and Streams"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-8-methods","text":"49: Check parameters for validity You should clearly document all restrictions and enforce them with checks at the beginning of the method body. For public and protected methods, use the Javadoc @throws tag to document the exception that will be thrown if a restriction on parameter values is violated. The Objects.requireNonNull method, added in Java 7, is flexible and convenient, so there\u2019s no reason to perform null checks manually anymore. In Java 9, a range-checking facility was added to java.util.Objects. This facility consists of three methods: checkFromIndexSize, checkFromToIndex, and checkIndex . Nonpublic methods can check their parameters using assertions, as shown below: // Private helper function for a recursive sort private static void sort ( long a [] , int offset , int length ) { assert a != null ; assert offset >= 0 && offset <= a . length ; assert length >= 0 && length <= a . length - offset ; ... // Do the computation } Assertions throw AssertionError if they fail. They have no effect and essentially no cost unless you enable them, which you do by passing the -ea (or -enableassertions) flag to the java command. 50: Make defensive copies when needed Even java is a safe language, you must program defensively, with the assumption that clients of your class will do their best to destroy its invariants. i.e. Date is a mutable class. If you implement something using Date that is intended to be immutable, it is still vulnerable. // Attack the internals of a Period instance Date start = new Date (); Date end = new Date (); Period p = new Period ( start , end ); end . setYear ( 78 ); // Modifies internals of p! As of Java 8, the obvious way to fix this problem is to use Instant (or Local-DateTime or ZonedDateTime) in place of a Date because Instant (and the other java.time classes) are immutable. Date is obsolete and should no longer be used in new code. It is essential to make a defensive copy of each mutable parameter to the constructor and to use the copies as components of the Period instance in place of the originals: // Repaired constructor - makes defensive copies of parameters public Period ( Date start , Date end ) { this . start = new Date ( start . getTime ()); this . end = new Date ( end . getTime ()); if ( this . start . compareTo ( this . end ) > 0 ) throw new IllegalArgumentException ( this . start + \" after \" + this . end ); } Note that defensive copies are made before checking the validity of the parameters, and the validity check is performed on the copies rather than on the originals. It protects the class against changes to the parameters from another thread during the window of vulnerability between the time the parameters are checked and the time they are copied. This is known as a time-of-check/time-of-use or TOCTOU attack . Do not use the clone method to make a defensive copy of a parameter whose type is subclassable by untrusted parties. While the replacement constructor successfully defends against the previous attack, it is still possible to mutate a Period instance, because its accessors offer access to its mutable internals: // Second attack on the internals of a Period instance Date start = new Date (); Date end = new Date (); Period p = new Period ( start , end ); p . end (). setYear ( 78 ); // Modifies internals of p! To defend against the second attack, merely modify the accessors to return defensive copies of mutable internal fields. // Repaired accessors - make defensive copies of internal fields public Date start () { return new Date ( start . getTime ()); } public Date end () { return new Date ( end . getTime ()); } In the accessors, unlike the constructor, it would be permissible to use the clone method to make the defensive copies. This is so because we know that the class of Period\u2019s internal Date objects is java.util.Date, and not some untrusted subclass. That said, you are generally better off using a constructor or static factory to copy an instance. Arguably, the real lesson in all of this is that you should, where possible, use immutable objects as components of your objects so that you that don\u2019t have to worry about defensive copying. In summary, if a class has mutable components that it gets from or returns to its clients, the class must defensively copy these components. If the cost of the copy would be prohibitive and the class trusts its clients not to modify the components inappropriately, then the defensive copy may be replaced by documentation outlining the client\u2019s responsibility not to modify the affected components. 51: Design method signatures carefully Choose method names carefully. Choose names that are understandable and consistent with other names in the same package. Avoid long method names. Don\u2019t go overboard in providing convenience methods. Too many methods make a class difficult to learn, use, document, test, and maintain. Avoid long parameter lists. Long sequences of identically typed parameters are especially harmful. Some techniques for shortening overly long parameter lists: break the method up into multiple methods, each of which requires only a subset of the parameters create helper classes to hold groups of parameters, typically static member classes adapt the Builder pattern For parameter types, favor interfaces over classes. i.e. there is no reason to ever write a method that takes HashMap on input\u2014use Map instead. Prefer two-element enum types to boolean parameters, unless the meaning of the boolean is clear from the method name. Enums make your code easier to read and to write. Also, they make it easy to add more options later. public enum TemperatureScale { FAHRENHEIT , CELSIUS } Thermometer . newInstance ( TemperatureScale . CELSIUS ) 52: Use overloading judiciously Because the following classify method is overloaded, and the choice of which overloading to invoke is made at compile time, it prints \"Unknown Collection\" three times. // Broken! public class CollectionClassifier { public static String classify ( Set <?> s ) { return \"Set\" ; } public static String classify ( List <?> lst ) { return \"List\" ; } public static String classify ( Collection <?> c ) { return \"Unknown Collection\" ; } public static void main ( String [] args ) { Collection <?>[] collections = { new HashSet < String > (), new ArrayList < BigInteger > (), new HashMap < String , String > (). values () }; for ( Collection <?> c : collections ) System . out . println ( classify ( c )); } } For all three iterations of the loop, the compile-time type of the parameter is the same: Collection<?>. The runtime type is different in each iteration, but this does not affect the choice of overloading. The best way to fix the CollectionClassifier program is to replace all three overloadings of classify with a single method that does explicit instanceof tests. Selection among overloaded methods is static, while selection among overridden methods is dynamic. A safe, conservative policy is never to export two overloadings with the same number of parameters. You can always give methods different names instead of overloading them. Do not overload methods to take different functional interfaces in the same argument position. 53: Use varargs judiciously Varargs methods, formally known as variable arity methods, accept zero or more arguments of a specified type. The varargs facility works by first creating an array whose size is the number of arguments passed at the call site, then putting the argument values into the array, and finally passing the array to the method. // The WRONG way to use varargs to pass one or more arguments! static int min ( int ... args ) { if ( args . length == 0 ) throw new IllegalArgumentException ( \"Too few arguments\" ); int min = args [ 0 ] ; for ( int i = 1 ; i < args . length ; i ++ ) if ( args [ i ] < min ) min = args [ i ] ; return min ; } // The right way to use varargs to pass one or more arguments static int min ( int firstArg , int ... remainingArgs ) { int min = firstArg ; for ( int arg : remainingArgs ) if ( arg < min ) min = arg ; return min ; } Exercise care when using varargs in performance-critical situations. Every invocation of a varargs method causes an array allocation and initialization. Suppose you\u2019ve determined that 95 percent of the calls to a method have three or fewer parameters. public void foo () { } public void foo ( int a1 ) { } public void foo ( int a1 , int a2 ) { } public void foo ( int a1 , int a2 , int a3 ) { } public void foo ( int a1 , int a2 , int a3 , int ... rest ) { } Like most performance optimizations, this technique usually isn\u2019t appropriate, but when it is, it\u2019s a lifesaver. In summary, varargs are invaluable when you need to define methods with a variable number of arguments. Precede the varargs parameter with any required parameters, and be aware of the performance consequences of using varargs. 54: Return empty COLLECTIONs or ARRAYs, not nulls In the unlikely event that you have evidence suggesting that allocating empty collections is harming performance, you can avoid the allocations by returning the same immutable empty collection repeatedly, as immutable objects may be shared freely. //The right way to return a possibly empty collection public List < Cheese > getCheeses () { return new ArrayList <> ( cheesesInStock ); } // Optimization - avoids allocating empty collections public List < Cheese > getCheeses () { return cheesesInStock . isEmpty () ? Collections . emptyList () : new ArrayList <> ( cheesesInStock ); } //The right way to return a possibly empty array public Cheese [] getCheeses () { return cheesesInStock . toArray ( new Cheese [ 0 ] ); } // Optimization - avoids allocating empty arrays private static final Cheese [] EMPTY_CHEESE_ARRAY = new Cheese [ 0 ] ; public Cheese [] getCheeses () { return cheesesInStock . toArray ( EMPTY_CHEESE_ARRAY ); } // Don\u2019t do this - preallocating the array harms performance! return cheesesInStock . toArray ( new Cheese [ cheesesInStock . size () ] ); In summary, never return null in place of an empty array or collection. It makes your API more difficult to use and more prone to error, and it has no performance advantages. 55: Return OPTIONALs judiciously Prior to Java 8, there were two approaches you could take when writing a method that was unable to return a value under certain circumstances. Either you could throw an exception, or you could return null. Exceptions should be reserved for exceptional conditions (Item 69), and throwing an exception is expensive because the entire stack trace is captured when an exception is created. If a method returns null, clients must contain special-case code to deal with the possibility of a null return. In Java 8, the Optional class represents an immutable container that can hold either a single non-null T reference or nothing at all. An optional that contains nothing is said to be empty. A value is said to be present in an optional that is not empty. An optional is essentially an immutable collection that can hold at most one element. Optional does not implement Collection , but it could in principle. A method that conceptually returns a T but may be unable to do so under certain circumstances can instead be declared to return an Optional . This allows the method to return an empty result to indicate that it couldn\u2019t return a valid result. An Optional-returning method is more flexible and easier to use than one that throws an exception, and it is less error-prone than one that returns null. // Returns maximum value in collection - throws exception if empty public static < E extends Comparable < E >> E max ( Collection < E > c ) { if ( c . isEmpty ()) throw new IllegalArgumentException ( \"Empty collection\" ); E result = null ; for ( E e : c ) if ( result == null || e . compareTo ( result ) > 0 ) result = Objects . requireNonNull ( e ); return result ; } // Returns maximum value in collection as an Optional<E> public static < E extends Comparable < E >> Optional < E > max ( Collection < E > c ) { if ( c . isEmpty ()) return Optional . empty (); E result = null ; for ( E e : c ) if ( result == null || e . compareTo ( result ) > 0 ) result = Objects . requireNonNull ( e ); return Optional . of ( result ); } If a method returns an optional, the client gets to choose what action to take if the method can\u2019t return a value. You can specify a default value. // Using an optional to provide a chosen default value String lastWordInLexicon = max ( words ). orElse ( \"No words...\" ); // Using an optional to throw a chosen exception Toy myToy = max ( toys ). orElseThrow ( TemperTantrumException :: new ); // Using optional when you know there\u2019s a return value Element lastNobleGas = max ( Elements . NOBLE_GASES ). get (); Occasionally you may be faced with a situation where it\u2019s expensive to get the default value, and you want to avoid that cost unless it\u2019s necessary. For these situations, Optional provides a method that takes a Supplier and invokes it only when necessary. Optional < ProcessHandle > parentProcess = ph . parent (); System . out . println ( \"Parent PID: \" + ( parentProcess . isPresent () ? String . valueOf ( parentProcess . get (). pid ()) : \"N/A\" )); // can be replaced by this one, which uses Optional\u2019s map function\\ System . out . println ( \"Parent PID: \" + ph . parent (). map ( h -> String . valueOf ( h . pid ())). orElse ( \"N/A\" )); Container types, including collections, maps, streams, arrays, and optionals should not be wrapped in optionals. Rather than returning an empty Optional >, you should simply return an empty List . As a rule, you should declare a method to return Optional if it might not be able to return a result and clients will have to perform special processing if no result is returned. You should never return an optional of a boxed primitive type, with the possible exception of the \"minor primitive types,\" Boolean, Byte, Character, Short, and Float. There are OptionalInt, OptionalLong, and OptionalDouble available for use. It is almost never appropriate to use an optional as a key, value, or element in a collection or array. In summary, if you find yourself writing a method that can\u2019t always return a value and you believe it is important that users of the method consider this possibility every time they call it, then you should probably return an optional. You should, however, be aware that there are real performance consequences associated with returning optionals; for performance-critical methods, it may be better to return a null or throw an exception. Finally, you should rarely use an optional in any other capacity than as a return value. 56: Write doc comments for all exposed API elements Javadoc generates API documentation automatically from source code with specially formatted documentation comments, more commonly known as doc comments. To document your API properly, you must precede every exported class, interface, constructor, method, and field declaration with a doc comment. The doc comment for a method should describe succinctly the contract between the method and its client. The contract should say what the method does rather than how it does its job. The doc comment should enumerate all of the method\u2019s preconditions. Methods should document any side effects. A side effect is an observable change in the state of the system that is not obviously required in order to achieve the postcondition. i.e. if a method starts new threads, it should be documented. To describe a method\u2019s contract fully, the doc comment should have an @param tag for every parameter, an @return tag unless the method has a void return type, and an @throws tag for every exception thrown by the method. By convention, the text following an @param tag or @return tag should be a noun phrase describing the value represented by the parameter or return value. The text following an @throws tag should consist of the word \"if,\" followed by a clause describing the conditions under which the exception is thrown. /** * Returns the element at the specified position in this list. * * <p>This method is <i>not</i> guaranteed to run in constant * time. In some implementations it may run in time proportional * to the element position. * * @param index index of element to return; must be * non-negative and less than the size of this list * @return the element at the specified position in this list * @throws IndexOutOfBoundsException if the index is out of range * ({@code index < 0 || index >= this.size()}) */ E get ( int index ); The Javadoc utility translates doc comments into HTML, and arbitrary HTML elements in doc comments end up in the resulting HTML document. The Javadoc {@code} tag causes the code fragment to be rendered in code font, and it suppresses processing of HTML markup and nested Javadoc tags in the code fragment. {@literal} tag has similar effect. When you design a class for inheritance, you must document its self-use patterns, so programmers know the semantics of overriding its methods. Should be documented using the @implSpec tag. As of Java 9, the Javadoc utility still ignores the @implSpec tag unless you pass the command line switch -tag \"implSpec Implementation Requirements:\". Doc comments should be readable both in the source code and in the generated documentation. To avoid confusion, no two members or constructors in a class or interface should have the same summary description. Be careful if the intended summary description contains a period, because the period can prematurely terminate the description. In Java 9, a client-side index was added to the HTML generated by Javadoc. This index, which eases the task of navigating large API documentation sets, takes the form of a search box in the upper-right corner of the page. API elements, such as classes, methods, and fields, are indexed automatically. To add index, use {@index} tag: * This method complies with the {@index IEEE 754} standard. Generics, enums, and annotations require special care in doc comments. When documenting a generic type or method, be sure to document all type parameters When documenting an enum type, be sure to document the constants as well as the type and any public methods When documenting an annotation type, be sure to document any members as well as the type itself. /** * (omitted) * @param <K> the type of keys maintained by this map * @param <V> the type of mapped values */ public interface Map < K , V > { ... } /** * An instrument section of a symphony orchestra. */ public enum OrchestraSection { /** Woodwinds, such as flute, clarinet, and oboe. */ WOODWIND , /** Brass instruments, such as french horn and trumpet. */ BRASS , /** Percussion instruments, such as timpani and cymbals. */ PERCUSSION , /** Stringed instruments, such as violin and cello. */ STRING ; } /** * Indicates that the annotated method is a test method that * must throw the designated exception to pass. */ @Retention ( RetentionPolicy . RUNTIME ) @Target ( ElementType . METHOD ) public @interface ExceptionTest { /** * The exception that the annotated test method must throw * in order to pass. (The test is permitted to throw any * subtype of the type described by this class object.) */ Class <? extends Throwable > value (); } Package-level doc comments should be placed in a file named package-info.java and it must contain a package declaration and may contain annotations on this declaration. Similarly, if you elect to use the module system, module-level comments should be placed in the module-info.java file. Whether or not a class or static method is thread-safe, you should document its thread-safety level. If a class is serializable, you should document its serialized form. To summarize, documentation comments are the best, most effective way to document your API. Their use should be considered mandatory for all exported API elements. Adopt a consistent style that adheres to standard conventions. Remember that arbitrary HTML is permissible in documentation comments and that HTML metacharacters must be escaped.","title":"Chapter 8. Methods"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-9-general-programming","text":"57: Minimize the scope of local variables The most powerful technique for minimizing the scope of a local variable is to declare it where it is first used. Nearly every local variable declaration should contain an initializer. A final technique to minimize the scope of local variables is to keep methods small and focused. If you combine two activities in the same method, local variables relevant to one activity may be in the scope of the code performing the other activity. 58: Prefer for-each loops to traditional for loops Unfortunately, there are three common situations where you can\u2019t use for-each: - Destructive filtering\u2014If you need to traverse a collection removing selected elements, then you need to use an explicit iterator so that you can call its remove method. - You can often avoid explicit traversal by using Collection\u2019s removeIf method, added in Java 8. - Transforming\u2014If you need to traverse a list or array and replace some or all of the values of its elements, then you need the list iterator or array index in order to replace the value of an element. - Parallel iteration\u2014If you need to traverse multiple collections in parallel, then you need explicit control over the iterator or index variable so that all iterators or index variables can be advanced in lockstep. In summary, the for-each loop provides compelling advantages over the traditional for loop in clarity, flexibility, and bug prevention, with no performance penalty. Use for-each loops in preference to for loops wherever you can. 59: Know and use the libraries By using a standard library, you take advantage of the knowledge of the experts who wrote it and the experience of those who used it before you. As of Java 7, you should no longer use Random. For most uses, the random number generator of choice is now ThreadLocalRandom. It produces higher quality random numbers, and it\u2019s very fast. A second advantage of using the libraries is that you don\u2019t have to waste your time writing ad hoc solutions to problems that are only marginally related to your work. A third advantage of using standard libraries is that their performance tends to improve over time, with no effort on your part. A fourth advantage of using libraries is that they tend to gain functionality over time. A final advantage of using the standard libraries is that you place your code in the mainstream. Such code is more easily readable, maintainable, and reusable by the multitude of developers. Numerous features are added to the libraries in every major release, and it pays to keep abreast of these additions. Each time there is a major release of the Java platform, a web page is published describing its new features. These pages are well worth reading. The libraries are too big to study all the documentation, but every programmer should be familiar with the basics of java.lang, java.util, and java.io, and their subpackages. If you can\u2019t find what you need in Java platform libraries, your next choice should be to look in high-quality third-party libraries, such as Google\u2019s excellent, open source Guava library To summarize, don\u2019t reinvent the wheel. If you need to do something that seems like it should be reasonably common, there may already be a facility in the libraries that does what you want. If there is, use it; if you don\u2019t know, check. 60: Avoid float and double if exact answers are required The float and double types are designed primarily for scientific and engineering calculations. They do not, however, provide exact results and should not be used where exact results are required. The float and double types are particularly ill-suited for monetary calculations because it is impossible to represent 0.1 (or any other negative power of ten) as a float or double exactly. The right way to solve this problem is to use BigDecimal, int, or long for monetary calculations. There are, however, two disadvantages to using BigDecimal: it\u2019s a lot less convenient than using a primitive arithmetic type, and it\u2019s a lot slower. 61: Prefer primitive types to boxed primitives There are three major differences between primitives and boxed primitives. primitives have only their values, whereas boxed primitives have identities distinct from their values. primitive types have only fully functional values, whereas each boxed primitive type has one nonfunctional value, which is null. primitives are more time- and space-efficient than boxed primitives. Applying the == operator to boxed primitives is almost always wrong. In practice, if you need a comparator to describe a type\u2019s natural order, you should simply call Comparator.naturalOrder(), and if you write a comparator yourself, you should use the comparator construction methods, or the static compare methods on primitive types. In nearly every case when you mix primitives and boxed primitives in an operation, the boxed primitive is auto-unboxed. In summary, use primitives in preference to boxed primitives whenever you have the choice. Primitive types are simpler and faster. Autoboxing reduces the verbosity, but not the danger, of using boxed primitives. When your program compares two boxed primitives with the == operator, it does an identity comparison, which is almost certainly not what you want. When your program does mixed-type computations involving boxed and unboxed primitives, it does unboxing, and when your program does unboxing, it can throw a NullPointerException. Finally, when your program boxes primitive values, it can result in costly and unnecessary object creations. 62: Avoid STRINGs where other types are more appropriate Strings are poor substitutes for other value types. When a piece of data comes into a program from a file, from the network, or from keyboard input, it is often in string form. If there\u2019s an appropriate value type, whether primitive or object reference, you should use it; if there isn\u2019t, you should write one. Strings are poor substitutes for enum types. Strings are poor substitutes for aggregate types. If an entity has multiple components, it is usually a bad idea to represent it as a single string. Strings are poor substitutes for capabilities. To summarize, avoid the natural tendency to represent objects as strings when better data types exist or can be written. Used inappropriately, strings are more cumbersome, less flexible, slower, and more error-prone than other types. Types for which strings are commonly misused include primitive types, enums, and aggregate types. 63: Beware the performance of STRING concatenation The string concatenation operator (+) is a convenient way to combine a few strings into one. It is fine for generating a single line of output or constructing the string representation of a small, fixed-size object, but it does not scale. Using the string concatenation operator repeatedly to concatenate n strings requires time quadratic in n. Strings are immutable. When two strings are concatenated, the contents of both are copied. To achieve acceptable performance, use a StringBuilder in place of a String. 64: Refer to objects by their interfaces If appropriate interface types exist, then parameters, return values, variables, and fields should all be declared using interface types. It will make your program much more flexible. The only time you really need to refer to an object\u2019s class is when you\u2019re creating it with a constructor. // Good - uses interface as type Set < Son > sonSet = new LinkedHashSet <> (); // Bad - uses class as type! LinkedHashSet < Son > sonSet = new LinkedHashSet <> (); If you decide that you want to switch implementations, all you have to do is change the class name in the constructor (or use a different static factory). It is entirely appropriate to refer to an object by a class rather than an interface if no appropriate interface exists. Value classes are rarely written with multiple implementations in mind. Objects belonging to a framework whose fundamental types are classes rather than interfaces. Classes that implement an interface but also provide extra methods not found in the interface. If there is no appropriate interface, just use the least specific class in the class hierarchy that provides the required functionality. 65: Prefer interfaces to reflection The core reflection facility, java.lang.reflect, offers programmatic access to arbitrary classes. Given a Class object, you can obtain Constructor, Method, and Field instances representing the constructors, methods, and fields of the class represented by the Class instance. These objects provide programmatic access to the class\u2019s member names, field types, method signatures, and so on. Moreover, Constructor, Method, and Field instances let you manipulate their underlying counterparts reflectively: you can construct instances, invoke methods, and access fields of the underlying class by invoking methods on the Constructor, Method, and Field instances. Reflection allows one class to use another, even if the latter class did not exist when the former was compiled. This power, however, comes at a price: You lose all the benefits of compile-time type checking, including exception checking. The code required to perform reflective access is clumsy and verbose. Performance suffers. You can obtain many of the benefits of reflection while incurring few of its costs by using it only in a very limited form. For many programs that must use a class that is unavailable at compile time, there exists at compile time an appropriate interface or superclass by which to refer to the class. If this is the case, you can create instances reflectively and access them normally via their interface or superclass. i.e. here is a program that creates a Set instance whose class is specified by the first command line argument. The program inserts the remaining command line arguments into the set and prints it. // Reflective instantiation with interface access public static void main ( String [] args ) { // Translate the class name into a Class object Class <? extends Set < String >> cl = null ; try { cl = ( Class <? extends Set < String >> ) // Unchecked cast! Class . forName ( args [ 0 ] ); } catch ( ClassNotFoundException e ) { fatalError ( \"Class not found.\" ); } // Get the constructor Constructor <? extends Set < String >> cons = null ; try { cons = cl . getDeclaredConstructor (); } catch ( NoSuchMethodException e ) { fatalError ( \"No parameterless constructor\" ); } // Instantiate the set Set < String > s = null ; try { s = cons . newInstance (); } catch ( IllegalAccessException e ) { fatalError ( \"Constructor not accessible\" ); } catch ( InstantiationException e ) { fatalError ( \"Class not instantiable.\" ); } catch ( InvocationTargetException e ) { fatalError ( \"Constructor threw \" + e . getCause ()); } catch ( ClassCastException e ) { fatalError ( \"Class doesn't implement Set\" ); } // Exercise the set s . addAll ( Arrays . asList ( args ). subList ( 1 , args . length )); System . out . println ( s ); } private static void fatalError ( String msg ) { System . err . println ( msg ); System . exit ( 1 ); } Usually, this technique is all that you need in the way of reflection. This example demonstrates two disadvantages of reflection: the example can generate six different exceptions at runtime, all of which would have been compile-time errors if reflective instantiation were not used; it takes twenty-five lines of tedious code to generate an instance of the class from its name, whereas a constructor invocation would fit neatly on a single line. If you are writing a program that has to work with classes unknown at compile time, you should, if at all possible, use reflection only to instantiate objects, and access the objects using some interface or superclass that is known at compile time. 66: Use native methods judiciously The Java Native Interface (JNI) allows Java programs to call native methods, which are methods written in native programming languages such as C or C++. Historically, native methods have had three main uses. They provide access to platform-specific facilities such as registries. They provide access to existing libraries of native code, including legacy libraries that provide access to legacy data. Finally, native methods are used to write performance-critical parts of applications in native languages for improved performance. It is legitimate to use native methods to access platform-specific facilities, but it is seldom necessary; it is also legitimate to use native methods to use native libraries when no equivalent libraries are available in Java. It is rarely advisable to use native methods for improved performance. The use of native methods has serious disadvantages. Because native languages are not safe, applications using native methods are no longer immune to memory corruption errors. Native languages are more platform-dependent than Java, programs using native methods are less portable. They are harder to debug too. 67: Optimize judiciously There are three aphorisms concerning optimization that everyone should know: More computing sins are committed in the name of efficiency (without necessarily achieving it) than for any other single reason\u2014including blind stupidity. \u2014William A. Wulf We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. \u2014Donald E. Knuth We follow two rules in the matter of optimization: Rule 1. Don\u2019t do it. Rule 2 (for experts only). Don\u2019t do it yet\u2014that is, not until you have a perfectly clear and unoptimized solution. Rule 3 (added by us). Measure performance before and after each attempted optimization. \u2014M. A. Jackson It is easy to do more harm than good, especially if you optimize prematurely. In the process, you may produce software that is neither fast nor correct and cannot easily be fixed. Strive to write good programs rather than fast ones. If a good program is not fast enough, its architecture will allow it to be optimized. Good programs embody the principle of information hiding: where possible, they localize design decisions within individual components, so individual decisions can be changed without affecting the remainder of the system. Therefore you must think about performance during the design process. Strive to avoid design decisions that limit performance. Consider the performance consequences of your API design decisions. Luckily, it is generally the case that good API design is consistent with good performance. It is a very bad idea to warp an API to achieve good performance. The performance issue that caused you to warp the API may go away in a future release of the platform or other underlying software, but the warped API and the support headaches that come with it will be with you forever. Common wisdom says that programs spend 90 percent of their time in 10 percent of their code. Profiling tools can help you decide where to focus your optimization efforts. These tools give you runtime information, such as roughly how much time each method is consuming and how many times it is invoked. In addition to focusing your tuning efforts, this can alert you to the need for algorithmic changes. The more code in the system, the more important it is to use a profiler. The need to measure the effects of attempted optimization is even greater in Java than in more traditional languages such as C and C++, because Java has a weaker performance model: The relative cost of the various primitive operations is less well defined. To summarize, do not strive to write fast programs\u2014strive to write good ones; speed will follow. But do think about performance while you\u2019re designing systems, especially while you\u2019re designing APIs, wire-level protocols, and persistent data formats. When you\u2019ve finished building the system, measure its performance. 68: Adhere to generally accepted naming conventions Loosely speaking, naming conventions fall into two categories: typographical and grammatical. Details can be found in the The Java Language Specification . Typographical Package and module names should be hierarchical with the components separated by periods. Components should consist of lowercase alphabetic characters and, rarely, digits. The name of any package that will be used outside your organization should begin with your organization\u2019s Internet domain name with the components reversed The remainder of a package name should consist of one or more components describing the package. Components should be short, generally eight or fewer characters. Meaningful abbreviations are encouraged. Components should generally consist of a single word or abbreviation. Class and interface names, including enum and annotation type names, should consist of one or more words, with the first letter of each word capitalized. Method and field names follow the same typographical conventions as class and interface names, except that the first letter of a method or field name should be lowercase. Constant fields names should consist of one or more uppercase words separated by the underscore character. Constant fields constitute the only recommended use of underscores. Local variable names have similar typographical naming conventions to member names, except that abbreviations are permitted, as are individual characters and short sequences of characters whose meaning depends on the context in which they occur. Input parameters are a special kind of local variable. They should be named much more carefully than ordinary local variables, as their names are an integral part of their method\u2019s documentation. Type parameter names usually consist of a single letter. Most commonly it is one of these five: T for an arbitrary type, E for the element type of a collection, K and V for the key and value types of a map, and X for an exception. The return type of a function is usually R. A sequence of arbitrary types can be T, U, V or T1, T2, T3. Grammatical There are no grammatical naming conventions to speak of for packages. Instantiable classes, including enum types, are generally named with a singular noun or noun phrase, such as Thread, PriorityQueue, or ChessPiece. Non-instantiable utility classes are often named with a plural noun, such as Collectors or Collections. Interfaces are named like classes, for example, Collection or Comparator, or with an adjective ending in able or ible, for example, Runnable, Iterable, or Accessible. Annotation types have so many uses, no part of speech predominates. Methods that perform some action are generally named with a verb or verb phrase (including object), for example, append or drawImage. Methods that return a boolean value usually have names that begin with the word is or, less commonly, has, followed by a noun, noun phrase, or any word or phrase that functions as an adjective, for example, isDigit. Methods that return a non-boolean function or attribute of the object on which they\u2019re invoked are usually named with a noun, a noun phrase, or a verb phrase beginning with the verb get, for example, size, hashCode, or getTime. Instance methods that convert the type of an object, returning an independent object of a different type, are often called toType, for example, toString or toArray. Methods that return a view whose type differs from that of the receiving object are often called asType, for example, asList. Methods that return a primitive with the same value as the object on which they\u2019re invoked are often called typeValue, for example, intValue. Common names for static factories include from, of, valueOf, instance, getInstance, newInstance, getType, and newType. To summarize, internalize the standard naming conventions and learn to use them as second nature. The typographical conventions are straightforward and largely unambiguous; the grammatical conventions are more complex and looser. Use common sense.","title":"Chapter 9. General Programming"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-10-exceptions","text":"69: Use EXCEPTIONs only for exceptional conditions When used to best advantage, exceptions can improve a program\u2019s readability, reliability, and maintainability. When used improperly, they can have the opposite effect. Exceptions are to be used only for exceptional conditions; they should never be used for ordinary control flow. A well-designed API must not force its clients to use exceptions for ordinary control flow. 70: Use checked exceptions for recoverable conditions and runtime exceptions for programming errors Java provides three kinds of throwables: checked exceptions, runtime exceptions, and errors. Use checked exceptions for conditions from which the caller can reasonably be expected to recover. By throwing a checked exception, you force the caller to handle the exception in a catch clause or to propagate it outward. Each checked exception declared to throw indicates to the user that it is a possible outcome. There are two kinds of unchecked throwables: runtime exceptions and errors. They both are throwables that needn\u2019t, and generally shouldn\u2019t, be caught. If a program throws an unchecked exception or an error, it is generally the case that recovery is impossible and continued execution would do more harm than good. Use runtime exceptions to indicate programming errors. The great majority of runtime exceptions indicate precondition violations. A precondition violation is simply a failure by the client of an API to adhere to the contract established by the API specification. i.e. the contract for array access specifies that the array index must be between zero and the array length minus one, inclusive. ArrayIndexOutOfBoundsException indicates that this precondition was violated. If you believe a condition is likely to allow for recovery, use a checked exception; if not, use a runtime exception. If it isn\u2019t clear whether recovery is possible, you\u2019re probably better off using an unchecked exception. All of the unchecked throwables you implement should subclass RuntimeException all of the unchecked throwables you implement should subclass RuntimeException. Not only shouldn\u2019t you define Error subclasses, but with the exception of AssertionError, you shouldn\u2019t throw them either. To summarize, throw checked exceptions for recoverable conditions and unchecked exceptions for programming errors. When in doubt, throw unchecked exceptions. Don\u2019t define any throwables that are neither checked exceptions nor runtime exceptions. Provide methods on your checked exceptions to aid in recovery. 71: Avoid unnecessary use of checked exceptions Used properly, they can improve APIs and programs. Unlike return codes and unchecked exceptions, checked exceptions force programmers to deal with problems, enhancing reliability. That said, overuse of checked exceptions in APIs can make them far less pleasant to use. This burden on the user may be justified if the exceptional condition cannot be prevented by proper use of the API and the programmer using the API can take some useful action once confronted with the exception. Unless both of these conditions are met, an unchecked exception is appropriate. The easiest way to eliminate a checked exception is to return an optional of the desired result type. Instead of throwing a checked exception, the method simply returns an empty optional. The disadvantage of this technique is that the method can\u2019t return any additional information detailing its inability to perform the desired computation. 72: Favor the use of standard exceptions The Java libraries provide a set of exceptions that covers most of the exception-throwing needs of most APIs. Reusing standard exceptions makes your API easier to learn and use because it matches the established conventions that programmers are already familiar with. Programs using your API are easier to read because they aren\u2019t cluttered with unfamiliar exceptions. Fewer exception classes means a smaller memory footprint and less time spent loading classes. IllegalArgumentException is generally thrown when the caller passes in an argument whose value is inappropriate. IllegalStateException is generally thrown if the invocation is illegal because of the state of the receiving object. NullPointerException should be thrown rather than IllegalArgumentException if a null value is passed to a method where an instance is expected. Similarly, if a caller passes an out-of-range value in a parameter representing an index into a sequence, IndexOutOfBoundsException should be thrown rather than IllegalArgumentException. ConcurrentModificationException should be thrown if an object that was designed for use by a single thread (or with external synchronization) detects that it is being modified concurrently. UnsupportedOperationException should be thrown if an object does not support an attempted operation. This exception is used by classes that fail to implement one or more optional operations defined by an interface they implement. Do not reuse Exception, RuntimeException, Throwable, or Error directly. Treat these classes as if they were abstract. If an exception fits your needs, go ahead and use it, but only if the conditions under which you would throw it are consistent with the exception\u2019s documentation: reuse must be based on documented semantics, not just on name. Also, feel free to subclass a standard exception if you want to add more detail, but remember that exceptions are serializable. That alone is reason not to write your own exception class without good reason. 73: Throw exceptions appropriate to the abstraction Higher layers should catch lower-level exceptions and, in their place, throw exceptions that can be explained in terms of the higher-level abstraction. // Exception Translation try { ... // Use lower-level abstraction to do our bidding } catch ( LowerLevelException e ) { throw new HigherLevelException (...); } // Exception Chaining try { ... // Use lower-level abstraction to do our bidding } catch ( LowerLevelException cause ) { throw new HigherLevelException ( cause ); } While exception translation is superior to mindless propagation of exceptions from lower layers, it should not be overused. Where possible, the best way to deal with exceptions from lower layers is to avoid them, by ensuring that lower-level methods succeed. Sometimes you can do this by checking the validity of the higher-level method\u2019s parameters before passing them on to lower layers. In summary, if it isn\u2019t feasible to prevent or to handle exceptions from lower layers, use exception translation, unless the lower-level method happens to guarantee that all of its exceptions are appropriate to the higher level. Chaining provides the best of both worlds: it allows you to throw an appropriate higher-level exception, while capturing the underlying cause for failure analysis. 74: Document all exceptions thrown by each method Always declare checked exceptions individually, and document precisely the conditions under which each one is thrown using the Javadoc @throws tag. Don\u2019t take the shortcut of declaring that a method throws some superclass of multiple exception classes that it can throw. As an extreme example, don\u2019t declare that a public method throws Exception or, worse, throws Throwable. It is particularly important that methods in interfaces document the unchecked exceptions they may throw. This documentation forms a part of the interface\u2019s general contract and enables common behavior among multiple implementations of the interface. Use the Javadoc @throws tag to document each exception that a method can throw, but do not use the throws keyword on unchecked exceptions. If an exception is thrown by many methods in a class for the same reason, you can document the exception in the class\u2019s documentation comment rather than documenting it individually for each method. 75: Include failure-capture information in detail messages To capture a failure, the detail message of an exception should contain the values of all parameters and fields that contributed to the exception. Do not include passwords, encryption keys, and the like in detail messages. One way to ensure that exceptions contain adequate failure-capture information in their detail messages is to require this information in their constructors instead of a string detail message. The detail message can then be generated automatically to include the information. 76: Strive for failure atomicity Generally speaking, a failed method invocation should leave the object in the state that it was in prior to the invocation. A method with this property is said to be failure-atomic. There are several ways to achieve this effect. The simplest is to design immutable objects. If an object is immutable, failure atomicity is free. For methods that operate on mutable objects, the most common way to achieve failure atomicity is to check parameters for validity before performing the operation. This causes most exceptions to get thrown before object modification commences. public Object pop () { if ( size == 0 ) throw new EmptyStackException (); Object result = elements [-- size ] ; elements [ size ] = null ; // Eliminate obsolete reference return result ; } A third approach to achieving failure atomicity is to perform the operation on a temporary copy of the object and to replace the contents of the object with the temporary copy once the operation is complete. A last and far less common approach to achieving failure atomicity is to write recovery code that intercepts a failure that occurs in the midst of an operation, and causes the object to roll back its state to the point before the operation began. In summary, as a rule, any generated exception that is part of a method\u2019s specification should leave the object in the same state it was in prior to the method invocation. Where this rule is violated, the API documentation should clearly indicate what state the object will be left in. 77: Don\u2019t ignore exceptions An empty catch block defeats the purpose of exceptions, which is to force you to handle exceptional conditions. If you choose to ignore an exception, the catch block should contain a comment explaining why it is appropriate to do so, and the variable should be named ignored. The advice in this item applies equally to checked and unchecked exceptions. Whether an exception represents a predictable exceptional condition or a programming error, ignoring it with an empty catch block will result in a program that continues silently in the face of error. Properly handling an exception can avert failure entirely. Merely letting an exception propagate outward can at least cause the program to fail swiftly, preserving information to aid in debugging the failure.","title":"Chapter 10. Exceptions"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-11-concurrency","text":"","title":"Chapter 11. Concurrency"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#synchronize-access-to-shared-mutable-data","text":"The synchronized keyword ensures that only a single thread can execute a method or block at one time. Many programmers think of synchronization solely as a means of mutual exclusion, to prevent an object from being seen in an inconsistent state by one thread while it\u2019s being modified by another. In this view, an object is created in a consistent state and locked by the methods that access it. These methods observe the state and optionally cause a state transition, transforming the object from one consistent state to another. Not only does synchronization prevent threads from observing an object in an inconsistent state, but it ensures that each thread entering a synchronized method or block sees the effects of all previous modifications that were guarded by the same lock. The language specification guarantees that reading or writing a variable is atomic unless the variable is of type long or double. Synchronization is required for reliable communication between threads as well as for mutual exclusion. The consequences of failing to synchronize access to shared mutable data can be dire even if the data is atomically readable and writable. // Broken! - How long would you expect this program to run? public class StopThread { private static boolean stopRequested ; public static void main ( String [] args ) throws InterruptedException { Thread backgroundThread = new Thread (() -> { int i = 0 ; while ( ! stopRequested ) // ends up running forever i ++ ; }); backgroundThread . start (); TimeUnit . SECONDS . sleep ( 1 ); stopRequested = true ; } } The problem is that in the absence of synchronization, there is no guarantee as to when, if ever, the background thread will see the change in the value of stopRequested made by the main thread. // added methods to make it work private static synchronized void requestStop () { stopRequested = true ; } private static synchronized boolean stopRequested () { return stopRequested ; } Synchronization is not guaranteed to work unless both read and write operations are synchronized. The above solution works, but the locking in the second version of StopThread can be omitted if stopRequested is declared volatile . While the volatile modifier performs no mutual exclusion, it guarantees that any thread that reads the field will see the most recently written value. // Cooperative thread termination with a volatile field public class StopThread { private static volatile boolean stopRequested ; public static void main ( String [] args ) throws InterruptedException { Thread backgroundThread = new Thread (() -> { int i = 0 ; while ( ! stopRequested ) i ++ ; }); backgroundThread . start (); TimeUnit . SECONDS . sleep ( 1 ); stopRequested = true ; } } However you need to be careful when using volatile . // Broken - requires synchronization! private static volatile int nextSerialNumber = 0 ; public static int generateSerialNumber () { return nextSerialNumber ++ ; } The method\u2019s state consists of a single atomically accessible field, nextSerialNumber, and all possible values of this field are legal. Therefore, no synchronization is necessary to protect its invariants. Still, the method won\u2019t work properly without synchronization. The problem is that the increment operator (++) is not atomic. It performs two operations on the nextSerialNumber field: first it reads the value, and then it writes back a new value. If a second thread reads the field between the time a thread reads the old value and writes back a new one, the second thread will see the same value as the first and return the same serial number. This is a safety failure: the program computes the wrong results. One way to fix generateSerialNumber is to add the synchronized modifier to its declaration and remove the volatile modifier from nextSerialNumber. Finally, the best solution is to use existing package: // Lock-free synchronization with java.util.concurrent.atomic private static final AtomicLong nextSerialNum = new AtomicLong (); public static long generateSerialNumber () { return nextSerialNum . getAndIncrement (); } It is acceptable for one thread to modify a data object for a while and then to share it with other threads, synchronizing only the act of sharing the object reference. Other threads can then read the object without further synchronization, so long as it isn\u2019t modified again. Such objects are said to be effectively immutable. Transferring such an object reference from one thread to others is called safe publication. There are many ways to safely publish an object reference: you can store it in a static field as part of class initialization; you can store it in a volatile field, a final field, or a field that is accessed with normal locking; or you can put it into a concurrent collection. In summary, when multiple threads share mutable data, each thread that reads or writes the data must perform synchronization. If you need only inter-thread communication, and not mutual exclusion, the volatile modifier is an acceptable form of synchronization, but it can be tricky to use correctly. 79: Avoid excessive synchronization Excessive synchronization can cause reduced performance, deadlock, or even nondeterministic behavior. To avoid liveness and safety failures, never cede control to the client within a synchronized method or block. In other words, inside a synchronized region, do not invoke a method that is designed to be overridden, or one provided by a client in the form of a function object. // Broken - invokes alien method from synchronized block! public class ObservableSet < E > extends ForwardingSet < E > { public ObservableSet ( Set < E > set ) { super ( set ); } private final List < SetObserver < E >> observers = new ArrayList <> (); public void addObserver ( SetObserver < E > observer ) { synchronized ( observers ) { observers . add ( observer ); } } public boolean removeObserver ( SetObserver < E > observer ) { synchronized ( observers ) { return observers . remove ( observer ); } } private void notifyElementAdded ( E element ) { synchronized ( observers ) { for ( SetObserver < E > observer : observers ) observer . added ( this , element ); } } @Override public boolean add ( E element ) { boolean added = super . add ( element ); if ( added ) notifyElementAdded ( element ); return added ; } @Override public boolean addAll ( Collection <? extends E > c ) { boolean result = false ; for ( E element : c ) result |= add ( element ); // Calls notifyElementAdded return result ; } } // runs and prints 0-99 public static void main ( String [] args ) { ObservableSet < Integer > set = new ObservableSet <> ( new HashSet <> ()); set . addObserver (( s , e ) -> System . out . println ( e )); for ( int i = 0 ; i < 100 ; i ++ ) set . add ( i ); } Observers subscribe to notifications by invoking the addObserver method and unsubscribe by invoking the removeObserver method. In both cases, an instance of this callback interface is passed to the method. // structurally identical to BiConsumer<ObservableSet<E>,E> @FunctionalInterface public interface SetObserver < E > { // Invoked when an element is added to the observable set void added ( ObservableSet < E > set , E element ); } Now if we make some changes to addObserver set . addObserver ( new SetObserver <> () { public void added ( ObservableSet < Integer > s , Integer e ) { System . out . println ( e ); if ( e == 23 ) s . removeObserver ( this ); } }); It throws ConcurrentModificationException after it prints 0-23. We are trying to remove an element from a list in the midst of iterating over it, which is illegal. // Observer that uses a background thread needlessly set . addObserver ( new SetObserver <> () { public void added ( ObservableSet < Integer > s , Integer e ) { System . out . println ( e ); if ( e == 23 ) { ExecutorService exec = Executors . newSingleThreadExecutor (); try { exec . submit (() -> s . removeObserver ( this )). get (); } catch ( ExecutionException | InterruptedException ex ) { throw new AssertionError ( ex ); } finally { exec . shutdown (); } } } }); When we run this program now, we don\u2019t get an exception; we get a deadlock. The background thread calls s.removeObserver, which attempts to lock observers, but it can\u2019t acquire the lock, because the main thread already has the lock. All the while, the main thread is waiting for the background thread to finish removing the observer. // Alien method moved outside of synchronized block - open calls private void notifyElementAdded ( E element ) { List < SetObserver < E >> snapshot = null ; synchronized ( observers ) { snapshot = new ArrayList <> ( observers ); } for ( SetObserver < E > observer : snapshot ) observer . added ( this , element ); } We can fix above two issues by taking a \"snapshot\" of the observers list that can then be safely traversed without a lock. With this change, both of the previous examples run without exception or deadlock. There is also a better way to move the alien method invocations out of the synchronized block. The libraries provide a concurrent collection known as CopyOnWriteArrayList that is tailor-made for this purpose. This List implementation is a variant of ArrayList in which all modification operations are implemented by making a fresh copy of the entire underlying array. Because the internal array is never modified, iteration requires no locking and is very fast. For most uses, the performance of CopyOnWriteArrayList would be atrocious, but it\u2019s perfect for observer lists, which are rarely modified and often traversed. // Thread-safe observable set with CopyOnWriteArrayList private final List < SetObserver < E >> observers = new CopyOnWriteArrayList <> (); public void addObserver ( SetObserver < E > observer ) { observers . add ( observer ); } public boolean removeObserver ( SetObserver < E > observer ) { return observers . remove ( observer ); } private void notifyElementAdded ( E element ) { for ( SetObserver < E > observer : observers ) observer . added ( this , element ); } An alien method invoked outside of a synchronized region is known as an open call. Besides preventing failures, open calls can greatly increase concurrency. An alien method might run for an arbitrarily long period. If the alien method were invoked from a synchronized region, other threads would be denied access to the protected resource unnecessarily. As a rule, you should do as little work as possible inside synchronized regions. Obtain the lock, examine the shared data, transform it as necessary, and drop the lock. If you must perform some time-consuming activity, find a way to move it out of the synchronized region. In a multicore world, the real cost of excessive synchronization is not the CPU time spent getting locks; it is contention: the lost opportunities for parallelism and the delays imposed by the need to ensure that every core has a consistent view of memory. Another hidden cost of oversynchronization is that it can limit the VM\u2019s ability to optimize code execution. If you are writing a mutable class, you have two options: you can omit all synchronization and allow the client to synchronize externally if concurrent use is desired, or you can synchronize internally, making the class thread-safe. - You should choose the latter option only if you can achieve significantly higher concurrency with internal synchronization than you could by having the client lock the entire object externally. - The collections in java.util (with the exception of the obsolete Vector and Hashtable) take the former approach, while those in java.util.concurrent take the latter. - When in doubt, do not synchronize your class, but document that it is not thread-safe. - If you do synchronize your class internally, you can use various techniques to achieve high concurrency, such as lock splitting, lock striping, and nonblocking concurrency control. If a method modifies a static field and there is any possibility that the method will be called from multiple threads, you must synchronize access to the field internally. The field is essentially a global variable even if it is private because it can be read and modified by unrelated clients. The nextSerialNumber field used by the method generateSerialNumber exemplifies this situation. In summary, to avoid deadlock and data corruption, never call an alien method from within a synchronized region. More generally, keep the amount of work that you do from within synchronized regions to a minimum. When you are designing a mutable class, think about whether it should do its own synchronization. In the multicore era, it is more important than ever not to oversynchronize. Synchronize your class internally only if there is a good reason to do so, and document your decision clearly. 80: Prefer executors, tasks, and streams to threads The Java Executor Framework in java.util.concurrent is a flexible interface-based task execution facility. ExecutorService exec = Executors . newSingleThreadExecutor (); // create exec . execute ( runnable ); // submit work exec . shutdown (); // terminate gracefully You can do many more things with an executor service. wait for a particular task to complete (with the get method) wait for any or all of a collection of tasks to complete (using the invokeAny or invokeAll methods) wait for the executor service to terminate (using the awaitTermination method) retrieve the results of tasks one by one as they complete (using an ExecutorCompletionService) schedule tasks to run at a particular time or to run periodically (using a ScheduledThreadPoolExecutor) create a thread pool with a fixed or variable number of threads use the ThreadPoolExecutor class directly to configure nearly every aspect of a thread pool\u2019s operation. A cached thread pool is not a good choice for a heavily loaded production server. Submitted tasks are not queued but immediately handed off to a thread for execution. If no threads are available, a new one is created. If a server is so heavily loaded that all of its CPUs are fully utilized and more tasks arrive, more threads will be created, which will only make matters worse. In the executor framework, the unit of work and the execution mechanism are separate. The key abstraction is the unit of work, which is the task. There are two kinds of tasks: Runnable and Callable (which is like Runnable, except that it returns a value and can throw arbitrary exceptions). In Java 7, the Executor Framework was extended to support fork-join tasks, which are run by a special kind of executor service known as a fork-join pool. A fork-join task, represented by a ForkJoinTask instance, may be split up into smaller subtasks, and the threads comprising a ForkJoinPool not only process these tasks but \"steal\" tasks from one another to ensure that all threads remain busy, resulting in higher CPU utilization, higher throughput, and lower latency. Writing and tuning fork-join tasks is tricky. Parallel streams are written atop fork join pools and allow you to take advantage of their performance benefits with little effort, assuming they are appropriate for the task at hand. 81: Prefer concurrency utilities to wait and notify Given the difficulty of using wait and notify correctly, you should use the higher-level concurrency utilities instead. The higher-level utilities in java.util.concurrent fall into three categories: the Executor Framework; concurrent collections; and synchronizers. The concurrent collections are high-performance concurrent implementations of standard collection interfaces such as List, Queue, and Map, which managing their own synchronization internally. It is impossible to exclude concurrent activity from a concurrent collection; locking it will only slow the program. Because you can\u2019t exclude concurrent activity on concurrent collections, you can\u2019t atomically compose method invocations on them either. Therefore, concurrent collection interfaces were outfitted with state-dependent modify operations, which combine several primitives into a single atomic operation. // Concurrent canonicalizing map atop ConcurrentMap - not optimal private static final ConcurrentMap < String , String > map = new ConcurrentHashMap <> (); public static String intern ( String s ) { String previousValue = map . putIfAbsent ( s , s ); return previousValue == null ? s : previousValue ; } ConcurrentHashMap is optimized for retrieval operations, such as get. Therefore, it is worth invoking get initially and calling putIfAbsent only if get indicates that it is necessary. // Concurrent canonicalizing map atop ConcurrentMap - faster! public static String intern ( String s ) { String result = map . get ( s ); if ( result == null ) { result = map . putIfAbsent ( s , s ); if ( result == null ) result = s ; } return result ; } Concurrent collections make synchronized collections largely obsolete. For example, use ConcurrentHashMap in preference to Collections.synchronizedMap. Simply replacing synchronized maps with concurrent maps can dramatically increase the performance of concurrent applications. Synchronizers are objects that enable threads to wait for one another, allowing them to coordinate their activities. The most commonly used synchronizers are CountDownLatch and Semaphore. Less commonly used are CyclicBarrier and Exchanger. The most powerful synchronizer is Phaser. Countdown latches are single-use barriers that allow one or more threads to wait for one or more other threads to do something. The sole constructor for CountDownLatch takes an int that is the number of times the countDown method must be invoked on the latch before all waiting threads are allowed to proceed. // Simple framework for timing concurrent execution public static long time ( Executor executor , int concurrency , Runnable action ) throws InterruptedException { CountDownLatch ready = new CountDownLatch ( concurrency ); CountDownLatch start = new CountDownLatch ( 1 ); CountDownLatch done = new CountDownLatch ( concurrency ); for ( int i = 0 ; i < concurrency ; i ++ ) { executor . execute (() -> { ready . countDown (); // Tell timer we're ready try { start . await (); // Wait till peers are ready action . run (); } catch ( InterruptedException e ) { Thread . currentThread (). interrupt (); } finally { done . countDown (); // Tell timer we're done } }); } ready . await (); // Wait for all workers to be ready long startNanos = System . nanoTime (); start . countDown (); // And they're off! done . await (); // Wait for all workers to finish return System . nanoTime () - startNanos ; } The first, ready, is used by worker threads to tell the timer thread when they\u2019re ready. The worker threads then wait on the second latch, which is start. When the last worker thread invokes ready.countDown, the timer thread records the start time and invokes start.countDown, allowing all of the worker threads to proceed. Then the timer thread waits on the third latch, done, until the last of the worker threads finishes running the action and calls done.countDown. As soon as this happens, the timer thread awakens and records the end time. A few more details bear noting. The executor passed to the time method must allow for the creation of at least as many threads as the given concurrency level, or the test will never complete. This is known as a thread starvation deadlock. For interval timing, always use System.nanoTime rather than System.currentTimeMillis. System.nanoTime is both more accurate and more precise and is unaffected by adjustments to the system\u2019s real-time clock. Note that the code in this example won\u2019t yield accurate timings unless action does a fair amount of work. Accurate microbenchmarking is notoriously hard and is best done with the aid of a specialized framework such as jmh the three countdown latches in the previous example could be replaced by a single CyclicBarrier or Phaser instance. The resulting code would be a bit more concise but perhaps more difficult to understand. In summary, using wait and notify directly is like programming in \"concurrency assembly language,\" as compared to the higher-level language provided by java.util.concurrent. There is seldom, if ever, a reason to use wait and notify in new code. If you maintain code that uses wait and notify, make sure that it always invokes wait from within a while loop using the standard idiom. The notifyAll method should generally be used in preference to notify. If notify is used, great care must be taken to ensure liveness. 82: Document thread safety The presence of the synchronized modifier in a method declaration is an implementation detail, not a part of its API. It does not reliably indicate that a method is thread-safe. To enable safe concurrent use, a class must clearly document what level of thread safety it supports: Immutable\u2014Instances of this class appear constant. No external synchronization is necessary. Unconditionally thread-safe\u2014Instances of this class are mutable, but the class has sufficient internal synchronization that its instances can be used concurrently without the need for any external synchronization. Conditionally thread-safe\u2014Like unconditionally thread-safe, except that some methods require external synchronization for safe concurrent use. Not thread-safe\u2014Instances of this class are mutable. To use them concurrently, clients must surround each method invocation (or invocation sequence) with external synchronization of the clients\u2019 choosing. Thread-hostile\u2014This class is unsafe for concurrent use even if every method invocation is surrounded by external synchronization. Thread hostility usually results from modifying static data without synchronization. When a class or method is found to be thread-hostile, it is typically fixed or deprecated. There are some thread safety annotations in Java Concurrency in Practice, which are @Immutable, @ThreadSafe, and @NotThreadSafe. Documenting a conditionally thread-safe class requires care. You must indicate which invocation sequences require external synchronization, and which lock (or in rare cases, locks) must be acquired to execute these sequences. The description of a class\u2019s thread safety generally belongs in the class\u2019s doc comment, but methods with special thread safety properties should describe these properties in their own documentation comments. Lock fields should always be declared final. This is true whether you use an ordinary monitor lock (as shown above) or a lock from the java.util.concurrent.locks package. 83: Use lazy initialization judiciously Lazy initialization is the act of delaying the initialization of a field until its value is needed. If the value is never needed, the field is never initialized. This technique is applicable to both static and instance fields. While lazy initialization is primarily an optimization, it can also be used to break harmful circularities in class and instance initialization. The best advice for lazy initialization is \"don\u2019t do it unless you need to\" because it decreases the cost of initializing a class or creating an instance, at the expense of increasing the cost of accessing the lazily initialized field. Under most circumstances, normal initialization is preferable to lazy initialization. If you use lazy initialization to break an initialization circularity, use a synchronized accessor because it is the simplest, clearest alternative. // Lazy initialization of instance field - synchronized accessor private FieldType field ; private synchronized FieldType getField () { if ( field == null ) field = computeFieldValue (); return field ; } If you need to use lazy initialization for performance on a static field, use the lazy initialization holder class idiom. This idiom exploits the guarantee that a class will not be initialized until it is used. // Lazy initialization holder class idiom for static fields private static class FieldHolder { static final FieldType field = computeFieldValue (); } private static FieldType getField () { return FieldHolder . field ; } The beauty of this idiom is that the getField method is not synchronized and performs only a field access, so lazy initialization adds practically nothing to the cost of access. A typical VM will synchronize field access only to initialize the class. If you need to use lazy initialization for performance on an instance field, use the double-check idiom. // Double-check idiom for lazy initialization of instance fields // needs volatile because there is no locking once the field is initialized private volatile FieldType field ; private FieldType getField () { // this local variable declaration is to ensure that field is read only // once in the common case where it\u2019s already initialized // thus increases performance FieldType result = field ; if ( result == null ) { // First check (no locking) synchronized ( this ) { if ( field == null ) // Second check (with locking) field = result = computeFieldValue (); } } return result ; } In summary, you should initialize most fields normally, not lazily. If you must initialize a field lazily in order to achieve your performance goals or to break a harmful initialization circularity, then use the appropriate lazy initialization technique. For instance fields, it is the double-check idiom; for static fields, the lazy initialization holder class idiom. For instance fields that can tolerate repeated initialization, you may also consider the single-check idiom. 84: Don\u2019t depend on the thread scheduler Any program that relies on the thread scheduler for correctness or performance is likely to be nonportable. The best way to write a robust, responsive, portable program is to ensure that the average number of runnable threads is not significantly greater than the number of processors. This leaves the thread scheduler with little choice: it simply runs the runnable threads till they\u2019re no longer runnable. The program\u2019s behavior doesn\u2019t vary too much, even under radically different thread-scheduling policies. Note that the number of runnable threads isn\u2019t the same as the total number of threads, which can be much higher. Threads that are waiting are not runnable. The main technique for keeping the number of runnable threads low is to have each thread do some useful work, and then wait for more. Threads should not run if they aren\u2019t doing useful work. Threads should not busy-wait, repeatedly checking a shared object waiting for its state to change. // Awful CountDownLatch implementation - busy-waits incessantly! public class SlowCountDownLatch { private int count ; public SlowCountDownLatch ( int count ) { if ( count < 0 ) throw new IllegalArgumentException ( count + \" < 0\" ); this . count = count ; } public void await () { while ( true ) { synchronized ( this ) { if ( count == 0 ) return ; } } } public synchronized void countDown () { if ( count != 0 ) count -- ; } } When faced with a program that barely works because some threads aren\u2019t getting enough CPU time relative to others, resist the temptation to \"fix\" the program by putting in calls to Thread.yield. A better course of action is to restructure the application to reduce the number of concurrently runnable threads. Thread priorities are among the least portable features of Java. It is not unreasonable to tune the responsiveness of an application by tweaking a few thread priorities, but it is rarely necessary and is not portable. In summary, do not depend on the thread scheduler for the correctness of your program. The resulting program will be neither robust nor portable. As a corollary, do not rely on Thread.yield or thread priorities. These facilities are merely hints to the scheduler. Thread priorities may be used sparingly to improve the quality of service of an already working program, but they should never be used to \"fix\" a program that barely works.","title":"Synchronize access to shared mutable data"},{"location":"Programming-Lang-Reference/Java/Best-Practices/#chapter-12-serialization","text":"85: Prefer alternatives to Java serialization Java deserialization is a clear and present danger as it is widely used both directly by applications and indirectly by Java subsystems such as RMI (Remote Method Invocation), JMX (Java Management Extension), and JMS (Java Messaging System). Deserialization of untrusted streams can result in remote code execution (RCE), denial-of-service (DoS), and a range of other exploits. Applications can be vulnerable to these attacks even if they did nothing wrong. -- Robert Seacord You open yourself up to attack whenever you deserialize a byte stream that you don\u2019t trust. The best way to avoid serialization exploits is never to deserialize anything. 86: Implement Serializable with great caution A major cost of implementing Serializable is that it decreases the flexibility to change a class\u2019s implementation once it has been released. When a class implements Serializable, its byte-stream encoding (or serialized form) becomes part of its exported API. Once you distribute a class widely, you are generally required to support the serialized form forever, just as you are required to support all other parts of the exported API. A second cost of implementing Serializable is that it increases the likelihood of bugs and security holes. Serialization is an extralinguistic mechanism for creating objects. Relying on the default deserialization mechanism can easily leave objects open to invariant corruption and illegal access. A third cost of implementing Serializable is that it increases the testing burden associated with releasing a new version of a class. You must ensure both that the serialization-deserialization process succeeds and that it results in a faithful replica of the original object. Classes representing active entities, such as thread pools, should rarely implement Serializable. Classes designed for inheritance should rarely implement Serializable, and interfaces should rarely extend it. Inner classes should not implement Serializable. 87: Consider using a custom serialized form Do not accept the default serialized form without first considering whether it is appropriate. The default serialized form is likely to be appropriate if an object\u2019s physical representation is identical to its logical content. Even if you decide that the default serialized form is appropriate, you often must provide a readObject method to ensure invariants and security. Using the default serialized form when an object\u2019s physical representation differs substantially from its logical data content has four disadvantages: It permanently ties the exported API to the current internal representation. It can consume excessive space. It can consume excessive time. It can cause stack overflows. Whether or not you use the default serialized form, you must impose any synchronization on object serialization that you would impose on any other method that reads the entire state of the object. Regardless of what serialized form you choose, declare an explicit serial version UID in every serializable class you write. Do not change the serial version UID unless you want to break compatibility with all existing serialized instances of a class. 88: Write readObject methods defensively Just as a constructor must check its arguments for validity and make defensive copies of parameters where appropriate, so must a readObject method. Loosely speaking, readObject is a constructor that takes a byte stream as its sole parameter. In normal use, the byte stream is generated by serializing a normally constructed instance. The problem arises when readObject is presented with a byte stream that is artificially constructed to generate an object that violates the invariants of its class. To summarize, anytime you write a readObject method, adopt the mind-set that you are writing a public constructor that must produce a valid instance regardless of what byte stream it is given. Do not assume that the byte stream represents an actual serialized instance. 89: For instance control, prefer enum types to readResolve To summarize, use enum types to enforce instance control invariants wherever possible. If this is not possible and you need a class to be both serializable and instance-controlled, you must provide a readResolve method and ensure that all of the class\u2019s instance fields are either primitive or transient. 90: Consider serialization proxies instead of serialized instances The serialization proxy pattern is reasonably straightforward. First, design a private static nested class that concisely represents the logical state of an instance of the enclosing class. It should have a single constructor, whose parameter type is the enclosing class. This constructor merely copies the data from its argument: it need not do any consistency checking or defensive copying. By design, the default serialized form of the serialization proxy is the perfect serialized form of the enclosing class. Both the enclosing class and its serialization proxy must be declared to implement Serializable. // Serialization proxy for Period class private static class SerializationProxy implements Serializable { private final Date start ; private final Date end ; SerializationProxy ( Period p ) { this . start = p . start ; this . end = p . end ; } private static final long serialVersionUID = 234098243823485285L ; // Any number will do (Item 87) } Next, add the following writeReplace method to the enclosing class. This method can be copied verbatim into any class with a serialization proxy: // writeReplace method for the serialization proxy pattern private Object writeReplace () { return new SerializationProxy ( this ); } The presence of this method on the enclosing class causes the serialization system to emit a SerializationProxy instance instead of an instance of the enclosing class. With this writeReplace method in place, the serialization system will never generate a serialized instance of the enclosing class, but an attacker might fabricate one in an attempt to violate the class\u2019s invariants. To guarantee that such an attack would fail, merely add this readObject method to the enclosing class: // readObject method for the serialization proxy pattern private void readObject ( ObjectInputStream stream ) throws InvalidObjectException { throw new InvalidObjectException ( \"Proxy required\" ); } Finally, provide a readResolve method on the SerializationProxy class that returns a logically equivalent instance of the enclosing class. The presence of this method causes the serialization system to translate the serialization proxy back into an instance of the enclosing class upon deserialization. // readResolve method for Period.SerializationProxy private Object readResolve () { return new Period ( start , end ); // Uses public constructor } In summary, consider the serialization proxy pattern whenever you find yourself having to write a readObject or writeObject method on a class that is not extendable by its clients. This pattern is perhaps the easiest way to robustly serialize objects with nontrivial invariants.","title":"Chapter 12. Serialization"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/","text":"Notes taken from the book Head First Design Patterns . Examples and source code http://www.headfirstlabs.com/books/hfdp/ Design patterns don't go directly into your code, they first go into your BRAIN. Once you've loaded your brain with a good working knowledge of patterns, you can then start to apply them to your new designs, and rework your old code when you fi nd it's degrading into an infl exible mess of jungle spaghetti code. One of the advantages of knowing design patterns is recognizing and quickly understanding the design movitation in your favorite libraries. A design pattern is really just a solution (that functions pretty well) to a type of problem in a certain context. Meanwhile, patterns are not meant to be laws or rules; they are guidelines that you can alter to fit your needs. WHen you do adopt patterns with changes to fit your need, be sure to document the changes which can help other developers to understand the differences between your pattern and the classic pattern. Center your thinking on design , not on patterns (that you feel obligated to have to use some sort of patterns to solve a problem). Use patterns when there is a natural need for them. Stick with simpler solutions when possible . There are more patterns for other domains such as concurrent systems and enterprise systems are not covered in this notes. The Strategy Pattern \u00b6 Separating what changes from what stays the same Identify the aspects of your application that vary and separate them from what stays the same . Take what varies and \"encapsulate\" it so it won't affect the rest of the code. This principle stays true for all the design patterns, to let some part of a system vary independently from all other parts. Program to an interface (supertype, a set of behaviors), not to a concrete implementation. The declared type of the variables should be a supertype, usually an abstract class or interface, so that the objects assigned to those variables can be of any concrete implementation of the supertype, which means the class declaring them doesn't have to know about the actual object types. The Strategy Pattern defines a family of algorithms, encapsulates each one, and makes them interchangeable. Strategy lets the algorithm vary independently from clients that use it. Start thinking the behaviors to implement are actually a family of different algorithms to express those behaviors. Encapsulate what varies and favor composition over inheritance. // programming to an implementation Dog d = new Dog (); // locking ourselves to a concrete type d . bark (); // programming to an interface/supertype Animal animal = new Dog (); animal . makeSound (); // assign concrete implementation at runtime Animal animal = getAnimal (); animal . makeSound (); Has-A can be better than Is-A relationship ( Favor composition over inheritance ) When you put two classes together like this you're using composition . Instead of inheriting the behaviors , the first class gets its behavior by being composed with the right behavior object. The Observer Pattern \u00b6 The Observer Pattern allow objects to be notified for specific events . Publishers (Subject) + Subscribers (Observer) = Observer Pattern. This pattern provides an object design where subjects and observers are lossely coupled. Strive for loosely coupled designs between objects that interact . When two objects are loosely coupled , they can interact with each other but have very little knowledge of each other. The Observer Pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependents are notified and updated automatically. The Observer Pattern achieves loose coupling by: the only thing the subject knows about an observer is that it implements a certain interface separating what varies: the complete state of the subject and the number and types of observers can add/remove observers at any time never need to modify the subject to add new types of observers You can push or pull data from the Subject when using the pattern (pulling by the observers is considered more \"correct\"). The Decorator Pattern \u00b6 Classes should be open for extension , but closed for modification . Be careful when choosing the areas of code that need to be extended; applying the Open-Closed Principle EVERYWHERE is wasteful and unnecessary, and can lead to complex, hard-to-understand code. The Decorator Pattern attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Composition and delegation can often be used to add new behaviors at runtime. Decorators are typically transparent to the client of the component; that is, unless the client is relying on the component's concrete type. Decorating objects gives objects new responsibilities without making code changes to the underlying classes. The decorator adds its own behavior either before and/or after delegating to the component object it decorates to do the rest of the job. Java's io package heavily uses the decorator pattern. i.e. To read contents from a zipped file, you might do FileInputStream wrapped with BufferedInputStream wrapped with ZipInputStream . The FileInputStream is the abstract component and the others are the concrete decorator implementations. One drawback of the Decorator Pattern is that it often result in a large number of small classes that can be overwhelming to a developer trying to use the Decorator-based API. Overuse of this pattern will make the system more complex. If some code logic is dependent on specific types, think twice before introducing decorators, as it hides the underlying types . Use of the Factory and Builder patterns can help reduce the complexity of instantiate a component with many decorators. The Factory Patterns \u00b6 When you use the new operator you are certainly instantiating a concrete class, and certainly an implementation rather than an interface. This is against the principle that to program against interface, not implementation. Tying your code to a concrete class can make it more fragile and less flexible. The Factory Method Pattern defines an interface for creating an object, but lets subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses . The Factory Patterns are really about encapsulating object creation . By encapsulating the object creation details in a separate and focused class, we have only one place to make modifications and everywhere that uses it stays untouched. The Dependency Inversion Principle: depend upon abstractions . Do not depend upon concrete classes. It suggests that our high-level components should not depend on our low-level components; rather, they should both depend on abstractions. Instead of starting at the top, start at the low-level concrete components and think about what you can abstract , then move on to design the high level with the abstraction in mind. A few guidelines for trying to adhere with the principle: No variable should hold a reference to a concrete class. No class should derive from a concrete class. No method should override an implemented method of any of its base classes. Those methods implemented in the base class are meant to be shared by all your subclasses. The Abstract Factory Pattern provides an interface for creating families of related or dependent objects without specifying their concrete classes. An Abstract Factory gives us an interface for creating a family of products. By writing code that uses this interface, we decouple our code from the actual factory that creates the products. It allows us to implement a variety of factories that produce products meant for different contexts. The differences between the two: how product object is created Factory Method Pattern do it through inheritance Abstract Factory Pattern do it through object composition the Factory Method Pattern is good for creating one product, while Abstract Factory Pattern is good for creating a family of products. how factory is created Factory Method Pattern do it through implementing the abstract creator method that makes use of the concrete types the subclasses create Abstract Factory Pattern do it through implementing factory methods Use Factory Method Pattern if the number of concrete classes is foreseeably finite, or you don't know ahead of time all the concrete classes going to need. Use Abstract Factory Method Pattern when need to create families of products, or those that make sense to belong in the same factory. Singleton Pattern \u00b6 Create objects that there is only one instance at run time, and create them only when they are needed. There are many objects we only need one of: thread pools, caches, dialog boxes, objects that handle preferences and registry settings, objects used for logging, and objects that act as device drivers to devices like printers and graphics cards. The Singleton Pattern ensures a class has only one instance, and provides a global point of access to it. If the singleton object is accessed by multiple threads, a few options to deal with it: Do nothing if the performance of getInstance() isn't critical to your application. Move to an eagerly created instance rather than a lazily created one. Use \"double-checked locking\" to reduce the use of synchronization in getInstance(). Other cases when there can be more than one singleton instances when there are multiple versions of class loaders reflection, serialization and deserialization It is also easy to use singleton but violate the loose coupling principle. Singletons are meant to be used sparingly. Enum is also a good way to create singletons Command Pattern \u00b6 This pattern is about encapsulating method invocation. It allows you to decouple the requester of an action from the object that actually performs the action. A command object encapsulates a request to do something on a specific object. The caller doesn't have any idea what the work is, it just has a command object that knows how to talk to the right object to get the work done. The Command Pattern encapsulates a request as an object, thereby letting you parameterize other objects with different requests, queue or log requests, and support undoable operations. From the outside, no other objects really know what actions get performed on what receiver; they just know that if they call the execute() method, their request will be serviced. The receiver of the request gets bound to the command it's encapsulated in. In general, we strive for \"dumb\" command objects that just invoke an action on a receiver. The NoCommand object is an example of a null object . A null object is useful when you don't have a meaningful object to return, and yet you want to remove the responsibility for handling null from the client. Java's lambda expressions can be used to populate commands. Say, use a function object as a command. You can only do this if your Command interface has one abstract method. Another flavor of this pattern, the Meta Command Pattern allows you to create macros of commands so that you can execute multiple commands at once. The idea is to create another command that keeps a list of other commands , and execute them when execute() is called Commands allows packaging a piece of computation and pass it around. The computation itself may be invoked asynchronously or invoked by a different thread. The commands can also be put inside a queue and executed in sequence or saved as a batch, useful for applications like logging or transactional systems. Adaper Pattern \u00b6 The client makes a request to the adapter by calling a method on it using the target interface. The adapter translates the request into one or more calls on the adaptee using the adaptee interface. The client receives the results of the call and never knows there is an adapter doing the translation. Note that the Client and Adaptee are decoupled \u2013 neither knows about the other. You may need to adapt many classes to provide the interface a client is coded to The Adapter Pattern converts the interface of a class into another interface the clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces . With class adapter we subclass the Target and the Adaptee, while with object adapter we use composition to pass requests to an Adaptee. There will be cases when a behavior is not implemented by the adaptee class, for which clients will have to watch out for potential exceptions ( UnsupportedOperationException ), but as long as the client is careful and the adapter is well documented this is a perfectly reasonable solution. Facade Pattern \u00b6 You can take a complex subsystem and make it easier to use by implementing a Facade class that provides one more reasonable interface. A facade not only simplifies an interface, it decouples a client from a subsystem of components. Facades and adapters may wrap multiple classes, but a facade's intent is to simplify , while an adapter's is to convert the interface to something different. The Facade Pattern provides a unified interface to a set of interfaces in a subsystem. Facade defines a higher-level interface that makes the subsystem easier to use. Principle of Least Knowledge: talk only to your immediate friends. When you are designing a system, for any object, be careful of the number of classes it interacts with and also how it comes to interact with those classes. Prevent designing a large number of classes coupled together. We should only invoke methods that belong to: The object itself Objects passed in as a parameter to the method Any object the method creates or instantiates Try NOT to call methods on objects that were returned from calling other methods. An adapter wraps an object to change its interface, a decorator wraps an object to add new behaviors and responsibilities, and a facade \"wraps\" a set of objects to simplify. Templating Pattern \u00b6 When there are code duplication, it is a good sign to do some clean up, share the common code. The Templating Pattern basically involves defining a set of methods that should be implemented. The Template Method defines the steps of an algorithm and allows subclasses to provide the implementation for one or more steps. This pattern makes algorithm live in one place and one place to change. It prevents duplication on more concrete subclasses. The Template Method Pattern defines the skeleton of an algorithm in a method, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm's structure. A hook is a method that is declared in the abstract class, but only given an empty or default implementation. This gives subclasses the ability to \"hook into\" the algorithm at various points, if they wish; a subclass is also free to ignore the hook. It is good to provide more granular and optional steps as hooks , so it places less burden on the subclass but also provide a flexible functionality. The Hollywood Principle: Don't call us, we'll call you. This principle is to prevent dependency rot. Dependency rot happens when you have high-level components depending on low-level components depending on high-level components depending on sideways components depending on low-level components, and so on. The Hollywood Principle allow low-level components to hook themselves into a system, but the high-level components determine when they are needed, and how. The Templating Pattern can also appear as having a static method as the templating method, and requires the concrete class to implement an interface. i.e. Java's Array.sort requires the object to implement the Comparable interface. To prevent subclasses from changing the algorithm in the template method, declare the template method as final. Iterator and Composite Patterns \u00b6 These patterns are about allowing your clients to iterate through your objects without ever getting a peek at how you store your objects. The Iterator Pattern provides a way to access the elements of an aggregate object sequentially without exposing its underlying representation. It also places the task of traversal on the iterator object, not on the aggregate, which simplifies the aggregate interface and implementation, and places the responsibility where it should be. External iterator, means that the client controls the iteration by calling next() to get the next element. An internal iterator is controlled by the iterator itself. In that case, because it's the iterator that's stepping through the elements, you have to tell the iterator what to do with those elements as it goes through them. A class should have only one reason to change, a single responsibility. Every responsibility of a class is an area of potential change. More than one responsibility means more than one area of change. The only way to succeed is to be diligent in examining your designs and to watch out for signals that a class is changing in more than one way as your system grows. We say that a module or class has high cohesion when it is designed around a set of related functions, and we say it has low cohesion when it is designed around a set of unrelated functions. Classes that adhere to the principle tend to have high cohesion and are more maintainable than classes that take on multiple responsibilities and have low cohesion. The Composite Pattern allows you to compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly. Using a composite structure, we can apply the same operations over both composites and individual objects. In other words, in most cases we can ignore the differences between compositions of objects and individual objects. By allowing the Component interface to contain the child management operations and the leaf operations, a client can treat both composites and leaf nodes uniformly; so whether an element is a composite or leaf node becomes transparent to the client. Otherwise, by enforcing that any inappropriate calls on elements would be caught at compile time or runtime, but we'd lose transparency and our code would have to use conditionals and the instanceof operator. There are many design tradeoffs in implementing Composite. You need to balance transparency and safety with your needs. State Pattern \u00b6 In a state diagram, all states are in different configurations of the machine that behave in a certain way and need some action to take them to another state. This pattern allows an object to have many different behaviors that are based on its internal state. By using the state pattern we want localize the behavior of each state into its own class remove troublesome if-statements on the machine's state variables close each state for modification The State Pattern allows an object to alter its behavior when its internal state changes. The object will appear to change its class. With the State Pattern, we have a set of behaviors encapsulated in state objects; at any time the context is delegating to one of those states. Over time, the current state changes across the set of state objects to reflect the internal state of the context, so the context's behavior changes over time as well. The client usually knows very little, if anything, about the state objects. With Strategy, the client usually specifies the strategy object that the context is composed with. Think of the State Pattern as an alternative to putting lots of conditionals in your context; by encapsulating the behaviors within state objects, you can simply change the state object in context to change its behavior. The disadvantage of having state transitions in the state classes is that we create dependencies between the state classes. Also by encapsulating state behavior into separate state classes, you'll always end up with more classes in your design. It is the Context's job to oversee its state, and you don't usually want a client changing the state of a Context without that Context's knowledge. Given we had no common functionality to put into an abstract class, we went with an interface. In your own implementation, you might want to consider an abstract class. Doing so has the benefit of allowing you to add methods to the abstract class later, without breaking the concrete state implementations. Proxy Pattern \u00b6 This pattern controlls object access. Your client object acts like it's making remote method calls. But what it's really doing is calling methods on a heap-local \u2018proxy' object that handles all the low-level details of network communication. Java's Remote Method Invocation (RMI) gives us a way to find objects in a remote JVM and allows us to invoke their methods. The nice thing about RMI is that you don't have to write any of the networking or I/O code yourself. With your client, you call remote methods just like normal method calls on objects running in the client's own local JVM. RMI Nomenclature: in RMI, the client helper is a \u2018stub' and the service helper is a \u2018skeleton'. To do this, we need: remote interface - defines methods that a client can call remotely and both the Stub and actual service will implement this extend java.rmi.Remote and declare all methods to throw a RemoteException if the methods in an interface declare exceptions, any code calling methods on a reference of that type (the interface type) must handle or declare the exceptions remote implementation - the object that client wants to remotely call methods on extend java.rmi.server.UnicastRemoteObject and inherit useful methods from that class the methods will handle the remote calls other RemoteObject classes exist register your service using the static rebind() method of the java.rmi.Naming class RMI registry - where the client gets the proxy object (stub/helper object) start rmiregistry in the class directory start remote service - registers the service with the RMI registry and available for clients start the service When making remote method calls, the arguments and return types must be serializable. The Proxy Pattern provides a surrogate or placeholder for another object to control access to it. Use the Proxy Pattern to create a representative object that controls access to another object, which may be remote, expensive to create, or in need of securing. The Proxy Pattern has many forms. What they all have in common is that they intercept a method invocation that the client is making on the subject. Some of them are: A remote proxy controls access to a remote object. the proxy acts as a local representative for an object that lives in a different JVM A virtual proxy controls access to a resource that is expensive to create. defers the creation of the object until it is needed and acts as a surrogate for the object before and while it is being created A protection proxy controls access to a resource based on access rights. Java's got its own proxy support right in the java.lang.reflect package. With this package, Java lets you create a proxy class on the fly that implements one or more interfaces and forwards method invocations to a class that you specify. This is called the dynamic proxy . To do this, we need: create InvocationHandlers that implement the behavior of the proxy, beacuase Java will create the actual proxy class and object that we don't control. write code to create the dynamic proxies and instantiate them. wrap the protected object with the appropriate proxy Compound Pattern \u00b6 This pattern is in fact pattern of patterns. A compound pattern is a set of a few patterns that are combined to solve a general problem. The Model-View-Controller pattern is actually a compound pattern. Model-View-Controller (MVC) Pattern \u00b6 User interact with the view The controller asks the model to change the state, or ask the view to change The model notifies the view when its state changed The view asks the model for state, and changes accordingly to the state The model uses Observer to keep the views and controllers updated on the latest state changes. The view and the controller, on the other hand, implement the Strategy Pattern. The Adaptor Pattern is also often used with the MVC Pattern. These patterns work together to decouple the three players in the MVC model, which keeps designs clear and flexible. The controller implements behavior for the view. It is the smarts that translates the actions from the view to actions on the model. The model takes those actions and implements the application logic to decide what to do in response to those actions. The controller might have to do a little work to determine what method calls to make on the model, but that's not considered the \"application logic.\" The application logic is the code that manages and manipulates your data and it lives in your model. It wasn't long after the Web was spun that developers started adapting the MVC to fit the browser/server model. The prevailing adaptation is known simply as \"Model 2\" and uses a combination of servlet and JSP technology to achieve the same separation of model, view and controller that we see in conventional GUIs. Collection of all wisdoms \u00b6 OO Basics Abstraction Encapsulation Polymorphism Inheritance OO Principles Encasulate what varies Favor composition over inheritance Program to interfaces, not implementations Strive for loosely coupled designs between objects that interact Depend on abstractions. Do not depend on concrete classes Principle of Least Knowledge: talk only to your immediate friends The Hollywood Principle: Don't call us, we'll call you A class should have only one reason to change, a single responzibility OO Patterns The Strategy Pattern defines a family of algorithms, encapsulates each one, and makes them interchangeable. Strategy lets the algorithm vary independently from clients that use it. The Observer Pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependents are notified and updated automatically. The Decorator Pattern attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. The Factory Method Pattern defines an interface for creating an object, but lets subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses. The Abstract Factory Pattern provides an interface for creating families of related or dependent objects without specifying their concrete classes. The Singleton Pattern ensures a class has only one instance, and provides a global point of access to it. The Command Pattern encapsulates a request as an object, thereby letting you parameterize other objects with different requests, queue or log requests, and support undoable operations. The Adapter Pattern converts the interface of a class into another interface the clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces. The Facade Pattern provides a unified interface to a set of interfaces in a subsystem. Facade defines a higher-level interface that makes the subsystem easier to use. The Template Method Pattern defines the skeleton of an algorithm in a method, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm's structure. The Iterator Pattern provides a way to access the elements of an aggregate object sequentially without exposing its underlying representation. The Composite Pattern allows you to compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly The State Pattern allows an object to alter its behavior when its internal state changes. The object will appear to change its class. The Proxy Pattern provides a surrogate or placeholder for another object to control access to it. Other Resources \u00b6 The Portland Patterns Repository The Hillside Group","title":"Java Design Patterns"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#the-strategy-pattern","text":"Separating what changes from what stays the same Identify the aspects of your application that vary and separate them from what stays the same . Take what varies and \"encapsulate\" it so it won't affect the rest of the code. This principle stays true for all the design patterns, to let some part of a system vary independently from all other parts. Program to an interface (supertype, a set of behaviors), not to a concrete implementation. The declared type of the variables should be a supertype, usually an abstract class or interface, so that the objects assigned to those variables can be of any concrete implementation of the supertype, which means the class declaring them doesn't have to know about the actual object types. The Strategy Pattern defines a family of algorithms, encapsulates each one, and makes them interchangeable. Strategy lets the algorithm vary independently from clients that use it. Start thinking the behaviors to implement are actually a family of different algorithms to express those behaviors. Encapsulate what varies and favor composition over inheritance. // programming to an implementation Dog d = new Dog (); // locking ourselves to a concrete type d . bark (); // programming to an interface/supertype Animal animal = new Dog (); animal . makeSound (); // assign concrete implementation at runtime Animal animal = getAnimal (); animal . makeSound (); Has-A can be better than Is-A relationship ( Favor composition over inheritance ) When you put two classes together like this you're using composition . Instead of inheriting the behaviors , the first class gets its behavior by being composed with the right behavior object.","title":"The Strategy Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#the-observer-pattern","text":"The Observer Pattern allow objects to be notified for specific events . Publishers (Subject) + Subscribers (Observer) = Observer Pattern. This pattern provides an object design where subjects and observers are lossely coupled. Strive for loosely coupled designs between objects that interact . When two objects are loosely coupled , they can interact with each other but have very little knowledge of each other. The Observer Pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependents are notified and updated automatically. The Observer Pattern achieves loose coupling by: the only thing the subject knows about an observer is that it implements a certain interface separating what varies: the complete state of the subject and the number and types of observers can add/remove observers at any time never need to modify the subject to add new types of observers You can push or pull data from the Subject when using the pattern (pulling by the observers is considered more \"correct\").","title":"The Observer Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#the-decorator-pattern","text":"Classes should be open for extension , but closed for modification . Be careful when choosing the areas of code that need to be extended; applying the Open-Closed Principle EVERYWHERE is wasteful and unnecessary, and can lead to complex, hard-to-understand code. The Decorator Pattern attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Composition and delegation can often be used to add new behaviors at runtime. Decorators are typically transparent to the client of the component; that is, unless the client is relying on the component's concrete type. Decorating objects gives objects new responsibilities without making code changes to the underlying classes. The decorator adds its own behavior either before and/or after delegating to the component object it decorates to do the rest of the job. Java's io package heavily uses the decorator pattern. i.e. To read contents from a zipped file, you might do FileInputStream wrapped with BufferedInputStream wrapped with ZipInputStream . The FileInputStream is the abstract component and the others are the concrete decorator implementations. One drawback of the Decorator Pattern is that it often result in a large number of small classes that can be overwhelming to a developer trying to use the Decorator-based API. Overuse of this pattern will make the system more complex. If some code logic is dependent on specific types, think twice before introducing decorators, as it hides the underlying types . Use of the Factory and Builder patterns can help reduce the complexity of instantiate a component with many decorators.","title":"The Decorator Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#the-factory-patterns","text":"When you use the new operator you are certainly instantiating a concrete class, and certainly an implementation rather than an interface. This is against the principle that to program against interface, not implementation. Tying your code to a concrete class can make it more fragile and less flexible. The Factory Method Pattern defines an interface for creating an object, but lets subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses . The Factory Patterns are really about encapsulating object creation . By encapsulating the object creation details in a separate and focused class, we have only one place to make modifications and everywhere that uses it stays untouched. The Dependency Inversion Principle: depend upon abstractions . Do not depend upon concrete classes. It suggests that our high-level components should not depend on our low-level components; rather, they should both depend on abstractions. Instead of starting at the top, start at the low-level concrete components and think about what you can abstract , then move on to design the high level with the abstraction in mind. A few guidelines for trying to adhere with the principle: No variable should hold a reference to a concrete class. No class should derive from a concrete class. No method should override an implemented method of any of its base classes. Those methods implemented in the base class are meant to be shared by all your subclasses. The Abstract Factory Pattern provides an interface for creating families of related or dependent objects without specifying their concrete classes. An Abstract Factory gives us an interface for creating a family of products. By writing code that uses this interface, we decouple our code from the actual factory that creates the products. It allows us to implement a variety of factories that produce products meant for different contexts. The differences between the two: how product object is created Factory Method Pattern do it through inheritance Abstract Factory Pattern do it through object composition the Factory Method Pattern is good for creating one product, while Abstract Factory Pattern is good for creating a family of products. how factory is created Factory Method Pattern do it through implementing the abstract creator method that makes use of the concrete types the subclasses create Abstract Factory Pattern do it through implementing factory methods Use Factory Method Pattern if the number of concrete classes is foreseeably finite, or you don't know ahead of time all the concrete classes going to need. Use Abstract Factory Method Pattern when need to create families of products, or those that make sense to belong in the same factory.","title":"The Factory Patterns"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#singleton-pattern","text":"Create objects that there is only one instance at run time, and create them only when they are needed. There are many objects we only need one of: thread pools, caches, dialog boxes, objects that handle preferences and registry settings, objects used for logging, and objects that act as device drivers to devices like printers and graphics cards. The Singleton Pattern ensures a class has only one instance, and provides a global point of access to it. If the singleton object is accessed by multiple threads, a few options to deal with it: Do nothing if the performance of getInstance() isn't critical to your application. Move to an eagerly created instance rather than a lazily created one. Use \"double-checked locking\" to reduce the use of synchronization in getInstance(). Other cases when there can be more than one singleton instances when there are multiple versions of class loaders reflection, serialization and deserialization It is also easy to use singleton but violate the loose coupling principle. Singletons are meant to be used sparingly. Enum is also a good way to create singletons","title":"Singleton Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#command-pattern","text":"This pattern is about encapsulating method invocation. It allows you to decouple the requester of an action from the object that actually performs the action. A command object encapsulates a request to do something on a specific object. The caller doesn't have any idea what the work is, it just has a command object that knows how to talk to the right object to get the work done. The Command Pattern encapsulates a request as an object, thereby letting you parameterize other objects with different requests, queue or log requests, and support undoable operations. From the outside, no other objects really know what actions get performed on what receiver; they just know that if they call the execute() method, their request will be serviced. The receiver of the request gets bound to the command it's encapsulated in. In general, we strive for \"dumb\" command objects that just invoke an action on a receiver. The NoCommand object is an example of a null object . A null object is useful when you don't have a meaningful object to return, and yet you want to remove the responsibility for handling null from the client. Java's lambda expressions can be used to populate commands. Say, use a function object as a command. You can only do this if your Command interface has one abstract method. Another flavor of this pattern, the Meta Command Pattern allows you to create macros of commands so that you can execute multiple commands at once. The idea is to create another command that keeps a list of other commands , and execute them when execute() is called Commands allows packaging a piece of computation and pass it around. The computation itself may be invoked asynchronously or invoked by a different thread. The commands can also be put inside a queue and executed in sequence or saved as a batch, useful for applications like logging or transactional systems.","title":"Command Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#adaper-pattern","text":"The client makes a request to the adapter by calling a method on it using the target interface. The adapter translates the request into one or more calls on the adaptee using the adaptee interface. The client receives the results of the call and never knows there is an adapter doing the translation. Note that the Client and Adaptee are decoupled \u2013 neither knows about the other. You may need to adapt many classes to provide the interface a client is coded to The Adapter Pattern converts the interface of a class into another interface the clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces . With class adapter we subclass the Target and the Adaptee, while with object adapter we use composition to pass requests to an Adaptee. There will be cases when a behavior is not implemented by the adaptee class, for which clients will have to watch out for potential exceptions ( UnsupportedOperationException ), but as long as the client is careful and the adapter is well documented this is a perfectly reasonable solution.","title":"Adaper Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#facade-pattern","text":"You can take a complex subsystem and make it easier to use by implementing a Facade class that provides one more reasonable interface. A facade not only simplifies an interface, it decouples a client from a subsystem of components. Facades and adapters may wrap multiple classes, but a facade's intent is to simplify , while an adapter's is to convert the interface to something different. The Facade Pattern provides a unified interface to a set of interfaces in a subsystem. Facade defines a higher-level interface that makes the subsystem easier to use. Principle of Least Knowledge: talk only to your immediate friends. When you are designing a system, for any object, be careful of the number of classes it interacts with and also how it comes to interact with those classes. Prevent designing a large number of classes coupled together. We should only invoke methods that belong to: The object itself Objects passed in as a parameter to the method Any object the method creates or instantiates Try NOT to call methods on objects that were returned from calling other methods. An adapter wraps an object to change its interface, a decorator wraps an object to add new behaviors and responsibilities, and a facade \"wraps\" a set of objects to simplify.","title":"Facade Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#templating-pattern","text":"When there are code duplication, it is a good sign to do some clean up, share the common code. The Templating Pattern basically involves defining a set of methods that should be implemented. The Template Method defines the steps of an algorithm and allows subclasses to provide the implementation for one or more steps. This pattern makes algorithm live in one place and one place to change. It prevents duplication on more concrete subclasses. The Template Method Pattern defines the skeleton of an algorithm in a method, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm's structure. A hook is a method that is declared in the abstract class, but only given an empty or default implementation. This gives subclasses the ability to \"hook into\" the algorithm at various points, if they wish; a subclass is also free to ignore the hook. It is good to provide more granular and optional steps as hooks , so it places less burden on the subclass but also provide a flexible functionality. The Hollywood Principle: Don't call us, we'll call you. This principle is to prevent dependency rot. Dependency rot happens when you have high-level components depending on low-level components depending on high-level components depending on sideways components depending on low-level components, and so on. The Hollywood Principle allow low-level components to hook themselves into a system, but the high-level components determine when they are needed, and how. The Templating Pattern can also appear as having a static method as the templating method, and requires the concrete class to implement an interface. i.e. Java's Array.sort requires the object to implement the Comparable interface. To prevent subclasses from changing the algorithm in the template method, declare the template method as final.","title":"Templating Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#iterator-and-composite-patterns","text":"These patterns are about allowing your clients to iterate through your objects without ever getting a peek at how you store your objects. The Iterator Pattern provides a way to access the elements of an aggregate object sequentially without exposing its underlying representation. It also places the task of traversal on the iterator object, not on the aggregate, which simplifies the aggregate interface and implementation, and places the responsibility where it should be. External iterator, means that the client controls the iteration by calling next() to get the next element. An internal iterator is controlled by the iterator itself. In that case, because it's the iterator that's stepping through the elements, you have to tell the iterator what to do with those elements as it goes through them. A class should have only one reason to change, a single responsibility. Every responsibility of a class is an area of potential change. More than one responsibility means more than one area of change. The only way to succeed is to be diligent in examining your designs and to watch out for signals that a class is changing in more than one way as your system grows. We say that a module or class has high cohesion when it is designed around a set of related functions, and we say it has low cohesion when it is designed around a set of unrelated functions. Classes that adhere to the principle tend to have high cohesion and are more maintainable than classes that take on multiple responsibilities and have low cohesion. The Composite Pattern allows you to compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly. Using a composite structure, we can apply the same operations over both composites and individual objects. In other words, in most cases we can ignore the differences between compositions of objects and individual objects. By allowing the Component interface to contain the child management operations and the leaf operations, a client can treat both composites and leaf nodes uniformly; so whether an element is a composite or leaf node becomes transparent to the client. Otherwise, by enforcing that any inappropriate calls on elements would be caught at compile time or runtime, but we'd lose transparency and our code would have to use conditionals and the instanceof operator. There are many design tradeoffs in implementing Composite. You need to balance transparency and safety with your needs.","title":"Iterator and Composite Patterns"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#state-pattern","text":"In a state diagram, all states are in different configurations of the machine that behave in a certain way and need some action to take them to another state. This pattern allows an object to have many different behaviors that are based on its internal state. By using the state pattern we want localize the behavior of each state into its own class remove troublesome if-statements on the machine's state variables close each state for modification The State Pattern allows an object to alter its behavior when its internal state changes. The object will appear to change its class. With the State Pattern, we have a set of behaviors encapsulated in state objects; at any time the context is delegating to one of those states. Over time, the current state changes across the set of state objects to reflect the internal state of the context, so the context's behavior changes over time as well. The client usually knows very little, if anything, about the state objects. With Strategy, the client usually specifies the strategy object that the context is composed with. Think of the State Pattern as an alternative to putting lots of conditionals in your context; by encapsulating the behaviors within state objects, you can simply change the state object in context to change its behavior. The disadvantage of having state transitions in the state classes is that we create dependencies between the state classes. Also by encapsulating state behavior into separate state classes, you'll always end up with more classes in your design. It is the Context's job to oversee its state, and you don't usually want a client changing the state of a Context without that Context's knowledge. Given we had no common functionality to put into an abstract class, we went with an interface. In your own implementation, you might want to consider an abstract class. Doing so has the benefit of allowing you to add methods to the abstract class later, without breaking the concrete state implementations.","title":"State Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#proxy-pattern","text":"This pattern controlls object access. Your client object acts like it's making remote method calls. But what it's really doing is calling methods on a heap-local \u2018proxy' object that handles all the low-level details of network communication. Java's Remote Method Invocation (RMI) gives us a way to find objects in a remote JVM and allows us to invoke their methods. The nice thing about RMI is that you don't have to write any of the networking or I/O code yourself. With your client, you call remote methods just like normal method calls on objects running in the client's own local JVM. RMI Nomenclature: in RMI, the client helper is a \u2018stub' and the service helper is a \u2018skeleton'. To do this, we need: remote interface - defines methods that a client can call remotely and both the Stub and actual service will implement this extend java.rmi.Remote and declare all methods to throw a RemoteException if the methods in an interface declare exceptions, any code calling methods on a reference of that type (the interface type) must handle or declare the exceptions remote implementation - the object that client wants to remotely call methods on extend java.rmi.server.UnicastRemoteObject and inherit useful methods from that class the methods will handle the remote calls other RemoteObject classes exist register your service using the static rebind() method of the java.rmi.Naming class RMI registry - where the client gets the proxy object (stub/helper object) start rmiregistry in the class directory start remote service - registers the service with the RMI registry and available for clients start the service When making remote method calls, the arguments and return types must be serializable. The Proxy Pattern provides a surrogate or placeholder for another object to control access to it. Use the Proxy Pattern to create a representative object that controls access to another object, which may be remote, expensive to create, or in need of securing. The Proxy Pattern has many forms. What they all have in common is that they intercept a method invocation that the client is making on the subject. Some of them are: A remote proxy controls access to a remote object. the proxy acts as a local representative for an object that lives in a different JVM A virtual proxy controls access to a resource that is expensive to create. defers the creation of the object until it is needed and acts as a surrogate for the object before and while it is being created A protection proxy controls access to a resource based on access rights. Java's got its own proxy support right in the java.lang.reflect package. With this package, Java lets you create a proxy class on the fly that implements one or more interfaces and forwards method invocations to a class that you specify. This is called the dynamic proxy . To do this, we need: create InvocationHandlers that implement the behavior of the proxy, beacuase Java will create the actual proxy class and object that we don't control. write code to create the dynamic proxies and instantiate them. wrap the protected object with the appropriate proxy","title":"Proxy Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#compound-pattern","text":"This pattern is in fact pattern of patterns. A compound pattern is a set of a few patterns that are combined to solve a general problem. The Model-View-Controller pattern is actually a compound pattern.","title":"Compound Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#model-view-controller-mvc-pattern","text":"User interact with the view The controller asks the model to change the state, or ask the view to change The model notifies the view when its state changed The view asks the model for state, and changes accordingly to the state The model uses Observer to keep the views and controllers updated on the latest state changes. The view and the controller, on the other hand, implement the Strategy Pattern. The Adaptor Pattern is also often used with the MVC Pattern. These patterns work together to decouple the three players in the MVC model, which keeps designs clear and flexible. The controller implements behavior for the view. It is the smarts that translates the actions from the view to actions on the model. The model takes those actions and implements the application logic to decide what to do in response to those actions. The controller might have to do a little work to determine what method calls to make on the model, but that's not considered the \"application logic.\" The application logic is the code that manages and manipulates your data and it lives in your model. It wasn't long after the Web was spun that developers started adapting the MVC to fit the browser/server model. The prevailing adaptation is known simply as \"Model 2\" and uses a combination of servlet and JSP technology to achieve the same separation of model, view and controller that we see in conventional GUIs.","title":"Model-View-Controller (MVC) Pattern"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#collection-of-all-wisdoms","text":"OO Basics Abstraction Encapsulation Polymorphism Inheritance OO Principles Encasulate what varies Favor composition over inheritance Program to interfaces, not implementations Strive for loosely coupled designs between objects that interact Depend on abstractions. Do not depend on concrete classes Principle of Least Knowledge: talk only to your immediate friends The Hollywood Principle: Don't call us, we'll call you A class should have only one reason to change, a single responzibility OO Patterns The Strategy Pattern defines a family of algorithms, encapsulates each one, and makes them interchangeable. Strategy lets the algorithm vary independently from clients that use it. The Observer Pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependents are notified and updated automatically. The Decorator Pattern attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. The Factory Method Pattern defines an interface for creating an object, but lets subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses. The Abstract Factory Pattern provides an interface for creating families of related or dependent objects without specifying their concrete classes. The Singleton Pattern ensures a class has only one instance, and provides a global point of access to it. The Command Pattern encapsulates a request as an object, thereby letting you parameterize other objects with different requests, queue or log requests, and support undoable operations. The Adapter Pattern converts the interface of a class into another interface the clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces. The Facade Pattern provides a unified interface to a set of interfaces in a subsystem. Facade defines a higher-level interface that makes the subsystem easier to use. The Template Method Pattern defines the skeleton of an algorithm in a method, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm's structure. The Iterator Pattern provides a way to access the elements of an aggregate object sequentially without exposing its underlying representation. The Composite Pattern allows you to compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly The State Pattern allows an object to alter its behavior when its internal state changes. The object will appear to change its class. The Proxy Pattern provides a surrogate or placeholder for another object to control access to it.","title":"Collection of all wisdoms"},{"location":"Programming-Lang-Reference/Java/Design-Patterns/#other-resources","text":"The Portland Patterns Repository The Hillside Group","title":"Other Resources"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/","text":"Notes taken from Java 8 Standard API Commonly used Data Structures \u00b6 Data Objects \u00b6 member Integer Double constants MAX_VALUE, MIN_VALUE MAX_VALUE, MIN_VALUE, NaN, POSITIVE_INFINITY, NEGATIVE_INFINITY methods parseInt, valueOf parseDouble, valueOf, intValue, isNaN, isInfinite String \u00b6 charAt, contains, startsWith, endsWith, format, indexOf, join, split, substring, toCharArray, toLowerCase, toUpperCase, valueOf Collection Subinterfaces \u00b6 member List Set Map Queue Stack Deque (has Queue, Stack behaviors) insert add add put, computeIfAbsent add, offer push addFirst, addLast, offerFirst, offerLast lookup contains, get, indexOf contains containsKey, get, getOrDefault, entrySet peek peek, search getFirst, getLast, peekFirst, peekLast update set - put, replace - - delete remove, clear remove, clear remove, clear poll pop pollFirst, pollLast, removeFirst, removeLast Collections methods \u00b6 addAll, clear, contains, containsAll, equals, hashCode, isEmpty, iterator, parallelStream, remove, removeAll, removeIf, retainAll, size, spliterator, stream, toArray, toArray Iterable methods \u00b6 forEach Comparable methods \u00b6 compareTo List \u00b6 member ArrayList LinkedList (has Deque behavior) ArrayBlockingQueue ArrayDeque constructor default, copy, capacity default, copy capacity default, copy, capacity additional methods clone, ensureCapacity, sort - take, drainTo - Hash \u00b6 member HashSet HashMap HashTable constructor default, copy, capacity default, copy, capacity default, copy, capacity additional methods - clone clone, rehash Tree \u00b6 member PriorityQueue (has Deque behavior) TreeSet (has Set behavior) constructor default, copy, capacity, comparator default, copy, comparator additional methods - ceiling, floor, first, last, pollFirst, pollLast, higher, lower Utility \u00b6 StringBuilder \u00b6 append, delete, deleteCharAt, reverse, setLength","title":"Java Std Lib Cheatsheet"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#commonly-used-data-structures","text":"","title":"Commonly used Data Structures"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#data-objects","text":"member Integer Double constants MAX_VALUE, MIN_VALUE MAX_VALUE, MIN_VALUE, NaN, POSITIVE_INFINITY, NEGATIVE_INFINITY methods parseInt, valueOf parseDouble, valueOf, intValue, isNaN, isInfinite","title":"Data Objects"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#string","text":"charAt, contains, startsWith, endsWith, format, indexOf, join, split, substring, toCharArray, toLowerCase, toUpperCase, valueOf","title":"String"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#collection-subinterfaces","text":"member List Set Map Queue Stack Deque (has Queue, Stack behaviors) insert add add put, computeIfAbsent add, offer push addFirst, addLast, offerFirst, offerLast lookup contains, get, indexOf contains containsKey, get, getOrDefault, entrySet peek peek, search getFirst, getLast, peekFirst, peekLast update set - put, replace - - delete remove, clear remove, clear remove, clear poll pop pollFirst, pollLast, removeFirst, removeLast","title":"Collection Subinterfaces"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#collections-methods","text":"addAll, clear, contains, containsAll, equals, hashCode, isEmpty, iterator, parallelStream, remove, removeAll, removeIf, retainAll, size, spliterator, stream, toArray, toArray","title":"Collections methods"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#iterable-methods","text":"forEach","title":"Iterable methods"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#comparable-methods","text":"compareTo","title":"Comparable methods"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#list","text":"member ArrayList LinkedList (has Deque behavior) ArrayBlockingQueue ArrayDeque constructor default, copy, capacity default, copy capacity default, copy, capacity additional methods clone, ensureCapacity, sort - take, drainTo -","title":"List"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#hash","text":"member HashSet HashMap HashTable constructor default, copy, capacity default, copy, capacity default, copy, capacity additional methods - clone clone, rehash","title":"Hash"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#tree","text":"member PriorityQueue (has Deque behavior) TreeSet (has Set behavior) constructor default, copy, capacity, comparator default, copy, comparator additional methods - ceiling, floor, first, last, pollFirst, pollLast, higher, lower","title":"Tree"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#utility","text":"","title":"Utility"},{"location":"Programming-Lang-Reference/Java/Java_Std_Lib_Cheatsheet/#stringbuilder","text":"append, delete, deleteCharAt, reverse, setLength","title":"StringBuilder"},{"location":"Programming-Lang-Reference/Java/Style-Guide/","text":"This notes is taken from google.github.io/styleguide Source files \u00b6 File name \u00b6 Use the case-sensitive name of the top-level class it contains, plus the .java extension. Structure \u00b6 From top down: License or copyright information, if present or needed (IDEs have settings to create this automatically) package statement must never be wrapped import statements do NOT use wildcard imports (turn OFF auto wildcard in IDE setting) group all static imports together group all non-static imports together import names are sorted in ASCII sort order NO static import for static nested classes Exactly ONE top-level class members and initializers should follow some logical order that helps as if the maintainer to explain the code logic group overloaded methods or constructors together Exactly ONE blank line separates each section that is present. Example license or copyright header: /* * Copyright (C) <year> <project/org name> * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ Formatting \u00b6 Braces \u00b6 Always use braces with if else and for do while loop statements. Lambda braces are optional. Non-empty blocks should follow the rules: NO line break before the opening brace exception is when using a nested block to create a local closure for limiting local variable scopes. Line break after the opening brace Line break before the closing brace Line break after the closing brace when terminating a statement or the body of a method, constructor, or named class. Empty blocks should be closed immediately after it is opened, or add a line break in between if it is part of a multi-block statment try catch , if else . Lines \u00b6 Block indentation should be 2 spaces. Each statement should be followed by a line break. Each line should have a column limit of 100 characters, except: long URLs in Javadoc package import statements commands in comment that can be copy-pasted to a shell very long identifiers The primary goal for line wrapping is to have clear code, not necessarily code that fits in the smallest number of lines. Line wrapping rules: break after an assignment operator, = : break before non-assignment operator, . :: | <> a method or constructor name stays attached to the open parenthesis ( that follows it a comma , stays attached to the token precedes it no immediate line breaks after the arrow operator, except when there is only one statement follows it indentation of continuation lines by adding +2 spaces for each level of parallism Whitespace \u00b6 A single blank line appears: always between consecutive members or initializers of a class: constructors, methods, nested classes, static initializers, and instance initializers optionally between fields to create logical groupings of fields anywhere when it improves readability A single whitespace appears: separating a reserved word such as if for catch from an open parenthesis ( separating a reserved word such as else catch from a closing curly brace } before open curly brace { except on annotations or except on initializing array with constants using array initializer syntax, i.e. String[][] x = {{\"foo\"}}; on both sides of any binary or ternary operator + - * / % & | : :: . -> after , : ; or closing ) after casting before and after // for single comment. Multiple spaces also allowed between a type and variable declaration optionally inside both braces of an array initializer between type annotation and [] or ... Grouping parentheses \u00b6 Adding grouping parentheses to emphasize precedence makes arithematic code logic more readable Enum class \u00b6 An enum class with no methods and no documentation on its constants may optionally be formatted as if it were an array initializer. After each comma that follows an enum constant, a line break is optional. Variable declarations \u00b6 One variable per declaration, do NOT do int a, b; declaration except in a for loop header. Local variables are declared close to the point they are first used to minimize their scope. They should be initizalized immediately after declaration. Arrays \u00b6 Any array initializer may optionally be formatted as if it were a \"block-like construct.\" Square brackets form a part of the array type, not the variable: String args[] should not be used. Switch statement \u00b6 Add comment when there is a non-empty case fall-through i.e. switch ( input ) { case 1 : case 2 : prepareOneOrTwo (); // fall through case 3 : handleOneTwoOrThree (); break ; default : handleLargeNumber ( input ); } Always include a default label statement group even if it is empty, except when all cases are covered exhaustively, for example, when switching on a Enum value. Annotations \u00b6 Type-use annotations (i.e. meta-annotated with @Target(ElementType.TYPE_USE) ) appear immediately before the annotated type. Annotations applying to a class, method, or constructor appear immediately after the documentation block one on each line, except when the signature can fit on one line such as method declarations in an Interface class. Annotations applying to a field also appear immediately after the documentation block, but multiple annotations (possibly parameterized) may be listed on the same line. There are no specific rules for formatting annotations on parameters or local variables. Modifiers ordering \u00b6 Java Language Specification recommended order: public protected private abstract default static final transient volatile synchronized native strictfp Numeric Literals \u00b6 long -valued integer literals use an uppercase L suffix Naming \u00b6 All identifiers use ASCII letters and digits, with some cases using underscores. Package names \u00b6 Only lowercase letters and digits (no underscores), and consecutive words are simply concatenated together. Class names \u00b6 Class names are written in UpperCamelCase. Class names are typically nouns or noun phrases i.e. HashMap . Additionally, Interfaces may sometimes be adjectives or adjective phrases instead i.e. Runnable . A test class has a name that ends with Test, i.e. HashIntegrationTest . Method names \u00b6 Method names are written in lowerCamelCase. Method names are typically verbs or verb phrases i.e. sendMail . Underscores may appear in test method names to separate logical components of the name, with each component written in lowerCamelCase i.e. transferMoney_deductsFromSource . Field names \u00b6 Constant names use UPPER_SNAKE_CASE: all uppercase letters, with each word separated from the next by a single underscore. And are typically nouns or noun phrases. Note that NOT all static final fields are deemed constants. What is a constant Constants are static final fields whose contents are deeply immutable and whose methods have no detectable side effects. Examples include primitives, strings, immutable value classes, and anything set to null. If any of the instance's observable state can change, it is not a constant. Merely intending to never mutate the object is not enough. // Constants static final int NUMBER = 5 ; static final ImmutableList < String > NAMES = ImmutableList . of ( \"Ed\" , \"Ann\" ); static final Map < String , Integer > AGES = ImmutableMap . of ( \"Ed\" , 35 , \"Ann\" , 32 ); static final Joiner COMMA_JOINER = Joiner . on ( ',' ); // because Joiner is immutable static final SomeMutableType [] EMPTY_ARRAY = {}; // Not constants static String nonFinal = \"non-final\" ; final String nonStatic = \"non-static\" ; static final Set < String > mutableCollection = new HashSet < String > (); static final ImmutableSet < SomeMutableType > mutableElements = ImmutableSet . of ( mutable ); static final ImmutableMap < String , SomeMutableType > mutableValues = ImmutableMap . of ( \"Ed\" , mutableInstance , \"Ann\" , mutableInstance2 ); static final Logger logger = Logger . getLogger ( MyClass . getName ()); static final String [] nonEmptyArray = { \"these\" , \"can\" , \"change\" }; Non-constant field names (static or otherwise) are written in lowerCamelCase and are typically nouns or noun phrases. Parameter names are written in lowerCamelCase and are typically nouns or noun phrases. One-character parameter names in public methods should be avoided. Local variable names are written in lowerCamelCase and are typically nouns or noun phrases. Even when final and immutable, local variables are not considered to be constants Type variable names are usually one single capital letter optionally followed by a single numeral, i.e. E, T, X, T2 , or a name in the form used for classes and followed by the capital letter T , i.e. RequestT Programming Practices \u00b6 @Override \u00b6 Always use @Override annotation when applicable, except when parent method is @Deprecated . Caught exceptions \u00b6 Do NOT ignore an caught exception even when it is logically impossible for the exception to be thrown. Typical responses are to log it or rethrow it as an AssertionError . When it is safe to do nothing, be sure to leave a comment for why that is okay, rather than leaving the catch block empty. In tests, you may leave an empty catch block if naming the exception variable expected . Static members \u00b6 Always access a static class member qualified with that class's name, do NOT access with a reference or expression which yields a reference of that class's type. Commenting \u00b6 Block comments \u00b6 Block comments are indented at the same level as the surrounding code. For multi-line /* ... */ comments, subsequent lines must start with * aligned with the * on the previous line. Comments should NOT be enclosed in boxes drawn with asterisks or other characters. JavaDoc \u00b6 The general form can span multiple lines or use up a single line (when there is no parameters or return value). Rules covers block comments also applies. A blank line should appear between paragraphs. Each paragraph except the first has <p> immediately before the first word, with no space after it, and end the last word in paragraph with </p> . Other HTML tags can be used rather than <p> , such as <ul> <table> Any of the standard \"block tags\" that are used appear in the order @param, @return, @throws, @deprecated and should always be followed with descriptions. When a block tag doesn't fit on a single line, continuation lines are indented four (or more) spaces from the position of the @ . Each Javadoc block begins with a brief summary fragment , which should be a noun phrase or verb phrase, NOT a complete sentence.. It is the only part of the text that appears in certain contexts such as class and method indexes. At the minimum, Javadoc is present for every public class, and every public or protected member of such a class, except: some self-explanatory members such as fields, getter and setter methods. method that overrides a supertype method","title":"Style Guide"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#source-files","text":"","title":"Source files"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#file-name","text":"Use the case-sensitive name of the top-level class it contains, plus the .java extension.","title":"File name"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#structure","text":"From top down: License or copyright information, if present or needed (IDEs have settings to create this automatically) package statement must never be wrapped import statements do NOT use wildcard imports (turn OFF auto wildcard in IDE setting) group all static imports together group all non-static imports together import names are sorted in ASCII sort order NO static import for static nested classes Exactly ONE top-level class members and initializers should follow some logical order that helps as if the maintainer to explain the code logic group overloaded methods or constructors together Exactly ONE blank line separates each section that is present. Example license or copyright header: /* * Copyright (C) <year> <project/org name> * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */","title":"Structure"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#formatting","text":"","title":"Formatting"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#braces","text":"Always use braces with if else and for do while loop statements. Lambda braces are optional. Non-empty blocks should follow the rules: NO line break before the opening brace exception is when using a nested block to create a local closure for limiting local variable scopes. Line break after the opening brace Line break before the closing brace Line break after the closing brace when terminating a statement or the body of a method, constructor, or named class. Empty blocks should be closed immediately after it is opened, or add a line break in between if it is part of a multi-block statment try catch , if else .","title":"Braces"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#lines","text":"Block indentation should be 2 spaces. Each statement should be followed by a line break. Each line should have a column limit of 100 characters, except: long URLs in Javadoc package import statements commands in comment that can be copy-pasted to a shell very long identifiers The primary goal for line wrapping is to have clear code, not necessarily code that fits in the smallest number of lines. Line wrapping rules: break after an assignment operator, = : break before non-assignment operator, . :: | <> a method or constructor name stays attached to the open parenthesis ( that follows it a comma , stays attached to the token precedes it no immediate line breaks after the arrow operator, except when there is only one statement follows it indentation of continuation lines by adding +2 spaces for each level of parallism","title":"Lines"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#whitespace","text":"A single blank line appears: always between consecutive members or initializers of a class: constructors, methods, nested classes, static initializers, and instance initializers optionally between fields to create logical groupings of fields anywhere when it improves readability A single whitespace appears: separating a reserved word such as if for catch from an open parenthesis ( separating a reserved word such as else catch from a closing curly brace } before open curly brace { except on annotations or except on initializing array with constants using array initializer syntax, i.e. String[][] x = {{\"foo\"}}; on both sides of any binary or ternary operator + - * / % & | : :: . -> after , : ; or closing ) after casting before and after // for single comment. Multiple spaces also allowed between a type and variable declaration optionally inside both braces of an array initializer between type annotation and [] or ...","title":"Whitespace"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#grouping-parentheses","text":"Adding grouping parentheses to emphasize precedence makes arithematic code logic more readable","title":"Grouping parentheses"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#enum-class","text":"An enum class with no methods and no documentation on its constants may optionally be formatted as if it were an array initializer. After each comma that follows an enum constant, a line break is optional.","title":"Enum class"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#variable-declarations","text":"One variable per declaration, do NOT do int a, b; declaration except in a for loop header. Local variables are declared close to the point they are first used to minimize their scope. They should be initizalized immediately after declaration.","title":"Variable declarations"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#arrays","text":"Any array initializer may optionally be formatted as if it were a \"block-like construct.\" Square brackets form a part of the array type, not the variable: String args[] should not be used.","title":"Arrays"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#switch-statement","text":"Add comment when there is a non-empty case fall-through i.e. switch ( input ) { case 1 : case 2 : prepareOneOrTwo (); // fall through case 3 : handleOneTwoOrThree (); break ; default : handleLargeNumber ( input ); } Always include a default label statement group even if it is empty, except when all cases are covered exhaustively, for example, when switching on a Enum value.","title":"Switch statement"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#annotations","text":"Type-use annotations (i.e. meta-annotated with @Target(ElementType.TYPE_USE) ) appear immediately before the annotated type. Annotations applying to a class, method, or constructor appear immediately after the documentation block one on each line, except when the signature can fit on one line such as method declarations in an Interface class. Annotations applying to a field also appear immediately after the documentation block, but multiple annotations (possibly parameterized) may be listed on the same line. There are no specific rules for formatting annotations on parameters or local variables.","title":"Annotations"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#modifiers-ordering","text":"Java Language Specification recommended order: public protected private abstract default static final transient volatile synchronized native strictfp","title":"Modifiers ordering"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#numeric-literals","text":"long -valued integer literals use an uppercase L suffix","title":"Numeric Literals"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#naming","text":"All identifiers use ASCII letters and digits, with some cases using underscores.","title":"Naming"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#package-names","text":"Only lowercase letters and digits (no underscores), and consecutive words are simply concatenated together.","title":"Package names"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#class-names","text":"Class names are written in UpperCamelCase. Class names are typically nouns or noun phrases i.e. HashMap . Additionally, Interfaces may sometimes be adjectives or adjective phrases instead i.e. Runnable . A test class has a name that ends with Test, i.e. HashIntegrationTest .","title":"Class names"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#method-names","text":"Method names are written in lowerCamelCase. Method names are typically verbs or verb phrases i.e. sendMail . Underscores may appear in test method names to separate logical components of the name, with each component written in lowerCamelCase i.e. transferMoney_deductsFromSource .","title":"Method names"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#field-names","text":"Constant names use UPPER_SNAKE_CASE: all uppercase letters, with each word separated from the next by a single underscore. And are typically nouns or noun phrases. Note that NOT all static final fields are deemed constants. What is a constant Constants are static final fields whose contents are deeply immutable and whose methods have no detectable side effects. Examples include primitives, strings, immutable value classes, and anything set to null. If any of the instance's observable state can change, it is not a constant. Merely intending to never mutate the object is not enough. // Constants static final int NUMBER = 5 ; static final ImmutableList < String > NAMES = ImmutableList . of ( \"Ed\" , \"Ann\" ); static final Map < String , Integer > AGES = ImmutableMap . of ( \"Ed\" , 35 , \"Ann\" , 32 ); static final Joiner COMMA_JOINER = Joiner . on ( ',' ); // because Joiner is immutable static final SomeMutableType [] EMPTY_ARRAY = {}; // Not constants static String nonFinal = \"non-final\" ; final String nonStatic = \"non-static\" ; static final Set < String > mutableCollection = new HashSet < String > (); static final ImmutableSet < SomeMutableType > mutableElements = ImmutableSet . of ( mutable ); static final ImmutableMap < String , SomeMutableType > mutableValues = ImmutableMap . of ( \"Ed\" , mutableInstance , \"Ann\" , mutableInstance2 ); static final Logger logger = Logger . getLogger ( MyClass . getName ()); static final String [] nonEmptyArray = { \"these\" , \"can\" , \"change\" }; Non-constant field names (static or otherwise) are written in lowerCamelCase and are typically nouns or noun phrases. Parameter names are written in lowerCamelCase and are typically nouns or noun phrases. One-character parameter names in public methods should be avoided. Local variable names are written in lowerCamelCase and are typically nouns or noun phrases. Even when final and immutable, local variables are not considered to be constants Type variable names are usually one single capital letter optionally followed by a single numeral, i.e. E, T, X, T2 , or a name in the form used for classes and followed by the capital letter T , i.e. RequestT","title":"Field names"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#programming-practices","text":"","title":"Programming Practices"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#override","text":"Always use @Override annotation when applicable, except when parent method is @Deprecated .","title":"@Override"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#caught-exceptions","text":"Do NOT ignore an caught exception even when it is logically impossible for the exception to be thrown. Typical responses are to log it or rethrow it as an AssertionError . When it is safe to do nothing, be sure to leave a comment for why that is okay, rather than leaving the catch block empty. In tests, you may leave an empty catch block if naming the exception variable expected .","title":"Caught exceptions"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#static-members","text":"Always access a static class member qualified with that class's name, do NOT access with a reference or expression which yields a reference of that class's type.","title":"Static members"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#commenting","text":"","title":"Commenting"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#block-comments","text":"Block comments are indented at the same level as the surrounding code. For multi-line /* ... */ comments, subsequent lines must start with * aligned with the * on the previous line. Comments should NOT be enclosed in boxes drawn with asterisks or other characters.","title":"Block comments"},{"location":"Programming-Lang-Reference/Java/Style-Guide/#javadoc","text":"The general form can span multiple lines or use up a single line (when there is no parameters or return value). Rules covers block comments also applies. A blank line should appear between paragraphs. Each paragraph except the first has <p> immediately before the first word, with no space after it, and end the last word in paragraph with </p> . Other HTML tags can be used rather than <p> , such as <ul> <table> Any of the standard \"block tags\" that are used appear in the order @param, @return, @throws, @deprecated and should always be followed with descriptions. When a block tag doesn't fit on a single line, continuation lines are indented four (or more) spaces from the position of the @ . Each Javadoc block begins with a brief summary fragment , which should be a noun phrase or verb phrase, NOT a complete sentence.. It is the only part of the text that appears in certain contexts such as class and method indexes. At the minimum, Javadoc is present for every public class, and every public or protected member of such a class, except: some self-explanatory members such as fields, getter and setter methods. method that overrides a supertype method","title":"JavaDoc"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/","text":"Two simple rules that helps software development: Write a failing automated test before you write any code Remove duplication Intro to TDD \u00b6 Quickly add a test Run all tests and see the new one fail Make a little change Run all tests and see them all succeed Refactor to remove duplication Introduce a red, then make it green fast. Then make it good and green fast. Prefer not to write a test when there is red. A good habit is to maintain a to-do list for tests and features, and make the items in-progress as bold and completed items as crossed off. When the list is empty is a good time to review the design. First, put on the list examples of every operation that you know you need to implement. Next, for those operations that don't already exist, put the null version of that operation on the list. Finally, list all of the refactorings that you think you will have to do in order to have clean code at the end of this session. If you don't find any test on the list that represents one step, then add some new tests that would represent progress toward the items there. Always start a test small . TDD is not about taking teeny-tiny steps, it's about being able to take teeny-tiny steps. It is okay if the test doesn't even compile. Then work to fix the compile errors (can start with stubs), then fix the test so the outcome matches the expected. In the end, it is expected to have roughly as many lines and functions in the test and functional code. It is important to get through steps 1-4 quickly, then spend time to make step 5 right. Keep in mind while refactoring in the good direction, try not to introduce more design until you had a better motivation to do so. It is okay if the code that made the test pass is not perfect yet. Because the next step will be adding more tests to cover general cases and edge cases, then going through the TDD cycle. It is possible that after a few iterations, the code you initially wrote had been changed drastically, and you will find the initial dumb code has become more sophisticated and clean, and there is a nice set of tests that guards you with more refacotring aginst breaking existing functionalities. If dependency is the problem, duplication is the symptom. Duplication most often takes the form of duplicate logic -- the same expression appearing in multiple places in the code. This can be code snippets or constants. Another goal is to be able to write another test that \u201cmakes sense\u201d to us, without having to change the code. By eliminating duplication before we go on to the next test, we maximize our chance of being able to get the next test running with one and only one change. You will often be implementing TDD in code that doesn't have adequate tests . You could make a refactoring mistake and the tests would all still run. Write the tests you wish you had. If you don't, you will eventually break something while refactoring. If you have a big system, then the parts that you touch all the time should be absolutely rock solid, so you can make daily changes confidently. TDD followed religiously should result in 100 percent statement coverage. You can also do defect insertion to evaluate test quality: change the meaning of a line of code and a test should break. The three approaches to making a test work cleanly : fake it, triangulation, and obvious implementation. Removing duplication between test and code as a way to drive the design. The ability to control the gap between tests to increase traction when the road gets slippery and cruise faster when conditions are clear. The running of tests should not affect one another at all times. Tests should be independent. Include expected and actual results in the test itself, and try to make their relationship apparent. You are writing tests for a reader, not just the computer. Later on when someone needs to maintain or update the test, it is easy to spot what you are trying to test. Best practices for TDD \u00b6 You should test: conditionals, loops, operations, polymorphism. Some common signs that code needs improvements: long test setup code - objects probably are too big and need to be split setup duplication - too many objects tightly coupled that it is hard to have a common setup long running tests - long running tests won't be run often and is bad for TDD, and indicates design flaws fragile tests - tests break unexpectedly suggest that one part of application is affecting another When you have a bunch of code that more or less works and you want to add tests to it, and the biggest problem is that code that isn't written with tests in mind typically isn't very testable. What you don't do is go write tests for the whole thing and refactor the whole thing, which would take months. Start adding tests and refactor the code parts that most likely would change all the time, then repeat on untested code. I think the reason for this is that working in a test-driven development style gives you this sense of keeping just one ball in the air at once, so you can concentrate on that ball properly and do a really good job with it. When I'm trying to add some new functionality, I'm not worried about what really makes a good design for this piece of function, I'm just trying to get a test to pass as easily as I can. When I switch to refactoring mode, I'm not worried about adding some new function, I'm just worried about getting the right design. With both of these I'm just focused on one thing at a time, and as a result I can concentrate better on that one thing. Patterns are always half baked, and need to be adapted in the oven of your own project. But a good way to do this is to first copy the pattern fairly blindly, and then use some mix of refactoring or test-first, to perform the adaptation. Starter Test \u00b6 Pick a Starter Test that will teach you something but that you are certain you can quickly get to work. Start by testing a variant of an operation that doesn't do anything. Then start asking the questions: Where does the operation belong What are the correct inputs What is the correct output given those inputs You will have a realistic test after answering all of the three questions with test code. Taking one question at a time helps with maintaining a fast Red/green/refactor loop. Explanation Test \u00b6 Attempt to alter existing tests, ask yourself for explanations of those tests and come up with new test cases. Learning Test \u00b6 Before using and coding with a new external library, attempt to write tests to verify its API works as expected. This way, if your application critically depends on some features or API provided by that library, then it is safe to verify those functionalities didn't change when upgrading to a newer version by running the existing learning tests. Regression Test \u00b6 When something fails, a code bug was caught during production, write the smallest possible test that fails and that, once run, will be repaired. Every time you have to write a regression test, think about how you could have known to write the test in the first place. Child Test \u00b6 When there is apparently a test case that turns out to be too big and requires multiple code changes to work, write a smaller test case that represents the broken part of the bigger test case. Then get the test passing and reintroduce a larger test case, repeat until you gets the complete large test case. Mock Object \u00b6 To test an object that relies on an expensive or complicated resource, create a fake version of the resource that answers constants. Mock Objects encourage you down the path of carefully considering the visibility of every object, reducing the coupling in your designs. http://www.mockobjects.com/ Crash Test Dummy \u00b6 Test error code or exceptions logic by running tests with a special object that throws the exception intead of doing real work. Refactoring \u00b6 Reconcile Differences \u00b6 When there are two similar looking pieces of code, gradually bring them closer and unify them when they are identical. Two loop structures are similar. By making them identical, you can merge them. Two branches of a conditional are similar. By making them identical, you can eliminate the conditional. Two methods are similar. By making them identical, you can eliminate one. Two classes are similar. By making them identical, you can eliminate one. Isolate Change \u00b6 Isolate the part that has to change when working on a multi-part method or object. Migrate Data \u00b6 When moving data from one representation to another, temporarily duplicate the data. Add an instance variable in the new format. Set the new format variable everywhere you set the old format. Use the new format variable everywhere you use the old format. Delete the old format. Change the external interface to reflect the new format. If changing API: Add a parameter in the new format. Translate from the new format parameter to the old format internal representation. Delete the old format parameter. Replace uses of the old format with the new format. Delete the old format. Extract Method \u00b6 Make long and complicated method easier to read by turnning a small part of it into a separate method and call the new method. Find a region of the method that would make sense as its own method. Bodies of loop, whole loops, and branches of conditionals are common candidates for extraction. Make sure that there are no assignments to temporary variables declared outside the scope of the region to be extracted. Copy the code from the old method to the new method. Compile it. For each temporary variable or parameter of the original method used in the new method, add a parameter to the new method. Call the new method from the original method. However, don't break methods too far that the code is not readable.","title":"Testing Design Patterns"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#intro-to-tdd","text":"Quickly add a test Run all tests and see the new one fail Make a little change Run all tests and see them all succeed Refactor to remove duplication Introduce a red, then make it green fast. Then make it good and green fast. Prefer not to write a test when there is red. A good habit is to maintain a to-do list for tests and features, and make the items in-progress as bold and completed items as crossed off. When the list is empty is a good time to review the design. First, put on the list examples of every operation that you know you need to implement. Next, for those operations that don't already exist, put the null version of that operation on the list. Finally, list all of the refactorings that you think you will have to do in order to have clean code at the end of this session. If you don't find any test on the list that represents one step, then add some new tests that would represent progress toward the items there. Always start a test small . TDD is not about taking teeny-tiny steps, it's about being able to take teeny-tiny steps. It is okay if the test doesn't even compile. Then work to fix the compile errors (can start with stubs), then fix the test so the outcome matches the expected. In the end, it is expected to have roughly as many lines and functions in the test and functional code. It is important to get through steps 1-4 quickly, then spend time to make step 5 right. Keep in mind while refactoring in the good direction, try not to introduce more design until you had a better motivation to do so. It is okay if the code that made the test pass is not perfect yet. Because the next step will be adding more tests to cover general cases and edge cases, then going through the TDD cycle. It is possible that after a few iterations, the code you initially wrote had been changed drastically, and you will find the initial dumb code has become more sophisticated and clean, and there is a nice set of tests that guards you with more refacotring aginst breaking existing functionalities. If dependency is the problem, duplication is the symptom. Duplication most often takes the form of duplicate logic -- the same expression appearing in multiple places in the code. This can be code snippets or constants. Another goal is to be able to write another test that \u201cmakes sense\u201d to us, without having to change the code. By eliminating duplication before we go on to the next test, we maximize our chance of being able to get the next test running with one and only one change. You will often be implementing TDD in code that doesn't have adequate tests . You could make a refactoring mistake and the tests would all still run. Write the tests you wish you had. If you don't, you will eventually break something while refactoring. If you have a big system, then the parts that you touch all the time should be absolutely rock solid, so you can make daily changes confidently. TDD followed religiously should result in 100 percent statement coverage. You can also do defect insertion to evaluate test quality: change the meaning of a line of code and a test should break. The three approaches to making a test work cleanly : fake it, triangulation, and obvious implementation. Removing duplication between test and code as a way to drive the design. The ability to control the gap between tests to increase traction when the road gets slippery and cruise faster when conditions are clear. The running of tests should not affect one another at all times. Tests should be independent. Include expected and actual results in the test itself, and try to make their relationship apparent. You are writing tests for a reader, not just the computer. Later on when someone needs to maintain or update the test, it is easy to spot what you are trying to test.","title":"Intro to TDD"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#best-practices-for-tdd","text":"You should test: conditionals, loops, operations, polymorphism. Some common signs that code needs improvements: long test setup code - objects probably are too big and need to be split setup duplication - too many objects tightly coupled that it is hard to have a common setup long running tests - long running tests won't be run often and is bad for TDD, and indicates design flaws fragile tests - tests break unexpectedly suggest that one part of application is affecting another When you have a bunch of code that more or less works and you want to add tests to it, and the biggest problem is that code that isn't written with tests in mind typically isn't very testable. What you don't do is go write tests for the whole thing and refactor the whole thing, which would take months. Start adding tests and refactor the code parts that most likely would change all the time, then repeat on untested code. I think the reason for this is that working in a test-driven development style gives you this sense of keeping just one ball in the air at once, so you can concentrate on that ball properly and do a really good job with it. When I'm trying to add some new functionality, I'm not worried about what really makes a good design for this piece of function, I'm just trying to get a test to pass as easily as I can. When I switch to refactoring mode, I'm not worried about adding some new function, I'm just worried about getting the right design. With both of these I'm just focused on one thing at a time, and as a result I can concentrate better on that one thing. Patterns are always half baked, and need to be adapted in the oven of your own project. But a good way to do this is to first copy the pattern fairly blindly, and then use some mix of refactoring or test-first, to perform the adaptation.","title":"Best practices for TDD"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#starter-test","text":"Pick a Starter Test that will teach you something but that you are certain you can quickly get to work. Start by testing a variant of an operation that doesn't do anything. Then start asking the questions: Where does the operation belong What are the correct inputs What is the correct output given those inputs You will have a realistic test after answering all of the three questions with test code. Taking one question at a time helps with maintaining a fast Red/green/refactor loop.","title":"Starter Test"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#explanation-test","text":"Attempt to alter existing tests, ask yourself for explanations of those tests and come up with new test cases.","title":"Explanation Test"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#learning-test","text":"Before using and coding with a new external library, attempt to write tests to verify its API works as expected. This way, if your application critically depends on some features or API provided by that library, then it is safe to verify those functionalities didn't change when upgrading to a newer version by running the existing learning tests.","title":"Learning Test"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#regression-test","text":"When something fails, a code bug was caught during production, write the smallest possible test that fails and that, once run, will be repaired. Every time you have to write a regression test, think about how you could have known to write the test in the first place.","title":"Regression Test"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#child-test","text":"When there is apparently a test case that turns out to be too big and requires multiple code changes to work, write a smaller test case that represents the broken part of the bigger test case. Then get the test passing and reintroduce a larger test case, repeat until you gets the complete large test case.","title":"Child Test"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#mock-object","text":"To test an object that relies on an expensive or complicated resource, create a fake version of the resource that answers constants. Mock Objects encourage you down the path of carefully considering the visibility of every object, reducing the coupling in your designs. http://www.mockobjects.com/","title":"Mock Object"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#crash-test-dummy","text":"Test error code or exceptions logic by running tests with a special object that throws the exception intead of doing real work.","title":"Crash Test Dummy"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#refactoring","text":"","title":"Refactoring"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#reconcile-differences","text":"When there are two similar looking pieces of code, gradually bring them closer and unify them when they are identical. Two loop structures are similar. By making them identical, you can merge them. Two branches of a conditional are similar. By making them identical, you can eliminate the conditional. Two methods are similar. By making them identical, you can eliminate one. Two classes are similar. By making them identical, you can eliminate one.","title":"Reconcile Differences"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#isolate-change","text":"Isolate the part that has to change when working on a multi-part method or object.","title":"Isolate Change"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#migrate-data","text":"When moving data from one representation to another, temporarily duplicate the data. Add an instance variable in the new format. Set the new format variable everywhere you set the old format. Use the new format variable everywhere you use the old format. Delete the old format. Change the external interface to reflect the new format. If changing API: Add a parameter in the new format. Translate from the new format parameter to the old format internal representation. Delete the old format parameter. Replace uses of the old format with the new format. Delete the old format.","title":"Migrate Data"},{"location":"Programming-Lang-Reference/Java/Testing-Design-Patterns/#extract-method","text":"Make long and complicated method easier to read by turnning a small part of it into a separate method and call the new method. Find a region of the method that would make sense as its own method. Bodies of loop, whole loops, and branches of conditionals are common candidates for extraction. Make sure that there are no assignments to temporary variables declared outside the scope of the region to be extracted. Copy the code from the old method to the new method. Compile it. For each temporary variable or parameter of the original method used in the new method, add a parameter to the new method. Call the new method from the original method. However, don't break methods too far that the code is not readable.","title":"Extract Method"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/","text":"Dcoumentation \u00b6 like shell, comments starts with a '#' Debug \u00b6 perl -de1 perl -MCPAN -e shell perl -c file_to_compile Type of Data \u00b6 Numbers Integers Base-10: 1, -4, 255, 25_000_000 ... Binary: 0b11111 Octal: 0123, 055 Hexadecimal: 0x4AF Floating-point numbers - 0.5, -0.0133, ... Strings no processing: 'simple raw string' string evaluated as-is with interpolation: \"Hello world\\n\" Alternative Delimiters Instead of bounding strings with ' or \", can define you own delimiters for strings use qq followed by an arbitrary non-alphanumeric character: print qq/'\"Hi,\" said Jack. \"Have you read Slashdot today?\"'\\n/; use q// works too Here-Documents, another way to specify a string, start with << and then a label . print << EOF ; This is a here-document. It starts on the line after the two arrows, and it ends when the text following the arrows is found at the beginning of a line, like this: EOF Numbers<->Strings - perl converts between strings, integers, and floating-point numbers behind the scenes automatically if necessary. - \"12\" > \"30\" yields false - string contains no digit will be evaluated as 0 in Integer, if evaluated in an arithmetic expression - Special function can be used on converting binary, octal, or Hexadecimal string into Integers: - hex(\"0x30\"), oct(\"030\"), oct(\"0b11010\"), oct(\"0x35AB\") Operators Arithmetic Operators: + - * / % **(power function) ++ -- Logical Operators: & | ~(NOT) ^ Bit-wise Operators: << >> Comparison Operators: == != > < >= <= <=>(like java's compareTo, returns -1, 0, 1 i.e. 6<=>9 yields -1) Boolean Operators: && || ! and or xor not String Operators: .(for concatenation) x[0-9]+(repetes previous string N times) String Compare Operators: lt gt ge le eq ne cmp(like java's compareTo, returns -1, 0, 1) ord(takes first character of input string, returns its ASCII) Range Operators: ... and ... Build-list Operator: ..., ..., ... Regex Operator: =~ !~ Pointer Operator: -> \\ Variables types Scalars $name , holds Numbers, Strings, limited by the size of your computer's memory Lists @list , holds Numbers, Strings, variables (42, 39) (\"cheese\", \"cake\") (42, 1.5, \"lalala\", $test) qw/one two three four/ will yield ('one', 'two', 'three', 'four') '/' can be replaced with other special chars; spaces can be replaced with tabs, new lines, or any number of white spaces Note that lists inside lists will be flattened to level one list list element can be accessed with [N]; negative index rewinds from the end of the list nit: use $a = $array[0] instead of $a = (@array)[0] prime rule is this: the prefix represents what you want to get, not what you've got. [N] N can be a list, or a range of numbers to access corresponding elements Swap elements in-place: @months[3,4] = @months[4,3] which isn't far from swapping variables using list assignment ($mone, $mtwo) = ($mtwo, $mone) list slicing: [(index1, index2)] will return a new list containing elements in those indexes from the old list Ranges: (1 .. 6) will yield a list of 1-6 ('a' .. 'z') will yield a list of a-z array functions change elements: push pop shift(taken from index 0) unshift(adding to index 0) other: reverse sort sort is based on alphabetic orders by default. If sorting numbers or others things where special rule is required, can pass a compareTo func: my @string_sorted = sort { $a cmp $b } @unsorted; my @number_sorted = sort { $a <=> $b } @unsorted; special variable: $#array gives the highest element index in @array this made it okay to use for (0..$#array) for traversal using $_ as index note that this value is 1 less than the value from salar @array , which gives the size of the array Hashes %hash can be created by: List with key value pairs separated by commas. %where = ( \"Gary\" , \"Dallas\" , \"Lucy\" , \"Exeter\" ); List with key value pairs more readable: %where = ( Gary => \"Dallas\" , Lucy => Exeter ); From an array: @array = qw(Gary Dallas Lucy Exeter Ian Reading Samantha Oregon) ; %where = @array ; access/set values with $where{Key} keys %where gives a list of keys in this hash exists can be used to test whether a key exists in a hash if (not exists $rates{$key}) Variable name must begin with alphabetic character or underscore, then can be followed by numbers, letters, underscores Note that $var @var %var are different variables Scalar vs. List Context: if a variable is evaluated in different context, its return value will be different Can force scalar context using scalar operator on another variable, like this print scalar @array print @array ; # list context, returns list elements $scalar = @array ; # scalar context, returns list size Variable scopes Variables declared within blocks {} are lexical (local) variables; otherwise are global. Locla variable overrides global variable value at evaluation, while it does not modify global variable (with the same variable name). $global_var { my $local_var my ( $var1 , $var2 , $var3 ) # () is needed if declaring many at a time } With use strict; set, have to declare global variable with our $global_var Special Variables $_ the default variable that functions read from/write to $! a way of getting various things Perl want to give, like error message <> an abbreviation for <ARGV> $/ defines your own line separator for I/O purposes note that the $/ being set as \"\" will make reading chunk as paragraph instead of lines @_ stores arguments passed into a subroutine Variable interpolation print \"My name is $name\\n\"; print \"This is the ${times}th time.\\n\"; print \"@array\" will add spaces between its elements $scalar = \"@array\" achieve the same thing above, while storing into a string I/O \u00b6 Printing call print(RawStr...) is implicitly print(STDOUT, RawStr...) can print to error with print STDERROR RawStr expressions evaluated as false will be printing nothing; otherwise printing 1 die can be used to print error message and exit out current project. @array = ( 4 , 6 , 3 , 9 ) print @array , \"\\n\" # yields 4639 print \"@array\\n\" yields 4 6 3 9 Reads input my $cash = <STDIN>; use chomp($var1, $var2, ...) to get rid of '\\n' read from STDIN Read File open FILE , \"nlexample.txt\" or die $! ; my $lineno = 1 ; while ( <FILE> ) { # equivalent as \"while (defined ($_ = <FILE>)) {\" print $lineno ++ ; print \": $_\" ; } Another shortcut for read/process files my $lineno = 1 ; while ( <> ) { print $lineno ++ ; print \": $_\" ; } Then just run the script and pass filenames as command-line args, the files will automatically be read in and processed in the loop Further reading here https://docs.google.com/viewer?url=https%3A%2F%2Fblob.perl.org%2Fbooks%2Fbeginning-perl%2F3145_Chap06.pdf Conditions \u00b6 if Condition { do this ; } elsif Condition { do this ; } else { do this ; } # alternatively Condition ? do this : do this defined can be used to test whether a variable is defined Boolean evaluations Empty string \"\" is false Number zero 0 and string \"0\" are false Empty list () is false Undefined value is false Everything else is true Statement modifier form die \"Something bad happened\" if $error Another fashion of condition check for ( $choice ) { $_ == 1 && print \"You chose number one\\n\" ; $_ == 2 && print \"You chose number two\\n\" ; $_ == 3 && print \"You chose number three\\n\" ; ... } Loops \u00b6 For-each loop for list traversal. Note that the variable $element is a reference to the element itself, so changes can be made directly on it for $element ( @array ) { print $element , \"\\n\" ; } # can also do for ( @array ) { $_ *= 2 } # for each loop foreach $i ( @array ) { print \"Element: $i\\n\" ; } # while loop my $countdown = 5 ; while ( $countdown > 0 ) { print \"Counting down: $countdown\\n\" ; $countdown -- ; } do { ... } while ( $_ ) # loop until loops until ( < condition ) { ... } for loop creates an alias rather than a value . The alias is just an iterator direct referencing its value. Changes made on it will be reflected somewhere else where it is used. loops can be break out using last : last if $_ eq \"STOP THIS NOW\"; loops can go to next iteration using next labeling OUTER: while ( <STDIN> ) { chomp ; INNER: for my $check ( @getout ) { last OUTER if $check eq $_ ; } print \"Hey, you said $_\\n\" ; } Statement modifier form $total += $_ for @ARGV <statement> while <condition> Regex \u00b6 Pattern regex patterns are defined in between '/'s, like this /regex/ patterns support interpolation, so variable can be put inside '/'s like this /$pattern/ 'i' tells that pattern is \"case insensitive\", like this /regex/i special chars that need to be escaped: . * ? + [ ] ( ) { } ^ $ | \\ alternatively, use \\Q and \\E to set range that these chars are matched as is /\\Q$pattern\\E/ variable interpolation still in effect s/regex/regex_replace/ will do in-place replacement of matched string, once s/regex/regex_replace/g will do it as many times (global) change delimiters s#/usr/local/share/#/usr/share/#g; other modifiers /m \u2013 treat the string as multiple lines. Normally, ^ and $ match the very start and very end of the string. If the /m modifier is in play, then they will match the starts and ends of individual lines (separated by \\n). For example, given the string: \"one\\ntwo\", the pattern /^two$/ will not match, but /^two$/m will. /s \u2013 treat the string as a single line. Normally, . does not match a new line character; when /s is given, then it will. /x \u2013 allow the use of whitespace and comments inside a match. look ahead/behind /fish(?= cake)/ will match only if fish is followed by cake /fish(?! cake)/ does the opposite Other functions that uses regex split(regex, target) Transliteration \u00b6 Works a lot like regex, except it defines a rule of translating things from one to another. What this does is to correlate the characters in its two arguments, one by one, and use these pairings to substitute individual characters in the referenced string. $string =~ tr /0123456789/ abcdefghij / ; # would turn, say, \"2011064\" into \"cabbage\". my $vowels = $string =~ tr /aeiou/ / ; # would count the number of vowels in a string $string =~ tr / / / d ; # would remove the spaces from the $string Reference \u00b6 Always scalar but can give the data stored in an array or hash. It differs from pointers in the sense that, only store memory locations for specific, clearly defined data structures \u2013 maybe not predefined, but defined nevertheless. You create a reference by putting a backslash in front of the variable. my @array = ( 1 , 2 , 3 , 4 , 5 ); my $array_r = \\ @array ; my %hash = ( apple => \"pomme\" , pear => \"poire\" ); my $hash_r = \\ %hash ; my $scalar = 42 ; my $scalar_r = \\ $scalar ; my $a = 3 ; my $b = 4 ; my $c = 5 ; my @refs = ( \\ $a , \\ $b , \\ $c ); my @refs2 = \\ ( $a , $b , $c ); Anonymous References To get an array reference instead of an array, use square brackets [] instead of parentheses. To get a hash reference instead of a hash, use curly braces {} instead of parentheses. my $array_r = [ 1 , 2 , 3 , 4 , 5 ]; my $hash_r = { apple => \"pomme\" , pear => \"poire\" }; my %months = ( english => [ \"January\" , \"February\" , \"March\" , \"April\" , \",May\" , \",June\" ], french => [ \"Janvier\" , \"Fevrier\" , \"Mars\" , \"Avril\" , \"Mai\" , \"Juin\" ] ); my @array = ( 100 , 200 ,[ 2 , 4 ,[ 1 , 2 ,[ 10 , 20 , 30 , 40 , 50 ], 3 , 4 ], 6 , 8 ], 300 , 400 ); To dereference data, put the reference in curly braces wherever you would normally use a variable's name. my @array2 = @{$array_r}; %{$hash_r} ${$href}{$_} You don't have to write the curly brackets. for ( @$array_r ) { print \"An element: $_\\n\" ; } for ( keys %$href ) { print \"Key: \" , $_ , \" \" ; print \"Hash: \" , $hash { $_ }, \" \" ; print \"Ref: \" , $$href { $_ }, \" \" ; print \"\\n\" ; } Instead of ${$ref} , we can say $ref-> my @array = ( 68 , 101 , 114 , 111 , 117 ); my $ref = \\ @array ; $ref -> [ 0 ] = 100 ; # compare to ${$ref}[0] = 100; print \"Array is now : @array\\n\" ; Between sets of brackets, the arrow is optional. $ref = [ 1 , 2 , [ 10 , 20 ] ]; $element = { $ref -> [ 2 ]} -> [ 1 ]; $element = $ref -> [ 2 ][ 1 ]; Destroy/GC a reference using undef $ref or delete $addressbook{$who} Autovivification my $ref ; $ref -> { UK } -> { England } -> { Oxford } -> [ 1999 ] -> { Population } = 500000 ; my @chessboard ; $chessboard [ 0 ] -> [ 0 ] = \"WR\" ; perl will automatically know that we need $ref to be a hash reference. So, it'll make us a nice new anonymous hash, and another... We don't have to worry about creating all the entries ourselves. Subroutines (user-defined functions) \u00b6 Like C, perl requires subroutines to be defined or declared before using them. You can choose to define them before using them, or just declare them, use them, then defined them at the end of the file. declare subroutines using sub marine ; # alternatively, use this statement at the top of the program use subs qw(marine setup teardown) ; # call subroutine setup ; # then define it later sub marine { ... } Now pass arguments to subroutines and use them. Arguments are stored in @_ : total ( 1 ... 100 ); sub total { my $total = 0 ; $total += $_ for @_ ; print \"The total is $total\\n\" ; $total ; } Can set default arg value using this: my $message = shift || \"Something's wrong\"; Named Parameters logon ( username => $name , password => $pass , host => $hostname ); sub logon { die \"Parameters to logon should be even\" if @_ % 2 ; my %args = @_ ; print \"Logging on to host $args{hostname}\\n\" ; ... } Finally we can return a value. We can return a list or a hash instead of a scalar. To do so implicitly was easy, just make the value we want to return the last thing in our subroutine, like above. To return explicitly, use the keyword return . sub secs2hms { my ( $h , $m ); my $seconds = shift ; # uses @_ implicitly if nothing is passed. # uses @ARGV implicitly if outside a subroutine $h = int ( $seconds / ( 60 * 60 )); $seconds % = 60 * 60 ; $m = int ( $seconds / 60 ); $seconds % = 60 ; return ( $h , $m , $seconds ); print \"This statement is never reached.\" ; } Just like a built-in function, when we're expecting a subroutine to return a list, we can use an array or list of variables to collect the return values. my ($hours, $minutes, $seconds) = secs2hms(3723); Context-aware Subroutines The function wantarray tells whether the context was array or scalar. It returns true if it is in an array context. Use this if there is a need to return different values for different context. Subroutine Prototype Define how many arguments a subroutine needs to consume using $ , \\@ , or % . The number of $ s defines the number of expected arguments. You can also use an @_ to denote any number of arguments is ok. sub sum_of_two_squares ($$) { my ( $a , $b ) = ( shift , shift ); return $a ** 2 + $b ** 2 ; } References to Subroutines Usually we can use this mechanism to do callbacks. sub something { print \"Wibble!\\n\" } my $ref = \\& something ; # reference to an anonymous subroutine my $ref = sub { print \"Wibble!\\n\" } # calling reference to a subroutine & { $ref }; & { $ref }( @parameters ); & $ref ( @parameters ); $ref -> (); $ref -> ( @parameters ); Recursion in perl \u00b6 An example program that use BFS search to validate all internal links are valid: #!/usr/bin/perl # webchecker.plx use warnings ; use strict ; my %seen ; print \"Web Checker, version 1.\\n\" ; die \"Usage: $0 <starting point> <site base>\\n\" unless @ARGV == 2 ; my ( $start , $base ) = @ARGV ; $base .= \"/\" unless $base =~ m |/ $| ; die \"$start appears not to be in $base\\n\" unless in_our_site ( $start ); traverse ( $start ); sub traverse { my $url = shift ; $url =~ s | /$|/i ndex . html | ; return if $seen { $url } ++ ; # Break circular links my $page = get ( $url ); if ( $page ) { print \"Link OK : $url\\n\" ; } else { print \"Link dead : $url\\n\" ; return ; # Terminating condition : if dead. } return unless in_our_site ( $url ); # Terminating condition : if external. my @links = extract_links ( $page , $url ); return unless @links ; # Terminating condition : no links for my $link ( @links ) { traverse ( $link ) # Recurse } } sub in_our_site { my $url = shift ; return index ( $url , $base ) == 0 ; } sub get { my $what = shift ; sleep 5 ; # Be friendly return `lynx -source $what` ; } sub extract_links { my ( $page , $url ) = @_ ; my $dir = $url ; my @links ; $dir =~ s | ( .* ) /.* ? $| $1 | ; for ( @links = ( $page =~ /<A HREF=[\"']?([^\\s\"'>]+)[\"']?/gi )) { $_ = $base . $_ if s |^/|| ; $_ = $dir . \"/\" . $_ if ! /^(ht|f)tp:/ ; } return @links ; } Modules \u00b6 Declare a package using package Wibble; at the top of the file. Three ways to import another package: do , require , and use do will look for a file by searching the @INC path (default contents of the search path). If the file can't be found, it'll silently move on. If it is found, it will run the file just as if it was placed in a block within our main program \u2013 but with one slight difference: we won't be able to see lexical variables from the main program once we're inside the additional code. require is like do, but it'll only do once. It'll record the fact that a file has been loaded and will ignore further requests to require it again. require Wibble ; # look for a file called Wibble.pm in the @INC path require Monty::Python ; # look for a file in directory Monty and a file Python.pm in @INC path use The way we normally use modules. This is like require, except that perl applies it before anything else in the program starts. If Perl sees a use statement anywhere in your program, it'll include that module. use takes place at compile time and not at run time. # both packages will be included if ( $graphical ) { use MyProgram::Graphical ; } else { use MyProgram::Text ; } Import particular subroutines and variables: use Wibble (\"wobble\", \"bounce\", \"boing\"); # if ever need to limit what can be imported by another package use Exporter ; our @ISA = qw(Exporter) ; our @EXPORT_OK = qw(wobble bounce boing) ; our @EXPORT = qw(bounce) # default imports sub wobble { print \"wobble\\n\" } sub bounce { warn \"bounce\\n\" } sub boing { die \"boing!\\n\" } You can always directly address the subroutine without importing it: Wibble::boing() Change \\@INC # BEGIN subroutine will always run at compile time sub BEGIN { push @INC , \"my/module/directory\" ; } Perl Standard Modules See page https://docs.google.com/viewer?url=https%3A%2F%2Fblob.perl.org%2Fbooks%2Fbeginning-perl%2F3145_Chap10.pdf Find more moduels from CPAN, the Comprehensive Perl Archive Network, http://www.cpan.org. command in shell \u00b6 To execute a command in shell, use system($command) function. It forks a child process, and then waits for the child process to terminate. The value 0 is returned if the command succeeds and the value 1 is returned if the command fails.","title":"Perl Language Reference"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#dcoumentation","text":"like shell, comments starts with a '#'","title":"Dcoumentation"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#debug","text":"perl -de1 perl -MCPAN -e shell perl -c file_to_compile","title":"Debug"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#type-of-data","text":"Numbers Integers Base-10: 1, -4, 255, 25_000_000 ... Binary: 0b11111 Octal: 0123, 055 Hexadecimal: 0x4AF Floating-point numbers - 0.5, -0.0133, ... Strings no processing: 'simple raw string' string evaluated as-is with interpolation: \"Hello world\\n\" Alternative Delimiters Instead of bounding strings with ' or \", can define you own delimiters for strings use qq followed by an arbitrary non-alphanumeric character: print qq/'\"Hi,\" said Jack. \"Have you read Slashdot today?\"'\\n/; use q// works too Here-Documents, another way to specify a string, start with << and then a label . print << EOF ; This is a here-document. It starts on the line after the two arrows, and it ends when the text following the arrows is found at the beginning of a line, like this: EOF Numbers<->Strings - perl converts between strings, integers, and floating-point numbers behind the scenes automatically if necessary. - \"12\" > \"30\" yields false - string contains no digit will be evaluated as 0 in Integer, if evaluated in an arithmetic expression - Special function can be used on converting binary, octal, or Hexadecimal string into Integers: - hex(\"0x30\"), oct(\"030\"), oct(\"0b11010\"), oct(\"0x35AB\") Operators Arithmetic Operators: + - * / % **(power function) ++ -- Logical Operators: & | ~(NOT) ^ Bit-wise Operators: << >> Comparison Operators: == != > < >= <= <=>(like java's compareTo, returns -1, 0, 1 i.e. 6<=>9 yields -1) Boolean Operators: && || ! and or xor not String Operators: .(for concatenation) x[0-9]+(repetes previous string N times) String Compare Operators: lt gt ge le eq ne cmp(like java's compareTo, returns -1, 0, 1) ord(takes first character of input string, returns its ASCII) Range Operators: ... and ... Build-list Operator: ..., ..., ... Regex Operator: =~ !~ Pointer Operator: -> \\ Variables types Scalars $name , holds Numbers, Strings, limited by the size of your computer's memory Lists @list , holds Numbers, Strings, variables (42, 39) (\"cheese\", \"cake\") (42, 1.5, \"lalala\", $test) qw/one two three four/ will yield ('one', 'two', 'three', 'four') '/' can be replaced with other special chars; spaces can be replaced with tabs, new lines, or any number of white spaces Note that lists inside lists will be flattened to level one list list element can be accessed with [N]; negative index rewinds from the end of the list nit: use $a = $array[0] instead of $a = (@array)[0] prime rule is this: the prefix represents what you want to get, not what you've got. [N] N can be a list, or a range of numbers to access corresponding elements Swap elements in-place: @months[3,4] = @months[4,3] which isn't far from swapping variables using list assignment ($mone, $mtwo) = ($mtwo, $mone) list slicing: [(index1, index2)] will return a new list containing elements in those indexes from the old list Ranges: (1 .. 6) will yield a list of 1-6 ('a' .. 'z') will yield a list of a-z array functions change elements: push pop shift(taken from index 0) unshift(adding to index 0) other: reverse sort sort is based on alphabetic orders by default. If sorting numbers or others things where special rule is required, can pass a compareTo func: my @string_sorted = sort { $a cmp $b } @unsorted; my @number_sorted = sort { $a <=> $b } @unsorted; special variable: $#array gives the highest element index in @array this made it okay to use for (0..$#array) for traversal using $_ as index note that this value is 1 less than the value from salar @array , which gives the size of the array Hashes %hash can be created by: List with key value pairs separated by commas. %where = ( \"Gary\" , \"Dallas\" , \"Lucy\" , \"Exeter\" ); List with key value pairs more readable: %where = ( Gary => \"Dallas\" , Lucy => Exeter ); From an array: @array = qw(Gary Dallas Lucy Exeter Ian Reading Samantha Oregon) ; %where = @array ; access/set values with $where{Key} keys %where gives a list of keys in this hash exists can be used to test whether a key exists in a hash if (not exists $rates{$key}) Variable name must begin with alphabetic character or underscore, then can be followed by numbers, letters, underscores Note that $var @var %var are different variables Scalar vs. List Context: if a variable is evaluated in different context, its return value will be different Can force scalar context using scalar operator on another variable, like this print scalar @array print @array ; # list context, returns list elements $scalar = @array ; # scalar context, returns list size Variable scopes Variables declared within blocks {} are lexical (local) variables; otherwise are global. Locla variable overrides global variable value at evaluation, while it does not modify global variable (with the same variable name). $global_var { my $local_var my ( $var1 , $var2 , $var3 ) # () is needed if declaring many at a time } With use strict; set, have to declare global variable with our $global_var Special Variables $_ the default variable that functions read from/write to $! a way of getting various things Perl want to give, like error message <> an abbreviation for <ARGV> $/ defines your own line separator for I/O purposes note that the $/ being set as \"\" will make reading chunk as paragraph instead of lines @_ stores arguments passed into a subroutine Variable interpolation print \"My name is $name\\n\"; print \"This is the ${times}th time.\\n\"; print \"@array\" will add spaces between its elements $scalar = \"@array\" achieve the same thing above, while storing into a string","title":"Type of Data"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#io","text":"Printing call print(RawStr...) is implicitly print(STDOUT, RawStr...) can print to error with print STDERROR RawStr expressions evaluated as false will be printing nothing; otherwise printing 1 die can be used to print error message and exit out current project. @array = ( 4 , 6 , 3 , 9 ) print @array , \"\\n\" # yields 4639 print \"@array\\n\" yields 4 6 3 9 Reads input my $cash = <STDIN>; use chomp($var1, $var2, ...) to get rid of '\\n' read from STDIN Read File open FILE , \"nlexample.txt\" or die $! ; my $lineno = 1 ; while ( <FILE> ) { # equivalent as \"while (defined ($_ = <FILE>)) {\" print $lineno ++ ; print \": $_\" ; } Another shortcut for read/process files my $lineno = 1 ; while ( <> ) { print $lineno ++ ; print \": $_\" ; } Then just run the script and pass filenames as command-line args, the files will automatically be read in and processed in the loop Further reading here https://docs.google.com/viewer?url=https%3A%2F%2Fblob.perl.org%2Fbooks%2Fbeginning-perl%2F3145_Chap06.pdf","title":"I/O"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#conditions","text":"if Condition { do this ; } elsif Condition { do this ; } else { do this ; } # alternatively Condition ? do this : do this defined can be used to test whether a variable is defined Boolean evaluations Empty string \"\" is false Number zero 0 and string \"0\" are false Empty list () is false Undefined value is false Everything else is true Statement modifier form die \"Something bad happened\" if $error Another fashion of condition check for ( $choice ) { $_ == 1 && print \"You chose number one\\n\" ; $_ == 2 && print \"You chose number two\\n\" ; $_ == 3 && print \"You chose number three\\n\" ; ... }","title":"Conditions"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#loops","text":"For-each loop for list traversal. Note that the variable $element is a reference to the element itself, so changes can be made directly on it for $element ( @array ) { print $element , \"\\n\" ; } # can also do for ( @array ) { $_ *= 2 } # for each loop foreach $i ( @array ) { print \"Element: $i\\n\" ; } # while loop my $countdown = 5 ; while ( $countdown > 0 ) { print \"Counting down: $countdown\\n\" ; $countdown -- ; } do { ... } while ( $_ ) # loop until loops until ( < condition ) { ... } for loop creates an alias rather than a value . The alias is just an iterator direct referencing its value. Changes made on it will be reflected somewhere else where it is used. loops can be break out using last : last if $_ eq \"STOP THIS NOW\"; loops can go to next iteration using next labeling OUTER: while ( <STDIN> ) { chomp ; INNER: for my $check ( @getout ) { last OUTER if $check eq $_ ; } print \"Hey, you said $_\\n\" ; } Statement modifier form $total += $_ for @ARGV <statement> while <condition>","title":"Loops"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#regex","text":"Pattern regex patterns are defined in between '/'s, like this /regex/ patterns support interpolation, so variable can be put inside '/'s like this /$pattern/ 'i' tells that pattern is \"case insensitive\", like this /regex/i special chars that need to be escaped: . * ? + [ ] ( ) { } ^ $ | \\ alternatively, use \\Q and \\E to set range that these chars are matched as is /\\Q$pattern\\E/ variable interpolation still in effect s/regex/regex_replace/ will do in-place replacement of matched string, once s/regex/regex_replace/g will do it as many times (global) change delimiters s#/usr/local/share/#/usr/share/#g; other modifiers /m \u2013 treat the string as multiple lines. Normally, ^ and $ match the very start and very end of the string. If the /m modifier is in play, then they will match the starts and ends of individual lines (separated by \\n). For example, given the string: \"one\\ntwo\", the pattern /^two$/ will not match, but /^two$/m will. /s \u2013 treat the string as a single line. Normally, . does not match a new line character; when /s is given, then it will. /x \u2013 allow the use of whitespace and comments inside a match. look ahead/behind /fish(?= cake)/ will match only if fish is followed by cake /fish(?! cake)/ does the opposite Other functions that uses regex split(regex, target)","title":"Regex"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#transliteration","text":"Works a lot like regex, except it defines a rule of translating things from one to another. What this does is to correlate the characters in its two arguments, one by one, and use these pairings to substitute individual characters in the referenced string. $string =~ tr /0123456789/ abcdefghij / ; # would turn, say, \"2011064\" into \"cabbage\". my $vowels = $string =~ tr /aeiou/ / ; # would count the number of vowels in a string $string =~ tr / / / d ; # would remove the spaces from the $string","title":"Transliteration"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#reference","text":"Always scalar but can give the data stored in an array or hash. It differs from pointers in the sense that, only store memory locations for specific, clearly defined data structures \u2013 maybe not predefined, but defined nevertheless. You create a reference by putting a backslash in front of the variable. my @array = ( 1 , 2 , 3 , 4 , 5 ); my $array_r = \\ @array ; my %hash = ( apple => \"pomme\" , pear => \"poire\" ); my $hash_r = \\ %hash ; my $scalar = 42 ; my $scalar_r = \\ $scalar ; my $a = 3 ; my $b = 4 ; my $c = 5 ; my @refs = ( \\ $a , \\ $b , \\ $c ); my @refs2 = \\ ( $a , $b , $c ); Anonymous References To get an array reference instead of an array, use square brackets [] instead of parentheses. To get a hash reference instead of a hash, use curly braces {} instead of parentheses. my $array_r = [ 1 , 2 , 3 , 4 , 5 ]; my $hash_r = { apple => \"pomme\" , pear => \"poire\" }; my %months = ( english => [ \"January\" , \"February\" , \"March\" , \"April\" , \",May\" , \",June\" ], french => [ \"Janvier\" , \"Fevrier\" , \"Mars\" , \"Avril\" , \"Mai\" , \"Juin\" ] ); my @array = ( 100 , 200 ,[ 2 , 4 ,[ 1 , 2 ,[ 10 , 20 , 30 , 40 , 50 ], 3 , 4 ], 6 , 8 ], 300 , 400 ); To dereference data, put the reference in curly braces wherever you would normally use a variable's name. my @array2 = @{$array_r}; %{$hash_r} ${$href}{$_} You don't have to write the curly brackets. for ( @$array_r ) { print \"An element: $_\\n\" ; } for ( keys %$href ) { print \"Key: \" , $_ , \" \" ; print \"Hash: \" , $hash { $_ }, \" \" ; print \"Ref: \" , $$href { $_ }, \" \" ; print \"\\n\" ; } Instead of ${$ref} , we can say $ref-> my @array = ( 68 , 101 , 114 , 111 , 117 ); my $ref = \\ @array ; $ref -> [ 0 ] = 100 ; # compare to ${$ref}[0] = 100; print \"Array is now : @array\\n\" ; Between sets of brackets, the arrow is optional. $ref = [ 1 , 2 , [ 10 , 20 ] ]; $element = { $ref -> [ 2 ]} -> [ 1 ]; $element = $ref -> [ 2 ][ 1 ]; Destroy/GC a reference using undef $ref or delete $addressbook{$who} Autovivification my $ref ; $ref -> { UK } -> { England } -> { Oxford } -> [ 1999 ] -> { Population } = 500000 ; my @chessboard ; $chessboard [ 0 ] -> [ 0 ] = \"WR\" ; perl will automatically know that we need $ref to be a hash reference. So, it'll make us a nice new anonymous hash, and another... We don't have to worry about creating all the entries ourselves.","title":"Reference"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#subroutines-user-defined-functions","text":"Like C, perl requires subroutines to be defined or declared before using them. You can choose to define them before using them, or just declare them, use them, then defined them at the end of the file. declare subroutines using sub marine ; # alternatively, use this statement at the top of the program use subs qw(marine setup teardown) ; # call subroutine setup ; # then define it later sub marine { ... } Now pass arguments to subroutines and use them. Arguments are stored in @_ : total ( 1 ... 100 ); sub total { my $total = 0 ; $total += $_ for @_ ; print \"The total is $total\\n\" ; $total ; } Can set default arg value using this: my $message = shift || \"Something's wrong\"; Named Parameters logon ( username => $name , password => $pass , host => $hostname ); sub logon { die \"Parameters to logon should be even\" if @_ % 2 ; my %args = @_ ; print \"Logging on to host $args{hostname}\\n\" ; ... } Finally we can return a value. We can return a list or a hash instead of a scalar. To do so implicitly was easy, just make the value we want to return the last thing in our subroutine, like above. To return explicitly, use the keyword return . sub secs2hms { my ( $h , $m ); my $seconds = shift ; # uses @_ implicitly if nothing is passed. # uses @ARGV implicitly if outside a subroutine $h = int ( $seconds / ( 60 * 60 )); $seconds % = 60 * 60 ; $m = int ( $seconds / 60 ); $seconds % = 60 ; return ( $h , $m , $seconds ); print \"This statement is never reached.\" ; } Just like a built-in function, when we're expecting a subroutine to return a list, we can use an array or list of variables to collect the return values. my ($hours, $minutes, $seconds) = secs2hms(3723); Context-aware Subroutines The function wantarray tells whether the context was array or scalar. It returns true if it is in an array context. Use this if there is a need to return different values for different context. Subroutine Prototype Define how many arguments a subroutine needs to consume using $ , \\@ , or % . The number of $ s defines the number of expected arguments. You can also use an @_ to denote any number of arguments is ok. sub sum_of_two_squares ($$) { my ( $a , $b ) = ( shift , shift ); return $a ** 2 + $b ** 2 ; } References to Subroutines Usually we can use this mechanism to do callbacks. sub something { print \"Wibble!\\n\" } my $ref = \\& something ; # reference to an anonymous subroutine my $ref = sub { print \"Wibble!\\n\" } # calling reference to a subroutine & { $ref }; & { $ref }( @parameters ); & $ref ( @parameters ); $ref -> (); $ref -> ( @parameters );","title":"Subroutines (user-defined functions)"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#recursion-in-perl","text":"An example program that use BFS search to validate all internal links are valid: #!/usr/bin/perl # webchecker.plx use warnings ; use strict ; my %seen ; print \"Web Checker, version 1.\\n\" ; die \"Usage: $0 <starting point> <site base>\\n\" unless @ARGV == 2 ; my ( $start , $base ) = @ARGV ; $base .= \"/\" unless $base =~ m |/ $| ; die \"$start appears not to be in $base\\n\" unless in_our_site ( $start ); traverse ( $start ); sub traverse { my $url = shift ; $url =~ s | /$|/i ndex . html | ; return if $seen { $url } ++ ; # Break circular links my $page = get ( $url ); if ( $page ) { print \"Link OK : $url\\n\" ; } else { print \"Link dead : $url\\n\" ; return ; # Terminating condition : if dead. } return unless in_our_site ( $url ); # Terminating condition : if external. my @links = extract_links ( $page , $url ); return unless @links ; # Terminating condition : no links for my $link ( @links ) { traverse ( $link ) # Recurse } } sub in_our_site { my $url = shift ; return index ( $url , $base ) == 0 ; } sub get { my $what = shift ; sleep 5 ; # Be friendly return `lynx -source $what` ; } sub extract_links { my ( $page , $url ) = @_ ; my $dir = $url ; my @links ; $dir =~ s | ( .* ) /.* ? $| $1 | ; for ( @links = ( $page =~ /<A HREF=[\"']?([^\\s\"'>]+)[\"']?/gi )) { $_ = $base . $_ if s |^/|| ; $_ = $dir . \"/\" . $_ if ! /^(ht|f)tp:/ ; } return @links ; }","title":"Recursion in perl"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#modules","text":"Declare a package using package Wibble; at the top of the file. Three ways to import another package: do , require , and use do will look for a file by searching the @INC path (default contents of the search path). If the file can't be found, it'll silently move on. If it is found, it will run the file just as if it was placed in a block within our main program \u2013 but with one slight difference: we won't be able to see lexical variables from the main program once we're inside the additional code. require is like do, but it'll only do once. It'll record the fact that a file has been loaded and will ignore further requests to require it again. require Wibble ; # look for a file called Wibble.pm in the @INC path require Monty::Python ; # look for a file in directory Monty and a file Python.pm in @INC path use The way we normally use modules. This is like require, except that perl applies it before anything else in the program starts. If Perl sees a use statement anywhere in your program, it'll include that module. use takes place at compile time and not at run time. # both packages will be included if ( $graphical ) { use MyProgram::Graphical ; } else { use MyProgram::Text ; } Import particular subroutines and variables: use Wibble (\"wobble\", \"bounce\", \"boing\"); # if ever need to limit what can be imported by another package use Exporter ; our @ISA = qw(Exporter) ; our @EXPORT_OK = qw(wobble bounce boing) ; our @EXPORT = qw(bounce) # default imports sub wobble { print \"wobble\\n\" } sub bounce { warn \"bounce\\n\" } sub boing { die \"boing!\\n\" } You can always directly address the subroutine without importing it: Wibble::boing() Change \\@INC # BEGIN subroutine will always run at compile time sub BEGIN { push @INC , \"my/module/directory\" ; } Perl Standard Modules See page https://docs.google.com/viewer?url=https%3A%2F%2Fblob.perl.org%2Fbooks%2Fbeginning-perl%2F3145_Chap10.pdf Find more moduels from CPAN, the Comprehensive Perl Archive Network, http://www.cpan.org.","title":"Modules"},{"location":"Programming-Lang-Reference/Perl/Perl-Lang-Specs/#command-in-shell","text":"To execute a command in shell, use system($command) function. It forks a child process, and then waits for the child process to terminate. The value 0 is returned if the command succeeds and the value 1 is returned if the command fails.","title":"command in shell"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/","text":"Big thanks to William Shotts Jr. from linuxcommand.org for this great educational resource. Here are my reading notes taken. Getting to Know Shell Script \u00b6 There is no way to remember every command and every option associated with it. The real power is to know how to quickly learn from man page and write scripts to do what you need to do. #!/bin/bash # A line of comment echo \"Hello World!\" The first line is called a shebang , indicating what program is used to interpret the script. Other scripting languages such as Perl, awk, tcl, Tk, and python also use this mechanism. After giving a file execution permission, it can be run using ./script_name PATH is where Linux scans for commands and executable scripts If a directory need to be added to the PATH var, do export PATH=$PATH:directory . It is better to add this line to the .bash_profile or .profile file in your home directory. Linux encourage each user to have a specific directory for programs the user personally uses, bin in the user's home directory. This is automatically included in the $PATH var. shell (environment) variables \u00b6 Linux is a multi-user system, each person logged-in will get a bash to use. Each bash will have a set of environment variables to make sure users workspace clean. To set a shell variable , a few things: var_name=value no spaces on two sides of '=' var_name can't start with number; var_name contains only alphanumeric chars if var's value contains spaces, use \"\" or '' to wrap it up; variables can be evaluated within \"\" , but not '' use escape char \\ to include special chars in variables created within \"\" . can assign variable value from command's output using $( <shell_expression> ) use export var_name to make the variable an environment variable usually capitalized VAR_NAME is an environment variable use unset to cancel/remove a variable use env to see a list of all environment vars $RANDOM is a special env var that gives a random number between 0-32767 every time used. use set to see a list of user defined vars $PS1 is the command character as well as anything before it! It can be customized to display more info at the beginning of each line of command in terminal. see Linux Terminal Tricks notes $$ itself is a variable showing current shell's PID. sub-program of bash can only inherit the parent's environment vars, not regular vars export normal vars as env vars if necessary read command allows user to enter something and store it as a variable. Useful for script requires user input. declare or typeset both can set the variable type of a variable -a : array -i : integer -x : environment variable -r : readonly variable +x : cancel env var setting; '+' means cancel here... i.e. declare -i sum=100+300+50 Modify variable content expression - - purpose ${variable#key_word} from begin to end, find shortest match and delete key_word ${variable##key_word} from begin to end, find longest match and delete key_word ${variable%key_word} from end to beginning, find shortest match and delete key_word ${variable%%key_word} from end to beginning, find longest match and delete key_word ${variable/key_word/new_word} find one match of key_word and replace with new_word ${variable//key_word/new_word} find all matches of key_word and replace with new_word Variable evaluation from another variable expression - - str not set - - str is empty - - str set and non-empty var=${str-expr} $var=expr $var= $var=$str var=${str:-expr} $var=expr $var=expr $var=$str var=${str+expr} $var= $var=expr $var=expr var=${str:+expr} $var= $var= $var=expr var=${str=expr} $str=expr, $var=expr $str unchanged, $var= $str unchanged, $var=$str var=${str:=expr} $str=expr, $var=expr $str=expr, $var=expr $str unchanged, $var=$str var=${str?expr} prints expr to stderr $var= $var=$str var=${str:?expr} prints expr to stderr prints expr to stderr $var=$str use ${#Var_Name} to get the string length of a variable. array/list variable can be created using syntax of VARS=() - initialize with values VARS=(a b c) - change index value with VARS[0]=\"values\" - access array indexes with ${VARS[0]} - get a string of all items of an array variable with ${VARS[@]} - get array length with ${#VARS[@]} - indexes start at 0. No need to specify initial capacity. use source to load configuration from a file immediately Filtering Commands \u00b6 Take std input and perform operations upon it and send results to std output Command - - What it does sort sorts std input then outputs sorted result uniq given a stream of data, removes duplicate lines grep examines each line receives, outputs lines containing specified patterns fmt reads text from input and outputs formatted text pr takes text input and splits data into pages with page breaks, headers and footers in preparation for printing head outputs first few lines of input tail outputs last few lines of input tr translate characters, such as upper/lowercase conversions, changing line termination chars. sed stream editor, perform more sophisticated text translations than tr . supports regex awk a programming lang designed for constructing filters Example: Printing from the command line cat report.txt | sort | uniq | fmt | pr | lpr cat is used to concatenate files and gives to output. here can be used to put single file content to std output fmt formats text into neat paragraphs pr splits text neatly into pages lpr sends std input to printer use of - use - can replace stdin and stdout without creating files. i.e. tar -cvf - /home | tar -xvf - will pack the files and pass it to next command without writing to a file. Expansion \u00b6 expansion is when your command typed-in expanded into something else before shell acts upon it. Pathname Expansion : we can use echo D* to show files starting with 'D' in the current directory. Tilde Expansion ~ char has special meanings: at beginning of a word, it expands to the name of the home directory of the named user. at the end of a filename, it means this file is a temporary backup if used alone it refers to the home directory of current user Arithmetic Expansion : the use of $((expression)) allows arithmetic evaluation inside the $() syntax if shell variables are used, their values must be numbers + - * / % **(exponential function) are supported by shell script. Brace Expansion create multiple text strings from a pattern containing braces i.e. echo Front-{A,B,C}-Back i.e. echo Front-{1..5}-Back Common good application is to make lists of files or directories to be created i.e. mkdir {2007..2009}-0{1..9} {2007..2009}-{10..12} Parameter Expansion make use of the system's ability to store small chunks of data and give each chunk a name (shell variables). printenv gives all available variables Command Substitution allows us to use the output of a command as an expansion, like this: echo $(ls) , ls -l $(which cp) alternatively, can also use back-quotes to do the same thing: ls -l `which cp` Quoting some special characters may need escapes using \\ or simply using double quotes \"\" to include the parts having special characters speical characters quoted within '' don't need to be escaped Here is a comparison of three levels of expansion suppressions echo text ~/*.txt { a,b } $( echo foo ) $(( 2 + 2 )) $USER # text /home/me/ls-output.txt a b foo 4 me echo \"text ~/*.txt {a,b} $( echo foo ) $(( 2 + 2 )) $USER \" # text ~/*.txt {a,b} foo 4 me echo 'text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER' # text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER use backslash also to start a new line for a single line of command. Like this: ls -l \\ --reverse \\ --human-readable \\ --full-time Sometimes use the long version of the options help you read and know its purpose instantly. Permissions \u00b6 Linux is a multitasking and multi-user system. chmod - modify file access rights rwx = 111, rw- = 110, r-x = 101, r-- 4, ... chmod 600 some_file makes the file only read/writable for the user, strips the rights for group and global for directory, x allows directory to be entered. r/w is for its contents to be listed/modified su - temporarily become the superuser sudo - temporarily become the superuser chown - change file ownership chown new_owner_username file superuser or owner can do this chgrp - change a file's group ownership chgrp new_group_name file only owner can do this Frequently used file permission settings Value - - Meaning 777 (rwxrwxrwx) No restriction on permission. 755 (rwxr-xr-x) Owner may read/write/exec, no one else may write the file but only read/exec 700 (rwx------) Only Owner has all access 666 (rw-rw-rw-) All users may read/write, not exec 644 (rw-r--r--) Owner may read/write, others can only read 600 (rw-------) Owner read/write only Job Control \u00b6 commands used to control processes. ps - list the processes running on the system kill - send a terminate signal to one or more processes (usually to \"kill\" a process) jobs - an alternate way of listing your own processes bg - put a process in the background fg - put a process in the forground Some commond kill signals Signal# - - Name - - Description 1 SIGHUP Hang up signal. Programs can listen for this signal and act upon it. This signal is sent to processes running in a terminal when you close the terminal. 2 SIGINT Interrupt signal. This signal is given to processes to interrupt them. Programs can process this signal and act upon it. You can also issue this signal directly by typing Ctrl-c in the terminal window where the program is running. 15 SIGTERM Termination signal. This signal is given to processes to terminate them. Again, programs can process this signal and act upon it. This is the default signal sent by the kill command if no signal is specified. 9 SIGKILL Kill signal. This signal causes the immediate termination of the process by the Linux kernel. Programs cannot listen for this signal. Editing Shell Scripts \u00b6 environment The Linux environment contains your path, your user name, etc. A complete list of the environment entries can be viewed using the command set . Two types of commands are often contained in the environment: aliases and shell functions . Login shells read one or more startup files: File - - Contents /etc/profile A global configuration script that applies to all users. ~/.bash_profile A user's personal startup file. Can be used to extend or override settings in the global configuration script. ~/.bash_login If ~/.bash_profile is not found, bash attempts to read this script. ~/.profile If neither ~/.bash_profile nor ~/.bash_login is found, bash attempts to read this file. Non-login shell sessions read the following: File - - Contents /etc/bash.bashrc A global configuration script that applies to all users. ~/.bashrc A user's personal startup file. Can be used to extend or override settings in the global configuration script. Creating alias for longer commands can be set like this alias today='date +\"%A, %B %-d, %Y\"' Creating shell functions \u00b6 inside a shell script, functions can be invoked by just using its name, or using $(func_name) shell commands can be called within functions simply with the command name. functions must be defined before using them. today () { echo -n \"Today's date is: \" date + \"%A, %B %-d, %Y\" } # same as function today { echo -n \"Today's date is: \" date + \"%A, %B %-d, %Y\" } A function local variables can be declared by appending local before it, like this: local argc=0 Here Script \u00b6 A here script (a.k.a. here document ) is an additional form of I/O redirection . It provides a way to include content that will be given to the standard input of a command. command << token content to be used as the command's standard input more lines here... until token token can be any string of characters, by convention is EOF Another trick to have the here script ignoring the leading tabs (not spaces) is by writing it this way: command <<- token line1 line2 ... token Variables \u00b6 define and use a variable: var_name = \"value\" # to define $var_name # to use it Rules for variables: names must start with a letter names contain no embedded spaces; use underscores instead names cannot use punctuation marks Environment Variables use printenv to view all environment variables loaded by shell These environment variables can be accessed directly in the shell script running within current shell session. Environment variables names are uppercase by convention. Inline commands we can have shell substitute the results of a command to existing script by using: $(command_here) can also assign the result of a command to a variable: var_1=$(command_here) variables inside $(()) don't need a '$' Variables can be directly replaced with its value inside \"\" quoted strings, like so \"Today's date: $date\" Control flow \u00b6 An if/else block if ( condition_expression ) ; then commands... elif ( condition_expression ) ; then commands... else commands... fi An switch/case block case $character in 1 ) echo \"character is: 1\" ;; 2 ) echo \"character is: 2\" ;; 3 ) echo \"character is: 3\" ;; 4 | 5 | 6 ) echo \"character matched one of: 4 5 6\" ;; * ) echo \"character matched to be anything else\" ;; esac Loops: while, until, and for An while block number = 0 while [ \" $number \" -lt 10 ] ; do echo \"Number = $number \" number = $(( number + 1 )) done An until block does exact the same thing as the while example number = 0 until [ \" $number \" -ge 10 ] ; do echo \"Number = $number \" number = $(( number + 1 )) done An for block assigns a word form a list of words to the specified variable, executes the commands, repeats until all words are exhausted. for variable in words ; do commands done for treats a string of text as a list of words by breaking by space chars count = 0 for i in $( cat ~/.bash_profile ) ; do count = $(( count + 1 )) echo \"Word $count ( $i ) contains $( echo -n $i | wc -c ) characters\" done Exit Status : a zero exit-code indicates success by convention. Exit status can be examined by variable $? after running a program/script. Shell provides true and false commands that do nothing except terminate with either a zero or one exit status exit command causes the script to terminate immediately and set the exit status to a value, like this exit 5 test/assertion test command is used most often as the condition check command. If the given expression is true, test exits with status 0; otherwise it exits with status 1. test expression # is the same as [ expression ] # spaces are required!! # or [[ expression ]] # spaces are required!! also use this everywhere, it is better than [ ] Most cases test can be used as a shortcut for if , like this test -e path_to_file && echo \"exist\" || echo \"not exist\" In the if statement, a exit code of 0 evaluated as 'true' while other exit codes are evaluated as 'false' if [ -f .bash_profile ] ; then echo \"You have a .bash_profile. Things are fine.\" else echo \"Yikes! You have no .bash_profile!\" fi ; the semicolon allows command statements to appear on the same line. A full list of test options option - - meaning -e exists? -f is a file ? -d is a directory ? -b is a block device? -c is a character device? -S is a Socket file? -P is a FIFO (pipe) file? -L is a link? -r read privilege ? -w write privilege ? -x execute privilege ? -u having SUID property? -g having SGID property? -k having sticky bit? -s exists and non empty ? $file1 -nt $file2 file1 newer than file2? $file1 -ot $file2 file1 older than file2? $file1 -ef $file2 file1 the same file as file2? check whether they point to the same inode $n1 -eq $n2 strings equal? $n1 -ne $n2 strings not equal? $n1 -gt $n2 greater than? $n1 -lt $n2 less than? $n1 -ge $n2 greater than or equal? $n1 -le $n2 less than or equal? -z string whether string is empty? -n string whether string is not empty? test $var1 == $var2 var1 is the same as var2? test $var1 != $var2 var1 is not the same as var2? -a and , same as && -o or , same as || ! not I/O Redirection \u00b6 redirect output of commands to files, devices, and input of other commands. Std I/O ls > file.txt write output to file.txt, create if not exist ls >> file.txt append output to file.txt, create if not exist sort < file.txt redirect file.txt contents to command sort as input ls -l | less piping output of ls to program less as input Whenever a new program is run on the system, the kernel creates a table of file descriptors for the program to use. File descriptors are pointers to files. By convention, descriptors 0 (STDIN) , 1 (STDOUT) , and 2 (STDERR) are available. Initially, all three descriptors point to the terminal device (which the system treats as a read/write file) Redirection is the process of manipulating the file descriptors so that input and output can be routed from/to different files. the shell assumes we want to redirect standard output if the file descriptor is omitted. command > file command 1 > file # the above two are equivalent, so as the following two lines command < file command 0 < file Duplicating File Descriptors i.e. command 1> file 2>&1 to achieve sending both STDOUT and STDERR to the file first redirects STDOUT to 1, which is to the file then redirects STDERR to 1 (the file ) Create additional File Descriptors exec 3 > some_file.txt # Open new file descriptor 3 command 1 > & 3 # redirect STDOUT to file descriptor 3 exec 3 > & - # Close file descriptor 3 Debug Mode Simply include #!/bin/bash -x at the beginning of the file. Alternatively, use set -x to turn tracing on and set +x to turn tracing off. Take user inputs \u00b6 read command takes input from the keyboard if -s option is passed, user's typing will not be displayed if -t option is passes, user has only specified seconds to enter the information. read text will store user's input into variable text read -p \"prompt\\t\" var will print a prompt before asking Positional parameters \u00b6 these are special variables $0 through $9 that contain the contents of the command line arguments $? contains the exit status of last executed command/script $# contains the number of items on the command line in addition to the name of the command $0 shift is a shell built-in that operates on the positional parameters. Each time calling shift , the arguments $2 becomes $1 , $3 becomes $2 , so on. shift can follow a number, shift 3 means move and discard 3 arguments while [ \" $1 \" -ne \"\" ] ; do echo \"Parameter 1 equals $1 \" echo \"now have $# positional parameters\" shift done Error Handling \u00b6 One trivial way is to check $? for status of last command alternatively, can directly check the command's exit code in the if [ command ]; then error_exit () { echo \" $1 \" 1 > & 2 exit 1 } if cd $some_directory ; then rm * else error_exit \"Cannot change directory! Aborting.\" fi here $1 in error_exit() function is the first argument By using && or || can simplify the above example to the following: cd $some_directory || error_exit \"Cannot change directory! Aborting\" rm * # or simply cd $some_directory && rm * A more slicker example of error_exit function: PROGNAME = $( basename $0 ) error_exit () { echo \" ${ PROGNAME } : ${ 1 :- \"Unknown Error\" } \" 1 > & 2 exit 1 } As shown before, the 1:- means if variable $1 is undefined, a default value \"Unknown Error\" is used. Signals and Traps \u00b6 Errors are not the only way a script can terminate unexpectedly. Signals can do too. SIGINT is a signal sent to the script when user pressed ctrl-c in the middle of the program execution trap is the command allows you to execute a command/function when a signal is received by your script trap <arg> <signals> <signals> is a list of signals to intercept <arg> is a command or function to execute when one of the signals is received. i.e. trap \"rm $TEMP_FILE; exit\" SIGHUP SIGINT SIGTERM The signals can be specified by number as well. signal 9 (SIGKILL) however, cannot be handled. many programs create lock files to prevent multiple copies of the program running at the same time. A program killed by SIGKILL doesn't get the chance to remove the lock file, which has to be manually removed for restarting the program it is better to write a function that does the clean up, and pass it to trap : trap clean_up SIGHUP SIGINT SIGTERM A best practice in shell scripting is to use absolute path instead of relative path to ensure the correctness!","title":"Shell Scripting"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#getting-to-know-shell-script","text":"There is no way to remember every command and every option associated with it. The real power is to know how to quickly learn from man page and write scripts to do what you need to do. #!/bin/bash # A line of comment echo \"Hello World!\" The first line is called a shebang , indicating what program is used to interpret the script. Other scripting languages such as Perl, awk, tcl, Tk, and python also use this mechanism. After giving a file execution permission, it can be run using ./script_name PATH is where Linux scans for commands and executable scripts If a directory need to be added to the PATH var, do export PATH=$PATH:directory . It is better to add this line to the .bash_profile or .profile file in your home directory. Linux encourage each user to have a specific directory for programs the user personally uses, bin in the user's home directory. This is automatically included in the $PATH var.","title":"Getting to Know Shell Script"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#shell-environment-variables","text":"Linux is a multi-user system, each person logged-in will get a bash to use. Each bash will have a set of environment variables to make sure users workspace clean. To set a shell variable , a few things: var_name=value no spaces on two sides of '=' var_name can't start with number; var_name contains only alphanumeric chars if var's value contains spaces, use \"\" or '' to wrap it up; variables can be evaluated within \"\" , but not '' use escape char \\ to include special chars in variables created within \"\" . can assign variable value from command's output using $( <shell_expression> ) use export var_name to make the variable an environment variable usually capitalized VAR_NAME is an environment variable use unset to cancel/remove a variable use env to see a list of all environment vars $RANDOM is a special env var that gives a random number between 0-32767 every time used. use set to see a list of user defined vars $PS1 is the command character as well as anything before it! It can be customized to display more info at the beginning of each line of command in terminal. see Linux Terminal Tricks notes $$ itself is a variable showing current shell's PID. sub-program of bash can only inherit the parent's environment vars, not regular vars export normal vars as env vars if necessary read command allows user to enter something and store it as a variable. Useful for script requires user input. declare or typeset both can set the variable type of a variable -a : array -i : integer -x : environment variable -r : readonly variable +x : cancel env var setting; '+' means cancel here... i.e. declare -i sum=100+300+50 Modify variable content expression - - purpose ${variable#key_word} from begin to end, find shortest match and delete key_word ${variable##key_word} from begin to end, find longest match and delete key_word ${variable%key_word} from end to beginning, find shortest match and delete key_word ${variable%%key_word} from end to beginning, find longest match and delete key_word ${variable/key_word/new_word} find one match of key_word and replace with new_word ${variable//key_word/new_word} find all matches of key_word and replace with new_word Variable evaluation from another variable expression - - str not set - - str is empty - - str set and non-empty var=${str-expr} $var=expr $var= $var=$str var=${str:-expr} $var=expr $var=expr $var=$str var=${str+expr} $var= $var=expr $var=expr var=${str:+expr} $var= $var= $var=expr var=${str=expr} $str=expr, $var=expr $str unchanged, $var= $str unchanged, $var=$str var=${str:=expr} $str=expr, $var=expr $str=expr, $var=expr $str unchanged, $var=$str var=${str?expr} prints expr to stderr $var= $var=$str var=${str:?expr} prints expr to stderr prints expr to stderr $var=$str use ${#Var_Name} to get the string length of a variable. array/list variable can be created using syntax of VARS=() - initialize with values VARS=(a b c) - change index value with VARS[0]=\"values\" - access array indexes with ${VARS[0]} - get a string of all items of an array variable with ${VARS[@]} - get array length with ${#VARS[@]} - indexes start at 0. No need to specify initial capacity. use source to load configuration from a file immediately","title":"shell (environment) variables"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#filtering-commands","text":"Take std input and perform operations upon it and send results to std output Command - - What it does sort sorts std input then outputs sorted result uniq given a stream of data, removes duplicate lines grep examines each line receives, outputs lines containing specified patterns fmt reads text from input and outputs formatted text pr takes text input and splits data into pages with page breaks, headers and footers in preparation for printing head outputs first few lines of input tail outputs last few lines of input tr translate characters, such as upper/lowercase conversions, changing line termination chars. sed stream editor, perform more sophisticated text translations than tr . supports regex awk a programming lang designed for constructing filters Example: Printing from the command line cat report.txt | sort | uniq | fmt | pr | lpr cat is used to concatenate files and gives to output. here can be used to put single file content to std output fmt formats text into neat paragraphs pr splits text neatly into pages lpr sends std input to printer use of - use - can replace stdin and stdout without creating files. i.e. tar -cvf - /home | tar -xvf - will pack the files and pass it to next command without writing to a file.","title":"Filtering Commands"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#expansion","text":"expansion is when your command typed-in expanded into something else before shell acts upon it. Pathname Expansion : we can use echo D* to show files starting with 'D' in the current directory. Tilde Expansion ~ char has special meanings: at beginning of a word, it expands to the name of the home directory of the named user. at the end of a filename, it means this file is a temporary backup if used alone it refers to the home directory of current user Arithmetic Expansion : the use of $((expression)) allows arithmetic evaluation inside the $() syntax if shell variables are used, their values must be numbers + - * / % **(exponential function) are supported by shell script. Brace Expansion create multiple text strings from a pattern containing braces i.e. echo Front-{A,B,C}-Back i.e. echo Front-{1..5}-Back Common good application is to make lists of files or directories to be created i.e. mkdir {2007..2009}-0{1..9} {2007..2009}-{10..12} Parameter Expansion make use of the system's ability to store small chunks of data and give each chunk a name (shell variables). printenv gives all available variables Command Substitution allows us to use the output of a command as an expansion, like this: echo $(ls) , ls -l $(which cp) alternatively, can also use back-quotes to do the same thing: ls -l `which cp` Quoting some special characters may need escapes using \\ or simply using double quotes \"\" to include the parts having special characters speical characters quoted within '' don't need to be escaped Here is a comparison of three levels of expansion suppressions echo text ~/*.txt { a,b } $( echo foo ) $(( 2 + 2 )) $USER # text /home/me/ls-output.txt a b foo 4 me echo \"text ~/*.txt {a,b} $( echo foo ) $(( 2 + 2 )) $USER \" # text ~/*.txt {a,b} foo 4 me echo 'text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER' # text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER use backslash also to start a new line for a single line of command. Like this: ls -l \\ --reverse \\ --human-readable \\ --full-time Sometimes use the long version of the options help you read and know its purpose instantly.","title":"Expansion"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#permissions","text":"Linux is a multitasking and multi-user system. chmod - modify file access rights rwx = 111, rw- = 110, r-x = 101, r-- 4, ... chmod 600 some_file makes the file only read/writable for the user, strips the rights for group and global for directory, x allows directory to be entered. r/w is for its contents to be listed/modified su - temporarily become the superuser sudo - temporarily become the superuser chown - change file ownership chown new_owner_username file superuser or owner can do this chgrp - change a file's group ownership chgrp new_group_name file only owner can do this Frequently used file permission settings Value - - Meaning 777 (rwxrwxrwx) No restriction on permission. 755 (rwxr-xr-x) Owner may read/write/exec, no one else may write the file but only read/exec 700 (rwx------) Only Owner has all access 666 (rw-rw-rw-) All users may read/write, not exec 644 (rw-r--r--) Owner may read/write, others can only read 600 (rw-------) Owner read/write only","title":"Permissions"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#job-control","text":"commands used to control processes. ps - list the processes running on the system kill - send a terminate signal to one or more processes (usually to \"kill\" a process) jobs - an alternate way of listing your own processes bg - put a process in the background fg - put a process in the forground Some commond kill signals Signal# - - Name - - Description 1 SIGHUP Hang up signal. Programs can listen for this signal and act upon it. This signal is sent to processes running in a terminal when you close the terminal. 2 SIGINT Interrupt signal. This signal is given to processes to interrupt them. Programs can process this signal and act upon it. You can also issue this signal directly by typing Ctrl-c in the terminal window where the program is running. 15 SIGTERM Termination signal. This signal is given to processes to terminate them. Again, programs can process this signal and act upon it. This is the default signal sent by the kill command if no signal is specified. 9 SIGKILL Kill signal. This signal causes the immediate termination of the process by the Linux kernel. Programs cannot listen for this signal.","title":"Job Control"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#editing-shell-scripts","text":"environment The Linux environment contains your path, your user name, etc. A complete list of the environment entries can be viewed using the command set . Two types of commands are often contained in the environment: aliases and shell functions . Login shells read one or more startup files: File - - Contents /etc/profile A global configuration script that applies to all users. ~/.bash_profile A user's personal startup file. Can be used to extend or override settings in the global configuration script. ~/.bash_login If ~/.bash_profile is not found, bash attempts to read this script. ~/.profile If neither ~/.bash_profile nor ~/.bash_login is found, bash attempts to read this file. Non-login shell sessions read the following: File - - Contents /etc/bash.bashrc A global configuration script that applies to all users. ~/.bashrc A user's personal startup file. Can be used to extend or override settings in the global configuration script. Creating alias for longer commands can be set like this alias today='date +\"%A, %B %-d, %Y\"'","title":"Editing Shell Scripts"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#creating-shell-functions","text":"inside a shell script, functions can be invoked by just using its name, or using $(func_name) shell commands can be called within functions simply with the command name. functions must be defined before using them. today () { echo -n \"Today's date is: \" date + \"%A, %B %-d, %Y\" } # same as function today { echo -n \"Today's date is: \" date + \"%A, %B %-d, %Y\" } A function local variables can be declared by appending local before it, like this: local argc=0","title":"Creating shell functions"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#here-script","text":"A here script (a.k.a. here document ) is an additional form of I/O redirection . It provides a way to include content that will be given to the standard input of a command. command << token content to be used as the command's standard input more lines here... until token token can be any string of characters, by convention is EOF Another trick to have the here script ignoring the leading tabs (not spaces) is by writing it this way: command <<- token line1 line2 ... token","title":"Here Script"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#variables","text":"define and use a variable: var_name = \"value\" # to define $var_name # to use it Rules for variables: names must start with a letter names contain no embedded spaces; use underscores instead names cannot use punctuation marks Environment Variables use printenv to view all environment variables loaded by shell These environment variables can be accessed directly in the shell script running within current shell session. Environment variables names are uppercase by convention. Inline commands we can have shell substitute the results of a command to existing script by using: $(command_here) can also assign the result of a command to a variable: var_1=$(command_here) variables inside $(()) don't need a '$' Variables can be directly replaced with its value inside \"\" quoted strings, like so \"Today's date: $date\"","title":"Variables"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#control-flow","text":"An if/else block if ( condition_expression ) ; then commands... elif ( condition_expression ) ; then commands... else commands... fi An switch/case block case $character in 1 ) echo \"character is: 1\" ;; 2 ) echo \"character is: 2\" ;; 3 ) echo \"character is: 3\" ;; 4 | 5 | 6 ) echo \"character matched one of: 4 5 6\" ;; * ) echo \"character matched to be anything else\" ;; esac Loops: while, until, and for An while block number = 0 while [ \" $number \" -lt 10 ] ; do echo \"Number = $number \" number = $(( number + 1 )) done An until block does exact the same thing as the while example number = 0 until [ \" $number \" -ge 10 ] ; do echo \"Number = $number \" number = $(( number + 1 )) done An for block assigns a word form a list of words to the specified variable, executes the commands, repeats until all words are exhausted. for variable in words ; do commands done for treats a string of text as a list of words by breaking by space chars count = 0 for i in $( cat ~/.bash_profile ) ; do count = $(( count + 1 )) echo \"Word $count ( $i ) contains $( echo -n $i | wc -c ) characters\" done Exit Status : a zero exit-code indicates success by convention. Exit status can be examined by variable $? after running a program/script. Shell provides true and false commands that do nothing except terminate with either a zero or one exit status exit command causes the script to terminate immediately and set the exit status to a value, like this exit 5 test/assertion test command is used most often as the condition check command. If the given expression is true, test exits with status 0; otherwise it exits with status 1. test expression # is the same as [ expression ] # spaces are required!! # or [[ expression ]] # spaces are required!! also use this everywhere, it is better than [ ] Most cases test can be used as a shortcut for if , like this test -e path_to_file && echo \"exist\" || echo \"not exist\" In the if statement, a exit code of 0 evaluated as 'true' while other exit codes are evaluated as 'false' if [ -f .bash_profile ] ; then echo \"You have a .bash_profile. Things are fine.\" else echo \"Yikes! You have no .bash_profile!\" fi ; the semicolon allows command statements to appear on the same line. A full list of test options option - - meaning -e exists? -f is a file ? -d is a directory ? -b is a block device? -c is a character device? -S is a Socket file? -P is a FIFO (pipe) file? -L is a link? -r read privilege ? -w write privilege ? -x execute privilege ? -u having SUID property? -g having SGID property? -k having sticky bit? -s exists and non empty ? $file1 -nt $file2 file1 newer than file2? $file1 -ot $file2 file1 older than file2? $file1 -ef $file2 file1 the same file as file2? check whether they point to the same inode $n1 -eq $n2 strings equal? $n1 -ne $n2 strings not equal? $n1 -gt $n2 greater than? $n1 -lt $n2 less than? $n1 -ge $n2 greater than or equal? $n1 -le $n2 less than or equal? -z string whether string is empty? -n string whether string is not empty? test $var1 == $var2 var1 is the same as var2? test $var1 != $var2 var1 is not the same as var2? -a and , same as && -o or , same as || ! not","title":"Control flow"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#io-redirection","text":"redirect output of commands to files, devices, and input of other commands. Std I/O ls > file.txt write output to file.txt, create if not exist ls >> file.txt append output to file.txt, create if not exist sort < file.txt redirect file.txt contents to command sort as input ls -l | less piping output of ls to program less as input Whenever a new program is run on the system, the kernel creates a table of file descriptors for the program to use. File descriptors are pointers to files. By convention, descriptors 0 (STDIN) , 1 (STDOUT) , and 2 (STDERR) are available. Initially, all three descriptors point to the terminal device (which the system treats as a read/write file) Redirection is the process of manipulating the file descriptors so that input and output can be routed from/to different files. the shell assumes we want to redirect standard output if the file descriptor is omitted. command > file command 1 > file # the above two are equivalent, so as the following two lines command < file command 0 < file Duplicating File Descriptors i.e. command 1> file 2>&1 to achieve sending both STDOUT and STDERR to the file first redirects STDOUT to 1, which is to the file then redirects STDERR to 1 (the file ) Create additional File Descriptors exec 3 > some_file.txt # Open new file descriptor 3 command 1 > & 3 # redirect STDOUT to file descriptor 3 exec 3 > & - # Close file descriptor 3 Debug Mode Simply include #!/bin/bash -x at the beginning of the file. Alternatively, use set -x to turn tracing on and set +x to turn tracing off.","title":"I/O Redirection"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#take-user-inputs","text":"read command takes input from the keyboard if -s option is passed, user's typing will not be displayed if -t option is passes, user has only specified seconds to enter the information. read text will store user's input into variable text read -p \"prompt\\t\" var will print a prompt before asking","title":"Take user inputs"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#positional-parameters","text":"these are special variables $0 through $9 that contain the contents of the command line arguments $? contains the exit status of last executed command/script $# contains the number of items on the command line in addition to the name of the command $0 shift is a shell built-in that operates on the positional parameters. Each time calling shift , the arguments $2 becomes $1 , $3 becomes $2 , so on. shift can follow a number, shift 3 means move and discard 3 arguments while [ \" $1 \" -ne \"\" ] ; do echo \"Parameter 1 equals $1 \" echo \"now have $# positional parameters\" shift done","title":"Positional parameters"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#error-handling","text":"One trivial way is to check $? for status of last command alternatively, can directly check the command's exit code in the if [ command ]; then error_exit () { echo \" $1 \" 1 > & 2 exit 1 } if cd $some_directory ; then rm * else error_exit \"Cannot change directory! Aborting.\" fi here $1 in error_exit() function is the first argument By using && or || can simplify the above example to the following: cd $some_directory || error_exit \"Cannot change directory! Aborting\" rm * # or simply cd $some_directory && rm * A more slicker example of error_exit function: PROGNAME = $( basename $0 ) error_exit () { echo \" ${ PROGNAME } : ${ 1 :- \"Unknown Error\" } \" 1 > & 2 exit 1 } As shown before, the 1:- means if variable $1 is undefined, a default value \"Unknown Error\" is used.","title":"Error Handling"},{"location":"Programming-Lang-Reference/Shell/Shell-Scripting/#signals-and-traps","text":"Errors are not the only way a script can terminate unexpectedly. Signals can do too. SIGINT is a signal sent to the script when user pressed ctrl-c in the middle of the program execution trap is the command allows you to execute a command/function when a signal is received by your script trap <arg> <signals> <signals> is a list of signals to intercept <arg> is a command or function to execute when one of the signals is received. i.e. trap \"rm $TEMP_FILE; exit\" SIGHUP SIGINT SIGTERM The signals can be specified by number as well. signal 9 (SIGKILL) however, cannot be handled. many programs create lock files to prevent multiple copies of the program running at the same time. A program killed by SIGKILL doesn't get the chance to remove the lock file, which has to be manually removed for restarting the program it is better to write a function that does the clean up, and pass it to trap : trap clean_up SIGHUP SIGINT SIGTERM A best practice in shell scripting is to use absolute path instead of relative path to ensure the correctness!","title":"Signals and Traps"},{"location":"Reading-Notes/DevOps_SRE/","text":"LF Course https://learning.edx.org/course/course-v1:LinuxFoundationX+LFS162x+3T2019 DevOps Today \u00b6 Reliability, Availability, Scalability, Observability Five Pillars of DevOps: Cloud, Container, CI/CD, Infrastructure as Code, Observability Cloud \u00b6 Major cloud providers: Google Cloud, Microsoft Azure, Amazon AWS Major cloud orchestration engins: Kubernetes, Docker - Swarm, Mesos - Marathon Benefits from Cloud \u00b6 Utility Computing Model - use as much of infrastructure that we want, and pay at the end of the month to the cloud provider, just like the utility providers. Plus, you get access to the global infrastructure. Without the cloud that would be very expensive and time-consuming. The platform itself that you host your infrastructure on is scalable OPEX versus CAPEX: capital expenses versus operational expenses; your finance team would definitely love the operational expenses model than the capital expenses. Business continuity, benefits from the multi-region datacenters available to you. Managed services. No headaches from setting up, configuring and managing, automating on services such as a database cluster that needs backups, high avilability, scalability. Especially useful when your team is small. Automation out-of-box. Types of Cloud \u00b6 With Infrastructure as a Service, what you can get from a service provider is the infrastructure components, like compute, network, storage, and databases. With Platform as a Service, what you get is basically a deployed application, it scales on its own as well, and you don't have a control over it and you may not need it. With Service as a Service, what you get is purely services that offers certain functionalities to consume for your service. Cloud deployment models \u00b6 Public cloud , rely entirely on IaaS - You could start with an idea, build your application, just go live, and serve your global customers within hours. It accelerated the growth of the startup world in general. Private cloud is what you build in-house or in the datacenter that you control, which can be completely private or exposed to outside. You get a self-service ability, automation, self-provisioning capacity, dynamic capacity in certain cases. Tools like OpenStack help you build your own private cloud platform. Hybrid cloud , for organizations who have their own datacenters and who have been running their own cloud in-house that is a private cloud, still need some capacity in certain cases, such as content delivery network (CDN), or bursting traffic. Major Cloud Services \u00b6 Block Storage - provision disks spaces, usually large storage space Object Storage - also disks to attach to, mostly store files or objects for connect and retrival Network Service - virtual network services in a multi-tenant environment, especially from the security and customization point of view, ability to set firewall, divide network into subnets, provision public IP, etc. User Access Management (IAM) - manages who can access the resources Container \u00b6 Major container technologies: Docker, runc Container technology like Docker defines a standard for creating and running the containers, so that it can be used anywhere that this standard is adopted. So, what appears as a container and what separates one container from another is a bunch of namespaces. Only those things are virtualized; everything else is from the system. The container is sharing the kernel with the system here, and it's just running a process, just like a process running on your system, but in an isolated environment. Cgroups, from Google's contribution, offer resource isolation through control groups A Docker image differs from a VM image in that it is layered, while VM is not. What an orchestration engine like Kubernetes offer: scheduling containers networking for accessing containers high availability of containers by auto scaling security: network policies scanning images admission control, athentication and authorization controller for different workloads extensibility through CRDs Infra as Code \u00b6 Major IaC tools: Ansible, Chef, Terraform With Infrastructure as Code, you manage infrastructure configurations and states through a centralized management system using the language it understands. Infra as Code allows you to template things, reuse, reproduce, and automate. Categories \u00b6 VM provisioning - with tools like Vagrant, configure VM of certain resources; with a tool like Packer, create a template and define the process of building your image as a code. Cloud provisioning - provision VMs running in a managed environment along with virtual private networks, network components, virtual private clouds, the security components. Tools available such as Cloudformation for AWS, Terraform as a generic tool, and Ansible Configuration management - configure the system further such as user creation, package management, package installation, application configuration. Popular tools are Chef, Puppet, Ansible, and SaltStack. Application configs are mostly baked within the containers nowadays, though container provisioning - deploy container-based apps, with tools like Docker Compose or Kubernetes YAML configs. CICD provisioning - define pipelines as code, notable ones are Jenkins's Jenkinsfile, Docker's Dockerfile and Docker-compose file, Vagrant's Vagrantfile, Spinnaker's YAML file. Features \u00b6 Declarative syntax - simply put, you define what you want and let the tool handle \"how\" to get there. It provides an abstracted or simplified language. Ability to store as code - the config can be easily stored in a version-control system like git, and benefit from IDE's autocompletion, tools that lint syntax. Idempotence - the same code applied and achieve the same thing, and the tool is smart enough to figure out the differences to apply the changes. It manages the state from code, by comparing the declarative code with the code from current state, and appropriate action is chosen based on the current state. Self-document - the code is mostly written in a readable and simple language, human-friendly. It saves effort on explaning a lot of things. Templating - the code can be generalized through templating to improve reusability. CI/CD \u00b6 Major CI/CD orchestrators: Jenkins, Spinnaker, Travis CI, Circle CI Continuous Integration pipeline allows many developers to work in parallel and push for integrations. It also gives quick and immediate feedback from tests, builds, and integrations for each commit merged, thus continuously and iteratively improve the quality of software. Continuous Integration helps find bugs early and fail early. Read on paper Simple Testing Can Prevent Most Critical Failures; An Analysis of Production Failures in Distributed Data-Intensive Systems . Static code analysis like SonarQube, and Canary deployment and analysis are also effective tools to add to the pipeline. Other kinds of tests such as acceptance tests, load tests, compliance tests, and chaos tests, are optional but ideal to add as well. The end goal is to ensure reliability before releasing to production. Agile is the fundamental practice that you start with in order to incorporate Continuous Integration. Another practice is to make builds quick. This becomes harder as you go up the testing pyramid, say, those integration tests that are expensive to run. And to fight that, you should add a unit test every time some integration test catches a bug, to shift left in the testing pyramid. In terms of Continuous Delivery, Spinnaker is a very useful automated deployment tool that supports multiple cloud platforms. It does not replace Jenkins, but rather extends it to achieve Continuous Delivery. Release Strategies \u00b6 Release with downtime by scheduled release that shuts users off when applying and configuring the update. Release with zero downtime by performing a rolling update and apply patches in batches. Chances are that the customers will see different versions of the application during this update. Blue/green release requires extra hardware capacity to bring up a set of v2 servers in stand by, then tell load balancer to switch all new traffic to the v2 servers and take down v1 servers to allow next update. Canary release aims updating only part of your servers to v2, let some beta customers get directed here. In other words, it is a test-in-production strategy. Additionally, canary analysis, aka A/B testing, can be performed between v1 and v2. Another release strategy is feature flagging. You first rolling deploy the new features turned off, then enable the feature by switching it on that reconfigure all servers to serve. A great advantage is that rollback is quick and easy. Observailibity \u00b6 Major observailibity tools: Nagios, Prometheus (Time-series DB) and Grafana (Visualizer), Jaeger (Traces), Zipkin (traces) Getting the metric-measuring done is a very important aspect of observability. It helps improve existing system and troubleshoot issues. Metrics (system resource, network, application health), logs, and traces are what to observe. The ELK stack - Elasticsearch, Logstash, and Kibana, is a very popular stack in opensource world. SLA SLO SLI \u00b6 SLA - Service Level Aggrement - a contract between the service providers and their clients, which defines a guarantee that the service infrastructure will be up and the services will be functional. i.e. a 99% SLA gives the provider some room of downtime for maintenance. When SLA is breached, you have an outage. It is a strict aggrement. To ensure you can guarantee the SLA, you can use a tighter Internal SLA , now it is called a SLO - Service Level Objective , which give you some buffer for guarantee the SLA. To know whether you comply with SLA or SLO, measure the service uptime, latencies, number of requests, number of errors, etc. And they will be your SLI - Service Level Indicators DevOps Future \u00b6 K8s stays strong. Service Mesh is new thing (Istio, Linkerd). Canary Releases and Canary Analyses integration in CI/CD pipelines. Intelligent routing, send certain traffic for certain users. Redirect or mirror prod traffic to staging for testing. Faults injection for Chaos testing.","title":"Intro to DevOps and SRE"},{"location":"Reading-Notes/DevOps_SRE/#devops-today","text":"Reliability, Availability, Scalability, Observability Five Pillars of DevOps: Cloud, Container, CI/CD, Infrastructure as Code, Observability","title":"DevOps Today"},{"location":"Reading-Notes/DevOps_SRE/#cloud","text":"Major cloud providers: Google Cloud, Microsoft Azure, Amazon AWS Major cloud orchestration engins: Kubernetes, Docker - Swarm, Mesos - Marathon","title":"Cloud"},{"location":"Reading-Notes/DevOps_SRE/#benefits-from-cloud","text":"Utility Computing Model - use as much of infrastructure that we want, and pay at the end of the month to the cloud provider, just like the utility providers. Plus, you get access to the global infrastructure. Without the cloud that would be very expensive and time-consuming. The platform itself that you host your infrastructure on is scalable OPEX versus CAPEX: capital expenses versus operational expenses; your finance team would definitely love the operational expenses model than the capital expenses. Business continuity, benefits from the multi-region datacenters available to you. Managed services. No headaches from setting up, configuring and managing, automating on services such as a database cluster that needs backups, high avilability, scalability. Especially useful when your team is small. Automation out-of-box.","title":"Benefits from Cloud"},{"location":"Reading-Notes/DevOps_SRE/#types-of-cloud","text":"With Infrastructure as a Service, what you can get from a service provider is the infrastructure components, like compute, network, storage, and databases. With Platform as a Service, what you get is basically a deployed application, it scales on its own as well, and you don't have a control over it and you may not need it. With Service as a Service, what you get is purely services that offers certain functionalities to consume for your service.","title":"Types of Cloud"},{"location":"Reading-Notes/DevOps_SRE/#cloud-deployment-models","text":"Public cloud , rely entirely on IaaS - You could start with an idea, build your application, just go live, and serve your global customers within hours. It accelerated the growth of the startup world in general. Private cloud is what you build in-house or in the datacenter that you control, which can be completely private or exposed to outside. You get a self-service ability, automation, self-provisioning capacity, dynamic capacity in certain cases. Tools like OpenStack help you build your own private cloud platform. Hybrid cloud , for organizations who have their own datacenters and who have been running their own cloud in-house that is a private cloud, still need some capacity in certain cases, such as content delivery network (CDN), or bursting traffic.","title":"Cloud deployment models"},{"location":"Reading-Notes/DevOps_SRE/#major-cloud-services","text":"Block Storage - provision disks spaces, usually large storage space Object Storage - also disks to attach to, mostly store files or objects for connect and retrival Network Service - virtual network services in a multi-tenant environment, especially from the security and customization point of view, ability to set firewall, divide network into subnets, provision public IP, etc. User Access Management (IAM) - manages who can access the resources","title":"Major Cloud Services"},{"location":"Reading-Notes/DevOps_SRE/#container","text":"Major container technologies: Docker, runc Container technology like Docker defines a standard for creating and running the containers, so that it can be used anywhere that this standard is adopted. So, what appears as a container and what separates one container from another is a bunch of namespaces. Only those things are virtualized; everything else is from the system. The container is sharing the kernel with the system here, and it's just running a process, just like a process running on your system, but in an isolated environment. Cgroups, from Google's contribution, offer resource isolation through control groups A Docker image differs from a VM image in that it is layered, while VM is not. What an orchestration engine like Kubernetes offer: scheduling containers networking for accessing containers high availability of containers by auto scaling security: network policies scanning images admission control, athentication and authorization controller for different workloads extensibility through CRDs","title":"Container"},{"location":"Reading-Notes/DevOps_SRE/#infra-as-code","text":"Major IaC tools: Ansible, Chef, Terraform With Infrastructure as Code, you manage infrastructure configurations and states through a centralized management system using the language it understands. Infra as Code allows you to template things, reuse, reproduce, and automate.","title":"Infra as Code"},{"location":"Reading-Notes/DevOps_SRE/#categories","text":"VM provisioning - with tools like Vagrant, configure VM of certain resources; with a tool like Packer, create a template and define the process of building your image as a code. Cloud provisioning - provision VMs running in a managed environment along with virtual private networks, network components, virtual private clouds, the security components. Tools available such as Cloudformation for AWS, Terraform as a generic tool, and Ansible Configuration management - configure the system further such as user creation, package management, package installation, application configuration. Popular tools are Chef, Puppet, Ansible, and SaltStack. Application configs are mostly baked within the containers nowadays, though container provisioning - deploy container-based apps, with tools like Docker Compose or Kubernetes YAML configs. CICD provisioning - define pipelines as code, notable ones are Jenkins's Jenkinsfile, Docker's Dockerfile and Docker-compose file, Vagrant's Vagrantfile, Spinnaker's YAML file.","title":"Categories"},{"location":"Reading-Notes/DevOps_SRE/#features","text":"Declarative syntax - simply put, you define what you want and let the tool handle \"how\" to get there. It provides an abstracted or simplified language. Ability to store as code - the config can be easily stored in a version-control system like git, and benefit from IDE's autocompletion, tools that lint syntax. Idempotence - the same code applied and achieve the same thing, and the tool is smart enough to figure out the differences to apply the changes. It manages the state from code, by comparing the declarative code with the code from current state, and appropriate action is chosen based on the current state. Self-document - the code is mostly written in a readable and simple language, human-friendly. It saves effort on explaning a lot of things. Templating - the code can be generalized through templating to improve reusability.","title":"Features"},{"location":"Reading-Notes/DevOps_SRE/#cicd","text":"Major CI/CD orchestrators: Jenkins, Spinnaker, Travis CI, Circle CI Continuous Integration pipeline allows many developers to work in parallel and push for integrations. It also gives quick and immediate feedback from tests, builds, and integrations for each commit merged, thus continuously and iteratively improve the quality of software. Continuous Integration helps find bugs early and fail early. Read on paper Simple Testing Can Prevent Most Critical Failures; An Analysis of Production Failures in Distributed Data-Intensive Systems . Static code analysis like SonarQube, and Canary deployment and analysis are also effective tools to add to the pipeline. Other kinds of tests such as acceptance tests, load tests, compliance tests, and chaos tests, are optional but ideal to add as well. The end goal is to ensure reliability before releasing to production. Agile is the fundamental practice that you start with in order to incorporate Continuous Integration. Another practice is to make builds quick. This becomes harder as you go up the testing pyramid, say, those integration tests that are expensive to run. And to fight that, you should add a unit test every time some integration test catches a bug, to shift left in the testing pyramid. In terms of Continuous Delivery, Spinnaker is a very useful automated deployment tool that supports multiple cloud platforms. It does not replace Jenkins, but rather extends it to achieve Continuous Delivery.","title":"CI/CD"},{"location":"Reading-Notes/DevOps_SRE/#release-strategies","text":"Release with downtime by scheduled release that shuts users off when applying and configuring the update. Release with zero downtime by performing a rolling update and apply patches in batches. Chances are that the customers will see different versions of the application during this update. Blue/green release requires extra hardware capacity to bring up a set of v2 servers in stand by, then tell load balancer to switch all new traffic to the v2 servers and take down v1 servers to allow next update. Canary release aims updating only part of your servers to v2, let some beta customers get directed here. In other words, it is a test-in-production strategy. Additionally, canary analysis, aka A/B testing, can be performed between v1 and v2. Another release strategy is feature flagging. You first rolling deploy the new features turned off, then enable the feature by switching it on that reconfigure all servers to serve. A great advantage is that rollback is quick and easy.","title":"Release Strategies"},{"location":"Reading-Notes/DevOps_SRE/#observailibity","text":"Major observailibity tools: Nagios, Prometheus (Time-series DB) and Grafana (Visualizer), Jaeger (Traces), Zipkin (traces) Getting the metric-measuring done is a very important aspect of observability. It helps improve existing system and troubleshoot issues. Metrics (system resource, network, application health), logs, and traces are what to observe. The ELK stack - Elasticsearch, Logstash, and Kibana, is a very popular stack in opensource world.","title":"Observailibity"},{"location":"Reading-Notes/DevOps_SRE/#sla-slo-sli","text":"SLA - Service Level Aggrement - a contract between the service providers and their clients, which defines a guarantee that the service infrastructure will be up and the services will be functional. i.e. a 99% SLA gives the provider some room of downtime for maintenance. When SLA is breached, you have an outage. It is a strict aggrement. To ensure you can guarantee the SLA, you can use a tighter Internal SLA , now it is called a SLO - Service Level Objective , which give you some buffer for guarantee the SLA. To know whether you comply with SLA or SLO, measure the service uptime, latencies, number of requests, number of errors, etc. And they will be your SLI - Service Level Indicators","title":"SLA SLO SLI"},{"location":"Reading-Notes/DevOps_SRE/#devops-future","text":"K8s stays strong. Service Mesh is new thing (Istio, Linkerd). Canary Releases and Canary Analyses integration in CI/CD pipelines. Intelligent routing, send certain traffic for certain users. Redirect or mirror prod traffic to staging for testing. Faults injection for Chaos testing.","title":"DevOps Future"},{"location":"Reading-Notes/Google-SRE-Book/","text":"Notes taken from Google SRE Book . Software engineering has this in common with having children: the labor before the birth is painful and difficult, but the labor after the birth is where you actually spend most of your effort. SREs are focused on operating services built atop the distributed computing systems that span globally across multiple regions and serve millions to billions of users. Much like security, the earlier you care about reliability, the better. Concept of Dev, Ops, and DevOps \u00b6 This systems administrator, or sysadmin, assembles existing software components (developed by the developers) and deploying them to produce a service, runs the service, and responds to events and updates as they occur. These tasks generally fall into the category of operations. Developers and sysadmins are therefore divided into discrete teams: \"development/dev\" and \"operations/ops\". As the system grows in complexity and traffic volume, generating a corresponding increase in events and updates, the sysadmin team grows to absorb the additional work and increases operational cost, which is a direct cost. The indirect costs arise from the fact that the two teams are quite different in background, skill set, and incentives. They use different vocabulary to describe situations; they carry different assumptions about both risk and possibilities for technical solutions; they have different assumptions about the target level of product stability. At their core, the development teams want to launch new features and see them adopted by users; the ops teams want to make sure the service doesn\u2019t break while they are holding the pager. DevOps is a set of principles guiding software development: involvement of the IT function in each phase of a system\u2019s design and development, heavy reliance on automation versus human effort, the application of engineering practices and tools to operations tasks. SRE \u00b6 How SRE Differs from DevOps \u00b6 DevOps's principles and practices are consistent with Site Reliability Engineering. DevOps is a generalization of several core SRE principles to a wider range of organizations, management structures, and personnel. In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their service(s). Site Reliability Engineering teams focus on hiring software engineers to deploy and run products and to create systems to automate work that otherwise would be performed manually. This structure also improves the product development teams: easy transfers between product development and SRE teams cross-train the entire group. UNIX system internals and networking (Layer 1 to Layer 3) expertise are the two most common types of alternate technical skills for software engineers hired into SRE roles. A diverse background of the SRE team frequently results in clever, high-quality systems. Google places a 50% cap on the aggregate \"ops\" work for all SREs\u2014tickets, on-call, manual tasks to ensure SRE team can focus on engineering work in building or improving existing systems. Eliminating toil is one of SRE\u2019s most important tasks. Systems should be automatic, not just automated. They should run and repair themselves. Postmortems should be written for all significant incidents, regardless of whether or not they paged; postmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps. Google operates under a blame-free postmortem culture, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing exposing them. Error budget \u00b6 Product development and SRE teams can enjoy a productive working relationship by eliminating the structural conflict in their respective goals between pace of innovation and product stability, with an agreed error budget. A service that\u2019s 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service\u2019s error budget. 100% error budget is the wrong reliability target for basically everything. Setting the correct error budget is a product question and should consider: level of availability to keep user satisfied alternatives for unhappy users with given availability user's accessible features of product at different availability levels Spend error budget taking risks with things to launch. As soon as SRE activities are conceptualized in this framework, freeing up the error budget through tactics such as phased rollouts and 1% experiments can optimize for quicker launches. An outage is no longer a \"bad\" thing\u2014it is an expected part of the process of innovation, and an occurrence that both development and SRE teams manage rather than fear. Monitoring \u00b6 Monitoring allows service owners keep track of a system\u2019s health and availability. Traditionally, monitoring watches for a specific value or condition, and then triggers an alert to engineers when that value is exceeded or that condition occurs. Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action. Alerts should be sent when immediate actions need to be taken by an engineer. Tickets should be issued when engineers action is required but not immediately. Logging should be recorded for diagnostic or root-cause-analysis purposes. Oncall, Emergency Response \u00b6 Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR). Humans add latency. A system that can avoid emergencies that require human intervention will have higher availability than a system that requires hands-on intervention. Thinking through and recording the best practices ahead of time in a \"Runbook\" that gives each possible fail case and steps for remediation, drastically improves the MTTR for incidents that involves human. Change Management \u00b6 Best practices implementing automation around changes: apply progressive rollouts detect problems quickly and accurately roll back changes safely and quickly upon detecting problems Capacity Planning \u00b6 Demand forecasting and capacity planning can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future demand with the required availability. Capacity planning should take both organic growth (natural product adoption from users) and inorganic growth (feature launches, marketing campaigns) into account. Regular load testing of the system should be performed to correlate raw resource capacity to service capacity. Provisioning \u00b6 Provisioning combines both change management and capacity planning. Provisioning must be conducted quickly and only when necessary, as capacity is expensive. Adding new capacity often involves spinning up a new instance or location, making significant modification to existing systems (load balancers, networking, configurations changes), and validating that the new capacity performs and delivers correct results. Efficiency and Performance \u00b6 Resource utilization provides insights of a service's efficiency, and is a function of demand (load), capacity, and software efficiency. SREs predict demand, provision capacity, and can modify the software. Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity. At some point, a slowing system stops serving, which corresponds to infinite slowness. SREs provision to meet a capacity target at a specific response speed (responses per second), and thus are keenly interested in a service\u2019s performance. SREs and product developers will (and should) monitor and modify a service to improve its performance, thus adding capacity and improving efficiency. Production Environment \u00b6 In a production environment, a machine refers to a piece of hardware or VM and a server refers to a piece of software that implements a service. Machines can run any server. Topology of a Google datacenter: Tens of machines are placed in a rack. Racks stand in a row. One or more rows form a cluster. Usually a datacenter building houses multiple clusters. Multiple datacenter buildings that are located close together form a campus. Managing Machines \u00b6 Borg is a distributed cluster operating system managing user jobs at the cluster level and allocate resources to jobs. Jobs can either be indefinitely running servers or batch processes like a MapReduce. Borg then continually monitors these tasks. If a task malfunctions, it is killed and restarted, possibly on a different machine. Borg allocates a name and index number to each task using the Borg Naming Service (BNS). Rather than using the IP address and port number, other processes connect to Borg tasks via the BNS name, which is translated to an IP address and port number by BNS. Storage \u00b6 The storage layer is responsible for offering users easy and reliable access to the storage available for a cluster: the lowest layer is called D (disk), a fileserver running on almost all machines in a cluster Colossus layer is on top of D and creates a cluster-wide filesystem that offers usual filesystem semantics, as well as replication and encryption next layer are several database-like services Bigtable, NoSQL database handles petabyte-grade data. It is a sparse, distributed, persistent multidimensional sorted map that is indexed by row key, column key, and timestamp; each value in the map is an uninterpreted array of bytes. Bigtable supports eventually consistent, cross-datacenter replication. Spanner, SQL-like distributed database for real consistency across the world Blobstore Networking \u00b6 Instead of using \"smart\" routing hardware, Google rely on less expensive \"dumb\" switching components in combination with a central (duplicated) controller that precomputes best paths across the network. Google directs users to the closest datacenter with available capacity. Our Global Software Load Balancer (GSLB) performs load balancing on three levels: geographic DNS requests user service (i.e. YouTube, Google Maps) RPC level Locking \u00b6 The Chubby lock service provides a filesystem-like API for maintaining locks. It uses the Paxos protocol for asynchronous Consensus. Chubby helps master election and provides a consistent storage. Monitoring \u00b6 Borgmon monitoring proram regularly scrapes metrics from monitored servers. These metrics can be used instantaneously for alerting and also stored for use in historic overviews. Software infrastructure \u00b6 Google software architecture is designed to make the most efficient use of the hardware infrastructure. Code is heavily multithreaded. Every server has an HTTP server that provides diagnostics and statistics for a given task. All of Google\u2019s services communicate using a Remote Procedure Call (RPC) infrastructure named Stubby (opensourced as gRPC). Often, an RPC call is made even when a call to a subroutine in the local program needs to be performed. This makes it easier to refactor the call into a different server if more modularity is needed, or when a server\u2019s codebase grows. A server receives RPC requests from its frontend and sends RPCs to its backend. Data is transferred to and from an RPC using protocol buffers (protobufs). Protocol buffers have many advantages over XML for serializing structured data: they are simpler to use, 3 to 10 times smaller, 20 to 100 times faster, and less ambiguous. Development Environment \u00b6 Development velocity is very important to Google. Google Software Engineers work from a single shared repository. If engineers encounter a problem in a component outside of their project, they can fix the problem, send the proposed changes (\"changelist/CL\", like a PR) to the owner for review, and submit (merge) the CL to the mainline. Changes to source code in an engineer\u2019s own project require a review. All software is reviewed before being submitted. When software is built, the build request is sent to build servers in a datacenter and executed in parallel. Each CL submition causes tests to run on all software that may depend on that CL. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests. Shakespeare \u00b6 Shakespeare: a sample service at google Principles \u00b6 A key principle of any effective software engineering, not only reliability-oriented engineering, simplicity is a quality that, once lost, can be extraordinarily difficult to recapture. Embracing Risk \u00b6 Cost does not increase linearly as reliability increments: an incremental improvement in reliability may cost 100x more than the previous increment. The cost comes from: redundant machine and computing resources opportunity cost for building systems or features usable by end users Measuring risks \u00b6 SRE manages service reliability largely by managing risk, conceptualizes risk as a continuum, and identifies the appropriate level of risk tolerance for the services to run. Service failures can have many potential effects, including user dissatisfaction, harm, or loss of trust; direct or indirect revenue loss; brand or reputational impact; and undesirable press coverage. For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime and is expressed by service availability in terms of number of \"nines\": 99.9%, 99.99%, etc. The availability can be calculated as (uptime / uptime + downtime) or (successful requests / total requests). The same principles also apply to nonserving systems with minimal modification. Risk tolerance \u00b6 Product managers are charged with understanding the users and the business, and for shaping the product for success in the marketplace. To identify the risk tolerance of a service, SREs must work with the product owners to turn a set of business goals into explicit objectives that can be engineered. Factors to consider on consumer services: required level of avalilability users' expectation impact on our revenue, or our users' revenue paid or free service how much does our competitors set is the service target consumers or enterprises does different types of failures have different effects on the service business resilience on service downtime impact from low rate of failures, or occasional full-site outage planned downtime locate a service cost on the risk continuum what would be the increase in revenue and the cost to increase the availability move forward when the revenue increase offsets the implementation cost service metrics to account for understand the importance of metrics under different use cases and focus on those important ones Error Budgets \u00b6 Error budget provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability. The tensions between feature delivery and site reliability reflect themselves in different opinions about the level of effort that should be put into engineering practices: software fault tolerance testing release frequency canary duration and size The Service Level Objectives express error budget in a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter. Product Management defines an SLO of expected uptime for the quarter Monitoring system measure the actual uptime The difference between the budget and the unreliability happened tells the remaining budget for the quarter As long as there are budget remaining, new releases can be pushed For example, imagine that a service\u2019s SLO is to successfully serve 99.999% of all queries per quarter. This means that the service\u2019s error budget is a failure rate of 0.001% for a given quarter. If a problem causes us to fail 0.0002% of the expected queries for the quarter, the problem spends 20% of the service\u2019s quarterly error budget. If product development wants to skimp on testing or increase push velocity and SRE is resistant, the error budget guides the decision. When the budget is large, the product developers can take more risks. When the budget is nearly drained, the product developers themselves will push for more testing or slower push velocity, as they don\u2019t want to risk using up the budget and stall their launch. Service Level Terminology \u00b6 Indicators (SLI) - a carefully defined quantitative measure of some aspect of the level of service that is provided. i.e. request latency, error rate, system throughput or rps. These metrics are often aggregated into a rate, average, or percentile. Understanding of what your users want from the system will inform the judicious selection of a few indicators: user-facing: availability, latency, throughput storage systems: latency, availability, durability big data systems, data pipelines: throughput, end-to-end latency generally for all systems: correctness Objectives (SLO) - a target value or range of values for a service level that is measured by an SLI. For maximum clarity, SLOs should specify how they\u2019re measured and the conditions under which they\u2019re valid. i.e. 99% (averaged over 1 minute) of Get RPC calls will complete in less than 100 ms (measured across all the backend servers). Choose just enough SLOs to provide good coverage of your system\u2019s attributes. Get rid of unnecessary SLOs that cannot indicate priorities. Keep a safety margin , use a tighter internal SLO than the SLO advertised to users gives you room to respond to chronic problems before they become visible externally. You can always refine SLO definitions and targets over time as you learn about a system\u2019s behavior. Start loose and tighten it later. Don't overachieve . If your service\u2019s actual performance is much better than its stated SLO, users will come to rely on its current performance. Understanding how well a system is meeting its expectations helps decide whether to invest in making the system faster, more available, and more resilient, on tasks such as paying off tech debts, adding new features, introducing other tools, workflows, or products. Agreements (SLA) - an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain. The consequences are most easily recognized when they are financial: a rebate or a penalty\u2014but they can take other forms. SLAs are closely tied to business and product decisions. It is wise to be conservative in what you advertise to users, as the broader the constituency, the harder it is to change or delete SLAs that prove to be unwise or difficult to work with. Indicators processing \u00b6 Indicator metrics are most naturally gathered on the server side using a monitoring system or with periodic log analysis. Some should be instrumented with client-side collection, because not measuring behavior at the client can miss a range of problems that affect users but don\u2019t affect server-side metrics. Most metrics are better thought of as distributions or percentiles rather than averages. For example, a simple average latency metric aggregated over a fixed window can obscure these tail latencies, as well as changes in them. Alerting based only on the average latency would show no change in behavior over the course of the day, when there are in fact significant changes in the tail latency. User studies have shown that people typically prefer a slightly slower system to one with high variance in response time. Indicators should be standardized to avoid repeated reasoning each time they are needed. i.e. make it a template: aggregation window/interval size: 1 minute aggregation region: all within a cluster measurement frequency: every 10 seconds requests measured: HTTP GETs on xxx API data collection: monitoring framework measured at the server data access latency: time to last byte received since request sent Eliminate Toil \u00b6 Overhead is often work not directly tied to running a production service, and includes tasks like team meetings, setting and grading goals, snippets, and HR paperwork. Toil is the kind of work tied to running a production service that tends to be: manual, such as manually running some commands or a script that completes some task repetitive, the work you do over and over manually automatable, when machines can do the task as well as human without relying on a human's judgement for choices tactical, interrupt-driven and reactive, rather than strategy-driven and proactive. i.e. pager duty is inevitable but with opportunity to minimize no enduring value, so the service remains in the same performance state after you have finished a task scales linearly as a service grows. Ideally the service should be able to grow by one order of magnitude with zero manual work to add resource Toil tends to expand if left unchecked and can quickly fill 100% of everyone\u2019s time. SRE's engineering work either reduce future toil or add service features. Feature development typically focuses on improving reliability, performance, or utilization, which often reduces toil as a second-order effect. SREs report that their top source of toil is interrupts (that is, non-urgent service-related messages and emails). The next leading source is on-call (urgent) response, followed by releases and pushes. Typical SRE duties \u00b6 Software engineering - writing or extending code, design document, or software documentation. i.e. automation scripts, developing tools or frameworks, adding service features for scalability and reliability, hardening service with infrastructure code System engineering - configuring production systems, modifying configurations, or documenting systems runbooks i.e. setting up monitoring, load balancer configuration, server configuration, tuning OS parameters, productionization for dev teams Toil - repetitive work in running a service i.e. mentioned in above section Overhead - administrative work not tied directly to running a service i.e. hiring, HR paperwork, trainings, meetings, peer reviews and self-assess Toil isn\u2019t always and invariably bad, and some amount of toil is unavoidable in any engineering role. Small amount of toil can be calming and rewards with a sense of quick wins. Toil becomes toxic when experienced in large quantities. If we all commit to eliminate a bit of toil each week with some good engineering, we\u2019ll steadily clean up our services, and we can shift our collective efforts to engineering for scale, architecting the next generation of services, and building cross-SRE toolchains. Monitoring \u00b6 Monitoring terminologies: monitoring - collecting, processing, aggregating, and displaying real-time quantitative data about a system white-box monitoring - base on metrics exposed by the internals of the system, such as logs black-box monitoring - testing externally visible behavior as a user would see it, such as end-to-end testing dashboard - an application provides a summary view of a service's core metrics alert - a notification intended to be read by a human and that is pushed to a system such as a bug or ticket queue, an email alias, or a pager root cause - a defect in a software or human system that, if repaired, instills confidence that this event won\u2019t happen again in the same way node, machine - used interchangeably to indicate a single instance of a running kernel in either a physical server, virtual machine, or container push - any change to a service\u2019s running software or its configuration. Monitoring helps: analyzing long-term trends comparing over time or experiment groups alerting building dashboards ad hoc retrospective analysis business analytics security analysis Effective alerting systems have good signal and very low noise (unnecessary pages). Rules that generate alerts for humans should be simple to understand and represent a clear failure. Paging a human is a quite expensive use of an employee's time. When meaningless pages occur too frequently, employees second-guess, skim, or even ignore incoming alerts. Outages can be prolonged because other noise interferes with a rapid diagnosis and fix. Every page should be about a novel problem and actionable and require intelligence. Someone should find and eliminate the root causes of the problem. Pages with rote, algorithmic responses should be a red flag. Unwillingness on the part of your team to automate such pages implies that the team lacks confidence that they can clean up their technical debt (i.e. put in quick short term fixes to automate the page response, but plan for a long term fix). This is a major problem worth escalating. Symptoms vs. Causes \u00b6 Your monitoring system should address two questions: what\u2019s broken, and why? For example, high 5xxs maps to a part of service is down or behaving unexpectedly; high latency maps to one of overloaded server, low network bandwidth, or packet loss; users in a region seeing service interruptions maps to blocked traffic or a network partition; private content made public maps to missing ACLs in configurations. Black-box monitoring is symptom-oriented and represents active problems. White-box monitoring depends on the ability to inspect the innards of the system with instrumentation and allows detection of imminent problems such as failures masked by retries. If web servers seem slow on database-heavy requests, you need to know both how fast the web server perceives the database to be, and how fast the database believes itself to be. Otherwise, you can\u2019t distinguish an actually slow database server from a network problem between your web server and your database. The four golden signals of monitoring: Latency - time it takes to service a request, and distinguish between successful and failed requests Traffic - service demand on the system, such as high level requests per second broken down by nature of request Errors - rate of requests that fail explicitly, implicitly (succeed but delivers wrong content), or by policy (rate limit policy) Saturation - how \"full\" the service is serving from its capacity. many systems degrade in performance before they achieve 100% utilization, utilization target should be set appropriately below 100% saturation is also concerned with predictions of impending saturation, i.e. at current rate, the disk will fill in about 4 hours. The simplest way to differentiate between a slow average and a very slow \"tail\" of requests is to collect request counts bucketed by latencies (rendering a histogram). Distributing the histogram boundaries approximately exponentially. Monitoring Resolutions \u00b6 Different aspects of a system should be measured with different levels of granularity. Very frequent measurements may be very expensive to collect, store, and analyze. As an example for an alternative way for high-granularity metric data: record the metric value each second use buckets of 5% granularity, increment the appropriate metric bucket each second aggregate those values every minute This strategy allows observing brief CPU hotspots without incurring very high cost due to collection and retention. Design your monitoring system with an eye toward simplicity: rules for alerting for incidents should be as simple, predictable, and reliable as possible data collection, aggregation, and alerting configuration that is rarely exercised should be up for removal signals that are collected, but not exposed in any prebaked dashboard nor used by any alert, are candidates for removal alerts should be actionable. If some condition is not actionable then they should not be alerts. Add filters to apply non-actionable conditions alerts should reflect its urgenciness and it should be easy to tell if it can wait until working hours for remediation make sure the same cause does not page multiple teams, causing duplicate work. Others can be informed but not paged. Alerts should page the most relevant teams. In Google\u2019s experience, basic collection and aggregation of metrics, paired with alerting and dashboards, has worked well as a relatively standalone system. It\u2019s better to spend much more effort on catching symptoms than causes ; when it comes to causes, only worry about very definite, very imminent causes. Actionable alerts should result in work to automate the actions or fix real issues. While short-term fix can be acceptable, long-term fix should also be tracked instead of getting forgotten and regression happens. Pages with rote, algorithmic responses should be a red flag. Achieving a successful on-call rotation and product includes choosing to alert on symptoms or imminent real problems, adapting your targets to goals that are actually achievable, and making sure that your monitoring supports rapid diagnosis. Automation \u00b6 Software-based automation is superior to manual operation in most circumstances, although doing automation thoughtlessly can create as many problems as it solves. Value of automation \u00b6 scales with the system consistency in executing procedures and results can evolve into a platform, extensible for more use cases and for profit, and adding monitoring and extract metrics faster than manual work, reduce mean time to repair (MTTR) and time to action massive amount of human time savings For truly large services, the factors of consistency, quickness, and reliability dominate most conversations about the trade-offs of performing automation. Reliability is the fundamental feature, and autonomous, resilient behavior is one useful way to get that. Use cases \u00b6 manage machines user accounts cluster scaling up and down software/hardware installation, provision, or decommission new software version rollout runtime configuration updates runtime dependency updates etc. A hierarchy of automation classes, taking service failover as an example: no automation, manual failover that touches multiple places externally maintained specific automation, i.e. a script owned by some SRE engineer to do failover one specific system externally maintained generic automation, a generic tool that can be used to failover any system that is properly onboarded internally maintained specific automation, i.e. a database's own failover mechanism that can be used for failover, but managed by the database owner system that needs no automation, or automatically detects faults and carries out the failover Automation needs to be careful about relying on implicit \"safety\" signals or it can make unwanted changes that may potentially harm your system. Case studies \u00b6 Google Ads SRE - MySQL on Borg : Borg's infrastructure does frequent restarts and shifting jobs around to optimize resource utilization. MySQL instances gets interrupted a lot on Borg. Quick failover becomes a requirement. The SRE team developed a daemon to automate the failover process and made the system highly available on Borg. It did come with a cost that all MySQL dependencies must implement more failure-handling logic in their code. The win is still obvious in hardware savings (60%), and hands-free maintenance. Cluster Infra SRE - Cluster Turnups : Setting up new clusters for large services such as BigTable is a long and complex process. Early automation focused on accelerating cluster delivery through scripted SSH steps to distribute packages and initialize services. Flags for fine-tuning configurations get added later on which caused wasted time in spotting misconfigurations causing out-of-memory fails. Prodtests gets introduced to allow unit testing of real-world services to verify the cluster configurations. New bugs found extends the prodtests set. With this it is possible to predict the time for a cluster to go from \"network-ready\" to \"live-traffic-ready\". With thousands of shell scripts owned by dozens of teams, reducing the turnup time to one week becomes hard to do, as bugs found by the unit tests takes time to be fixed. Then the idea of \"code fixing misconfigurations\" arose. So each test is paired with a fix, and each fix is idempotent and safe to resolve; which means teams must be comfortable to run the fix-scans every 15 minute without fearing anything. The flaws in this process are that: 1) the latency between a test -> fix -> another test sometimes introduced flaky tests; 2) not all tests are idempotent and a flaky test with fix may render the system in an inconsistent state; 3) test code dies when they are not in sync with the codebase that it covers Due to some security requirement, the Admin Server becomes a mandate of service teams' workflows. SREs moved from writing shell scripts in their home directories to building peer-reviewed RPC servers with fine-grained ACLs. It becomes clear that the turnup processes had to be owned by the teams that owned the services. A Service-Oriented Architecture is built around the Admin Server: Admin Server to handle cluster turnup/turndown RPCs, while each team would provide the contract (API) that the turnup automation needed, while still being free to change the underlying implementation. Release Engineering \u00b6 Release engineering can be concisely described as building and delivering software. It touches on source code management, compilers, build configuration languages, automated build tools, package managers, and installers. It requires skills from domains: development, configuration management, test integration, system administration, and customer support. At Google. Release engineers work with software engineers (SWEs) in product development and SREs to define all the steps required to release software\u2014from how the software is stored in the source code repository, to build rules for compilation, to how testing, packaging, and deployment are conducted. Release engineers define best practices for using internal tools in order to make sure projects are released using consistent and repeatable methodologies. Teams should budget for release engineering resources at the beginning of the product development cycle. It\u2019s cheaper to put good practices and process in place early, rather than have to retrofit your system later. It is essential that the developers, SREs, and release engineers work together. Philosophy \u00b6 Release engineering is guided by four major principles: Self-Service Model teams must be self-sufficient to achieve high release velocity release processes can be automated invole engineer only when problems arise High Velocity frequent releases with fewer changes between versions better testing and troubleshooting, less changes to look through between releases can accumulate builds then pick a version to deploy, or simply \"push on green\" Hermetic Builds build tools must ensure consistency and repeatability build process is self-contained and does not rely on external services outside the build environment Enforcement of Policies and Procedures gate operations ensure security and access control approve merging code changes release process actions selection creating a new release reploying a new release updates build configuration report what has changed in a release speeds up troubleshooting CI, CD \u00b6 Goolge's software lifecycle: Building binaries, define build targets also saves build date, revision number, and build id for record-keeping Branching all code branches off from the main source code tree major projects branch from mainline at a specific revision and never merge from mainline again bug fixes in mainline are cherry-picked into the project branch for inclusion in releases Testing mainline runs unit tests at each submitted change create releases at the revision number of last continuous test build that completed all tests release re-run the unit tests and create an audit trail for all tests passed Packaging software is distributed via Midas Package Manager (MPM) packaged files along with owners and permissions, named, versioned with unique hash, labeled, and signed for authenticity labels are used to indicate the environment intended for that package, i.e. dev, canary, or production Rapid the CI/CD platform. Blueprints configures and defines the build and test targets, rule for deployment, and admin information role-based access control determine who can perform actions on a project compilation and testing occur in parallel and each in their dedicated environments artifacts then gets through system testing and canary deployments each step results are logged, a report is created for what changed since last release Deployment for more compicated deployments, Sisyphus kicks in as a general-purpose rollout automation framework Rapid creates a rollout in a long-running Sisyphus job, and pass on the MPM package with the versioned build label rollout can be simple fan out or progressive depending on the service's risk profile Configuration Management \u00b6 Although sounds simple, configuration changes are a potential source of instability. Configuration management at Google requires storing configuration in the source code repository and enforcing a strict code review. Here are some strategies: update configuration at the mainline decouples binary releases from configuration changes may lead to skew between checked-in version and running version of configuration files pack configuration files with binary files built into MPM package simplifies deployment, limits flexibility pack configuration files as configuration MPMs dedicated MPM package for configuration files can pair with a binary MPM, both package can be built independently then deployed together within the same release read configuration from an external store good for projects that need frequent or dynamically updated configurations Simplicity \u00b6 Changes have a side effect of introducing bugs and instability to the system. A good summary of the SRE approach to managing systems is: \"At the end of the day, our job is to keep agility and stability in balance in the system.\" SREs work to create procedures, practices, and tools that render software more reliable. At the same time, SREs ensure that this work has as little impact on developer agility as possible. Reliable processes tend to actually increase developer agility: rapid, reliable production rollouts make changes in production easier to see. Once a bug surfaces, it takes less time to find and fix that bug. \"Boring\" virtue \u00b6 \"Borning\" is a desireable property when it comes to software source code. The lack of excitement, suspense, and puzzles, and minimizing accidental complexity helps predictably accomplish the software's business goals. Remove bloat \u00b6 When code are bound to delete, delete them, never do commenting them out, or put a flag and hope they can be used at some point later. SRE should promote practices that ensure existing code all serve the essential purpose, routinely removing dead code, and building bloat detection into all levels of testing. Software bloat are the tendency of software to become slower and bigger over time as a result of constant additional features. A smaller project is easier to understand, easier to test, and frequently has fewer defects. Minimal, Modularity, Simple Release \u00b6 The ability to make changes to parts of the system in isolation is essential to creating a supportable system. Loose coupling between binaries, or between binaries and configuration, is a simplicity pattern that simultaneously promotes developer agility and system stability. Versioning APIs allows developers to continue to use the version that their system depends upon while they upgrade to a newer version in a safe and considered way. One of the central strengths and design goals of Google\u2019s protocol buffers was to create a wire format that was backward and forward compatible. Writing clear, minimal APIs is an essential aspect of managing simplicity in a software system. Prefer simple releases. It is much easier to measure and understand the impact of a single change rather than a batch of changes released simultaneously. Best Practices \u00b6 Successfully operating a service entails a wide range of activities: developing monitoring systems, planning capacity, responding to incidents, ensuring the root causes of outages are addressed, and so on. A healthy way to operate a service permits self-actualization and takes active control of the direction of the service rather than reactively fights fires. Service Reliability Hierarchy: the elements that go into making a service reliable: Monitoring - you want to be aware of problems before your users notice them Incident Response - it is a tool we use to achieve our larger mission and remain in touch with how distributed computing systems actually work and fail Postmortem and RCA - building a blameless postmortem culture is the first step in understanding what went wrong and prevent same issue gets popped up Testing - offer some assurance that our software isn\u2019t making certain classes of errors before it\u2019s released Capacity Planning - how requests are load-balanced and potential overload handled, prevent cascading failures Development - large-scale system design and implementation Product - reliable product launched at scale Practical Alerting \u00b6 Monitoring is the fundamental element to running a stable service. Monitoring a large system is challenging: the sheer number of components to analyze maintain low maintenance burden on engineers responsible for the system The Borgmon story \u00b6 Borgmon is the monitoring system for the job scheduling infrasture. It relies on a common data exposition format which allowed mass data collection with low overheads and avoids the costs of subprocess execution and network connection setup. The data is used both for rendering charts and creating alerts. The history of the collected data can be used for alert computation as well. A Borgmon can collect from other Borgmon, so it can build hierarchies that follow the topology of the service, aggregating and summarizing information and discarding some strategically at each level. Some very large services shard below the cluster level into many scraper Borgmon, which in turn feed to the cluster-level Borgmon. App instrumentation \u00b6 Borgmon used a format to export metrics in plain text as space-separated keys and values, one metric per line. Adding a metric to a program only requires a single declaration in the code where the metric is needed. The decoupling of the variable definition from its use in Borgmon rules requires careful change management, and this trade-off has been offset with proper tools to validate and generate monitoring rules. Borgmon uses service discover to figure out the targets to scrape metric data from. The target list is dynamic whic hallows the monitoring to scale automatically. Additional \"synthetic\" metrics variables for each target helps detect if the monitored tasks are unavailable. Time-series data storage \u00b6 Borgmon stores all the data in an in-memory database, regularly checkpointed to disk. Data points are of form (timestamp, value) , and stored in chronological lists aka time-series. Each time-series is named by a unique set of labels of form name=value . A time-series is conceptually a one-dimensional matrix of numbers, progressing through time. As you add permutations of labels to this time-series, the matrix becomes multidimensional. In practice, the structure is a fixed-sized block of memory, known as the time-series arena, with a garbage collector that expires the oldest entries once the arena is full. The time interval between the most recent and oldest entries in the arena is the horizon, which indicates how much queryable data is kept in RAM. Periodically, the in-memory state is archived to an external system known as the Time-Series Database (TSDB). Borgmon can query TSDB for older data and, while slower, TSDB is cheaper and larger than a Borgmon\u2019s RAM. Time-series are stored as sequences of numbers and timestamps, which are referred to as vectors. The name of a time-series is a labelset. To make a time-series identifiable, it must have labels: var - name of the variable/metric job - type of server being monitored service - collection of jobs that provide a service to users zone - datacenter location/region Together these label variables appear like this {var=http_requests,job=webserver,instance=host0:80,service=web,zone=us-west}[10m] called variable expression. A search for a labelset returns all matching time-series in a vector and does not require all labels to be specified. A duration can be specified to limit the range of data to query from. Rules \u00b6 Borgmon rules consists of simple algebraic expressions that compute time-series from other time-series. Rules run in a parallel threadpool where possible. Borgmon rules create new time-series, so the results of the computations are kept in the time-series arena and can be inspected just as the source time-series are. The ability to do so allows for ad hoc querying, evaluation, and exploration as tables or charts. Aggregation is the cornerstone of rule evaluation in a distributed environment. A counter is any monotonically non-decreasing variable. Gauges may take any value they like. Alerting \u00b6 When an alerting rule is evaluated by a Borgmon, the result is either true, in which case the alert is triggered, or false. Alerts sometimes toggle their state quickly, thus the rules allow a minimum duration (at least two rule evalutation cycles) for which the alerting rule must be true before the alert is sent. The alert rule allows templating for filling out a message template with contextual information when the alert fires and sent to alerting RPC. Alerts get first sent as \"triggering\" and then as \"firing\". The Alertmanager is responsible for routing the alert notification to the correct destination, and alerts will be dedupted, snoozed, or fan out/in base on the labelsets. Sharding Monitoring \u00b6 A Borgmon can import time-series data from other Borgmon. To avoid scaling bottlenecks, a streaming protocol is used to transmit time-series data between Borgmon. Such deployment uses two or more global Borgmon for top-level aggregation and one Borgmon in each datacenter to monitor all the jobs running at that location. Upper-tier Borgmon can filter the data they want to stream from the lower-tier Borgmon, so that the global Borgmon does not fill its arena with all the per-task time-series from the lower tiers. Thus, the aggregation hierarchy builds local caches of relevant time-series that can be drilled down into when required. Black-box monitoring \u00b6 Borgmon is a white-box monitoring system\u2014it inspects the internal state of the target service, and the rules are written with knowledge of the internals in mind. Prober is used to run a protocol check against a target and reports success or failure. Prober can also validate the response payload of the protocol to verify that the contents are expected, and even extract and export values as time-series. The prober can send alerts directly to Alertmanager, or its own varz can be collected by a Borgmon. Teams often use Prober to export histograms of response times by operation type and payload size so that they can slice and dice the user-visible performance. Prober is a hybrid of the check-and-test model with some richer variable extraction to create time-series. Prober can be pointed at either the frontend domain or behind the load balancer to know that traffic is still served when a datacenter fails, or to quickly isolate an edge in the traffic flow graph where a failure has occurred. Configuration \u00b6 Borgmon configuration separates the definition of the rules from the targets being monitored. The same sets of rules can be applied to many targets at once, instead of writing nearly identical configuration over and over. Borgmon also supports language templates to reduce repetition and promote rules reuse. Borgmon adds labels indicating the target\u2019s breakdown (data type), instance name (source of data), and the shard and datacenter (locality or aggregation) it occupies, which can be used to group and aggregate those time-series together. The templated nature of these libraries allows flexibility in their use. The same template can be used to aggregate from each tier. Borgmon of Ten Years \u00b6 Borgmon transposed the model of check-and-alert per target into mass variable collection and a centralized rule evaluation across the time-series for alerting and diagnostics. This decoupling allows the size of the system being monitored to scale independently of the size of alerting rules. New applications come ready with metric exports in all components and libraries to which they link, and well-traveled aggregation and console templates, which further reduces the burden of implementation. Ensuring that the cost of maintenance scales sublinearly with the size of the service is key to making monitoring (and all sustaining operations work) maintainable. This theme recurs in all SRE work, as SREs work to scale all aspects of their work to the global scale. The idea of treating time-series data as a data source for generating alerts is now accessible to everyone through those open source tools like Prometheus, Riemann, Heka, and Bosun. On-Calls \u00b6 Historically, On-calls in IT context are performed by dedicated Ops teams tasked with the primary responsibility of keeping the service(s) for which they are responsible in good health. The SRE teams are quite different from purely operational teams in that they place heavy emphasis on the use of engineering to approach problems that exist at a scale and would be intractable without software engineering solutions. Duty \u00b6 When on-call, an engineer is available to triage the problem and perform operations on production systems possibly involving other team members and escalating as needed, within minutes. Typical values are 5 minutes for user-facing or otherwise highly time-critical services, and 30 minutes for less time-sensitive systems, depending on a service's SLO. Nonpaging production events, such as lower priority alerts or software releases, can also be handled and/or vetted by the on-call engineer during business hours. Many teams have both a primary and a secondary on-call rotation to serve as a fall-through for the pages. It is also common for two related teams to serve as secondary on-call for each other, with fall-through handling duties, rather than keeping a dedicated secondary rotation. Balance \u00b6 The quantity of on-call can be calculated by the percent of time spent by engineers on on-call duties. The quality of on-call can be calculated by the number of incidents that occur during an on-call shift. Google strive to invest at least 50% of SRE time into engineering: of the remainder, no more than 25% can be spent on-call, leaving up to another 25% on other types of operational, nonproject work. Using the 25% rule to derive the minimum number of SREs requried to sustain a 24/7 on-call rotation. Prefer a multi-site team of on-call shifts for these reasons: a multi-site \"follow the sun\" rotation allows teams to avoid night shifts altogether limiting the number of engineers in the on-call rotation ensures that engineers do not lose touch with the production systems For each on-call shift, an engineer should have sufficient time to deal with any incidents and follow-up activities such as writing postmortems. Feeling Safe \u00b6 It\u2019s important that on-call SREs understand that they can rely on several resources that make the experience of being on-call less daunting than it may seem. The most important on-call resources are: Clear escalation paths Well-defined incident-management procedures A blameless postmortem culture Look into adopting a formal incident-management protocol that offers an easy-to-follow and well-defined set of steps that aid an on-call engineer to rationally pursue a satisfactory incident resolution with all the required help. Build or adopt tools that automates most of the incident management actions, so the on-call engineer can focus on dealing with the incident, rather than spending time and cognitive effort on mundane actions such as formatting emails or updating several communication channels at once. When an incident occurs, it\u2019s important to evaluate what went wrong, recognize what went well, and take action to prevent the same errors from recurring in the future. SRE teams must write postmortems after significant incidents and detail a full timeline of the events that occurred. Mistakes happen, and software should make sure that we make as few mistakes as possible. Recognizing automation opportunities is one of the best ways to prevent human errors. Avoid Overload \u00b6 The SRE team and leadership are responsible for including concrete objectives in quarterly work planning in order to make sure that the workload returns to sustainable levels. Misconfigured monitoring is a common cause of operational overload. Paging alerts should be aligned with the symptoms that threaten a service\u2019s SLOs. All paging alerts should also be actionable. Low-priority alerts that bother the on-call engineer every hour (or more frequently) disrupt productivity, and the fatigue such alerts induce can also cause serious alerts to be treated with less attention than necessary. It is also important to control the number of alerts that the on-call engineers receive for a single incident, regulate the alert fan-out by ensuring that related alerts are grouped together by the monitoring or alerting system. Noisy alerts that systematically generate more than one alert per incident should be tweaked to approach a 1:1 alert/incident ratio. In extreme cases, SRE teams may have the option to \"give back the pager\"\u2014SRE can ask the developer team to be exclusively on-call for the system until it meets the standards of the SRE team in question. It is appropriate to negotiate the reorganization of on-call responsibilities with the development team, possibly routing some or all paging alerts to the developer on-call. Avoid Underload \u00b6 When SREs are not on-call often enough, they start losing confidence in operations touching the production, and creating knowledge gaps. SRE teams should be sized to allow every engineer to be on-call at least once or twice a quarter, thus ensuring that each team member is sufficiently exposed to production. Regular trainings and exercises should also be conducted to help improve troubleshooting skills and knowledge of the services. Effective Troubleshooting \u00b6 Be warned that being an expert is more than understanding how a system is supposed to work. Expertise is gained by investigating why a system doesn't work. While you can investigate a problem using only the generic process and derivation from first principles, it is less efficient and less effective than understanding how things are supposed to work. Theory \u00b6 Think of the troubleshooting process as an application of the hypothetico-deductive method: given a set of observations about a system and a theoretical basis for understanding system behavior, we iteratively hypothesize potential causes for the failure and try to test those hypotheses. We\u2019d start with a problem report telling us that something is wrong with the system. Then we can look at the system\u2019s telemetry and logs to understand its current state. This information, combined with our knowledge of how the system is built, how it should operate, and its failure modes, enables us to identify some possible causes. Avoid Pitfalls \u00b6 The following are common pitfalls to avoid: Looking at symptoms that aren\u2019t relevant or misunderstanding the meaning of system metrics Misunderstanding how to change the system, its inputs, or its environment, so as to safely and effectively test hypotheses Coming up with wildly improbable theories about what\u2019s wrong, or latching on to causes of past problems, reasoning that since it happened once, it must be happening again Hunting down spurious correlations that are actually coincidences or are correlated with shared causes Understanding failures in our reasoning process is the first step to avoiding them and becoming more effective in solving problems. A methodical approach to knowing what we do know, what we don\u2019t know, and what we need to know, makes it simpler and more straightforward to figure out what\u2019s gone wrong and how to fix it. In Practice \u00b6 Problem Report \u00b6 An effective problem report should tell you the expected behavior, the actual behavior, and, if possible, how to reproduce the behavior. Ideally, the reports should have a consistent form and be stored in a searchable location. It\u2019s common practice at Google to open a bug for every issue, even those received via email or instant messaging. Doing so creates a log of investigation and remediation activities that can be referenced in the future. Discourage reporting problems directly to a person, which introduces an additional step of transcribing the report into a bug, produces lower-quality reports that aren\u2019t visible to other members of the team, and tends to concentrate the problem-solving load on a handful of team members that the reporters happen to know, rather than the person currently on duty. Triage \u00b6 Problems can vary in severity: an issue might affect only one user under very specific circumstances, or it might entail a complete global outage for a service. Your response should be appropriate for the problem\u2019s impact and your course of action should be to make the system work as well as it can under the circumstances. For example, if a bug is leading to possibly unrecoverable data corruption, freezing the system to prevent further failure may be better than letting this behavior continue. Examine \u00b6 Graphing time-series and operations on time-series can be an effective way to understand the behavior of specific pieces of a system and find correlations that might suggest where problems began. Logging and exporting information about each operation and about system state makes it possible to understand exactly what a process was doing at a given point in time. Text logs are very helpful for reactive debugging in real time, while storing logs in a structured binary format can make it possible to build tools to conduct retrospective analysis with much more information. It\u2019s really useful to have multiple verbosity levels available, along with a way to increase these levels on the fly. This functionality enables you to examine any or all operations in incredible detail without having to restart your process, while still allowing you to dial back the verbosity levels when your service is operating normally. Exposing current state is the third trick. Google servers have endpoints that show a sample of RPCs recently sent or received, to help understand how any one server is communicating with others without referencing an architecture diagram. These endpoints also show histograms of error rates and latency for each type of RPC, their current configuration or allow examination of their data. Lastly, you may even need to instrument a client to experiment with, in order to discover what a component is returning in response to requests. Diagnose \u00b6 Ideally, components in a system have well-defined interfaces and perform known transformations from their input to their output. It's then possible to look at the data flows between components to determine whether a given component is working properly. Injecting known test data in order to check that the resulting output is expected at each step can be especially effective (a form of black-box testing). Dividing and conquering is a very useful general-purpose solution technique. An alternative, bisection, splits the system in half and examines the communication paths between components on one side and the other. Finding out what a malfunctioning system is doing (symptom), then asking why (cause of symptom) it\u2019s doing that and where (locate the code) its resources are being used or where its output is going can help you understand how things have gone wrong and forge a solution. A working computer system tends to remain in motion until acted upon by an external force, such as a configuration change or a shift in the type of load served. Recent changes to a system can be a productive place to start identifying what\u2019s going wrong. Correlating changes in a system\u2019s performance and behavior with other events in the system and environment can also be helpful in constructing monitoring dashboards, i.e. annotate a graph showing the system\u2019s error rates with the start and end times of a deployment of a new version. Test and Treat \u00b6 Using the experimental method, we can try to rule in or rule out our hypothetic list of possible causes. Following the code and trying to imitate the code flow, step-by-step, may point to exactly what\u2019s going wrong. Consider these when designing tests: a test should have mutually exclusive alternatives, so that it can rule one group of hypotheses in and rule another set out obvious first: perform the tests in decreasing order of likelihood an experiment may provide misleading results due to confounding factors i.e. firewall rules against your workstation but not for the application server side effects that change future test results if you performed active testing by changing a system\u2014for instance by giving more resources to a process\u2014making changes in a systematic and documented fashion will help you return the system to its pre-test setup tests can be suggestive rather than definitive i.e. it can be very difficult to make race conditions or deadlocks happen in a timely and reproducible manner Negative Results \u00b6 Negative results should not be ignored or discounted. Realizing you\u2019re wrong has much value: a clear negative result can resolve some of the hardest design questions. Often a team has two seemingly reasonable designs but progress in one direction has to address vague and speculative questions about whether the other direction might be better. Experiments with negative results are conclusive. They tell us something certain about production, or the design space, or the performance limits of an existing system. They can help others determine whether their own experiments or designs are worthwhile. Microbenchmarks, documented antipatterns, and project postmortems all fit this category. Tools and methods can outlive the experiment and inform future work. As an example, benchmarking tools and load generators can result just as easily from a disconfirming experiment as a supporting one. Publishing negative results improves our industry\u2019s data-driven culture. Accounting for negative results and statistical insignificance reduces the bias in our metrics and provides an example to others of how to maturely accept uncertainty. Cure \u00b6 Often, we can only find probable root cause factors than definitive factor, given that a production system can be complex and reproducing the problem in a live production system may not be an option. Once you\u2019ve found the factors that caused the problem, it\u2019s time to write up notes on what went wrong with the system, how you tracked down the problem, how you fixed the problem, and how to prevent it from happening again (a postmortem). Emergency Response \u00b6 What to Do When Systems Break? Don't panic. If you can\u2019t think of a solution, cast your net farther. Involve more of your teammates, seek help, do whatever you have to do, but do it quickly. The highest priority is to resolve the issue at hand quickly. Oftentimes, the person with the most state is the one whose actions somehow triggered the event. Utilize that person. Keep a History of Outages. History is about learning from everyone\u2019s mistakes. Be thorough, be honest, but most of all, ask hard questions. Look for specific actions that might prevent such an outage from recurring, not just tactically, but also strategically. Hold yourself and others accountable to following up on the specific actions detailed in these postmortems. Doing so will prevent a future outage that\u2019s nearly identical to, and caused by nearly the same triggers as, an outage that has already been documented. Managing Incidents \u00b6","title":"Google SRE Book"},{"location":"Reading-Notes/Google-SRE-Book/#concept-of-dev-ops-and-devops","text":"This systems administrator, or sysadmin, assembles existing software components (developed by the developers) and deploying them to produce a service, runs the service, and responds to events and updates as they occur. These tasks generally fall into the category of operations. Developers and sysadmins are therefore divided into discrete teams: \"development/dev\" and \"operations/ops\". As the system grows in complexity and traffic volume, generating a corresponding increase in events and updates, the sysadmin team grows to absorb the additional work and increases operational cost, which is a direct cost. The indirect costs arise from the fact that the two teams are quite different in background, skill set, and incentives. They use different vocabulary to describe situations; they carry different assumptions about both risk and possibilities for technical solutions; they have different assumptions about the target level of product stability. At their core, the development teams want to launch new features and see them adopted by users; the ops teams want to make sure the service doesn\u2019t break while they are holding the pager. DevOps is a set of principles guiding software development: involvement of the IT function in each phase of a system\u2019s design and development, heavy reliance on automation versus human effort, the application of engineering practices and tools to operations tasks.","title":"Concept of Dev, Ops, and DevOps"},{"location":"Reading-Notes/Google-SRE-Book/#sre","text":"","title":"SRE"},{"location":"Reading-Notes/Google-SRE-Book/#how-sre-differs-from-devops","text":"DevOps's principles and practices are consistent with Site Reliability Engineering. DevOps is a generalization of several core SRE principles to a wider range of organizations, management structures, and personnel. In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their service(s). Site Reliability Engineering teams focus on hiring software engineers to deploy and run products and to create systems to automate work that otherwise would be performed manually. This structure also improves the product development teams: easy transfers between product development and SRE teams cross-train the entire group. UNIX system internals and networking (Layer 1 to Layer 3) expertise are the two most common types of alternate technical skills for software engineers hired into SRE roles. A diverse background of the SRE team frequently results in clever, high-quality systems. Google places a 50% cap on the aggregate \"ops\" work for all SREs\u2014tickets, on-call, manual tasks to ensure SRE team can focus on engineering work in building or improving existing systems. Eliminating toil is one of SRE\u2019s most important tasks. Systems should be automatic, not just automated. They should run and repair themselves. Postmortems should be written for all significant incidents, regardless of whether or not they paged; postmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps. Google operates under a blame-free postmortem culture, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing exposing them.","title":"How SRE Differs from DevOps"},{"location":"Reading-Notes/Google-SRE-Book/#error-budget","text":"Product development and SRE teams can enjoy a productive working relationship by eliminating the structural conflict in their respective goals between pace of innovation and product stability, with an agreed error budget. A service that\u2019s 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service\u2019s error budget. 100% error budget is the wrong reliability target for basically everything. Setting the correct error budget is a product question and should consider: level of availability to keep user satisfied alternatives for unhappy users with given availability user's accessible features of product at different availability levels Spend error budget taking risks with things to launch. As soon as SRE activities are conceptualized in this framework, freeing up the error budget through tactics such as phased rollouts and 1% experiments can optimize for quicker launches. An outage is no longer a \"bad\" thing\u2014it is an expected part of the process of innovation, and an occurrence that both development and SRE teams manage rather than fear.","title":"Error budget"},{"location":"Reading-Notes/Google-SRE-Book/#monitoring","text":"Monitoring allows service owners keep track of a system\u2019s health and availability. Traditionally, monitoring watches for a specific value or condition, and then triggers an alert to engineers when that value is exceeded or that condition occurs. Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action. Alerts should be sent when immediate actions need to be taken by an engineer. Tickets should be issued when engineers action is required but not immediately. Logging should be recorded for diagnostic or root-cause-analysis purposes.","title":"Monitoring"},{"location":"Reading-Notes/Google-SRE-Book/#oncall-emergency-response","text":"Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR). Humans add latency. A system that can avoid emergencies that require human intervention will have higher availability than a system that requires hands-on intervention. Thinking through and recording the best practices ahead of time in a \"Runbook\" that gives each possible fail case and steps for remediation, drastically improves the MTTR for incidents that involves human.","title":"Oncall, Emergency Response"},{"location":"Reading-Notes/Google-SRE-Book/#change-management","text":"Best practices implementing automation around changes: apply progressive rollouts detect problems quickly and accurately roll back changes safely and quickly upon detecting problems","title":"Change Management"},{"location":"Reading-Notes/Google-SRE-Book/#capacity-planning","text":"Demand forecasting and capacity planning can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future demand with the required availability. Capacity planning should take both organic growth (natural product adoption from users) and inorganic growth (feature launches, marketing campaigns) into account. Regular load testing of the system should be performed to correlate raw resource capacity to service capacity.","title":"Capacity Planning"},{"location":"Reading-Notes/Google-SRE-Book/#provisioning","text":"Provisioning combines both change management and capacity planning. Provisioning must be conducted quickly and only when necessary, as capacity is expensive. Adding new capacity often involves spinning up a new instance or location, making significant modification to existing systems (load balancers, networking, configurations changes), and validating that the new capacity performs and delivers correct results.","title":"Provisioning"},{"location":"Reading-Notes/Google-SRE-Book/#efficiency-and-performance","text":"Resource utilization provides insights of a service's efficiency, and is a function of demand (load), capacity, and software efficiency. SREs predict demand, provision capacity, and can modify the software. Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity. At some point, a slowing system stops serving, which corresponds to infinite slowness. SREs provision to meet a capacity target at a specific response speed (responses per second), and thus are keenly interested in a service\u2019s performance. SREs and product developers will (and should) monitor and modify a service to improve its performance, thus adding capacity and improving efficiency.","title":"Efficiency and Performance"},{"location":"Reading-Notes/Google-SRE-Book/#production-environment","text":"In a production environment, a machine refers to a piece of hardware or VM and a server refers to a piece of software that implements a service. Machines can run any server. Topology of a Google datacenter: Tens of machines are placed in a rack. Racks stand in a row. One or more rows form a cluster. Usually a datacenter building houses multiple clusters. Multiple datacenter buildings that are located close together form a campus.","title":"Production Environment"},{"location":"Reading-Notes/Google-SRE-Book/#managing-machines","text":"Borg is a distributed cluster operating system managing user jobs at the cluster level and allocate resources to jobs. Jobs can either be indefinitely running servers or batch processes like a MapReduce. Borg then continually monitors these tasks. If a task malfunctions, it is killed and restarted, possibly on a different machine. Borg allocates a name and index number to each task using the Borg Naming Service (BNS). Rather than using the IP address and port number, other processes connect to Borg tasks via the BNS name, which is translated to an IP address and port number by BNS.","title":"Managing Machines"},{"location":"Reading-Notes/Google-SRE-Book/#storage","text":"The storage layer is responsible for offering users easy and reliable access to the storage available for a cluster: the lowest layer is called D (disk), a fileserver running on almost all machines in a cluster Colossus layer is on top of D and creates a cluster-wide filesystem that offers usual filesystem semantics, as well as replication and encryption next layer are several database-like services Bigtable, NoSQL database handles petabyte-grade data. It is a sparse, distributed, persistent multidimensional sorted map that is indexed by row key, column key, and timestamp; each value in the map is an uninterpreted array of bytes. Bigtable supports eventually consistent, cross-datacenter replication. Spanner, SQL-like distributed database for real consistency across the world Blobstore","title":"Storage"},{"location":"Reading-Notes/Google-SRE-Book/#networking","text":"Instead of using \"smart\" routing hardware, Google rely on less expensive \"dumb\" switching components in combination with a central (duplicated) controller that precomputes best paths across the network. Google directs users to the closest datacenter with available capacity. Our Global Software Load Balancer (GSLB) performs load balancing on three levels: geographic DNS requests user service (i.e. YouTube, Google Maps) RPC level","title":"Networking"},{"location":"Reading-Notes/Google-SRE-Book/#locking","text":"The Chubby lock service provides a filesystem-like API for maintaining locks. It uses the Paxos protocol for asynchronous Consensus. Chubby helps master election and provides a consistent storage.","title":"Locking"},{"location":"Reading-Notes/Google-SRE-Book/#monitoring_1","text":"Borgmon monitoring proram regularly scrapes metrics from monitored servers. These metrics can be used instantaneously for alerting and also stored for use in historic overviews.","title":"Monitoring"},{"location":"Reading-Notes/Google-SRE-Book/#software-infrastructure","text":"Google software architecture is designed to make the most efficient use of the hardware infrastructure. Code is heavily multithreaded. Every server has an HTTP server that provides diagnostics and statistics for a given task. All of Google\u2019s services communicate using a Remote Procedure Call (RPC) infrastructure named Stubby (opensourced as gRPC). Often, an RPC call is made even when a call to a subroutine in the local program needs to be performed. This makes it easier to refactor the call into a different server if more modularity is needed, or when a server\u2019s codebase grows. A server receives RPC requests from its frontend and sends RPCs to its backend. Data is transferred to and from an RPC using protocol buffers (protobufs). Protocol buffers have many advantages over XML for serializing structured data: they are simpler to use, 3 to 10 times smaller, 20 to 100 times faster, and less ambiguous.","title":"Software infrastructure"},{"location":"Reading-Notes/Google-SRE-Book/#development-environment","text":"Development velocity is very important to Google. Google Software Engineers work from a single shared repository. If engineers encounter a problem in a component outside of their project, they can fix the problem, send the proposed changes (\"changelist/CL\", like a PR) to the owner for review, and submit (merge) the CL to the mainline. Changes to source code in an engineer\u2019s own project require a review. All software is reviewed before being submitted. When software is built, the build request is sent to build servers in a datacenter and executed in parallel. Each CL submition causes tests to run on all software that may depend on that CL. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.","title":"Development Environment"},{"location":"Reading-Notes/Google-SRE-Book/#shakespeare","text":"Shakespeare: a sample service at google","title":"Shakespeare"},{"location":"Reading-Notes/Google-SRE-Book/#principles","text":"A key principle of any effective software engineering, not only reliability-oriented engineering, simplicity is a quality that, once lost, can be extraordinarily difficult to recapture.","title":"Principles"},{"location":"Reading-Notes/Google-SRE-Book/#embracing-risk","text":"Cost does not increase linearly as reliability increments: an incremental improvement in reliability may cost 100x more than the previous increment. The cost comes from: redundant machine and computing resources opportunity cost for building systems or features usable by end users","title":"Embracing Risk"},{"location":"Reading-Notes/Google-SRE-Book/#measuring-risks","text":"SRE manages service reliability largely by managing risk, conceptualizes risk as a continuum, and identifies the appropriate level of risk tolerance for the services to run. Service failures can have many potential effects, including user dissatisfaction, harm, or loss of trust; direct or indirect revenue loss; brand or reputational impact; and undesirable press coverage. For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime and is expressed by service availability in terms of number of \"nines\": 99.9%, 99.99%, etc. The availability can be calculated as (uptime / uptime + downtime) or (successful requests / total requests). The same principles also apply to nonserving systems with minimal modification.","title":"Measuring risks"},{"location":"Reading-Notes/Google-SRE-Book/#risk-tolerance","text":"Product managers are charged with understanding the users and the business, and for shaping the product for success in the marketplace. To identify the risk tolerance of a service, SREs must work with the product owners to turn a set of business goals into explicit objectives that can be engineered. Factors to consider on consumer services: required level of avalilability users' expectation impact on our revenue, or our users' revenue paid or free service how much does our competitors set is the service target consumers or enterprises does different types of failures have different effects on the service business resilience on service downtime impact from low rate of failures, or occasional full-site outage planned downtime locate a service cost on the risk continuum what would be the increase in revenue and the cost to increase the availability move forward when the revenue increase offsets the implementation cost service metrics to account for understand the importance of metrics under different use cases and focus on those important ones","title":"Risk tolerance"},{"location":"Reading-Notes/Google-SRE-Book/#error-budgets","text":"Error budget provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability. The tensions between feature delivery and site reliability reflect themselves in different opinions about the level of effort that should be put into engineering practices: software fault tolerance testing release frequency canary duration and size The Service Level Objectives express error budget in a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter. Product Management defines an SLO of expected uptime for the quarter Monitoring system measure the actual uptime The difference between the budget and the unreliability happened tells the remaining budget for the quarter As long as there are budget remaining, new releases can be pushed For example, imagine that a service\u2019s SLO is to successfully serve 99.999% of all queries per quarter. This means that the service\u2019s error budget is a failure rate of 0.001% for a given quarter. If a problem causes us to fail 0.0002% of the expected queries for the quarter, the problem spends 20% of the service\u2019s quarterly error budget. If product development wants to skimp on testing or increase push velocity and SRE is resistant, the error budget guides the decision. When the budget is large, the product developers can take more risks. When the budget is nearly drained, the product developers themselves will push for more testing or slower push velocity, as they don\u2019t want to risk using up the budget and stall their launch.","title":"Error Budgets"},{"location":"Reading-Notes/Google-SRE-Book/#service-level-terminology","text":"Indicators (SLI) - a carefully defined quantitative measure of some aspect of the level of service that is provided. i.e. request latency, error rate, system throughput or rps. These metrics are often aggregated into a rate, average, or percentile. Understanding of what your users want from the system will inform the judicious selection of a few indicators: user-facing: availability, latency, throughput storage systems: latency, availability, durability big data systems, data pipelines: throughput, end-to-end latency generally for all systems: correctness Objectives (SLO) - a target value or range of values for a service level that is measured by an SLI. For maximum clarity, SLOs should specify how they\u2019re measured and the conditions under which they\u2019re valid. i.e. 99% (averaged over 1 minute) of Get RPC calls will complete in less than 100 ms (measured across all the backend servers). Choose just enough SLOs to provide good coverage of your system\u2019s attributes. Get rid of unnecessary SLOs that cannot indicate priorities. Keep a safety margin , use a tighter internal SLO than the SLO advertised to users gives you room to respond to chronic problems before they become visible externally. You can always refine SLO definitions and targets over time as you learn about a system\u2019s behavior. Start loose and tighten it later. Don't overachieve . If your service\u2019s actual performance is much better than its stated SLO, users will come to rely on its current performance. Understanding how well a system is meeting its expectations helps decide whether to invest in making the system faster, more available, and more resilient, on tasks such as paying off tech debts, adding new features, introducing other tools, workflows, or products. Agreements (SLA) - an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain. The consequences are most easily recognized when they are financial: a rebate or a penalty\u2014but they can take other forms. SLAs are closely tied to business and product decisions. It is wise to be conservative in what you advertise to users, as the broader the constituency, the harder it is to change or delete SLAs that prove to be unwise or difficult to work with.","title":"Service Level Terminology"},{"location":"Reading-Notes/Google-SRE-Book/#indicators-processing","text":"Indicator metrics are most naturally gathered on the server side using a monitoring system or with periodic log analysis. Some should be instrumented with client-side collection, because not measuring behavior at the client can miss a range of problems that affect users but don\u2019t affect server-side metrics. Most metrics are better thought of as distributions or percentiles rather than averages. For example, a simple average latency metric aggregated over a fixed window can obscure these tail latencies, as well as changes in them. Alerting based only on the average latency would show no change in behavior over the course of the day, when there are in fact significant changes in the tail latency. User studies have shown that people typically prefer a slightly slower system to one with high variance in response time. Indicators should be standardized to avoid repeated reasoning each time they are needed. i.e. make it a template: aggregation window/interval size: 1 minute aggregation region: all within a cluster measurement frequency: every 10 seconds requests measured: HTTP GETs on xxx API data collection: monitoring framework measured at the server data access latency: time to last byte received since request sent","title":"Indicators processing"},{"location":"Reading-Notes/Google-SRE-Book/#eliminate-toil","text":"Overhead is often work not directly tied to running a production service, and includes tasks like team meetings, setting and grading goals, snippets, and HR paperwork. Toil is the kind of work tied to running a production service that tends to be: manual, such as manually running some commands or a script that completes some task repetitive, the work you do over and over manually automatable, when machines can do the task as well as human without relying on a human's judgement for choices tactical, interrupt-driven and reactive, rather than strategy-driven and proactive. i.e. pager duty is inevitable but with opportunity to minimize no enduring value, so the service remains in the same performance state after you have finished a task scales linearly as a service grows. Ideally the service should be able to grow by one order of magnitude with zero manual work to add resource Toil tends to expand if left unchecked and can quickly fill 100% of everyone\u2019s time. SRE's engineering work either reduce future toil or add service features. Feature development typically focuses on improving reliability, performance, or utilization, which often reduces toil as a second-order effect. SREs report that their top source of toil is interrupts (that is, non-urgent service-related messages and emails). The next leading source is on-call (urgent) response, followed by releases and pushes.","title":"Eliminate Toil"},{"location":"Reading-Notes/Google-SRE-Book/#typical-sre-duties","text":"Software engineering - writing or extending code, design document, or software documentation. i.e. automation scripts, developing tools or frameworks, adding service features for scalability and reliability, hardening service with infrastructure code System engineering - configuring production systems, modifying configurations, or documenting systems runbooks i.e. setting up monitoring, load balancer configuration, server configuration, tuning OS parameters, productionization for dev teams Toil - repetitive work in running a service i.e. mentioned in above section Overhead - administrative work not tied directly to running a service i.e. hiring, HR paperwork, trainings, meetings, peer reviews and self-assess Toil isn\u2019t always and invariably bad, and some amount of toil is unavoidable in any engineering role. Small amount of toil can be calming and rewards with a sense of quick wins. Toil becomes toxic when experienced in large quantities. If we all commit to eliminate a bit of toil each week with some good engineering, we\u2019ll steadily clean up our services, and we can shift our collective efforts to engineering for scale, architecting the next generation of services, and building cross-SRE toolchains.","title":"Typical SRE duties"},{"location":"Reading-Notes/Google-SRE-Book/#monitoring_2","text":"Monitoring terminologies: monitoring - collecting, processing, aggregating, and displaying real-time quantitative data about a system white-box monitoring - base on metrics exposed by the internals of the system, such as logs black-box monitoring - testing externally visible behavior as a user would see it, such as end-to-end testing dashboard - an application provides a summary view of a service's core metrics alert - a notification intended to be read by a human and that is pushed to a system such as a bug or ticket queue, an email alias, or a pager root cause - a defect in a software or human system that, if repaired, instills confidence that this event won\u2019t happen again in the same way node, machine - used interchangeably to indicate a single instance of a running kernel in either a physical server, virtual machine, or container push - any change to a service\u2019s running software or its configuration. Monitoring helps: analyzing long-term trends comparing over time or experiment groups alerting building dashboards ad hoc retrospective analysis business analytics security analysis Effective alerting systems have good signal and very low noise (unnecessary pages). Rules that generate alerts for humans should be simple to understand and represent a clear failure. Paging a human is a quite expensive use of an employee's time. When meaningless pages occur too frequently, employees second-guess, skim, or even ignore incoming alerts. Outages can be prolonged because other noise interferes with a rapid diagnosis and fix. Every page should be about a novel problem and actionable and require intelligence. Someone should find and eliminate the root causes of the problem. Pages with rote, algorithmic responses should be a red flag. Unwillingness on the part of your team to automate such pages implies that the team lacks confidence that they can clean up their technical debt (i.e. put in quick short term fixes to automate the page response, but plan for a long term fix). This is a major problem worth escalating.","title":"Monitoring"},{"location":"Reading-Notes/Google-SRE-Book/#symptoms-vs-causes","text":"Your monitoring system should address two questions: what\u2019s broken, and why? For example, high 5xxs maps to a part of service is down or behaving unexpectedly; high latency maps to one of overloaded server, low network bandwidth, or packet loss; users in a region seeing service interruptions maps to blocked traffic or a network partition; private content made public maps to missing ACLs in configurations. Black-box monitoring is symptom-oriented and represents active problems. White-box monitoring depends on the ability to inspect the innards of the system with instrumentation and allows detection of imminent problems such as failures masked by retries. If web servers seem slow on database-heavy requests, you need to know both how fast the web server perceives the database to be, and how fast the database believes itself to be. Otherwise, you can\u2019t distinguish an actually slow database server from a network problem between your web server and your database. The four golden signals of monitoring: Latency - time it takes to service a request, and distinguish between successful and failed requests Traffic - service demand on the system, such as high level requests per second broken down by nature of request Errors - rate of requests that fail explicitly, implicitly (succeed but delivers wrong content), or by policy (rate limit policy) Saturation - how \"full\" the service is serving from its capacity. many systems degrade in performance before they achieve 100% utilization, utilization target should be set appropriately below 100% saturation is also concerned with predictions of impending saturation, i.e. at current rate, the disk will fill in about 4 hours. The simplest way to differentiate between a slow average and a very slow \"tail\" of requests is to collect request counts bucketed by latencies (rendering a histogram). Distributing the histogram boundaries approximately exponentially.","title":"Symptoms vs. Causes"},{"location":"Reading-Notes/Google-SRE-Book/#monitoring-resolutions","text":"Different aspects of a system should be measured with different levels of granularity. Very frequent measurements may be very expensive to collect, store, and analyze. As an example for an alternative way for high-granularity metric data: record the metric value each second use buckets of 5% granularity, increment the appropriate metric bucket each second aggregate those values every minute This strategy allows observing brief CPU hotspots without incurring very high cost due to collection and retention. Design your monitoring system with an eye toward simplicity: rules for alerting for incidents should be as simple, predictable, and reliable as possible data collection, aggregation, and alerting configuration that is rarely exercised should be up for removal signals that are collected, but not exposed in any prebaked dashboard nor used by any alert, are candidates for removal alerts should be actionable. If some condition is not actionable then they should not be alerts. Add filters to apply non-actionable conditions alerts should reflect its urgenciness and it should be easy to tell if it can wait until working hours for remediation make sure the same cause does not page multiple teams, causing duplicate work. Others can be informed but not paged. Alerts should page the most relevant teams. In Google\u2019s experience, basic collection and aggregation of metrics, paired with alerting and dashboards, has worked well as a relatively standalone system. It\u2019s better to spend much more effort on catching symptoms than causes ; when it comes to causes, only worry about very definite, very imminent causes. Actionable alerts should result in work to automate the actions or fix real issues. While short-term fix can be acceptable, long-term fix should also be tracked instead of getting forgotten and regression happens. Pages with rote, algorithmic responses should be a red flag. Achieving a successful on-call rotation and product includes choosing to alert on symptoms or imminent real problems, adapting your targets to goals that are actually achievable, and making sure that your monitoring supports rapid diagnosis.","title":"Monitoring Resolutions"},{"location":"Reading-Notes/Google-SRE-Book/#automation","text":"Software-based automation is superior to manual operation in most circumstances, although doing automation thoughtlessly can create as many problems as it solves.","title":"Automation"},{"location":"Reading-Notes/Google-SRE-Book/#value-of-automation","text":"scales with the system consistency in executing procedures and results can evolve into a platform, extensible for more use cases and for profit, and adding monitoring and extract metrics faster than manual work, reduce mean time to repair (MTTR) and time to action massive amount of human time savings For truly large services, the factors of consistency, quickness, and reliability dominate most conversations about the trade-offs of performing automation. Reliability is the fundamental feature, and autonomous, resilient behavior is one useful way to get that.","title":"Value of automation"},{"location":"Reading-Notes/Google-SRE-Book/#use-cases","text":"manage machines user accounts cluster scaling up and down software/hardware installation, provision, or decommission new software version rollout runtime configuration updates runtime dependency updates etc. A hierarchy of automation classes, taking service failover as an example: no automation, manual failover that touches multiple places externally maintained specific automation, i.e. a script owned by some SRE engineer to do failover one specific system externally maintained generic automation, a generic tool that can be used to failover any system that is properly onboarded internally maintained specific automation, i.e. a database's own failover mechanism that can be used for failover, but managed by the database owner system that needs no automation, or automatically detects faults and carries out the failover Automation needs to be careful about relying on implicit \"safety\" signals or it can make unwanted changes that may potentially harm your system.","title":"Use cases"},{"location":"Reading-Notes/Google-SRE-Book/#case-studies","text":"Google Ads SRE - MySQL on Borg : Borg's infrastructure does frequent restarts and shifting jobs around to optimize resource utilization. MySQL instances gets interrupted a lot on Borg. Quick failover becomes a requirement. The SRE team developed a daemon to automate the failover process and made the system highly available on Borg. It did come with a cost that all MySQL dependencies must implement more failure-handling logic in their code. The win is still obvious in hardware savings (60%), and hands-free maintenance. Cluster Infra SRE - Cluster Turnups : Setting up new clusters for large services such as BigTable is a long and complex process. Early automation focused on accelerating cluster delivery through scripted SSH steps to distribute packages and initialize services. Flags for fine-tuning configurations get added later on which caused wasted time in spotting misconfigurations causing out-of-memory fails. Prodtests gets introduced to allow unit testing of real-world services to verify the cluster configurations. New bugs found extends the prodtests set. With this it is possible to predict the time for a cluster to go from \"network-ready\" to \"live-traffic-ready\". With thousands of shell scripts owned by dozens of teams, reducing the turnup time to one week becomes hard to do, as bugs found by the unit tests takes time to be fixed. Then the idea of \"code fixing misconfigurations\" arose. So each test is paired with a fix, and each fix is idempotent and safe to resolve; which means teams must be comfortable to run the fix-scans every 15 minute without fearing anything. The flaws in this process are that: 1) the latency between a test -> fix -> another test sometimes introduced flaky tests; 2) not all tests are idempotent and a flaky test with fix may render the system in an inconsistent state; 3) test code dies when they are not in sync with the codebase that it covers Due to some security requirement, the Admin Server becomes a mandate of service teams' workflows. SREs moved from writing shell scripts in their home directories to building peer-reviewed RPC servers with fine-grained ACLs. It becomes clear that the turnup processes had to be owned by the teams that owned the services. A Service-Oriented Architecture is built around the Admin Server: Admin Server to handle cluster turnup/turndown RPCs, while each team would provide the contract (API) that the turnup automation needed, while still being free to change the underlying implementation.","title":"Case studies"},{"location":"Reading-Notes/Google-SRE-Book/#release-engineering","text":"Release engineering can be concisely described as building and delivering software. It touches on source code management, compilers, build configuration languages, automated build tools, package managers, and installers. It requires skills from domains: development, configuration management, test integration, system administration, and customer support. At Google. Release engineers work with software engineers (SWEs) in product development and SREs to define all the steps required to release software\u2014from how the software is stored in the source code repository, to build rules for compilation, to how testing, packaging, and deployment are conducted. Release engineers define best practices for using internal tools in order to make sure projects are released using consistent and repeatable methodologies. Teams should budget for release engineering resources at the beginning of the product development cycle. It\u2019s cheaper to put good practices and process in place early, rather than have to retrofit your system later. It is essential that the developers, SREs, and release engineers work together.","title":"Release Engineering"},{"location":"Reading-Notes/Google-SRE-Book/#philosophy","text":"Release engineering is guided by four major principles: Self-Service Model teams must be self-sufficient to achieve high release velocity release processes can be automated invole engineer only when problems arise High Velocity frequent releases with fewer changes between versions better testing and troubleshooting, less changes to look through between releases can accumulate builds then pick a version to deploy, or simply \"push on green\" Hermetic Builds build tools must ensure consistency and repeatability build process is self-contained and does not rely on external services outside the build environment Enforcement of Policies and Procedures gate operations ensure security and access control approve merging code changes release process actions selection creating a new release reploying a new release updates build configuration report what has changed in a release speeds up troubleshooting","title":"Philosophy"},{"location":"Reading-Notes/Google-SRE-Book/#ci-cd","text":"Goolge's software lifecycle: Building binaries, define build targets also saves build date, revision number, and build id for record-keeping Branching all code branches off from the main source code tree major projects branch from mainline at a specific revision and never merge from mainline again bug fixes in mainline are cherry-picked into the project branch for inclusion in releases Testing mainline runs unit tests at each submitted change create releases at the revision number of last continuous test build that completed all tests release re-run the unit tests and create an audit trail for all tests passed Packaging software is distributed via Midas Package Manager (MPM) packaged files along with owners and permissions, named, versioned with unique hash, labeled, and signed for authenticity labels are used to indicate the environment intended for that package, i.e. dev, canary, or production Rapid the CI/CD platform. Blueprints configures and defines the build and test targets, rule for deployment, and admin information role-based access control determine who can perform actions on a project compilation and testing occur in parallel and each in their dedicated environments artifacts then gets through system testing and canary deployments each step results are logged, a report is created for what changed since last release Deployment for more compicated deployments, Sisyphus kicks in as a general-purpose rollout automation framework Rapid creates a rollout in a long-running Sisyphus job, and pass on the MPM package with the versioned build label rollout can be simple fan out or progressive depending on the service's risk profile","title":"CI, CD"},{"location":"Reading-Notes/Google-SRE-Book/#configuration-management","text":"Although sounds simple, configuration changes are a potential source of instability. Configuration management at Google requires storing configuration in the source code repository and enforcing a strict code review. Here are some strategies: update configuration at the mainline decouples binary releases from configuration changes may lead to skew between checked-in version and running version of configuration files pack configuration files with binary files built into MPM package simplifies deployment, limits flexibility pack configuration files as configuration MPMs dedicated MPM package for configuration files can pair with a binary MPM, both package can be built independently then deployed together within the same release read configuration from an external store good for projects that need frequent or dynamically updated configurations","title":"Configuration Management"},{"location":"Reading-Notes/Google-SRE-Book/#simplicity","text":"Changes have a side effect of introducing bugs and instability to the system. A good summary of the SRE approach to managing systems is: \"At the end of the day, our job is to keep agility and stability in balance in the system.\" SREs work to create procedures, practices, and tools that render software more reliable. At the same time, SREs ensure that this work has as little impact on developer agility as possible. Reliable processes tend to actually increase developer agility: rapid, reliable production rollouts make changes in production easier to see. Once a bug surfaces, it takes less time to find and fix that bug.","title":"Simplicity"},{"location":"Reading-Notes/Google-SRE-Book/#boring-virtue","text":"\"Borning\" is a desireable property when it comes to software source code. The lack of excitement, suspense, and puzzles, and minimizing accidental complexity helps predictably accomplish the software's business goals.","title":"\"Boring\" virtue"},{"location":"Reading-Notes/Google-SRE-Book/#remove-bloat","text":"When code are bound to delete, delete them, never do commenting them out, or put a flag and hope they can be used at some point later. SRE should promote practices that ensure existing code all serve the essential purpose, routinely removing dead code, and building bloat detection into all levels of testing. Software bloat are the tendency of software to become slower and bigger over time as a result of constant additional features. A smaller project is easier to understand, easier to test, and frequently has fewer defects.","title":"Remove bloat"},{"location":"Reading-Notes/Google-SRE-Book/#minimal-modularity-simple-release","text":"The ability to make changes to parts of the system in isolation is essential to creating a supportable system. Loose coupling between binaries, or between binaries and configuration, is a simplicity pattern that simultaneously promotes developer agility and system stability. Versioning APIs allows developers to continue to use the version that their system depends upon while they upgrade to a newer version in a safe and considered way. One of the central strengths and design goals of Google\u2019s protocol buffers was to create a wire format that was backward and forward compatible. Writing clear, minimal APIs is an essential aspect of managing simplicity in a software system. Prefer simple releases. It is much easier to measure and understand the impact of a single change rather than a batch of changes released simultaneously.","title":"Minimal, Modularity, Simple Release"},{"location":"Reading-Notes/Google-SRE-Book/#best-practices","text":"Successfully operating a service entails a wide range of activities: developing monitoring systems, planning capacity, responding to incidents, ensuring the root causes of outages are addressed, and so on. A healthy way to operate a service permits self-actualization and takes active control of the direction of the service rather than reactively fights fires. Service Reliability Hierarchy: the elements that go into making a service reliable: Monitoring - you want to be aware of problems before your users notice them Incident Response - it is a tool we use to achieve our larger mission and remain in touch with how distributed computing systems actually work and fail Postmortem and RCA - building a blameless postmortem culture is the first step in understanding what went wrong and prevent same issue gets popped up Testing - offer some assurance that our software isn\u2019t making certain classes of errors before it\u2019s released Capacity Planning - how requests are load-balanced and potential overload handled, prevent cascading failures Development - large-scale system design and implementation Product - reliable product launched at scale","title":"Best Practices"},{"location":"Reading-Notes/Google-SRE-Book/#practical-alerting","text":"Monitoring is the fundamental element to running a stable service. Monitoring a large system is challenging: the sheer number of components to analyze maintain low maintenance burden on engineers responsible for the system","title":"Practical Alerting"},{"location":"Reading-Notes/Google-SRE-Book/#the-borgmon-story","text":"Borgmon is the monitoring system for the job scheduling infrasture. It relies on a common data exposition format which allowed mass data collection with low overheads and avoids the costs of subprocess execution and network connection setup. The data is used both for rendering charts and creating alerts. The history of the collected data can be used for alert computation as well. A Borgmon can collect from other Borgmon, so it can build hierarchies that follow the topology of the service, aggregating and summarizing information and discarding some strategically at each level. Some very large services shard below the cluster level into many scraper Borgmon, which in turn feed to the cluster-level Borgmon.","title":"The Borgmon story"},{"location":"Reading-Notes/Google-SRE-Book/#app-instrumentation","text":"Borgmon used a format to export metrics in plain text as space-separated keys and values, one metric per line. Adding a metric to a program only requires a single declaration in the code where the metric is needed. The decoupling of the variable definition from its use in Borgmon rules requires careful change management, and this trade-off has been offset with proper tools to validate and generate monitoring rules. Borgmon uses service discover to figure out the targets to scrape metric data from. The target list is dynamic whic hallows the monitoring to scale automatically. Additional \"synthetic\" metrics variables for each target helps detect if the monitored tasks are unavailable.","title":"App instrumentation"},{"location":"Reading-Notes/Google-SRE-Book/#time-series-data-storage","text":"Borgmon stores all the data in an in-memory database, regularly checkpointed to disk. Data points are of form (timestamp, value) , and stored in chronological lists aka time-series. Each time-series is named by a unique set of labels of form name=value . A time-series is conceptually a one-dimensional matrix of numbers, progressing through time. As you add permutations of labels to this time-series, the matrix becomes multidimensional. In practice, the structure is a fixed-sized block of memory, known as the time-series arena, with a garbage collector that expires the oldest entries once the arena is full. The time interval between the most recent and oldest entries in the arena is the horizon, which indicates how much queryable data is kept in RAM. Periodically, the in-memory state is archived to an external system known as the Time-Series Database (TSDB). Borgmon can query TSDB for older data and, while slower, TSDB is cheaper and larger than a Borgmon\u2019s RAM. Time-series are stored as sequences of numbers and timestamps, which are referred to as vectors. The name of a time-series is a labelset. To make a time-series identifiable, it must have labels: var - name of the variable/metric job - type of server being monitored service - collection of jobs that provide a service to users zone - datacenter location/region Together these label variables appear like this {var=http_requests,job=webserver,instance=host0:80,service=web,zone=us-west}[10m] called variable expression. A search for a labelset returns all matching time-series in a vector and does not require all labels to be specified. A duration can be specified to limit the range of data to query from.","title":"Time-series data storage"},{"location":"Reading-Notes/Google-SRE-Book/#rules","text":"Borgmon rules consists of simple algebraic expressions that compute time-series from other time-series. Rules run in a parallel threadpool where possible. Borgmon rules create new time-series, so the results of the computations are kept in the time-series arena and can be inspected just as the source time-series are. The ability to do so allows for ad hoc querying, evaluation, and exploration as tables or charts. Aggregation is the cornerstone of rule evaluation in a distributed environment. A counter is any monotonically non-decreasing variable. Gauges may take any value they like.","title":"Rules"},{"location":"Reading-Notes/Google-SRE-Book/#alerting","text":"When an alerting rule is evaluated by a Borgmon, the result is either true, in which case the alert is triggered, or false. Alerts sometimes toggle their state quickly, thus the rules allow a minimum duration (at least two rule evalutation cycles) for which the alerting rule must be true before the alert is sent. The alert rule allows templating for filling out a message template with contextual information when the alert fires and sent to alerting RPC. Alerts get first sent as \"triggering\" and then as \"firing\". The Alertmanager is responsible for routing the alert notification to the correct destination, and alerts will be dedupted, snoozed, or fan out/in base on the labelsets.","title":"Alerting"},{"location":"Reading-Notes/Google-SRE-Book/#sharding-monitoring","text":"A Borgmon can import time-series data from other Borgmon. To avoid scaling bottlenecks, a streaming protocol is used to transmit time-series data between Borgmon. Such deployment uses two or more global Borgmon for top-level aggregation and one Borgmon in each datacenter to monitor all the jobs running at that location. Upper-tier Borgmon can filter the data they want to stream from the lower-tier Borgmon, so that the global Borgmon does not fill its arena with all the per-task time-series from the lower tiers. Thus, the aggregation hierarchy builds local caches of relevant time-series that can be drilled down into when required.","title":"Sharding Monitoring"},{"location":"Reading-Notes/Google-SRE-Book/#black-box-monitoring","text":"Borgmon is a white-box monitoring system\u2014it inspects the internal state of the target service, and the rules are written with knowledge of the internals in mind. Prober is used to run a protocol check against a target and reports success or failure. Prober can also validate the response payload of the protocol to verify that the contents are expected, and even extract and export values as time-series. The prober can send alerts directly to Alertmanager, or its own varz can be collected by a Borgmon. Teams often use Prober to export histograms of response times by operation type and payload size so that they can slice and dice the user-visible performance. Prober is a hybrid of the check-and-test model with some richer variable extraction to create time-series. Prober can be pointed at either the frontend domain or behind the load balancer to know that traffic is still served when a datacenter fails, or to quickly isolate an edge in the traffic flow graph where a failure has occurred.","title":"Black-box monitoring"},{"location":"Reading-Notes/Google-SRE-Book/#configuration","text":"Borgmon configuration separates the definition of the rules from the targets being monitored. The same sets of rules can be applied to many targets at once, instead of writing nearly identical configuration over and over. Borgmon also supports language templates to reduce repetition and promote rules reuse. Borgmon adds labels indicating the target\u2019s breakdown (data type), instance name (source of data), and the shard and datacenter (locality or aggregation) it occupies, which can be used to group and aggregate those time-series together. The templated nature of these libraries allows flexibility in their use. The same template can be used to aggregate from each tier.","title":"Configuration"},{"location":"Reading-Notes/Google-SRE-Book/#borgmon-of-ten-years","text":"Borgmon transposed the model of check-and-alert per target into mass variable collection and a centralized rule evaluation across the time-series for alerting and diagnostics. This decoupling allows the size of the system being monitored to scale independently of the size of alerting rules. New applications come ready with metric exports in all components and libraries to which they link, and well-traveled aggregation and console templates, which further reduces the burden of implementation. Ensuring that the cost of maintenance scales sublinearly with the size of the service is key to making monitoring (and all sustaining operations work) maintainable. This theme recurs in all SRE work, as SREs work to scale all aspects of their work to the global scale. The idea of treating time-series data as a data source for generating alerts is now accessible to everyone through those open source tools like Prometheus, Riemann, Heka, and Bosun.","title":"Borgmon of Ten Years"},{"location":"Reading-Notes/Google-SRE-Book/#on-calls","text":"Historically, On-calls in IT context are performed by dedicated Ops teams tasked with the primary responsibility of keeping the service(s) for which they are responsible in good health. The SRE teams are quite different from purely operational teams in that they place heavy emphasis on the use of engineering to approach problems that exist at a scale and would be intractable without software engineering solutions.","title":"On-Calls"},{"location":"Reading-Notes/Google-SRE-Book/#duty","text":"When on-call, an engineer is available to triage the problem and perform operations on production systems possibly involving other team members and escalating as needed, within minutes. Typical values are 5 minutes for user-facing or otherwise highly time-critical services, and 30 minutes for less time-sensitive systems, depending on a service's SLO. Nonpaging production events, such as lower priority alerts or software releases, can also be handled and/or vetted by the on-call engineer during business hours. Many teams have both a primary and a secondary on-call rotation to serve as a fall-through for the pages. It is also common for two related teams to serve as secondary on-call for each other, with fall-through handling duties, rather than keeping a dedicated secondary rotation.","title":"Duty"},{"location":"Reading-Notes/Google-SRE-Book/#balance","text":"The quantity of on-call can be calculated by the percent of time spent by engineers on on-call duties. The quality of on-call can be calculated by the number of incidents that occur during an on-call shift. Google strive to invest at least 50% of SRE time into engineering: of the remainder, no more than 25% can be spent on-call, leaving up to another 25% on other types of operational, nonproject work. Using the 25% rule to derive the minimum number of SREs requried to sustain a 24/7 on-call rotation. Prefer a multi-site team of on-call shifts for these reasons: a multi-site \"follow the sun\" rotation allows teams to avoid night shifts altogether limiting the number of engineers in the on-call rotation ensures that engineers do not lose touch with the production systems For each on-call shift, an engineer should have sufficient time to deal with any incidents and follow-up activities such as writing postmortems.","title":"Balance"},{"location":"Reading-Notes/Google-SRE-Book/#feeling-safe","text":"It\u2019s important that on-call SREs understand that they can rely on several resources that make the experience of being on-call less daunting than it may seem. The most important on-call resources are: Clear escalation paths Well-defined incident-management procedures A blameless postmortem culture Look into adopting a formal incident-management protocol that offers an easy-to-follow and well-defined set of steps that aid an on-call engineer to rationally pursue a satisfactory incident resolution with all the required help. Build or adopt tools that automates most of the incident management actions, so the on-call engineer can focus on dealing with the incident, rather than spending time and cognitive effort on mundane actions such as formatting emails or updating several communication channels at once. When an incident occurs, it\u2019s important to evaluate what went wrong, recognize what went well, and take action to prevent the same errors from recurring in the future. SRE teams must write postmortems after significant incidents and detail a full timeline of the events that occurred. Mistakes happen, and software should make sure that we make as few mistakes as possible. Recognizing automation opportunities is one of the best ways to prevent human errors.","title":"Feeling Safe"},{"location":"Reading-Notes/Google-SRE-Book/#avoid-overload","text":"The SRE team and leadership are responsible for including concrete objectives in quarterly work planning in order to make sure that the workload returns to sustainable levels. Misconfigured monitoring is a common cause of operational overload. Paging alerts should be aligned with the symptoms that threaten a service\u2019s SLOs. All paging alerts should also be actionable. Low-priority alerts that bother the on-call engineer every hour (or more frequently) disrupt productivity, and the fatigue such alerts induce can also cause serious alerts to be treated with less attention than necessary. It is also important to control the number of alerts that the on-call engineers receive for a single incident, regulate the alert fan-out by ensuring that related alerts are grouped together by the monitoring or alerting system. Noisy alerts that systematically generate more than one alert per incident should be tweaked to approach a 1:1 alert/incident ratio. In extreme cases, SRE teams may have the option to \"give back the pager\"\u2014SRE can ask the developer team to be exclusively on-call for the system until it meets the standards of the SRE team in question. It is appropriate to negotiate the reorganization of on-call responsibilities with the development team, possibly routing some or all paging alerts to the developer on-call.","title":"Avoid Overload"},{"location":"Reading-Notes/Google-SRE-Book/#avoid-underload","text":"When SREs are not on-call often enough, they start losing confidence in operations touching the production, and creating knowledge gaps. SRE teams should be sized to allow every engineer to be on-call at least once or twice a quarter, thus ensuring that each team member is sufficiently exposed to production. Regular trainings and exercises should also be conducted to help improve troubleshooting skills and knowledge of the services.","title":"Avoid Underload"},{"location":"Reading-Notes/Google-SRE-Book/#effective-troubleshooting","text":"Be warned that being an expert is more than understanding how a system is supposed to work. Expertise is gained by investigating why a system doesn't work. While you can investigate a problem using only the generic process and derivation from first principles, it is less efficient and less effective than understanding how things are supposed to work.","title":"Effective Troubleshooting"},{"location":"Reading-Notes/Google-SRE-Book/#theory","text":"Think of the troubleshooting process as an application of the hypothetico-deductive method: given a set of observations about a system and a theoretical basis for understanding system behavior, we iteratively hypothesize potential causes for the failure and try to test those hypotheses. We\u2019d start with a problem report telling us that something is wrong with the system. Then we can look at the system\u2019s telemetry and logs to understand its current state. This information, combined with our knowledge of how the system is built, how it should operate, and its failure modes, enables us to identify some possible causes.","title":"Theory"},{"location":"Reading-Notes/Google-SRE-Book/#avoid-pitfalls","text":"The following are common pitfalls to avoid: Looking at symptoms that aren\u2019t relevant or misunderstanding the meaning of system metrics Misunderstanding how to change the system, its inputs, or its environment, so as to safely and effectively test hypotheses Coming up with wildly improbable theories about what\u2019s wrong, or latching on to causes of past problems, reasoning that since it happened once, it must be happening again Hunting down spurious correlations that are actually coincidences or are correlated with shared causes Understanding failures in our reasoning process is the first step to avoiding them and becoming more effective in solving problems. A methodical approach to knowing what we do know, what we don\u2019t know, and what we need to know, makes it simpler and more straightforward to figure out what\u2019s gone wrong and how to fix it.","title":"Avoid Pitfalls"},{"location":"Reading-Notes/Google-SRE-Book/#in-practice","text":"","title":"In Practice"},{"location":"Reading-Notes/Google-SRE-Book/#problem-report","text":"An effective problem report should tell you the expected behavior, the actual behavior, and, if possible, how to reproduce the behavior. Ideally, the reports should have a consistent form and be stored in a searchable location. It\u2019s common practice at Google to open a bug for every issue, even those received via email or instant messaging. Doing so creates a log of investigation and remediation activities that can be referenced in the future. Discourage reporting problems directly to a person, which introduces an additional step of transcribing the report into a bug, produces lower-quality reports that aren\u2019t visible to other members of the team, and tends to concentrate the problem-solving load on a handful of team members that the reporters happen to know, rather than the person currently on duty.","title":"Problem Report"},{"location":"Reading-Notes/Google-SRE-Book/#triage","text":"Problems can vary in severity: an issue might affect only one user under very specific circumstances, or it might entail a complete global outage for a service. Your response should be appropriate for the problem\u2019s impact and your course of action should be to make the system work as well as it can under the circumstances. For example, if a bug is leading to possibly unrecoverable data corruption, freezing the system to prevent further failure may be better than letting this behavior continue.","title":"Triage"},{"location":"Reading-Notes/Google-SRE-Book/#examine","text":"Graphing time-series and operations on time-series can be an effective way to understand the behavior of specific pieces of a system and find correlations that might suggest where problems began. Logging and exporting information about each operation and about system state makes it possible to understand exactly what a process was doing at a given point in time. Text logs are very helpful for reactive debugging in real time, while storing logs in a structured binary format can make it possible to build tools to conduct retrospective analysis with much more information. It\u2019s really useful to have multiple verbosity levels available, along with a way to increase these levels on the fly. This functionality enables you to examine any or all operations in incredible detail without having to restart your process, while still allowing you to dial back the verbosity levels when your service is operating normally. Exposing current state is the third trick. Google servers have endpoints that show a sample of RPCs recently sent or received, to help understand how any one server is communicating with others without referencing an architecture diagram. These endpoints also show histograms of error rates and latency for each type of RPC, their current configuration or allow examination of their data. Lastly, you may even need to instrument a client to experiment with, in order to discover what a component is returning in response to requests.","title":"Examine"},{"location":"Reading-Notes/Google-SRE-Book/#diagnose","text":"Ideally, components in a system have well-defined interfaces and perform known transformations from their input to their output. It's then possible to look at the data flows between components to determine whether a given component is working properly. Injecting known test data in order to check that the resulting output is expected at each step can be especially effective (a form of black-box testing). Dividing and conquering is a very useful general-purpose solution technique. An alternative, bisection, splits the system in half and examines the communication paths between components on one side and the other. Finding out what a malfunctioning system is doing (symptom), then asking why (cause of symptom) it\u2019s doing that and where (locate the code) its resources are being used or where its output is going can help you understand how things have gone wrong and forge a solution. A working computer system tends to remain in motion until acted upon by an external force, such as a configuration change or a shift in the type of load served. Recent changes to a system can be a productive place to start identifying what\u2019s going wrong. Correlating changes in a system\u2019s performance and behavior with other events in the system and environment can also be helpful in constructing monitoring dashboards, i.e. annotate a graph showing the system\u2019s error rates with the start and end times of a deployment of a new version.","title":"Diagnose"},{"location":"Reading-Notes/Google-SRE-Book/#test-and-treat","text":"Using the experimental method, we can try to rule in or rule out our hypothetic list of possible causes. Following the code and trying to imitate the code flow, step-by-step, may point to exactly what\u2019s going wrong. Consider these when designing tests: a test should have mutually exclusive alternatives, so that it can rule one group of hypotheses in and rule another set out obvious first: perform the tests in decreasing order of likelihood an experiment may provide misleading results due to confounding factors i.e. firewall rules against your workstation but not for the application server side effects that change future test results if you performed active testing by changing a system\u2014for instance by giving more resources to a process\u2014making changes in a systematic and documented fashion will help you return the system to its pre-test setup tests can be suggestive rather than definitive i.e. it can be very difficult to make race conditions or deadlocks happen in a timely and reproducible manner","title":"Test and Treat"},{"location":"Reading-Notes/Google-SRE-Book/#negative-results","text":"Negative results should not be ignored or discounted. Realizing you\u2019re wrong has much value: a clear negative result can resolve some of the hardest design questions. Often a team has two seemingly reasonable designs but progress in one direction has to address vague and speculative questions about whether the other direction might be better. Experiments with negative results are conclusive. They tell us something certain about production, or the design space, or the performance limits of an existing system. They can help others determine whether their own experiments or designs are worthwhile. Microbenchmarks, documented antipatterns, and project postmortems all fit this category. Tools and methods can outlive the experiment and inform future work. As an example, benchmarking tools and load generators can result just as easily from a disconfirming experiment as a supporting one. Publishing negative results improves our industry\u2019s data-driven culture. Accounting for negative results and statistical insignificance reduces the bias in our metrics and provides an example to others of how to maturely accept uncertainty.","title":"Negative Results"},{"location":"Reading-Notes/Google-SRE-Book/#cure","text":"Often, we can only find probable root cause factors than definitive factor, given that a production system can be complex and reproducing the problem in a live production system may not be an option. Once you\u2019ve found the factors that caused the problem, it\u2019s time to write up notes on what went wrong with the system, how you tracked down the problem, how you fixed the problem, and how to prevent it from happening again (a postmortem).","title":"Cure"},{"location":"Reading-Notes/Google-SRE-Book/#emergency-response","text":"What to Do When Systems Break? Don't panic. If you can\u2019t think of a solution, cast your net farther. Involve more of your teammates, seek help, do whatever you have to do, but do it quickly. The highest priority is to resolve the issue at hand quickly. Oftentimes, the person with the most state is the one whose actions somehow triggered the event. Utilize that person. Keep a History of Outages. History is about learning from everyone\u2019s mistakes. Be thorough, be honest, but most of all, ask hard questions. Look for specific actions that might prevent such an outage from recurring, not just tactically, but also strategically. Hold yourself and others accountable to following up on the specific actions detailed in these postmortems. Doing so will prevent a future outage that\u2019s nearly identical to, and caused by nearly the same triggers as, an outage that has already been documented.","title":"Emergency Response"},{"location":"Reading-Notes/Google-SRE-Book/#managing-incidents","text":"","title":"Managing Incidents"},{"location":"Reading-Notes/How-Netflix-Works/","text":"This notes it taken from an article on High Scalability It's far more complicated and interesting than you might imagine, when you press play on a Netflix video (or press start on a Netflix title ). Control the entire stack \u00b6 The three parts of Netflix: client , backend , content delivery network (CDN) ; Netflix controls all of the three and achieved vertical integration and scaling and ensures the users get the contents reliably. - The client is the user interface on any device used to browse and play Netflix videos. - Everything that happens before you hit play happens in the backend , which runs in AWS. - Everything that happens after you hit play is handled by Open Connect, Netflix's CDN . Focus on what they do the best \u00b6 Netflix started online-streaming service with building their own data centers and failed and experienced all the problems that can happen when building data centers. It just doesn't work when you are growing rapidly. Undifferentiated heavy lifting are those things that have to be done, but don't provide any advantage to the core business of providing a quality video watching experience. Netflix then move to AWS for taking away the headache of building reliable infrastructure for their service and remove any single point of failure (SPF); and AWS offered highly reliable databases, storage and redundant datacenters. To Netflix they just avoided the undifferentiated heavy lifting and focus on providing business value that they are good at . Ensure the service can be always served \u00b6 Netflix operates out of three AWS regions: one in North Virginia, one in Portland Oregon, and one in Dublin Ireland. Having three regions, you get reliable service that when any one region fails, the other regions will step in handle all the service traffic in the failed region. Plus, serving traffic from the region closest to where the user is provides faster content delivery. This is referred as business continuity plan (BCP) for some companies. And Netflix calls this their global services model . Any customer can be served out of any region. And it doesn't happen automatically; Netflix did their work to guarantee the automatic fail-over happens. Netflix even intentionally takes down one of their regions every month to ensure the system work reliably; they calls it the chaos testing . Netflix's server-side heavy lifting \u00b6 Netflix takes advantage of the elasticity of the cloud service that AWS offered. Rather than have a lot of extra computers hanging around doing nothing and wait for the peak load, Netflix only had to pay for what was needed, when it was needed , by scaling up and down for their service instances. Plus, anything that doesn't involve serving video is handled in AWS. This includes scalable computing, scalable storage, business logic, scalable distributed databases, big data processing and analytics, recommendations, transcoding, and hundreds of other functions. Scalable computing is EC2 and scalable storage is S3. The client devices from users --Smart phones, TV, Tablet, PC, etc-- each request talks to a Netflix service running in EC2. Netflix uses both DynamoDB and Cassandra for their scalable distributed databases so they can just scale up when more data storage is necessary on all their regions, and have enough redundancy to rest assure that the data are safe. Netflix knows what everyone has watched when they watched it and where they were when they watched and lots more information. Netflix uses machine learning to power big data processing and generates analytics to improve their products and suggest more relevant contents to its users. Their ultimate goal is to keep users subscribed and attract new users. Netflix is known for being a data-driven company. One interesting fact is that a movie title image users see might be different for the same movie among some users, and this is because the image displayed is based on analyzing your watch behavior and choosing the one that data suggests can attract you the most. Netflix also use the analytics to choose the best movie images to display from time to time. The work done to prepare the contents \u00b6 Netflix gets its contents from the production houses and studios, aka the source media. The videos produced come in a high definition format that's many terabytes in size and need to go through a process of source media -> video validation -> accept/reject -> media pipeline -> validation -> encoded files Validation is a rigorous process to make sure the video is in good quality in terms of color, digital artifacts, frames, etc. And video is rejected if defects are found. The media pipeline runs as many as 70+ pieces of software operations to produce the result files. And the giant video must be break into much smaller chunks first then the chunks are processed in parallel . This consumes lots of CPU power on AWS. Netflix says a source media file can be encoded and pushed to their CDN in as little as 30 minutes. The result is a pile of files. Netflix need a video in each format that works best for every internet-connected device; to name a few, Windows, Roku, LG, Samsung Blu-ray, Mac, Xbox 360, LG DTV, Sony PS3, Nintendo Wii, iPad, iPhone, Apple TV, Android, Kindle Fire, Comcast X1, etc. And Netflix supports 2200+ different devices. Netflix also creates files optimized for different network speeds. There are also files for different audio formats. Audio is encoded into different levels of quality and in different languages. There are also files included for subtitles. A video may have subtitles in a number of different languages. Just for the movie The Crown , Netflix stores around 1,200 files. The strategies for serving the contents \u00b6 Netflix uses CDN to put video as close as possible to users by spreading computers throughout the world. The biggest benefits of a CDN are speed and reliability . Each location with a computer storing video content is called a PoP or point of presence. It houses servers, routers, and other telecommunications equipment. Netflix has tried three different video streaming strategies: its own small CDN; third-party CDNs; and Open Connect. They started with their own CDN when the video catalog was small enough that each location contained all of its content, and that soon become not enough as more users subscribed and more contents were added. Then Netflix contracted with companies like Akamai, Limelight, and Level 3 to provide CDN services and focused on other higher priority projects. However, 3rd-party CDN solutions are not easily scalable as how Netflix wanted; and they are not utilized well enough. So they built Open Connect. Open Connect is less expensive, delivers better quality video, and more scalable. Netflix developed special hardware for delivering large videos for Open Connect, called OCAs . OCAs use the FreeBSD operating system and NGINX for the web server. Netflix delivers huge amounts of video traffic from thousands of servers in more than 1,000 locations around the world. Unlike Netflix, YouTube and Amazon built their own backbone network to deliver video around the globe, which is very complicated and expensive. So Netflix borrowed the major internet service providers' (ISPs) network by asking them to put OCAs in their datacenters, or put OCAs close to internet exchange locations (IXPs). This is a brilliant idea and big deal. First, Netflix placed their content very close to their users ; so when a user ask for a video, the stream traffic never left their ISP's network and therefore is the fastest and the most reliable than any 3rd-party CDNs. Second, it is because the streaming traffics are (for most of the cases) within the ISP's network , the ISPs don't need to add more infrastructure to handle the massive video streaming traffic on the Internet. It is a win-win. Netflix has all this video sitting in S3, and Netflix uses a process called proactive caching to efficiently copy videos to OCAs. It basically means they cache the videos on each OCA based on the data prediction of the most likely watched videos around that location. This list at each OCA is refreshed every night and so is the video cache. The more popular a video, the more OCAs it will be copied to. Netflix operates what is called a tiered caching system : OCA units -> Small Peering Location -> Large Peering Location (Origin) -> AWS S3 . The OCAs at ISPs and close to IXPs are small ones that won't fit in the entire video catalog. They always ask videos from the closest available video cache of equivalent or larger in size. For Netflix, a video isn't considered live when it's copied to just one OCA. Only when there are a sufficient number of OCAs with enough copies of the video to serve it appropriately, will the video be considered live and ready for members to watch. There's never a cache miss in Open Connect. Since Netflix knows where a video is cached at any time, if a small OCA doesn't have a video, one of the larger OCA location guarantees to have it. Netflix also do load-balancing on the OCAs so to prevent some OCAs from being overwhelmed by unusually high traffic. Open Connect is a very reliable and resilient system. Netflix controls the client \u00b6 Netflix handles failures gracefully because it controls the client on every device running Netflix. They do so from the apps they developed and through the SDK they offered to consume their services. From there, Netflix can adapt consistently and transparently to slow networks, failed OCAs, and any other problems that might arise. Netflix uses the client's IP address and information from ISPs to identify which OCA clusters are the best to use. The client then tests the quality of the network connection to each OCA and select the fastest and most reliable OCA and determine the best way to receive content from that OCA considering the network quality. The client keeps running these tests throughout the video streaming process.","title":"How Netflix Works"},{"location":"Reading-Notes/How-Netflix-Works/#control-the-entire-stack","text":"The three parts of Netflix: client , backend , content delivery network (CDN) ; Netflix controls all of the three and achieved vertical integration and scaling and ensures the users get the contents reliably. - The client is the user interface on any device used to browse and play Netflix videos. - Everything that happens before you hit play happens in the backend , which runs in AWS. - Everything that happens after you hit play is handled by Open Connect, Netflix's CDN .","title":"Control the entire stack"},{"location":"Reading-Notes/How-Netflix-Works/#focus-on-what-they-do-the-best","text":"Netflix started online-streaming service with building their own data centers and failed and experienced all the problems that can happen when building data centers. It just doesn't work when you are growing rapidly. Undifferentiated heavy lifting are those things that have to be done, but don't provide any advantage to the core business of providing a quality video watching experience. Netflix then move to AWS for taking away the headache of building reliable infrastructure for their service and remove any single point of failure (SPF); and AWS offered highly reliable databases, storage and redundant datacenters. To Netflix they just avoided the undifferentiated heavy lifting and focus on providing business value that they are good at .","title":"Focus on what they do the best"},{"location":"Reading-Notes/How-Netflix-Works/#ensure-the-service-can-be-always-served","text":"Netflix operates out of three AWS regions: one in North Virginia, one in Portland Oregon, and one in Dublin Ireland. Having three regions, you get reliable service that when any one region fails, the other regions will step in handle all the service traffic in the failed region. Plus, serving traffic from the region closest to where the user is provides faster content delivery. This is referred as business continuity plan (BCP) for some companies. And Netflix calls this their global services model . Any customer can be served out of any region. And it doesn't happen automatically; Netflix did their work to guarantee the automatic fail-over happens. Netflix even intentionally takes down one of their regions every month to ensure the system work reliably; they calls it the chaos testing .","title":"Ensure the service can be always served"},{"location":"Reading-Notes/How-Netflix-Works/#netflixs-server-side-heavy-lifting","text":"Netflix takes advantage of the elasticity of the cloud service that AWS offered. Rather than have a lot of extra computers hanging around doing nothing and wait for the peak load, Netflix only had to pay for what was needed, when it was needed , by scaling up and down for their service instances. Plus, anything that doesn't involve serving video is handled in AWS. This includes scalable computing, scalable storage, business logic, scalable distributed databases, big data processing and analytics, recommendations, transcoding, and hundreds of other functions. Scalable computing is EC2 and scalable storage is S3. The client devices from users --Smart phones, TV, Tablet, PC, etc-- each request talks to a Netflix service running in EC2. Netflix uses both DynamoDB and Cassandra for their scalable distributed databases so they can just scale up when more data storage is necessary on all their regions, and have enough redundancy to rest assure that the data are safe. Netflix knows what everyone has watched when they watched it and where they were when they watched and lots more information. Netflix uses machine learning to power big data processing and generates analytics to improve their products and suggest more relevant contents to its users. Their ultimate goal is to keep users subscribed and attract new users. Netflix is known for being a data-driven company. One interesting fact is that a movie title image users see might be different for the same movie among some users, and this is because the image displayed is based on analyzing your watch behavior and choosing the one that data suggests can attract you the most. Netflix also use the analytics to choose the best movie images to display from time to time.","title":"Netflix's server-side heavy lifting"},{"location":"Reading-Notes/How-Netflix-Works/#the-work-done-to-prepare-the-contents","text":"Netflix gets its contents from the production houses and studios, aka the source media. The videos produced come in a high definition format that's many terabytes in size and need to go through a process of source media -> video validation -> accept/reject -> media pipeline -> validation -> encoded files Validation is a rigorous process to make sure the video is in good quality in terms of color, digital artifacts, frames, etc. And video is rejected if defects are found. The media pipeline runs as many as 70+ pieces of software operations to produce the result files. And the giant video must be break into much smaller chunks first then the chunks are processed in parallel . This consumes lots of CPU power on AWS. Netflix says a source media file can be encoded and pushed to their CDN in as little as 30 minutes. The result is a pile of files. Netflix need a video in each format that works best for every internet-connected device; to name a few, Windows, Roku, LG, Samsung Blu-ray, Mac, Xbox 360, LG DTV, Sony PS3, Nintendo Wii, iPad, iPhone, Apple TV, Android, Kindle Fire, Comcast X1, etc. And Netflix supports 2200+ different devices. Netflix also creates files optimized for different network speeds. There are also files for different audio formats. Audio is encoded into different levels of quality and in different languages. There are also files included for subtitles. A video may have subtitles in a number of different languages. Just for the movie The Crown , Netflix stores around 1,200 files.","title":"The work done to prepare the contents"},{"location":"Reading-Notes/How-Netflix-Works/#the-strategies-for-serving-the-contents","text":"Netflix uses CDN to put video as close as possible to users by spreading computers throughout the world. The biggest benefits of a CDN are speed and reliability . Each location with a computer storing video content is called a PoP or point of presence. It houses servers, routers, and other telecommunications equipment. Netflix has tried three different video streaming strategies: its own small CDN; third-party CDNs; and Open Connect. They started with their own CDN when the video catalog was small enough that each location contained all of its content, and that soon become not enough as more users subscribed and more contents were added. Then Netflix contracted with companies like Akamai, Limelight, and Level 3 to provide CDN services and focused on other higher priority projects. However, 3rd-party CDN solutions are not easily scalable as how Netflix wanted; and they are not utilized well enough. So they built Open Connect. Open Connect is less expensive, delivers better quality video, and more scalable. Netflix developed special hardware for delivering large videos for Open Connect, called OCAs . OCAs use the FreeBSD operating system and NGINX for the web server. Netflix delivers huge amounts of video traffic from thousands of servers in more than 1,000 locations around the world. Unlike Netflix, YouTube and Amazon built their own backbone network to deliver video around the globe, which is very complicated and expensive. So Netflix borrowed the major internet service providers' (ISPs) network by asking them to put OCAs in their datacenters, or put OCAs close to internet exchange locations (IXPs). This is a brilliant idea and big deal. First, Netflix placed their content very close to their users ; so when a user ask for a video, the stream traffic never left their ISP's network and therefore is the fastest and the most reliable than any 3rd-party CDNs. Second, it is because the streaming traffics are (for most of the cases) within the ISP's network , the ISPs don't need to add more infrastructure to handle the massive video streaming traffic on the Internet. It is a win-win. Netflix has all this video sitting in S3, and Netflix uses a process called proactive caching to efficiently copy videos to OCAs. It basically means they cache the videos on each OCA based on the data prediction of the most likely watched videos around that location. This list at each OCA is refreshed every night and so is the video cache. The more popular a video, the more OCAs it will be copied to. Netflix operates what is called a tiered caching system : OCA units -> Small Peering Location -> Large Peering Location (Origin) -> AWS S3 . The OCAs at ISPs and close to IXPs are small ones that won't fit in the entire video catalog. They always ask videos from the closest available video cache of equivalent or larger in size. For Netflix, a video isn't considered live when it's copied to just one OCA. Only when there are a sufficient number of OCAs with enough copies of the video to serve it appropriately, will the video be considered live and ready for members to watch. There's never a cache miss in Open Connect. Since Netflix knows where a video is cached at any time, if a small OCA doesn't have a video, one of the larger OCA location guarantees to have it. Netflix also do load-balancing on the OCAs so to prevent some OCAs from being overwhelmed by unusually high traffic. Open Connect is a very reliable and resilient system.","title":"The strategies for serving the contents"},{"location":"Reading-Notes/How-Netflix-Works/#netflix-controls-the-client","text":"Netflix handles failures gracefully because it controls the client on every device running Netflix. They do so from the apps they developed and through the SDK they offered to consume their services. From there, Netflix can adapt consistently and transparently to slow networks, failed OCAs, and any other problems that might arise. Netflix uses the client's IP address and information from ISPs to identify which OCA clusters are the best to use. The client then tests the quality of the network connection to each OCA and select the fastest and most reliable OCA and determine the best way to receive content from that OCA considering the network quality. The client keeps running these tests throughout the video streaming process.","title":"Netflix controls the client"},{"location":"Reading-Notes/Leading-without-formal-authority/","text":"Notes taken from LinkedIn course Leading Without Formal Authority Cultivating informal leadership doesn't happen overnight, so don't wait until you have the title or the salary or the courage, and don't wait until you feel 100% confident you can do it perfectly, just start. Someone who is actively trying to be a good leader is better than someone who isn't trying. Leading Basics \u00b6 Develop a leadship mindset \u00b6 Why should you develop a leadership mindset if you're not in charge of a formal team? Improve the ability to set direction, adapt to change, and win the hearts and minds of the people around you More likely to engage on your projects and get support from your peers Set the stage for more formal leadership Open possibilities that can lead to some major breakthroughs and leave a lasting impact Even if you don't have a formal team reporting to you, viewing yourself as a leader in your organization, a driver of change, and a supporter of your colleagues helps you rise to the occasion to create big impact and do your best work. Find opportunities to lead \u00b6 Points for enthusiasm, but you only have so many hours in the day and you want to make sure that your leadership is providing a lot of value to your organization while helping you grow as a professional. When opportunities to head up project teams, lead initiatives, or take charge of a task force arise, step up to the leadership plate if you: have expertise , or experience in whatever the objective or opportunity or challenge is say something like In my past project xxx, we experienced a similar challenge. I learned a lot through that, and I think I could help us avoid a few of the hiccups by leading this project. have connections , or personally know a lot of the key stakeholders and have strong relationships with them, your network and influence can be a huge value say something like I know the engagement of such and such a team will be important. I worked with them closely before and I'd be happy to connect with them to kick off this project. have the strong will to lead and grow from this project say something like I've been working on developing a deeper understanding of our customer base. Because this project will involve a high level of customer analysis, I'd like to take the opportunity to lead the initiative. One of the best ways to learn is actually to lead. It forces you to do your homework, ask for support, and step outside of your comfort zone. Avoid waiting to be asked to do something . While that is said, stay away from investing time in things that will suck up all your energy or don't offer a lot of value to the organization or provide little opportunity to grow professionally. Take a big look at the strategic goals of your team and your organization. Look beyond the task and ask yourself How can I leverage my strengths and make a big impact? Find or become a mentor \u00b6 Mentoring is a great way to develop your organizational influence. To find a mentor, the most obvious place to look is your boss but expanding your horizons can be a great thing. A good mentor can help hold you accountable for your goals or help you think about long-term career planning from an unbiased point of view. Think about people who share your role, who have more experience, or people who are in a different role but have been in the industry a long time. Avoid directly asking someone to be your mentor. Instead, find a way to make it easy for them like asking for help for an issue, then ask if it is okay to reach out from time to time with other questions. The relationship gradually grows. The key to attracting a great mentor is to demonstrate that you are coachable . That you take advice and feedback and critique and you act on it. You can also be a mentor. Even if you're new to the organization or your role, offer to help out new employees, less experienced team members, or anyone who has expressed an interest in mentorship. Being a mentor help you think more deeply about your own job or career and practice your leadership. Develop leadership brand \u00b6 Leadership isn't defined by how you look, the position you have or how long you've been with your organization. It's defined by how you behave and the sum of many small interactions from your brand . There's no personality type that's best suited to leadership. When you're defining your leadership brand, you want to build on your natural strengths: organizational acumen, which comes when you have a lot of experience with your company to get the insight on how to get things accomplished, what is available, who to go to, what for, and the nuances of your org chart expertise in your role brainstorm and ideas, the will to think differently and adds insights on solving problems creatively feedback, give good feedback to people and help their work and growth empathy, ability to emotionally calm people when things get intense Navigate org hierarchy \u00b6 To lead without authority, you need to build credibility both up and down through the organizational hierarchy. Think about the advantages and knowledge you've gathered, and how that can best serve your organization at all levels. To move your expertise up the hierarchy, the best bet is moving through your boss, aka leading up . Your conversations with your boss will influence their conversations with their boss. The more strategic and helpful you are in those conversations, the more likely your ideas will rise up through the ladder. Talk strategically about the areas where you have information and expertise. You want to take a similar strategy when you share your ideas down the hierarchy. The conversations you have with your subordinates will likely be at a more tactical level. Cite your experience and provide examples from your role. Speak with intention and be very clear, make it easy for others to spread your words. Focus on themes and big picture initiatives that you can speak to with the expertise you've gathered in your role. Leader Language and Behaviors \u00b6 Add value in meetings \u00b6 As an informal leader, you can create the opportunity to reframe meetings from blah to strategic, even if you're not the one running them. If you are organizing a meeting, planning is important. Send out an agenda and explain what you want to accomplish as a result of the meeting and include an accurate timeframe. Make sure the meeting is comprehensive and actionable. Ask in a collaborative tone, and then listen. Remember to end the meeting positively and thank the person who organized the meeting or the people who attended. Finally after the meeting, summarize the meeting and make sure there are action steps moving forward. Mindful listening \u00b6 A leader is not always the loudest voice in the room. When you're mindfully listening, you're paying attention to a person's words, their tone of voice, their body language, small changes in their facial expressions and other small nuances that give you a more holistic picture of the person and the conversation. To mindfully listen, try and rid yourself of all kinds of distractions. Before you engage in the conversation, just take 10 seconds to clear your mind and think about the importance of that conversation and instead of thinking about what you want to say in the conversation, try to think about what you want to understand. During the conversation, look into the person's eyes. Repeat what they're saying in your own head and think about how they feel. Mindful listening can boost your self-esteem, help you retain more information and also increase your attention ability, and also help you sort and frame information before you finally speak. Inspire others \u00b6 As an informal leader you want people to be inspired to help you, to buy into your ideas and work hard for your organization. You'll help others feel inspired and you'll also feel more inspired yourself. People are looking for meaning and inspiration and they always gravitate to the people who provide it. To get emotional engagement, paint a mental picture of the customers you help, the impacts of the projects you've done or in some cases, the consequences of not doing them. When you can, include names, details, talk about how the other people feel and the lasting impact that you're having on them. Adapt to change \u00b6 When presented with a change, your first decision is to determine where you stand. As a rule of thumb, unless there are huge flaws, your best bet is to support changes brought by senior leadership. Even if the change has some small potential errors, your public support at the beginning will create enthusiasm. Later, you can raise those smaller issues at the right time and with the appropriate party. If you genuinely believe your organization is making a big mistake, it's your duty to say something, but say something privately to the person who enacted the change. If you've made the decision that you are mostly supportive of the change, be all in. There are no benefits to holding back and there are great advantages to displaying enthusiasm. If you've expressed your concerns to the appropriate senior party, there is no need for you to repeat them to everyone on your team. Also don't ignore the challenges. Refusing to acknowledge challenges only makes them more pronounced, and ignoring challenges erodes trust. Setting and giving expectations \u00b6 The more you clearly communicate expectations up front the better you will be able to avoid challenges of authority down the road. Connect the expectations to the outcome. Those around you should know why the expectations are important and who will be impacted by the result of fulfilling those expectations. Hold yourself accountable to the same expectations you are giving out. If you want everyone around you to be on time, prepared and efficient, hold yourself to that same standard. Don't be afraid to set high expectations. Setting a high bar leads people to rise to the occasion. Done right, the way you communicate with those around you will set your team up for success. Avoid small mistakes \u00b6 Small things like being late to a meeting matters and can undermine your long-term leadership. The same is true for others like missing an internal deadline and holding up a project, forgetting a small promise like promising to proof a peer's presentation, and not responding to emails or returning phone calls. These mistakes might not seem like a big deal in the moment, but others form an opinion of us based on a combination of a lot of small actions . Overcome and avoid these mistakes makes you look good in front of your peers, your boss, and senior leaders always helps you in the long run. Give/Ask for feedback \u00b6 Giving feedback is a great way to establish leadership as an individual contributor. Give feedback on things that matter , and be sure to give feedback when you're asked. It shows a good sign since natural leaders are often asked for feedback since they give good feedback. If you do know one or two things that could help, let them know kindly. Walk the other person through your thought process and your rationale . Meanwhile, consider giving feedback on high stakes things, even if you aren't asked. The best leaders, both formal and informal, rely heavily on those around them. Asking for feedback will help you excel and show people that you have a growth mindset. High-Stakes Situations \u00b6 Lead through challenges \u00b6 Good leaders identify potential roadblocks early on and they help people redirect without shame or blame. If you see failure on the horizon, please keep the following in mind: rectify, offer the opportunity to fix the problem instead of lecturing about the mistakes reiterate, avoid sounding like you're issuing directives remedy, offer the help and ideas without taking over the project Look after boss's mistakes \u00b6 There are two types of mistakes: private and public. Private mistakes are when your boss is about to make a bad decision or do something that will hurt them. Stepping up and help your boss avoid a potential blunder that could be public later; pull them aside and have a conversation about it. Position yourself as someone who has your boss's best interest at heart . Public mistakes are more challenging to navigate, since it is easy to discredit your boss and make them look bad in front of others. It's not your job to take over and fix the problem, but you can help your boss get on the right track . When you jump in in public, do keep in mind: validate your boss, make your intervention to be conversational, and flip the conversation back to your boss. Ask for helps \u00b6 In any leadership journey, outside support is necessary. Support and help from others makes you better and stronger. When asking for help, framing the situation by stating about how the situation impacts you, the organization, or a customer, then make a specific ask. Finally, include a thank you in terms of the impact that help is making. Making sure you're clear, specific, and strategic in the way you ask for help makes people happier to help you and it makes you a stronger leader. Long-last Influence \u00b6 Collaboration culture \u00b6 As an informal leader, your ability to shape a culture of collaboration can create a huge ripple of positive impact throughout your organization. You and your team will be more effective when you put your brains together, and you'll also develop more influence throughout your organization. Being intentional with your language can help you foster the trust and respect required for successful teamwork. You can help foster psychological safety for your team by nipping toxic negativity, underhanded remarks, and gossip right in the bud, and don't lose the tether. Always give credit to the originator of the idea. If you're finding that your team doesn't collaborate as much as you'd like, you can model the behavior you'd like to see. Begin asking for their input on your work and model receiving that feedback well. Thank people for their feedback and act on it, follow up with them, and highlight the difference that feedback made and how you're grateful for their help. This lays the groundwork for others to do the same, and conveys the belief that we're a team who helps each other get better. Seek diverse perspectives. Ask yourself, who has worked on a project like this before? Continue Learning \u00b6 As you grow in your role, your organization, or even just in your ability to lead, you'll need to focus on both the hard and soft skills. Hard skills are things like software training, sales training, and product knowledge. These hard skills are foundational. Soft skills are things like leadership language, prioritizing, delegating, and communication. Balancing your time between soft skills and hard skills not only ensures you have the competencies necessary to succeed in your own role, it also makes you a valuable leader. Create Influence \u00b6 During tough times, people tend to focus on blame and reactionary activities, which often leads to despair. Start by listening. Staying calm while others air their feelings of disappointment or frustration can help everyone move forward faster. To add empathy, you'll need to put your view on pause just for a second and validate what you've heard, that they're concerned. Address the root of repeated issues if you have seen it. In a crisis, doing something to help or at least lessen the blow. Build Trust \u00b6 Building trust among people takes time and effort. keep your word . It can be keeping something confidential or promising to take care of something. use body language . When speaking to someone, make eye contact and leaning into the conversation. be competent and warm . The first step in getting someone to trust you at work is to be competent and good at your job; the second step is to be warm, or being human and admitting vulnerability. For example, if you're an engineer, don't show your vulnerability in your math skills, instead be open about speaking of your fear of public speaking or something else that makes you a little bit more human. don't gossip. admit when you were wrong . If you made a mistake, which we all do, own up to it. trust others , until they give you a reason not to. If you want someone to trust you, show that you trust them.","title":"Leading Without Formal Authority"},{"location":"Reading-Notes/Leading-without-formal-authority/#leading-basics","text":"","title":"Leading Basics"},{"location":"Reading-Notes/Leading-without-formal-authority/#develop-a-leadship-mindset","text":"Why should you develop a leadership mindset if you're not in charge of a formal team? Improve the ability to set direction, adapt to change, and win the hearts and minds of the people around you More likely to engage on your projects and get support from your peers Set the stage for more formal leadership Open possibilities that can lead to some major breakthroughs and leave a lasting impact Even if you don't have a formal team reporting to you, viewing yourself as a leader in your organization, a driver of change, and a supporter of your colleagues helps you rise to the occasion to create big impact and do your best work.","title":"Develop a leadship mindset"},{"location":"Reading-Notes/Leading-without-formal-authority/#find-opportunities-to-lead","text":"Points for enthusiasm, but you only have so many hours in the day and you want to make sure that your leadership is providing a lot of value to your organization while helping you grow as a professional. When opportunities to head up project teams, lead initiatives, or take charge of a task force arise, step up to the leadership plate if you: have expertise , or experience in whatever the objective or opportunity or challenge is say something like In my past project xxx, we experienced a similar challenge. I learned a lot through that, and I think I could help us avoid a few of the hiccups by leading this project. have connections , or personally know a lot of the key stakeholders and have strong relationships with them, your network and influence can be a huge value say something like I know the engagement of such and such a team will be important. I worked with them closely before and I'd be happy to connect with them to kick off this project. have the strong will to lead and grow from this project say something like I've been working on developing a deeper understanding of our customer base. Because this project will involve a high level of customer analysis, I'd like to take the opportunity to lead the initiative. One of the best ways to learn is actually to lead. It forces you to do your homework, ask for support, and step outside of your comfort zone. Avoid waiting to be asked to do something . While that is said, stay away from investing time in things that will suck up all your energy or don't offer a lot of value to the organization or provide little opportunity to grow professionally. Take a big look at the strategic goals of your team and your organization. Look beyond the task and ask yourself How can I leverage my strengths and make a big impact?","title":"Find opportunities to lead"},{"location":"Reading-Notes/Leading-without-formal-authority/#find-or-become-a-mentor","text":"Mentoring is a great way to develop your organizational influence. To find a mentor, the most obvious place to look is your boss but expanding your horizons can be a great thing. A good mentor can help hold you accountable for your goals or help you think about long-term career planning from an unbiased point of view. Think about people who share your role, who have more experience, or people who are in a different role but have been in the industry a long time. Avoid directly asking someone to be your mentor. Instead, find a way to make it easy for them like asking for help for an issue, then ask if it is okay to reach out from time to time with other questions. The relationship gradually grows. The key to attracting a great mentor is to demonstrate that you are coachable . That you take advice and feedback and critique and you act on it. You can also be a mentor. Even if you're new to the organization or your role, offer to help out new employees, less experienced team members, or anyone who has expressed an interest in mentorship. Being a mentor help you think more deeply about your own job or career and practice your leadership.","title":"Find or become a mentor"},{"location":"Reading-Notes/Leading-without-formal-authority/#develop-leadership-brand","text":"Leadership isn't defined by how you look, the position you have or how long you've been with your organization. It's defined by how you behave and the sum of many small interactions from your brand . There's no personality type that's best suited to leadership. When you're defining your leadership brand, you want to build on your natural strengths: organizational acumen, which comes when you have a lot of experience with your company to get the insight on how to get things accomplished, what is available, who to go to, what for, and the nuances of your org chart expertise in your role brainstorm and ideas, the will to think differently and adds insights on solving problems creatively feedback, give good feedback to people and help their work and growth empathy, ability to emotionally calm people when things get intense","title":"Develop leadership brand"},{"location":"Reading-Notes/Leading-without-formal-authority/#navigate-org-hierarchy","text":"To lead without authority, you need to build credibility both up and down through the organizational hierarchy. Think about the advantages and knowledge you've gathered, and how that can best serve your organization at all levels. To move your expertise up the hierarchy, the best bet is moving through your boss, aka leading up . Your conversations with your boss will influence their conversations with their boss. The more strategic and helpful you are in those conversations, the more likely your ideas will rise up through the ladder. Talk strategically about the areas where you have information and expertise. You want to take a similar strategy when you share your ideas down the hierarchy. The conversations you have with your subordinates will likely be at a more tactical level. Cite your experience and provide examples from your role. Speak with intention and be very clear, make it easy for others to spread your words. Focus on themes and big picture initiatives that you can speak to with the expertise you've gathered in your role.","title":"Navigate org hierarchy"},{"location":"Reading-Notes/Leading-without-formal-authority/#leader-language-and-behaviors","text":"","title":"Leader Language and Behaviors"},{"location":"Reading-Notes/Leading-without-formal-authority/#add-value-in-meetings","text":"As an informal leader, you can create the opportunity to reframe meetings from blah to strategic, even if you're not the one running them. If you are organizing a meeting, planning is important. Send out an agenda and explain what you want to accomplish as a result of the meeting and include an accurate timeframe. Make sure the meeting is comprehensive and actionable. Ask in a collaborative tone, and then listen. Remember to end the meeting positively and thank the person who organized the meeting or the people who attended. Finally after the meeting, summarize the meeting and make sure there are action steps moving forward.","title":"Add value in meetings"},{"location":"Reading-Notes/Leading-without-formal-authority/#mindful-listening","text":"A leader is not always the loudest voice in the room. When you're mindfully listening, you're paying attention to a person's words, their tone of voice, their body language, small changes in their facial expressions and other small nuances that give you a more holistic picture of the person and the conversation. To mindfully listen, try and rid yourself of all kinds of distractions. Before you engage in the conversation, just take 10 seconds to clear your mind and think about the importance of that conversation and instead of thinking about what you want to say in the conversation, try to think about what you want to understand. During the conversation, look into the person's eyes. Repeat what they're saying in your own head and think about how they feel. Mindful listening can boost your self-esteem, help you retain more information and also increase your attention ability, and also help you sort and frame information before you finally speak.","title":"Mindful listening"},{"location":"Reading-Notes/Leading-without-formal-authority/#inspire-others","text":"As an informal leader you want people to be inspired to help you, to buy into your ideas and work hard for your organization. You'll help others feel inspired and you'll also feel more inspired yourself. People are looking for meaning and inspiration and they always gravitate to the people who provide it. To get emotional engagement, paint a mental picture of the customers you help, the impacts of the projects you've done or in some cases, the consequences of not doing them. When you can, include names, details, talk about how the other people feel and the lasting impact that you're having on them.","title":"Inspire others"},{"location":"Reading-Notes/Leading-without-formal-authority/#adapt-to-change","text":"When presented with a change, your first decision is to determine where you stand. As a rule of thumb, unless there are huge flaws, your best bet is to support changes brought by senior leadership. Even if the change has some small potential errors, your public support at the beginning will create enthusiasm. Later, you can raise those smaller issues at the right time and with the appropriate party. If you genuinely believe your organization is making a big mistake, it's your duty to say something, but say something privately to the person who enacted the change. If you've made the decision that you are mostly supportive of the change, be all in. There are no benefits to holding back and there are great advantages to displaying enthusiasm. If you've expressed your concerns to the appropriate senior party, there is no need for you to repeat them to everyone on your team. Also don't ignore the challenges. Refusing to acknowledge challenges only makes them more pronounced, and ignoring challenges erodes trust.","title":"Adapt to change"},{"location":"Reading-Notes/Leading-without-formal-authority/#setting-and-giving-expectations","text":"The more you clearly communicate expectations up front the better you will be able to avoid challenges of authority down the road. Connect the expectations to the outcome. Those around you should know why the expectations are important and who will be impacted by the result of fulfilling those expectations. Hold yourself accountable to the same expectations you are giving out. If you want everyone around you to be on time, prepared and efficient, hold yourself to that same standard. Don't be afraid to set high expectations. Setting a high bar leads people to rise to the occasion. Done right, the way you communicate with those around you will set your team up for success.","title":"Setting and giving expectations"},{"location":"Reading-Notes/Leading-without-formal-authority/#avoid-small-mistakes","text":"Small things like being late to a meeting matters and can undermine your long-term leadership. The same is true for others like missing an internal deadline and holding up a project, forgetting a small promise like promising to proof a peer's presentation, and not responding to emails or returning phone calls. These mistakes might not seem like a big deal in the moment, but others form an opinion of us based on a combination of a lot of small actions . Overcome and avoid these mistakes makes you look good in front of your peers, your boss, and senior leaders always helps you in the long run.","title":"Avoid small mistakes"},{"location":"Reading-Notes/Leading-without-formal-authority/#giveask-for-feedback","text":"Giving feedback is a great way to establish leadership as an individual contributor. Give feedback on things that matter , and be sure to give feedback when you're asked. It shows a good sign since natural leaders are often asked for feedback since they give good feedback. If you do know one or two things that could help, let them know kindly. Walk the other person through your thought process and your rationale . Meanwhile, consider giving feedback on high stakes things, even if you aren't asked. The best leaders, both formal and informal, rely heavily on those around them. Asking for feedback will help you excel and show people that you have a growth mindset.","title":"Give/Ask for feedback"},{"location":"Reading-Notes/Leading-without-formal-authority/#high-stakes-situations","text":"","title":"High-Stakes Situations"},{"location":"Reading-Notes/Leading-without-formal-authority/#lead-through-challenges","text":"Good leaders identify potential roadblocks early on and they help people redirect without shame or blame. If you see failure on the horizon, please keep the following in mind: rectify, offer the opportunity to fix the problem instead of lecturing about the mistakes reiterate, avoid sounding like you're issuing directives remedy, offer the help and ideas without taking over the project","title":"Lead through challenges"},{"location":"Reading-Notes/Leading-without-formal-authority/#look-after-bosss-mistakes","text":"There are two types of mistakes: private and public. Private mistakes are when your boss is about to make a bad decision or do something that will hurt them. Stepping up and help your boss avoid a potential blunder that could be public later; pull them aside and have a conversation about it. Position yourself as someone who has your boss's best interest at heart . Public mistakes are more challenging to navigate, since it is easy to discredit your boss and make them look bad in front of others. It's not your job to take over and fix the problem, but you can help your boss get on the right track . When you jump in in public, do keep in mind: validate your boss, make your intervention to be conversational, and flip the conversation back to your boss.","title":"Look after boss's mistakes"},{"location":"Reading-Notes/Leading-without-formal-authority/#ask-for-helps","text":"In any leadership journey, outside support is necessary. Support and help from others makes you better and stronger. When asking for help, framing the situation by stating about how the situation impacts you, the organization, or a customer, then make a specific ask. Finally, include a thank you in terms of the impact that help is making. Making sure you're clear, specific, and strategic in the way you ask for help makes people happier to help you and it makes you a stronger leader.","title":"Ask for helps"},{"location":"Reading-Notes/Leading-without-formal-authority/#long-last-influence","text":"","title":"Long-last Influence"},{"location":"Reading-Notes/Leading-without-formal-authority/#collaboration-culture","text":"As an informal leader, your ability to shape a culture of collaboration can create a huge ripple of positive impact throughout your organization. You and your team will be more effective when you put your brains together, and you'll also develop more influence throughout your organization. Being intentional with your language can help you foster the trust and respect required for successful teamwork. You can help foster psychological safety for your team by nipping toxic negativity, underhanded remarks, and gossip right in the bud, and don't lose the tether. Always give credit to the originator of the idea. If you're finding that your team doesn't collaborate as much as you'd like, you can model the behavior you'd like to see. Begin asking for their input on your work and model receiving that feedback well. Thank people for their feedback and act on it, follow up with them, and highlight the difference that feedback made and how you're grateful for their help. This lays the groundwork for others to do the same, and conveys the belief that we're a team who helps each other get better. Seek diverse perspectives. Ask yourself, who has worked on a project like this before?","title":"Collaboration culture"},{"location":"Reading-Notes/Leading-without-formal-authority/#continue-learning","text":"As you grow in your role, your organization, or even just in your ability to lead, you'll need to focus on both the hard and soft skills. Hard skills are things like software training, sales training, and product knowledge. These hard skills are foundational. Soft skills are things like leadership language, prioritizing, delegating, and communication. Balancing your time between soft skills and hard skills not only ensures you have the competencies necessary to succeed in your own role, it also makes you a valuable leader.","title":"Continue Learning"},{"location":"Reading-Notes/Leading-without-formal-authority/#create-influence","text":"During tough times, people tend to focus on blame and reactionary activities, which often leads to despair. Start by listening. Staying calm while others air their feelings of disappointment or frustration can help everyone move forward faster. To add empathy, you'll need to put your view on pause just for a second and validate what you've heard, that they're concerned. Address the root of repeated issues if you have seen it. In a crisis, doing something to help or at least lessen the blow.","title":"Create Influence"},{"location":"Reading-Notes/Leading-without-formal-authority/#build-trust","text":"Building trust among people takes time and effort. keep your word . It can be keeping something confidential or promising to take care of something. use body language . When speaking to someone, make eye contact and leaning into the conversation. be competent and warm . The first step in getting someone to trust you at work is to be competent and good at your job; the second step is to be warm, or being human and admitting vulnerability. For example, if you're an engineer, don't show your vulnerability in your math skills, instead be open about speaking of your fear of public speaking or something else that makes you a little bit more human. don't gossip. admit when you were wrong . If you made a mistake, which we all do, own up to it. trust others , until they give you a reason not to. If you want someone to trust you, show that you trust them.","title":"Build Trust"},{"location":"Reading-Notes/System-Design-Concepts/","text":"Beginner Guide to System Design \u00b6 Characteristics of Distributed System \u00b6 Scalability, Reliability, Availability, Efficiency, and Manageability. Design a large system different architectural pieces can be used how these pieces work with each other how to best utilize these pieces, what are trade-offs Scalability capability of a system, process, or network to grow and manage increased demand. increased data vol, increased work amount, increased work load... ensure performance horizontal scaling: adding more servers into pool of resources. easier and dynamical vertical scaling: adding more power (CPU, RAM, Storage, etc.) to existing servers. requires downtime and having an upper limit Reliability probability a system will fail in a given period. keeping delivering services even when some components fail. achieves thru redundancy, both software and data comes at a cost on removing single point of failure. Availability the time/percentage a system remains operational to perform required function. when taking down, or failed, considered unavailable during that time Reliability is availability over time availability - not necessarily reliable. reliable -> available Efficiency Two standard measures of efficiency: response time (latency), delay to obtain resp throughput (bandwidth), numb of delivery in a given time unit Serviceability or Manageability how to operate and maintain ease of diagnosing problems, ease of updating, how to operate, etc. auto failure detection can decrease or avoid system downtime CAP Theorem \u00b6 It is impossible for distributed system to simultaneously provide more than two of: Consistency, Availability, and Partition tolerance. Consistency: all nodes see the same data at the same time achieved by updating nodes before further reads Availability: every request gets a response on success/failure; system continues function even with node failures by replicating data across servers Partition tolerance: system continues to work despite message loss or partial failure. survive network failures that doesn't fail entire network Caching \u00b6 locality of ref principle: recently request data likely accessed again can be at all levels in an architecture, mostly found close to front-end App Server cache place local storage on a request layer node quickly return local cached data if exists, else query db for the case of LB, use global caches or distributed caches Distributed cache cache is divided using consistent hashing function, so a node can quickly know where to look for data existence in a dist cache. advantage: ease of adding nodes to expand cache space disadvantage: missing node; sol multiple copies of data on diff nodes Global cache all nodes use single cache space. need a server dedicated for this. no good as # of requests inc two forms, on cache miss either: server fetch from DB, or request nodes fetch from DB Content Distribution Network (CDN) for sites serving large amounts of static media server does query on miss can use Nginx (a light w HTTP server) for your app Cache Invalidation data modified in DB should be invalidated in cache: Write-through cache: data written into cache & DB at same time. (higher latency, write 2ice) Write-around cache: data written to DB only. When accessed, must query DB Write-back cache: data written to cache first, then write back to DB in an interval/cond. risk of data loss in crash of cache node Cache eviction policies First In First Out Last In First Out Least Recently Used Most Recently Used Least Frequently Used Random Replacement Indexes \u00b6 well-known DB indexing, improves queries performance index on a table makes it faster to search through the table and find the rows desired provides sorted list of data that is easily searchable by relevant info index is a data structure that points to location where data lives disadvantage: slower to add rows or updates/delete rows, because the need to update the index. better read performance, worse write performance unnecessary indexes should be avoided/removed Consistent Hashing \u00b6 Distributed Hash Table (DHT): key, value, hash function index = hash_function(key) 'key % n' hashing approach drawbacks: not horizontally scalability. new cache host adding to system requires re-hash all existing mappings may not be load balanced, especially non-uniformly distributed data (some caches busy while others idle) Consistent Hashing definition allows distributing data across a cluster and minimize reorganization when nodes are added/removed easier scale up/down caching system when hash table resized, only k/n keys need to be remapped (k = total keys, n = total numb of servers) Given a list of cache servers, hash them to integers in the range map a key to a server - Hash it to a single integer - Move clockwise on the ring until finding the first cache it encounters - That cache is the one contains the key add a cache server on the ring, causing the portion of keys previously mapping to next server map to this new server, and removed from the next server removing a cache server on the ring, causing the keys mapping to this one mapping to next server on the ring Virtual replicas of the servers on the ring, enables more evenly distributed keys to each server, and remains distributed when servers being added/removed from the ring Long-Polling vs. Web Sockets vs. Server-Sent Events \u00b6 all are popular and common protocols between client and web server. Ajax Polling client repeatedly polls a server for data. client opens a connection & request data from server request page sends requests to server at regular intervals server calculate resp and sends back client repeated 1-3 periodically to get updates from server problem: many requests could be empty data and creating HTTP overhead HTTP Long-Polling aka. \"Hanging GET\", client requests expected that server may not resp immediately server holds client request if no data available, then sends it when it becomes available. client sends new request immediately follows receiving the resp, to make sure server can always push updated data to client immediately when its available. Long-Polling request can be timeout so new request has to be sent when previous one timeouts. Web Sockets use handshake to establish persistent connection so both can exchange data in both direction any time. two-way ongoing conversation can take place b/w two machines. Server-Sent Events (SSEs) use a persistent long-term connection b/w client and server. Server can send data to client any time, but not client to server, which requires separate Http connection. best when need real-traffic from server to client or when server is generating data in a loop to be sent to client. Queues \u00b6 manage requests in large-scale distributed system. high performance requires different components work asynchronously thru queues. provide some protection from service outages and failures. retry service requests that failed implemented on asynchronous common protocol, client get Acknowledgement when request received, serves as a reference for the results of work when client requires it. open source tools like (RabbitMQ, ZeroMQ, Active MQ, & BeanstalkD) Proxy \u00b6 intermediary piece of hardware/software b/w client & back-end server. receives requests, relays to servers typically used to filter requests, log requests, transform requests (add/remove headers, encryption/decryption, compression). cache can serve lots of requests. coordinating requests, optimize request traffic. (ex. collapse similar data access into one \"collapsed forwarding\") particularly useful when under high load or have limited caching available (batch several requests into one) Load Balancing (LB) \u00b6 spread traffic across a cluster of servers improves responsiveness & availability of app, web, or db keeps track of resource status stop distribute to failed server immediately sits between client & server, balancing app requests reduces individual server load, prevents single pt of failure detects bottlenecks before they happen Three places for LB b/w user & web servers b/w web servers & internal platform layer (ex. app servers or cache servers) b/w internal platform layer and db LB algorithms Prerequisite: Health Checks: keep list of alive and healthy server. remove unresponsive servers Least Connection: fewest active connections Least Response Time: fewest active connections & lowest avg resp time Least bandwidth: serving least amount of traffic in Mbps Round Robin: cycles thru list of servers, good for equal spec servers & non-persist connections Weighted Round Robin: each server assigned a weight (processing capacity), higher weight get new connections first and more connections IP Hash: calculate hash of client IP to assign server Redundant LB multi-LB setup, each monitors health of the others, one active, one passive(s) in case of active LB failure, another one takes over and becomes active one Ways to implement LB Smart Clients: client take a pool of service hosts and balances load across them detect failed hosts and recovered hosts, adding new hosts Hardware LB: most expensive, high performance (ex. Citrix NetScaler) Software LB: (ex. HAProxy) SQL vs. NoSQL \u00b6 SQL Relational DB has predefined schemas, structured stores data in rows and columns, each row about one entity, columns being data points NoSQL Non-relational DB are unstructured, distributed, and have dynamic schema. Key-Value Stores: data stored in an array of key-value pairs (ex. Redis, Voldemort, Dynamo) Document Databases: data stored in documents, & docs are grouped together in collections each doc can have different structure (ex. CouchDB, MongoDB) Wide-Column Databases: column families, containers for rows. no need to know all columns up front, each row unnecessary same numb of columns. good for aanalyzing large daatasets (ex. Cassandra, HBase) Graph Databases: data relations represented in a graph nodes (entities), properties (entity info), & lines (entities connections). (ex. Neo4J, InfiniteGraph) High level differences Storage approach Schema, fixed/dynamic Querying Scalability, SQL(vertical); NoSQL(horizontal) Reliability or ACID Compliance (Atomicity, Consistency, Isolation, Durability). SQL still better at data reliability and transactions performance, and NoSQL sacrifice ACID compliance for performance and scalability When to use which SQL: need to ensure ACID, which reduces anomalies and protects db integrity data is structured and unchanging. no massive growth expected NoSQL: prevent data from being bottleneck by distributing DBs large volumes of data storage having little or no structure, no need to define type in advance use of cloud computing and storage for scaling up rapid development, little prep ahead of time, little downtime b/w versions Redundancy and Replication \u00b6 duplicate critical data or services to increase reliability remove single pt of failure, fail-overs when one instance down shared-nothing architecture, where each node can operate independent of one another new servers can be added without conditions Sharding (Data Partitioning) \u00b6 break up big DB into smaller parts improve manageability, performance, availability, & LB after certain scale pt, cheaper to scale horizontally (more same machines) than grow vertically (upgrade server gears) Partitioning Methods horizontal partitioning: range based sharding. best for evenly distributed ranges disadvantage: can lead to unbalanced servers for bad range selection vertical partitioning: divide into tables for data related to a specific feature to their own server disadvantage: when app grows, may need to partition feature specific DB across various servers dictionary based partitioning: loosely coupled approach lookup service which knows partitioning scheme, holds mapping between each tuple key to its DB server Partitioning Criteria Key or Hash-based partitioning: apply a hash function to some key attribute of the entries need redistribution of data when adding new DB servers List partitioning: each partition assigned list of values look up partition contains our key and store in its list Round-robin partitioning: ensures uniform data dist. i mod n Composite partitioning: combine above schemes. a consistent hashing is hash and list: hash reduces key space to a smaller size Common Problems of Sharding operations across multiple tables or multiple rows in the same table, no longer run on the same server Joins and Denormalization: joins is inefficient across multiple servers. Need to denormalize the database so that previous operation requires joins can perform from a single table Referential integrity: foreign keys in sharded db can be difficult. need to enforce it in application code Rebalancing: when distribution is no longer uniform, or lots of load on one shard, either have to create more shards, or rebalance existing shards. these are cost of using sharding. using directory based partitioning can make rebalancing easier at the cost of increasing system complexity and creating new pt of failure. System Design Interviews, how to approach \u00b6 Scoping the problem ask questions to understand constraints and use cases Sketching up an abstract design illustrate building blocks and relationships b/w them Identifying & addressing bottlenecks no absolute answers, open-ended As a candidate learn from existing systems prepare ahead, learn based on real-life products, issues, & challenges foster analytical ability and questioning on the problem lead the conversation communicate your idea to interviewer, your thought process, what you are considering solve by breaking it down top-down, modularize into modules, tackle each independently deal with bottlenecks talk about possible solutions to these, and trade-offs and their impacts on the sys. try understand interviewer's intention and direction","title":"System Design Guide"},{"location":"Reading-Notes/System-Design-Concepts/#beginner-guide-to-system-design","text":"","title":"Beginner Guide to System Design"},{"location":"Reading-Notes/System-Design-Concepts/#characteristics-of-distributed-system","text":"Scalability, Reliability, Availability, Efficiency, and Manageability. Design a large system different architectural pieces can be used how these pieces work with each other how to best utilize these pieces, what are trade-offs Scalability capability of a system, process, or network to grow and manage increased demand. increased data vol, increased work amount, increased work load... ensure performance horizontal scaling: adding more servers into pool of resources. easier and dynamical vertical scaling: adding more power (CPU, RAM, Storage, etc.) to existing servers. requires downtime and having an upper limit Reliability probability a system will fail in a given period. keeping delivering services even when some components fail. achieves thru redundancy, both software and data comes at a cost on removing single point of failure. Availability the time/percentage a system remains operational to perform required function. when taking down, or failed, considered unavailable during that time Reliability is availability over time availability - not necessarily reliable. reliable -> available Efficiency Two standard measures of efficiency: response time (latency), delay to obtain resp throughput (bandwidth), numb of delivery in a given time unit Serviceability or Manageability how to operate and maintain ease of diagnosing problems, ease of updating, how to operate, etc. auto failure detection can decrease or avoid system downtime","title":"Characteristics of Distributed System"},{"location":"Reading-Notes/System-Design-Concepts/#cap-theorem","text":"It is impossible for distributed system to simultaneously provide more than two of: Consistency, Availability, and Partition tolerance. Consistency: all nodes see the same data at the same time achieved by updating nodes before further reads Availability: every request gets a response on success/failure; system continues function even with node failures by replicating data across servers Partition tolerance: system continues to work despite message loss or partial failure. survive network failures that doesn't fail entire network","title":"CAP Theorem"},{"location":"Reading-Notes/System-Design-Concepts/#caching","text":"locality of ref principle: recently request data likely accessed again can be at all levels in an architecture, mostly found close to front-end App Server cache place local storage on a request layer node quickly return local cached data if exists, else query db for the case of LB, use global caches or distributed caches Distributed cache cache is divided using consistent hashing function, so a node can quickly know where to look for data existence in a dist cache. advantage: ease of adding nodes to expand cache space disadvantage: missing node; sol multiple copies of data on diff nodes Global cache all nodes use single cache space. need a server dedicated for this. no good as # of requests inc two forms, on cache miss either: server fetch from DB, or request nodes fetch from DB Content Distribution Network (CDN) for sites serving large amounts of static media server does query on miss can use Nginx (a light w HTTP server) for your app Cache Invalidation data modified in DB should be invalidated in cache: Write-through cache: data written into cache & DB at same time. (higher latency, write 2ice) Write-around cache: data written to DB only. When accessed, must query DB Write-back cache: data written to cache first, then write back to DB in an interval/cond. risk of data loss in crash of cache node Cache eviction policies First In First Out Last In First Out Least Recently Used Most Recently Used Least Frequently Used Random Replacement","title":"Caching"},{"location":"Reading-Notes/System-Design-Concepts/#indexes","text":"well-known DB indexing, improves queries performance index on a table makes it faster to search through the table and find the rows desired provides sorted list of data that is easily searchable by relevant info index is a data structure that points to location where data lives disadvantage: slower to add rows or updates/delete rows, because the need to update the index. better read performance, worse write performance unnecessary indexes should be avoided/removed","title":"Indexes"},{"location":"Reading-Notes/System-Design-Concepts/#consistent-hashing","text":"Distributed Hash Table (DHT): key, value, hash function index = hash_function(key) 'key % n' hashing approach drawbacks: not horizontally scalability. new cache host adding to system requires re-hash all existing mappings may not be load balanced, especially non-uniformly distributed data (some caches busy while others idle) Consistent Hashing definition allows distributing data across a cluster and minimize reorganization when nodes are added/removed easier scale up/down caching system when hash table resized, only k/n keys need to be remapped (k = total keys, n = total numb of servers) Given a list of cache servers, hash them to integers in the range map a key to a server - Hash it to a single integer - Move clockwise on the ring until finding the first cache it encounters - That cache is the one contains the key add a cache server on the ring, causing the portion of keys previously mapping to next server map to this new server, and removed from the next server removing a cache server on the ring, causing the keys mapping to this one mapping to next server on the ring Virtual replicas of the servers on the ring, enables more evenly distributed keys to each server, and remains distributed when servers being added/removed from the ring","title":"Consistent Hashing"},{"location":"Reading-Notes/System-Design-Concepts/#long-polling-vs-web-sockets-vs-server-sent-events","text":"all are popular and common protocols between client and web server. Ajax Polling client repeatedly polls a server for data. client opens a connection & request data from server request page sends requests to server at regular intervals server calculate resp and sends back client repeated 1-3 periodically to get updates from server problem: many requests could be empty data and creating HTTP overhead HTTP Long-Polling aka. \"Hanging GET\", client requests expected that server may not resp immediately server holds client request if no data available, then sends it when it becomes available. client sends new request immediately follows receiving the resp, to make sure server can always push updated data to client immediately when its available. Long-Polling request can be timeout so new request has to be sent when previous one timeouts. Web Sockets use handshake to establish persistent connection so both can exchange data in both direction any time. two-way ongoing conversation can take place b/w two machines. Server-Sent Events (SSEs) use a persistent long-term connection b/w client and server. Server can send data to client any time, but not client to server, which requires separate Http connection. best when need real-traffic from server to client or when server is generating data in a loop to be sent to client.","title":"Long-Polling vs. Web Sockets vs. Server-Sent Events"},{"location":"Reading-Notes/System-Design-Concepts/#queues","text":"manage requests in large-scale distributed system. high performance requires different components work asynchronously thru queues. provide some protection from service outages and failures. retry service requests that failed implemented on asynchronous common protocol, client get Acknowledgement when request received, serves as a reference for the results of work when client requires it. open source tools like (RabbitMQ, ZeroMQ, Active MQ, & BeanstalkD)","title":"Queues"},{"location":"Reading-Notes/System-Design-Concepts/#proxy","text":"intermediary piece of hardware/software b/w client & back-end server. receives requests, relays to servers typically used to filter requests, log requests, transform requests (add/remove headers, encryption/decryption, compression). cache can serve lots of requests. coordinating requests, optimize request traffic. (ex. collapse similar data access into one \"collapsed forwarding\") particularly useful when under high load or have limited caching available (batch several requests into one)","title":"Proxy"},{"location":"Reading-Notes/System-Design-Concepts/#load-balancing-lb","text":"spread traffic across a cluster of servers improves responsiveness & availability of app, web, or db keeps track of resource status stop distribute to failed server immediately sits between client & server, balancing app requests reduces individual server load, prevents single pt of failure detects bottlenecks before they happen Three places for LB b/w user & web servers b/w web servers & internal platform layer (ex. app servers or cache servers) b/w internal platform layer and db LB algorithms Prerequisite: Health Checks: keep list of alive and healthy server. remove unresponsive servers Least Connection: fewest active connections Least Response Time: fewest active connections & lowest avg resp time Least bandwidth: serving least amount of traffic in Mbps Round Robin: cycles thru list of servers, good for equal spec servers & non-persist connections Weighted Round Robin: each server assigned a weight (processing capacity), higher weight get new connections first and more connections IP Hash: calculate hash of client IP to assign server Redundant LB multi-LB setup, each monitors health of the others, one active, one passive(s) in case of active LB failure, another one takes over and becomes active one Ways to implement LB Smart Clients: client take a pool of service hosts and balances load across them detect failed hosts and recovered hosts, adding new hosts Hardware LB: most expensive, high performance (ex. Citrix NetScaler) Software LB: (ex. HAProxy)","title":"Load Balancing (LB)"},{"location":"Reading-Notes/System-Design-Concepts/#sql-vs-nosql","text":"SQL Relational DB has predefined schemas, structured stores data in rows and columns, each row about one entity, columns being data points NoSQL Non-relational DB are unstructured, distributed, and have dynamic schema. Key-Value Stores: data stored in an array of key-value pairs (ex. Redis, Voldemort, Dynamo) Document Databases: data stored in documents, & docs are grouped together in collections each doc can have different structure (ex. CouchDB, MongoDB) Wide-Column Databases: column families, containers for rows. no need to know all columns up front, each row unnecessary same numb of columns. good for aanalyzing large daatasets (ex. Cassandra, HBase) Graph Databases: data relations represented in a graph nodes (entities), properties (entity info), & lines (entities connections). (ex. Neo4J, InfiniteGraph) High level differences Storage approach Schema, fixed/dynamic Querying Scalability, SQL(vertical); NoSQL(horizontal) Reliability or ACID Compliance (Atomicity, Consistency, Isolation, Durability). SQL still better at data reliability and transactions performance, and NoSQL sacrifice ACID compliance for performance and scalability When to use which SQL: need to ensure ACID, which reduces anomalies and protects db integrity data is structured and unchanging. no massive growth expected NoSQL: prevent data from being bottleneck by distributing DBs large volumes of data storage having little or no structure, no need to define type in advance use of cloud computing and storage for scaling up rapid development, little prep ahead of time, little downtime b/w versions","title":"SQL vs. NoSQL"},{"location":"Reading-Notes/System-Design-Concepts/#redundancy-and-replication","text":"duplicate critical data or services to increase reliability remove single pt of failure, fail-overs when one instance down shared-nothing architecture, where each node can operate independent of one another new servers can be added without conditions","title":"Redundancy and Replication"},{"location":"Reading-Notes/System-Design-Concepts/#sharding-data-partitioning","text":"break up big DB into smaller parts improve manageability, performance, availability, & LB after certain scale pt, cheaper to scale horizontally (more same machines) than grow vertically (upgrade server gears) Partitioning Methods horizontal partitioning: range based sharding. best for evenly distributed ranges disadvantage: can lead to unbalanced servers for bad range selection vertical partitioning: divide into tables for data related to a specific feature to their own server disadvantage: when app grows, may need to partition feature specific DB across various servers dictionary based partitioning: loosely coupled approach lookup service which knows partitioning scheme, holds mapping between each tuple key to its DB server Partitioning Criteria Key or Hash-based partitioning: apply a hash function to some key attribute of the entries need redistribution of data when adding new DB servers List partitioning: each partition assigned list of values look up partition contains our key and store in its list Round-robin partitioning: ensures uniform data dist. i mod n Composite partitioning: combine above schemes. a consistent hashing is hash and list: hash reduces key space to a smaller size Common Problems of Sharding operations across multiple tables or multiple rows in the same table, no longer run on the same server Joins and Denormalization: joins is inefficient across multiple servers. Need to denormalize the database so that previous operation requires joins can perform from a single table Referential integrity: foreign keys in sharded db can be difficult. need to enforce it in application code Rebalancing: when distribution is no longer uniform, or lots of load on one shard, either have to create more shards, or rebalance existing shards. these are cost of using sharding. using directory based partitioning can make rebalancing easier at the cost of increasing system complexity and creating new pt of failure.","title":"Sharding (Data Partitioning)"},{"location":"Reading-Notes/System-Design-Concepts/#system-design-interviews-how-to-approach","text":"Scoping the problem ask questions to understand constraints and use cases Sketching up an abstract design illustrate building blocks and relationships b/w them Identifying & addressing bottlenecks no absolute answers, open-ended As a candidate learn from existing systems prepare ahead, learn based on real-life products, issues, & challenges foster analytical ability and questioning on the problem lead the conversation communicate your idea to interviewer, your thought process, what you are considering solve by breaking it down top-down, modularize into modules, tackle each independently deal with bottlenecks talk about possible solutions to these, and trade-offs and their impacts on the sys. try understand interviewer's intention and direction","title":"System Design Interviews, how to approach"},{"location":"Reading-Notes/Why-Latency-Matters/","text":"This notes it taken from an article on High Scalability Latency matters. Amazon found every 100ms of latency cost them 1% in sales. Google found an extra .5 seconds in search page generation time dropped traffic by 20%. A broker could lose $4 million in revenues per millisecond if their electronic trading platform is 5 milliseconds behind the competition. The less interactive a site becomes the more likely users are to click away and do something else. Slow sites generally lead to higher customer defection rates, which lead to lower conversation rates, which results in lower sales. Low-latency and high-latency are relative terms. A system has low-latency if it's low enough to meet requirements, otherwise it's a high-latency system. If you have a network link with low bandwidth then it's an easy matter of putting several in parallel to make a combined link with higher bandwidth, but if you have a network link with bad latency then no amount of money can turn any number of them into a link with good latency. --Stuart Cheshire Possible sources of latency : - low level infrastructure - OS, CPU, Memory, Storage I/O, Network I/O - high level infrastructure - DNS, TCP, Web server, network links, routers - in most cases, half of time is spent from the network hops; the bottleneck is the Internet itself - incentive for edge-computing, putting servers closer to users - software processing - the server's code processing time limited by the speed of the processor and the level of optimization of the software after compilation - algorithm and logic plays a big part - frontend - to process the backend response and display accordingly for the user to see can take time - very important, as the end-user response time takes up to 80% of the time - most likely speeding up the frontend is more effective than the backend - service dependency latency - more dependent components increase latency - propagation latency - the speed at which data travels through the link at physical layer - every 20km adds about 100ms propagation latency - transmission latency - the speed at which data is transmitted on a communication link - not related to distance; more like the transmission speed limited by the hardware (or how much paid) - geographical distribution - BCP requires running in multiple datacenters and add WAN latency constraints - messaging latency - Intermediaries, Garbage Collection, Retransmissions, Reordering, Batching, CPU Scheduling, Socket Buffers, Network Queuing, Network Access Control, Serialization. If we want to increase interactivity we have to address every component in the system that introduces latency and minimize or remove it's contribution. As latency increases work stays queued at all levels of the system which puts stress everywhere. Some of the problems caused by higher latency : Queues grow; Memory grows; Timeouts cascade; Memory grows; Paging increases; Retries cascade; State machines reset; Locks are held longer; Threads block; Deadlock occurs; Predictability declines; Throughput declines; Messages drop; Quality plummets. That's how it costs sells and bad user-experiences. The general algorithm for managing latency : - continually map, monitor, and characterize all sources of latency - remove and/or minimize all latency sources that are found - target your latency slimming efforts where it matters the most Dan Pritchett's Lessons for Managing Latency - Create Loosely Couple Components - failure at one place won't fail the whole system or other components - can be independently scaled and engineered for latency - Use Asynchronous Interfaces - set expectation of asynchronous behavior between components - synchronous low-latency interactions doesn't allow architecture flexibility - Horizontally Scale from the Start - will be easier to scale later, when the system grows - Create an Active/Active Architecture - differs from typical BCP approach (active/backup, aka hot/warm) - all data centers operate simultaneously - users are served from the closest data center - much lower latency for all users - Use a BASE (basically available, soft state, eventually consistent) instead of ACID (atomicity, consistency, isolation, durability) Shared Storage Model - more tolerant to latency - makes one update to one partition and return - no latency from coordinating a transaction across multiple database servers Database services cannot ensure all three of the following properties at once: Consistency, Availability, Partition tolerance --The CAP Theorem Latency Reduction Ideas - Use Application-level Caches - each app instance serves out of cache unless misses - need time to warm up the cache for new app instances - Use a CDN to distribute some contents - better speed and latency for users everywhere around the world - Use Caching Proxy Server(s) - each app instance call the cache proxy server to get expensive contents - Optimize Virtual Machines - virtualized I/O can suffer a substantial performance penalty - Use Ajax to minimize perceived latency to the user - clever UI design can make a site feel faster than it really is - Optimize firewalls - Use Small Memory Chunks When Using Java - GC in Java kills latency - use more VMs and less memory in each VM instead of VM with a lot of memory, avoids large GC Some Suggestions on Server Architecture to Reduce Latency - Stop Serializing/Deserializing Messages - leave messages in a binary compressed format and decode only on access - Load Balance Across Read Replicas - the more copies of objects you have the more work you can perform in parallel - consider keeping objects replicas for both high availability and high scalability - Don't Block - block for any reason and your performance tanks because not only do you incur the latency of the operation but there's added rescheduling latency as well - Minimize Memory Paging - avoid memory thrashing - difficult to achieve - Minimize/Remove locking - locks add latency and variability to a processing pipeline - Colocate Application - servers adds less latency when communicating with each other - use a cloud","title":"Why Latency Matters"},{"location":"System-Design/Kafka/","text":"Kafka is an event streaming platform that implements the key capabilities end-to-end, and all the functionalities are provided in a distributed , highly scalable , elastic , fault-tolerant , and secure mannor. Kafka is a distributed system consisting of servers and clients that communicate via a high-performance TCP network protocol. Kafka servers can span multiple regions. Some form the storage layer, called brokers . Others run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. Kafka clients can be distributed applications and microservices you built that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner. Kafka ships with some such clients included; there are also Kafka Streams library and REST APIs for many languages. Kafka is widely used in software industry and is backed by a large and active community of opensource engineers. Main Concepts \u00b6 An event records the fact that \"something happened\" in the world or in your business. An event has a key, value, timestamp , and optional metadata headers. Producers are those client applications that publish (write) events to Kafka. Clients talk to Producers/Brokers to insert events to Kafka, and several other components work with Producers to ensure the non-duplicated successful deliver of a client message: A Client sends the message to Producer and get an ACK. Client sends a synchronous blocking request to the Future server Once Producer gets a message, it append it to the RecordAccumulator Once certain condition is met (time, numb of records), the RecordAccumulator prepares and make the accumulated records as a RecordAppendResult to persist to storage Once it is complete, it sends a confirmation to the Future server The Future server responses to the Client that the message send is complete. Consumers are those that subscribe to (read and process) these events. Producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability. Consumer group is the unit which Kafka uses to map one topic partition to. Each partition can only be connected and consumed by one of the consumers within a consumer group. A consumer group can have one or more consumers. A consumer can connect to multiple partitions from multiple topics. A partition, however, can ONLY have one consumer connected to it. Events are organized and durably stored in topics . Topics in Kafka are always multi-producer and multi-subscriber. Events in a topic can be read as often as needed and are NOT deleted after consumption; an event retain limit can be defined instead. Kafka's performance is effectively constant with respect to data size. Topics are partitioned , meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers. Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written. Every topic can be replicated for fault-tolerance and availability. Kafka APIs \u00b6 Producer API - publish (write) a stream of events to one or more Kafka topics Consumer API - subscribe to (read) one or more topics and to process the stream of events produced Streams API - for implementing stream processing applications and microservices, provides higher-level functions including transformations, stateful operations like aggregations and joins, windowing, processing based on event-time, etc., effectively transforming the input streams to output streams alternative open source stream processing tools include Apache Storm and Apache Samza Connect API - build and run reusable data import/export connectors that consume (read) or produce (write) streams of events from and to external systems and applications Admin API - manage and inspect topics, brokers, and other Kafka objects talks to the Zookeeper Kafka Deeper Look \u00b6 Kafka's High Throughput \u00b6 Log-based message \u00b6 log-based event message format allows append-only operation, fast to write. Log files are saved in directories of the name topic name appended with partition number . The log files in each partition are indexed for fast search. Each log message is of 1+4+n bytes long: \"magic\" value of 1 byte: version number CRC (Cyclic Redundancy Check) of 4 bytes: ensures message integrity payload of n bytes: actual message body Topic Partitioning \u00b6 Topics are partitioned to distribute the load and allow fast search and send. Each partition logs are saved as same-sized segments , the number of messages in each segment may vary depends on the message length. Logs in each partition are append-only and write to the last segment; the segment got written to disk once a condition is met. Logs are read in order. Log indexes stores the start location of each message based on its id. There is also a timeIndex that stores the start location based on its timestamp. Batch Send/Receive \u00b6 Batch sending and receiving with data compression saves number of network calls and bandwidth sendfile syscall \u00b6 A typical read file and send file over network flow is roughly as such: Server program instructs Kernel to load a file from disk Kernel does syscall to load file into Kernel buffer memory File content is copied from Kernel buffer to User space buffer memory for the Server program Kernel returns control back to the Server program Server program initiates send to the Network interface File content is optionally serialized and put back to memory File content is copied to socket buffer File content is sent through Network interface Data pass through the Internet hops and reach the target user Use Linux sendfile system call to pass files to network socket buffer, avoid re-copying IO from filesystem to user space memory and then copied to the network socket buffer memory, which saves two copy operations. Use Cases \u00b6 Messing Queue \u00b6 Unlike other MessageQueue solutions such as ActiveMQ, Kafka does NOT guarantee the strict order of messages being processed, because Kafka holds messages for potential re-processing. Message order on each partition is ordered. Use key and offset to keep a rough order of messages during read. Activity Tracking \u00b6 Metrics Collection \u00b6 Log Aggregation \u00b6 Stream Processing \u00b6 Event Sourcing \u00b6 Commit Logs \u00b6","title":"Kafka"},{"location":"System-Design/Kafka/#main-concepts","text":"An event records the fact that \"something happened\" in the world or in your business. An event has a key, value, timestamp , and optional metadata headers. Producers are those client applications that publish (write) events to Kafka. Clients talk to Producers/Brokers to insert events to Kafka, and several other components work with Producers to ensure the non-duplicated successful deliver of a client message: A Client sends the message to Producer and get an ACK. Client sends a synchronous blocking request to the Future server Once Producer gets a message, it append it to the RecordAccumulator Once certain condition is met (time, numb of records), the RecordAccumulator prepares and make the accumulated records as a RecordAppendResult to persist to storage Once it is complete, it sends a confirmation to the Future server The Future server responses to the Client that the message send is complete. Consumers are those that subscribe to (read and process) these events. Producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability. Consumer group is the unit which Kafka uses to map one topic partition to. Each partition can only be connected and consumed by one of the consumers within a consumer group. A consumer group can have one or more consumers. A consumer can connect to multiple partitions from multiple topics. A partition, however, can ONLY have one consumer connected to it. Events are organized and durably stored in topics . Topics in Kafka are always multi-producer and multi-subscriber. Events in a topic can be read as often as needed and are NOT deleted after consumption; an event retain limit can be defined instead. Kafka's performance is effectively constant with respect to data size. Topics are partitioned , meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers. Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written. Every topic can be replicated for fault-tolerance and availability.","title":"Main Concepts"},{"location":"System-Design/Kafka/#kafka-apis","text":"Producer API - publish (write) a stream of events to one or more Kafka topics Consumer API - subscribe to (read) one or more topics and to process the stream of events produced Streams API - for implementing stream processing applications and microservices, provides higher-level functions including transformations, stateful operations like aggregations and joins, windowing, processing based on event-time, etc., effectively transforming the input streams to output streams alternative open source stream processing tools include Apache Storm and Apache Samza Connect API - build and run reusable data import/export connectors that consume (read) or produce (write) streams of events from and to external systems and applications Admin API - manage and inspect topics, brokers, and other Kafka objects talks to the Zookeeper","title":"Kafka APIs"},{"location":"System-Design/Kafka/#kafka-deeper-look","text":"","title":"Kafka Deeper Look"},{"location":"System-Design/Kafka/#kafkas-high-throughput","text":"","title":"Kafka's High Throughput"},{"location":"System-Design/Kafka/#log-based-message","text":"log-based event message format allows append-only operation, fast to write. Log files are saved in directories of the name topic name appended with partition number . The log files in each partition are indexed for fast search. Each log message is of 1+4+n bytes long: \"magic\" value of 1 byte: version number CRC (Cyclic Redundancy Check) of 4 bytes: ensures message integrity payload of n bytes: actual message body","title":"Log-based message"},{"location":"System-Design/Kafka/#topic-partitioning","text":"Topics are partitioned to distribute the load and allow fast search and send. Each partition logs are saved as same-sized segments , the number of messages in each segment may vary depends on the message length. Logs in each partition are append-only and write to the last segment; the segment got written to disk once a condition is met. Logs are read in order. Log indexes stores the start location of each message based on its id. There is also a timeIndex that stores the start location based on its timestamp.","title":"Topic Partitioning"},{"location":"System-Design/Kafka/#batch-sendreceive","text":"Batch sending and receiving with data compression saves number of network calls and bandwidth","title":"Batch Send/Receive"},{"location":"System-Design/Kafka/#sendfile-syscall","text":"A typical read file and send file over network flow is roughly as such: Server program instructs Kernel to load a file from disk Kernel does syscall to load file into Kernel buffer memory File content is copied from Kernel buffer to User space buffer memory for the Server program Kernel returns control back to the Server program Server program initiates send to the Network interface File content is optionally serialized and put back to memory File content is copied to socket buffer File content is sent through Network interface Data pass through the Internet hops and reach the target user Use Linux sendfile system call to pass files to network socket buffer, avoid re-copying IO from filesystem to user space memory and then copied to the network socket buffer memory, which saves two copy operations.","title":"sendfile syscall"},{"location":"System-Design/Kafka/#use-cases","text":"","title":"Use Cases"},{"location":"System-Design/Kafka/#messing-queue","text":"Unlike other MessageQueue solutions such as ActiveMQ, Kafka does NOT guarantee the strict order of messages being processed, because Kafka holds messages for potential re-processing. Message order on each partition is ordered. Use key and offset to keep a rough order of messages during read.","title":"Messing Queue"},{"location":"System-Design/Kafka/#activity-tracking","text":"","title":"Activity Tracking"},{"location":"System-Design/Kafka/#metrics-collection","text":"","title":"Metrics Collection"},{"location":"System-Design/Kafka/#log-aggregation","text":"","title":"Log Aggregation"},{"location":"System-Design/Kafka/#stream-processing","text":"","title":"Stream Processing"},{"location":"System-Design/Kafka/#event-sourcing","text":"","title":"Event Sourcing"},{"location":"System-Design/Kafka/#commit-logs","text":"","title":"Commit Logs"},{"location":"System-Design/System_Design/","text":"System Design is the heart of software engineering. Each successful and sophisticated software system takes careful and thorough design process. Some good extra resources for system design https://www.educative.io/courses/grokking-the-system-design-interview/m2ygV4E81AR Approach System Design \u00b6 Write down the functional requirements of the system of interest what is the user/audience what I/O interface we are giving to the user what functionalities are provided to the user go into more detail for those may affect the design Write down the non-functional requirements highly available low latency analytics, monitoring and alerts Possible constraints what is the estimated usage capacity start from small estimate, then extend to larger time range look at RPS for number of servers, latency for whether we want to use cache, request size for network bandwidth, data unit storage size for total storage what is the service SLA think of ways to meet this SLA later in design Detailed design thought process high level components System APIs what we are offerring for data exchange interaction between components Things should be touched on Should it be a pull or push based architecture Database whether to use relational database or NoSQL database is the system expected to be read-heavy or write-heavy define the DB schema early will help understand the data flow between components and guides towards data partitioning Data structure and algorithm used in main components can we improve the efficiency by doing some parallel processing or producer consumer to take away some of the expensive calculations during serving the request? what special cases we need to consider to be treated differently from the majority of use cases? How would a normal case transform into a special case? Do we need to consider separating read and write into different servers Do we need data partitioning/sharding how we can ensure fairly evenly distributed, consistent hashing partition on userId, objectId, hash Do we need data replication Do we need Cache Cache strategy, eviction policy Cache memory, how much keys to cache the flow to update cache Load Balancer where we should place the load balancer Bottlenecks any places where a SPF can happen, how to resolve this bottleneck Monitoring how to make sure we know when the system performance has degraded Cleanup offline jobs to cleanup, keep system healthy Security abuse protection SQL vs. NoSQL \u00b6 Comparison in table Parameter SQL NOSQL Definition SQL databases are primarily called RDBMS or Relational Databases NoSQL databases are primarily called as Non-relational or distributed database Design Traditional RDBMS uses SQL syntax and queries to analyze and get the data for further insights. They are used for OLAP systems. NoSQL database system consists of various kind of database technologies. These databases were developed in response to the demands presented for the development of the modern application. Query Language Structured query language (SQL) No declarative query language Type SQL databases are table based databases NoSQL databases can be document based, key-value pairs, graph databases Schema SQL databases have a predefined schema NoSQL databases use dynamic schema for unstructured data. Scalability SQL databases are vertically scalable NoSQL databases are horizontally scalable Examples Oracle, Postgres, and MS-SQL. MongoDB, Redis, Neo4j, Cassandra, Hbase. Use cases An ideal choice for the complex query intensive environment. It is not good fit complex queries. Hierarchical data storage SQL databases are not suitable for hierarchical data storage. More suitable for the hierarchical data store as it supports key-value pair method. Variations One type with minor variations. Many different types which include key-value stores, document databases, and graph databases. Open-source A mix of open-source like Postgres & MySQL, and commercial like Oracle Database. Open-source Consistency It should be configured for strong consistency. It depends on DBMS as some offers strong consistency like MongoDB, whereas others offer only offers eventual consistency, like Cassandra. Best Used for RDBMS database is the right option for solving ACID problems. NoSQL is a best used for solving data availability problems Importance It should be used when data validity is super important Use when it\u2019s more important to have fast data than correct data Hardware Specialized DB hardware(Oracle Exadata, etc.) Commodity hardware Network Highly available network(Infiniband, Fabric Path, etc.) Commodity network(Ethernet, etc.) Storage Type Highly Available Storage (SAN, RAID, etc.) Commodity drives storage (standardHDDs, JBOD) Best features Cross-platform support, Secure and free Easy to use, High performance, and Flexible tool. ACID vs. BASE Model ACID( Atomicity, Consistency, Isolation, and Durability) is a standard for RDBMS Base ( Basically Available, Soft state, Eventually Consistent) is a model of many NoSQL systems source: https://www.guru99.com/sql-vs-nosql.html#:~:text=SQL%20pronounced%20as%20%E2%80%9CS%2DQ%2DL%E2%80%9D%20or,%2Dvalue%20pairs%2C%20graph%20databases Stream Processing \u00b6 Event streaming is the practice of: capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; routing the event streams to different destination technologies as needed. Event streaming ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time. An event streaming system is 'always-on'. Key capabilities of event streaming architecture publish (write) events and subscribe to (read) topics of streams of events store streams of events durably and reliably process streams of events as they occur or retrospectively","title":"System Design"},{"location":"System-Design/System_Design/#approach-system-design","text":"Write down the functional requirements of the system of interest what is the user/audience what I/O interface we are giving to the user what functionalities are provided to the user go into more detail for those may affect the design Write down the non-functional requirements highly available low latency analytics, monitoring and alerts Possible constraints what is the estimated usage capacity start from small estimate, then extend to larger time range look at RPS for number of servers, latency for whether we want to use cache, request size for network bandwidth, data unit storage size for total storage what is the service SLA think of ways to meet this SLA later in design Detailed design thought process high level components System APIs what we are offerring for data exchange interaction between components Things should be touched on Should it be a pull or push based architecture Database whether to use relational database or NoSQL database is the system expected to be read-heavy or write-heavy define the DB schema early will help understand the data flow between components and guides towards data partitioning Data structure and algorithm used in main components can we improve the efficiency by doing some parallel processing or producer consumer to take away some of the expensive calculations during serving the request? what special cases we need to consider to be treated differently from the majority of use cases? How would a normal case transform into a special case? Do we need to consider separating read and write into different servers Do we need data partitioning/sharding how we can ensure fairly evenly distributed, consistent hashing partition on userId, objectId, hash Do we need data replication Do we need Cache Cache strategy, eviction policy Cache memory, how much keys to cache the flow to update cache Load Balancer where we should place the load balancer Bottlenecks any places where a SPF can happen, how to resolve this bottleneck Monitoring how to make sure we know when the system performance has degraded Cleanup offline jobs to cleanup, keep system healthy Security abuse protection","title":"Approach System Design"},{"location":"System-Design/System_Design/#sql-vs-nosql","text":"Comparison in table Parameter SQL NOSQL Definition SQL databases are primarily called RDBMS or Relational Databases NoSQL databases are primarily called as Non-relational or distributed database Design Traditional RDBMS uses SQL syntax and queries to analyze and get the data for further insights. They are used for OLAP systems. NoSQL database system consists of various kind of database technologies. These databases were developed in response to the demands presented for the development of the modern application. Query Language Structured query language (SQL) No declarative query language Type SQL databases are table based databases NoSQL databases can be document based, key-value pairs, graph databases Schema SQL databases have a predefined schema NoSQL databases use dynamic schema for unstructured data. Scalability SQL databases are vertically scalable NoSQL databases are horizontally scalable Examples Oracle, Postgres, and MS-SQL. MongoDB, Redis, Neo4j, Cassandra, Hbase. Use cases An ideal choice for the complex query intensive environment. It is not good fit complex queries. Hierarchical data storage SQL databases are not suitable for hierarchical data storage. More suitable for the hierarchical data store as it supports key-value pair method. Variations One type with minor variations. Many different types which include key-value stores, document databases, and graph databases. Open-source A mix of open-source like Postgres & MySQL, and commercial like Oracle Database. Open-source Consistency It should be configured for strong consistency. It depends on DBMS as some offers strong consistency like MongoDB, whereas others offer only offers eventual consistency, like Cassandra. Best Used for RDBMS database is the right option for solving ACID problems. NoSQL is a best used for solving data availability problems Importance It should be used when data validity is super important Use when it\u2019s more important to have fast data than correct data Hardware Specialized DB hardware(Oracle Exadata, etc.) Commodity hardware Network Highly available network(Infiniband, Fabric Path, etc.) Commodity network(Ethernet, etc.) Storage Type Highly Available Storage (SAN, RAID, etc.) Commodity drives storage (standardHDDs, JBOD) Best features Cross-platform support, Secure and free Easy to use, High performance, and Flexible tool. ACID vs. BASE Model ACID( Atomicity, Consistency, Isolation, and Durability) is a standard for RDBMS Base ( Basically Available, Soft state, Eventually Consistent) is a model of many NoSQL systems source: https://www.guru99.com/sql-vs-nosql.html#:~:text=SQL%20pronounced%20as%20%E2%80%9CS%2DQ%2DL%E2%80%9D%20or,%2Dvalue%20pairs%2C%20graph%20databases","title":"SQL vs. NoSQL"},{"location":"System-Design/System_Design/#stream-processing","text":"Event streaming is the practice of: capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; routing the event streams to different destination technologies as needed. Event streaming ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time. An event streaming system is 'always-on'. Key capabilities of event streaming architecture publish (write) events and subscribe to (read) topics of streams of events store streams of events durably and reliably process streams of events as they occur or retrospectively","title":"Stream Processing"}]}